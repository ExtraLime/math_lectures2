text,start,stop
[MUSIC],00:00:00.000,00:00:04.915
Stanford University.,00:00:04.915,00:00:07.888
&gt;&gt; Alright!,00:00:10.809,00:00:11.322
"Hello, everybody.",00:00:11.322,00:00:13.150
Welcome to lecture three.,00:00:13.150,00:00:14.590
"I'm Richard, and today we'll talk
a little bit more about word vectors.",00:00:14.590,00:00:18.340
"But before that, let's do three
little organizational Items.",00:00:18.340,00:00:23.470
"First we'll have our first
coding session this week.",00:00:23.470,00:00:27.100
"Next, the problem set one has
a bunch of programming for",00:00:27.100,00:00:31.640
"you, as the first and only one where
you will do everything from scratch.",00:00:31.640,00:00:36.150
"So, do get started early on it.",00:00:36.150,00:00:37.830
"The coding session is mostly
to help you chat with other",00:00:39.800,00:00:42.660
people go through small bugs.,00:00:42.660,00:00:44.230
"Make sure you have everything set
up properly, your environments and",00:00:44.230,00:00:47.260
"everything, so you can get into the
exciting deep learning parts right away.",00:00:47.260,00:00:51.760
"Then there's the career fair,
the computer science forum.",00:00:53.550,00:00:56.080
"It's excited to help you find companies
to work at, and talk about your career.",00:00:56.080,00:01:04.200
"And then my first project advice office
hour's today, I'll just grab a quick",00:01:04.200,00:01:08.176
"dinner after this and then I'll be back
here in the Huang basement to chat.",00:01:08.176,00:01:12.140
"Mostly about projects, so we encourage you
to think about your projects early and so",00:01:12.140,00:01:16.910
we'll start that today.,00:01:16.910,00:01:19.120
"Very excited to chat with you if wanna
just bounce off ideas in the beginning,",00:01:19.120,00:01:22.632
that will be great.,00:01:22.632,00:01:23.560
"Any questions around organization, yes.",00:01:25.600,00:01:27.327
"I think just like outside,
yeah like, you can't miss it,",00:01:29.847,00:01:33.359
like right here in front of the class.,00:01:33.359,00:01:35.620
Any other organizational questions?,00:01:38.599,00:01:40.543
"Yeah.
He will hold office hours too.",00:01:46.629,00:01:47.982
"And we have a calendar on the website, and",00:01:47.982,00:01:50.266
"you can find all our office
hours on the calendar.",00:01:50.266,00:01:53.019
"Okay.
We'll fix that.",00:01:57.447,00:01:58.360
"We'll add the names of who's doing
the office hours, especially for",00:01:58.360,00:02:02.624
"Chris and mine All right, great.",00:02:02.624,00:02:08.200
So we'll finish word2vec.,00:02:08.200,00:02:10.290
"But then where it gets
really interesting is,",00:02:10.290,00:02:12.600
"we're actually asked what
word2vec really captures.",00:02:12.600,00:02:16.200
"We have these objective
functions we're optimizing.",00:02:16.200,00:02:18.612
"And we'll take a bit of a look and
analyze what's going on there.",00:02:18.612,00:02:21.613
"And then we'll try to actually
capture the essence of word2vec,",00:02:21.613,00:02:25.216
a little more effectively.,00:02:25.216,00:02:27.190
"And then also look at our first analysis,
of intrinsic and",00:02:27.190,00:02:30.720
extrinsic evaluations for word vectors.,00:02:30.720,00:02:34.130
"So, it'll be really exciting.",00:02:34.130,00:02:36.200
"By the end, you actually have a good sense
of how to evaluate word vectors, and",00:02:36.200,00:02:40.920
"you have at least two methods under
your belt on how to train them.",00:02:40.920,00:02:43.990
So let's do a quick review of word2vec.,00:02:45.580,00:02:47.780
"We ended with this following equation
here, where we wanted to basically predict",00:02:47.780,00:02:52.570
"the outside vectors from the center word,
and",00:02:53.710,00:02:57.260
"so lets just recap really
quickly what that meant.",00:02:57.260,00:03:01.400
"So let's say I have the beginning of
a corpus, and it says something like,",00:03:01.400,00:03:06.040
"I like deep learning,",00:03:06.040,00:03:12.960
or just and NLP.,00:03:12.960,00:03:15.930
"Now, what we were gonna do is, we
basically wanna compute the probability.",00:03:17.850,00:03:21.890
"Let's say, we start with these word
vectors in this is our first center word,",00:03:21.890,00:03:26.840
and that's deep.,00:03:26.840,00:03:28.430
"So, we wanna first compute
the probability of",00:03:28.430,00:03:31.960
"the first outside word,
I given the word deep and",00:03:33.410,00:03:37.860
"that was something like
the exponent here Of UO.",00:03:37.860,00:03:42.540
"So the U vector is the outside word and
so that's,",00:03:43.970,00:03:48.852
"in our case, I here transposed the deep.",00:03:48.852,00:03:52.931
"And then we had this big sum here and
the sum is always the same,",00:03:52.931,00:03:57.803
given for a certain VC.,00:03:57.803,00:03:59.698
So that is the center word.,00:03:59.698,00:04:01.260
"Now, how do we get this V and this U?",00:04:01.260,00:04:03.510
"We basically have a large matrix here,",00:04:03.510,00:04:08.390
"with all the different word vectors for
all the different words.",00:04:08.390,00:04:11.330
So it starts with vector for aardvark.,00:04:11.330,00:04:13.700
"And a and so on,
all the way to maybe the vector for zebra.",00:04:16.050,00:04:21.150
"And we had basically all
our center words v in here.",00:04:21.150,00:04:24.960
"And then we have one large matrix,
where we have again, all the vectors",00:04:24.960,00:04:30.300
"starting with aardvark and A,
and so on, all the way to zebra.",00:04:30.300,00:04:36.240
"And when we start in our first window
through this corpus, we basically collect,",00:04:38.300,00:04:44.127
"take that vector for deep here
this vector V plug it in here and",00:04:44.127,00:04:48.435
then we wanna maximize this probability.,00:04:48.435,00:04:51.487
"And now, we'll take the vectors for
U for all these different words like I,",00:04:54.689,00:04:59.380
"like, learning, and and.",00:04:59.380,00:05:01.720
"So the next thing would be, I for like or
the probability of like given deep.",00:05:01.720,00:05:06.990
"And that'll be the exponent of
U like transpose of v deep.",00:05:08.020,00:05:13.900
"And again, we have to divide by this",00:05:15.190,00:05:18.130
"pretty large sum over
the entire vocabulary.",00:05:18.130,00:05:20.580
"So, it's essentially little
classification problems all over.",00:05:20.580,00:05:23.915
So that's the first window of this corpus.,00:05:23.915,00:05:26.050
"Now, when we move to the next window,
we basically move one over.",00:05:27.830,00:05:32.130
"And now the center word is learning, and
we wanna predict these outside words.",00:05:33.550,00:05:39.778
"So now we'll take for this next,
the second window here.",00:05:41.453,00:05:46.720
"This was the first window,
the second window.",00:05:46.720,00:05:49.400
"We'll now take the vector V for learning
and the U vector for like, deep and NLP.",00:05:49.400,00:05:55.620
"So that was the skip gram model that
we talked about in the last lecture,",00:05:57.340,00:06:01.030
"just explained again
with the same notation.",00:06:01.030,00:06:05.410
"But basically, you take one window
at a time, you move that window and",00:06:05.410,00:06:08.350
"you keep trying to predict
the outside words.",00:06:08.350,00:06:11.050
Next to the center word.,00:06:11.050,00:06:11.940
Are there any questions around this?,00:06:14.290,00:06:15.450
"Cuz we'll move, yep?",00:06:15.450,00:06:17.689
"That's a good question, so
how do you actually develop that?",00:06:23.951,00:06:26.420
"You start with all the numbers,
all these vectors are just random.",00:06:26.420,00:06:30.090
"Little small random numbers, often sampled
uniformly between two small numbers.",00:06:30.090,00:06:35.310
"And then, you take the derivatives with
respect to these vectors in order to",00:06:36.360,00:06:40.200
increase these probabilities.,00:06:40.200,00:06:41.883
"And you essentially take the gradient
here, of each of these windows with SGD.",00:06:44.159,00:06:50.078
"And so, when you take the derivatives that
we went through in Latin last lecture,",00:06:50.078,00:06:55.287
"with respect to all these different
vectors here, you get this very,",00:06:55.287,00:06:59.741
very large sparse update.,00:06:59.741,00:07:02.100
"Cuz all your parameters
are essentially all the word vectors.",00:07:02.100,00:07:06.410
"And basically these two matrices,
with all these different column vectors.",00:07:06.410,00:07:10.952
"And so let's say you have
100 dimensional vectors, and",00:07:10.952,00:07:14.818
"you have a vocabulary of
let's say 20,000 words.",00:07:14.818,00:07:18.542
"So that's a lot of different
numbers that you have to optimize.",00:07:18.542,00:07:22.170
"And so these updates are very, very large.",00:07:22.170,00:07:25.110
"But, they're also very
sparse cuz each window,",00:07:25.110,00:07:27.420
"you usually only see five words
if your window size is two.",00:07:27.420,00:07:31.410
"yeah?
&gt;&gt; [INAUDIBLE]",00:07:33.300,00:07:36.420
&gt;&gt; That's a good question.,00:07:36.420,00:07:37.160
"We'll get to that once we look at
the evaluation of these word vectors.",00:07:37.160,00:07:39.950
"This cost function is not
convex It doesn't matter,",00:07:44.304,00:07:50.140
"sorry, I should repeat all the questions,
sorry for the people on the video.",00:07:50.140,00:07:52.870
"So the first question was,
how do we choose the dimensionality?",00:07:52.870,00:07:55.780
We'll get to that very soon.,00:07:55.780,00:07:58.030
"And then, this question here.",00:07:58.030,00:08:00.120
Was how do we start?,00:08:00.120,00:08:01.707
And how much does it matter?,00:08:01.707,00:08:03.903
"It turns out most of the objective
functions, pretty much almost of them in",00:08:03.903,00:08:08.666
"this lecture, are not convex, and
so initialization does matter.",00:08:08.666,00:08:13.240
And we'll go through tips and,00:08:13.240,00:08:14.820
"tricks on how to circumvent getting
stuck in very bad local optima.",00:08:14.820,00:08:20.240
"But it turns out in practice,
as long as you initialize with small",00:08:20.240,00:08:23.340
"random numbers especially in these word
vectors, it does not tend to be a problem.",00:08:23.340,00:08:27.320
"All right, so we basically run SGD,
it's just a recap of last lecture.",00:08:30.780,00:08:34.130
"Run SGD, we update now our
cost function here at each",00:08:34.130,00:08:39.020
"window as we move through the corpus,
right?",00:08:39.020,00:08:43.630
"And so when you think about these updates
and you think about implementing that,",00:08:43.630,00:08:47.090
"which you'll very soon for
problem set one, you'll realize well,",00:08:47.090,00:08:50.880
"if I have this entire matrix,
this entire vector here, sorry.",00:08:50.880,00:08:55.020
"This vector of all these
different numbers and",00:08:55.020,00:08:57.406
"I explicitly actually
keep around these zeros,",00:08:57.406,00:08:59.916
"you have very, very large updates, and
you'll run out of memory very quickly.",00:08:59.916,00:09:04.096
"And so what instead you
wanna do is either have very",00:09:04.096,00:09:06.784
"sparse matrix operations where
you update only specific columns.",00:09:06.784,00:09:11.581
"For this second window, you only have
to update the outside vectors for",00:09:11.581,00:09:16.911
"like, deep and NLP and
inside vector for learning.",00:09:16.911,00:09:20.199
"Or you could also implement this as
essentially a hash where you have keys and",00:09:20.199,00:09:26.051
values.,00:09:26.051,00:09:26.790
"And the values are the vectors,
and the keys are the word strings.",00:09:26.790,00:09:30.580
"All right, now, when I told you
this is the skip-gram model,",00:09:33.850,00:09:37.990
"I actually kind of lied a little bit
to teach it to you one step at a time.",00:09:37.990,00:09:43.060
"It turns out when you do
this computation here,",00:09:43.060,00:09:47.150
"the upper part is pretty simple, right?",00:09:48.890,00:09:50.440
"This is just
the hundred-dimensional vector, and",00:09:50.440,00:09:52.180
"you multiply that with another
hundred-dimensional vector.",00:09:52.180,00:09:54.980
So that's pretty fast.,00:09:54.980,00:09:56.355
"But at each window, and again you
go through an entire corpus, right?",00:09:56.355,00:10:00.318
"You do this one step at a time,
one word at a time.",00:10:00.318,00:10:03.479
"And for each window,
you do this computation.",00:10:03.479,00:10:05.265
And you do also this gigantic sum.,00:10:05.265,00:10:07.251
"And this sum goes over
the entire vocabulary.",00:10:07.251,00:10:09.698
"Again, potentially 20,000 maybe
even a million different words",00:10:09.698,00:10:14.687
in your whole corpus.,00:10:14.687,00:10:16.248
"All right, so each window,",00:10:16.248,00:10:18.329
"you have to make 20,000 times
this inner product down here.",00:10:18.329,00:10:23.202
And that's not very efficient.,00:10:23.202,00:10:25.320
"And it turns out,
you also don't teach the model that much.",00:10:25.320,00:10:28.780
"At each window you say, deep learning, or
learning does not co-occur with zebra.",00:10:28.780,00:10:35.086
It does not co-occur of aardvark.,00:10:35.086,00:10:36.491
"It does not co-occur
with 20,000 other words.",00:10:36.491,00:10:39.310
"And it's kind of repetitive, right?",00:10:39.310,00:10:40.185
"Cuz most words don't actually appear with
most other words, it's pretty sparse.",00:10:40.185,00:10:44.930
"And so the main idea behind skip-gram is a
very neat trick, which is we'll just train",00:10:44.930,00:10:50.150
"a couple of binary logistic
regressions for the true pairs.",00:10:50.150,00:10:53.670
"So we keep this idea of
wanting to optimize and",00:10:53.670,00:10:57.370
"maximize this inner product of
the center word and the outside words.",00:10:57.370,00:11:01.790
"But instead of going through all,",00:11:01.790,00:11:03.340
"we'll actually just take
a couple of random words and say,",00:11:03.340,00:11:05.610
"how about these random words from
the rest of the corpus don't co-occur.",00:11:05.610,00:11:11.120
"And this leads us to the original
objective function of the skip-gram model,",00:11:11.120,00:11:15.740
"which sort of as a software
package is often called Word2vec.",00:11:15.740,00:11:19.810
"And the original paper title was
Distributed Representations of Words and",00:11:19.810,00:11:23.200
"Phrases, and their compositionality.",00:11:23.200,00:11:25.300
"And so the overall objective
function is as follows.",00:11:25.300,00:11:27.870
Let's walk through this slowly together.,00:11:27.870,00:11:29.860
"Basically, you go again
through each window.",00:11:30.970,00:11:33.410
"So T here corresponds to each window
as you go through the corpus,",00:11:33.410,00:11:38.730
and then we have two terms here.,00:11:38.730,00:11:39.880
"The first one is essentially just a log
probability of these two center words and",00:11:39.880,00:11:44.710
outside words co-occurring.,00:11:44.710,00:11:46.134
"And so the sigmoid here is
a simple element wise function.",00:11:47.240,00:11:50.870
We'll become very good friends.,00:11:50.870,00:11:51.743
We'll use the sigmoid function a lot.,00:11:51.743,00:11:53.690
"You'll have to really be able to
take derivatives of it and so on.",00:11:53.690,00:11:57.040
"But essentially what it does,
it just takes any real number and",00:11:57.040,00:12:00.040
squashes it to be between zero and one.,00:12:00.040,00:12:02.200
"And that's for you learning people,
good enough to call it a probability.",00:12:02.200,00:12:06.182
"If you're reading statistics,
you wanna have proper measures and so on,",00:12:06.182,00:12:09.395
"so it's not quite that much, but
it's a number between zero and one.",00:12:09.395,00:12:12.413
We'll call it a probability.,00:12:12.413,00:12:13.445
"And then we basically can call this
here a term that we basically wanna",00:12:13.445,00:12:19.364
"maximize the log probability of
these two words co-occurring.",00:12:19.364,00:12:25.020
Any questions about the first term?,00:12:25.020,00:12:26.234
"This is very similar to before, but
then we have the second term here.",00:12:30.282,00:12:33.792
"And the original description
was this expected value here.",00:12:33.792,00:12:37.580
"But really, we can have some clear
notation that essentially just shows that",00:12:37.580,00:12:41.581
"we're going to randomly sub sample
a couple of the words from the corpus.",00:12:41.581,00:12:45.790
"And for each of these,",00:12:45.790,00:12:47.334
"we will essentially try to minimize
their probability of co-occurring.",00:12:47.334,00:12:52.470
And so one good exercise is actually for,00:12:52.470,00:12:56.696
you in preparation for midterms.,00:12:56.696,00:13:00.414
"And what not to prove to
yourself that one of sigmoid",00:13:00.414,00:13:05.968
"of minus x is the same as
one minus sigmoid of x.",00:13:05.968,00:13:11.019
"That is a nice little quick
proof to get into the zone.",00:13:11.019,00:13:14.960
"And so basically this is one
minus the probability of this.",00:13:14.960,00:13:18.470
"So we'd subsample a couple of random words
from our corpus instead of going through",00:13:18.470,00:13:22.041
"all the different ones saying
an aardvark doesn't appear.",00:13:22.041,00:13:24.656
"Zebra doesn't appear with learning and
so on.",00:13:24.656,00:13:26.960
"We'll just sample five, or ten, or so,
and then we minimize their probabilities.",00:13:26.960,00:13:31.140
"And so usually, we take and",00:13:33.070,00:13:34.200
"this is again a hyperparameter, one that
will have to evaluate how much it matters.",00:13:34.200,00:13:39.120
I will take k negative samples for,00:13:39.120,00:13:41.420
"the second part here of the objective
functions for each window.",00:13:41.420,00:13:45.020
"And then we minimize the probability
that these random words appear",00:13:45.020,00:13:49.683
around the center word.,00:13:49.683,00:13:51.415
"And then the way we sample them is
actually from a simple uniform or",00:13:51.415,00:13:55.059
unigram distribution here.,00:13:55.059,00:13:56.723
"We basically look at how often do
the words generally appear, and",00:13:56.723,00:14:00.092
then we sample them based on that.,00:14:00.092,00:14:01.910
"But we also take the power
of three-fourth.",00:14:01.910,00:14:03.439
It's kind of a hacky term.,00:14:03.439,00:14:04.962
"If you play around with this model for
long enough, you say, well,",00:14:04.962,00:14:08.726
"maybe it should more often sample some
of these rare words cuz otherwise,",00:14:08.726,00:14:12.556
"it would very, very often sample THE and
A and other stop words.",00:14:12.556,00:14:16.240
"And would probably never, ever sample
aardvark and zebra in our corpus,",00:14:16.240,00:14:20.850
"so you take this to
the power of three-fourth.",00:14:20.850,00:14:25.970
"And you don't have to
implement this function,",00:14:25.970,00:14:27.460
"we'll just give it to you cuz you kind
of have to compute the statistics",00:14:27.460,00:14:31.260
"of how often each word
appears in the corpus.",00:14:31.260,00:14:33.940
"But we'll give this to
you in the problem set.",00:14:33.940,00:14:35.610
"All right, so
any questions around the skip-gram model?",00:14:37.080,00:14:40.297
Yeah?,00:14:40.297,00:14:40.797
"That's right, so the question is,
is it a choice of how to define p of w?",00:14:46.692,00:14:53.920
"And it is a choice, you could do
a lot of different things there.",00:14:53.920,00:14:57.240
"But it turns out a very simple thing,
like just taking the unigram distribution.",00:14:57.240,00:15:02.224
"How often does this word
appear works well enough.",00:15:02.224,00:15:05.830
"So people haven't really explored
more complex versions than that.",00:15:05.830,00:15:10.349
That's a good question.,00:15:19.712,00:15:20.990
"Should we make sure that
the random samples here",00:15:20.990,00:15:24.430
aren't the same as exactly this word?,00:15:24.430,00:15:27.170
"Yes, but it turns out that the probability
for a very large corpora is so",00:15:28.500,00:15:32.460
"tiny that the very, very few times that
ever happens is kind of irrelevant.",00:15:32.460,00:15:36.590
"Cuz we randomly sub-sample so
much that it doesn't change.",00:15:36.590,00:15:39.918
Orders of magnitude for which part?,00:15:48.573,00:15:50.200
"K, it's ten.",00:15:51.617,00:15:53.224
"It's relatively small, and
it's an interesting trade-off that",00:15:53.224,00:15:56.436
"you'll observe in actually
several deep learning models.",00:15:56.436,00:15:59.317
"Often, As you go through the corpus,
you could do an update after each window,",00:15:59.317,00:16:04.460
"but you could also say let's go through
five windows collect the updates and",00:16:04.460,00:16:08.290
"then make a really, a step in your...",00:16:08.290,00:16:10.820
"Mini batch of your stochastic
gradient descent and",00:16:10.820,00:16:12.971
"we'll go through a lot these kind
of options later in the class.",00:16:12.971,00:16:15.681
"All right, last question on skip
gram What does Jt(theta) represent?",00:16:17.375,00:16:25.942
It's a good question.,00:16:25.942,00:16:26.890
"So theta is often a parameter that we
use for all the variables in our model.",00:16:26.890,00:16:33.650
"So in our case here for
the skip-gram model,",00:16:33.650,00:16:35.320
"it's essentially all the U vectors and
all the V vectors.",00:16:35.320,00:16:39.825
"Later on, when we call,
we'll call a theta,",00:16:39.825,00:16:42.400
"it might have other parameters of
the neural network, layers and so on.",00:16:42.400,00:16:46.310
"And J is just our cost function and
T is at the Tth time step or",00:16:46.310,00:16:50.360
"the Tth window as we
go through our corpus.",00:16:50.360,00:16:53.540
"So in the end, our overall objective
function that we actually optimize is",00:16:53.540,00:16:56.953
the sum of all of them.,00:16:56.953,00:16:57.995
"But again, we don't wanna do one large
update of the entire corpus, right?",00:16:57.995,00:17:02.942
"We don't wanna go through all the windows,
collect all the updates and",00:17:02.942,00:17:06.378
"then make one gigantic step cuz that
usually doesn't work very well.",00:17:06.378,00:17:09.770
"So, good question I think, last lecture
we talked a lot about minimization.",00:17:12.162,00:17:16.730
"Here, we have these log probabilities and
in the paper you wanna maximize that.",00:17:16.730,00:17:20.200
"And it's often very intuitive, right?",00:17:24.286,00:17:25.540
"Once you have probabilities,
you usually wanna maximize the probability",00:17:25.540,00:17:29.350
"of the actual thing that you
see in your corpus happening.",00:17:29.350,00:17:32.930
"And then other times,",00:17:32.930,00:17:33.680
"when we call it a cost function,
we wanna minimize the cost and so on.",00:17:33.680,00:17:37.560
"All right so, in word2vector's,
another model,",00:17:39.550,00:17:43.830
"which you won't have to implement
unless you want to get bonus points.",00:17:43.830,00:17:46.690
"But we will ask you to take
derivatives of, and so",00:17:46.690,00:17:49.565
"it's good to understand it at least
in a very simple conceptual level.",00:17:49.565,00:17:53.682
"And it's very similar
to the skip-gram model.",00:17:53.682,00:17:57.550
"Basically, we want to predict",00:17:57.550,00:18:00.170
"the center word from the sum
of the surrounding words.",00:18:00.170,00:18:04.040
"So very simply here, we sum up
the vector of And of NLP and of deep and",00:18:04.040,00:18:08.280
"of like and
we have the sum of these vectors.",00:18:08.280,00:18:10.770
"And then we have some inner products
with just the vector of the inside.",00:18:10.770,00:18:14.640
"And basically that's called
the continuous bag of words model.",00:18:14.640,00:18:18.250
"You'll learn all about the details and
the definition of that in the problem set.",00:18:18.250,00:18:22.510
"So what actually happens when we
train these word vectors, right?",00:18:23.920,00:18:27.036
"We optimize this objective function and
we take gradients and",00:18:27.036,00:18:31.939
"after a while, something kind of
magical happens to these word vectors.",00:18:31.939,00:18:37.917
"And that is that they actually start to
cluster around similar kinds of meaning,",00:18:37.917,00:18:43.524
"and sometimes also similar
kinds of syntactic functions.",00:18:43.524,00:18:47.980
"So when we zoom in, and again, this is,
usually these vectors are 25 to even",00:18:47.980,00:18:53.030
"500 or thousand dimensional, this is just
a PCA visualization of these vectors.",00:18:53.030,00:18:58.480
"And what we'll observe is that Tuesday and
Thursday and",00:18:58.480,00:19:03.210
"weekdays cluster together,
number terms cluster together,",00:19:03.210,00:19:07.040
first names cluster together and so on.,00:19:08.170,00:19:12.090
"So basically, words that appear
in similar context turn out to",00:19:12.090,00:19:16.240
"often have dissimilar meaning as
we discussed in previous lecture.",00:19:16.240,00:19:19.820
"And so
they essentially get similar vectors",00:19:19.820,00:19:24.280
"after we train this model for
a sufficient number of sets.",00:19:24.280,00:19:28.670
"All right, let's summarize word2vec.",00:19:30.926,00:19:33.160
"Basically, we went through
each word in the corpus.",00:19:33.160,00:19:36.270
"We looked at the surrounding
words in the window.",00:19:36.270,00:19:38.310
We predict the surrounding words.,00:19:38.310,00:19:40.320
"Now, what we are essentially
doing there is",00:19:40.320,00:19:43.480
"trying to capture
the coocurrence of words.",00:19:43.480,00:19:46.130
"How often does this word
cooccur with the other word?",00:19:46.130,00:19:49.112
And we did that one count at a time.,00:19:49.112,00:19:51.156
"It's like, I see the deep and
learning happen.",00:19:51.156,00:19:56.020
I make an update to both of this vectors.,00:19:56.020,00:19:58.440
"And then you go over the corpus and then
you probably will eventually see deep and",00:19:58.440,00:20:02.320
"learning coocurring again and
you make again a separate update step.",00:20:02.320,00:20:06.600
"When you think about that,
it's not very efficient, right?",00:20:06.600,00:20:08.488
"Why now we just go to the entire corpus
once, count how often this deep and",00:20:08.488,00:20:12.860
"learning cooccur, of these two
words cooccur, and then we make one",00:20:12.860,00:20:18.340
"update step that captures the entire
count instead of one sample at the time.",00:20:18.340,00:20:23.810
"And, yes we can do that and",00:20:25.220,00:20:27.910
"that is actually a method that
came historically before word2vec.",00:20:27.910,00:20:31.960
"And there are different
options of how we can do this.",00:20:33.060,00:20:36.070
The simplest one or,00:20:36.070,00:20:37.730
"the one that is similar to word2vec at
least is that we again use a window around",00:20:37.730,00:20:41.680
"each word and we basically just
go through the entire corpus.",00:20:41.680,00:20:45.460
"We don't update anything,
we don't do any SGD.",00:20:45.460,00:20:47.200
We just collect the counts first.,00:20:47.200,00:20:49.670
"And once we have the counts,
then we do something to that matrix.",00:20:49.670,00:20:53.080
"And so when we look at just
the window of length maybe two,",00:20:53.080,00:20:56.720
"like in this example here, or maybe five,
some small window size around each word,",00:20:56.720,00:21:01.980
"what we'll do is we'll capture,
not just the semantics, but",00:21:01.980,00:21:05.080
"also some of the syntactic
information of each word.",00:21:05.080,00:21:07.560
"Namely, what kind of
part of speech tag is it.",00:21:07.560,00:21:10.210
"So verbs are going to be
closer to one another.",00:21:10.210,00:21:13.530
"Then the verbs are to nouns, for instance.",00:21:13.530,00:21:16.320
"If, on the other hand, we look at
co-occurrence counts that aren't just",00:21:18.460,00:21:21.690
"around the window, but entire document,
so I don't just look at each window.",00:21:21.690,00:21:25.720
"But i say, this Word appears with all
these other words in this entire Wikipedia",00:21:25.720,00:21:30.705
"article, for instance, or
this entire Word document.",00:21:30.705,00:21:33.905
"Then, what you'll capture is actually
more topics, and this is often",00:21:33.905,00:21:39.315
"called Latent Semantic Analysis,
a big popular model from a while back.",00:21:39.315,00:21:44.919
"And basically what you'll get there is,
you'll ignore the part of",00:21:44.919,00:21:48.868
"speech that you ignore any kind of
syntactic information and just say,",00:21:48.868,00:21:53.026
"well swimming and boat and
water and weather and the sun,",00:21:53.026,00:21:56.353
"they're all kind of appear in this topic
together, in this document together.",00:21:56.353,00:22:01.890
"So we won't go into too many details for
these cuz they turn out for",00:22:01.890,00:22:05.310
"a lot of other downstream tasks
like machine translation or so and",00:22:05.310,00:22:08.060
"we really want to use these windows,
but it's good knowledge to have.",00:22:08.060,00:22:12.070
"So let's go over a simple example of
what we would do if we had a very small",00:22:12.070,00:22:18.190
"corpus and wanna collect these windows and
then compute word vectors from that.",00:22:18.190,00:22:23.070
"So it is technically not cosine cuz we
are not normalizing over the length, and",00:22:44.928,00:22:48.115
"technically we are not optimizing inner
products of these probabilities and so on.",00:22:48.115,00:22:51.560
But continue.,00:22:51.560,00:22:52.599
"That's right.
So the question is,",00:23:00.151,00:23:01.500
"in all these visualizations here,
we kind of look at Euclidean distance.",00:23:01.500,00:23:05.690
"And it's true, we're actually often
are going to use inner products",00:23:05.690,00:23:10.220
kinds of similarities.,00:23:10.220,00:23:12.050
"So yes, in some cases, Euclidean
distance works reasonably well still,",00:23:12.050,00:23:17.220
"despite not doing this in fact we'll see
one evaluation that is entirely based or",00:23:17.220,00:23:22.000
"partly based on Euclidean distances and
partly inner products.",00:23:22.000,00:23:25.400
"So it turns out both work well despite
our objective function only having this.",00:23:25.400,00:23:30.700
"And even more surprising there're a lot
of things that work quite well on this",00:23:30.700,00:23:33.593
"despite starting with this
kind of objective function.",00:23:33.593,00:23:35.740
"We often yeah, so if despite having
only this inner product optimizations,",00:23:41.429,00:23:45.864
"we will actually also do often very
well in terms of Euclidean distances.",00:23:45.864,00:23:50.110
Yep.,00:23:50.110,00:23:52.891
"Well, it get's complicated but there
are some interesting relationships between",00:23:54.287,00:23:59.402
"the ratios of the co-occurence counts
We don't have enough time to dive into",00:23:59.402,00:24:04.150
"the details, but if you are interested
in that I will talk about a paper.",00:24:04.150,00:24:07.770
"I mentioned the title of the paper in
five or ten slides, that will help",00:24:07.770,00:24:12.199
"you understand that a little better and
gain some more intuition, yep.",00:24:12.199,00:24:15.817
"All right, so,
window based co-occurrence matrices.",00:24:15.817,00:24:20.950
"So, let's say,
we have this corpus here, and",00:24:20.950,00:24:22.930
"that's to find our window length
as just 1, for simplicity.",00:24:22.930,00:24:26.210
"Usually, we have more commonly
5 to 10 windows around there.",00:24:26.210,00:24:29.370
"And we assume we have
a symmetric window so,",00:24:30.560,00:24:34.250
"we don't care if a word is to the left or
to the right of our center word.",00:24:34.250,00:24:38.040
And we have this corpus.,00:24:38.040,00:24:39.510
"So, this is essentially what a window
based co-occurrence matrix would be, for",00:24:39.510,00:24:44.430
"this very, very simple corpus.",00:24:44.430,00:24:46.540
"We just look at the word I and then,
we look at which words appear next to I.",00:24:46.540,00:24:51.650
"And so, we look at I, we see like
twice so, we have number two here.",00:24:51.650,00:24:56.190
"And we see enjoy once so,
we put the count one here.",00:24:56.190,00:25:00.260
"And then, we know we have the word like.",00:25:00.260,00:25:02.130
"And so, like co-occurs twice
with the word I on it's left and",00:25:02.130,00:25:07.390
once with deep and once with NLP.,00:25:07.390,00:25:09.290
"And so, this is essentially we go through
all the words in a very large corpus and",00:25:09.290,00:25:14.316
"we compute all these counts, super simple.",00:25:14.316,00:25:17.177
"Now, you could say, well,
that's a vector already, right?",00:25:17.177,00:25:20.420
"You have a list of numbers here and that
list of numbers now represents that word.",00:25:20.420,00:25:24.770
"And you already kinda capture things like,
well, like and enjoy have some overlap so,",00:25:24.770,00:25:29.174
maybe they're more similar.,00:25:29.174,00:25:30.731
"So, you already have a word vector, right?",00:25:30.731,00:25:32.870
"But now, it's not a very ideal word
vector for a couple of reasons.",00:25:32.870,00:25:36.840
"The first one is,
if you have a new word in your vocabulary,",00:25:36.840,00:25:39.950
that word vector changes.,00:25:39.950,00:25:41.090
"So, if you have some downstream machine
learning models now to take that",00:25:41.090,00:25:44.680
"vector's input, they always have to change
and there's always some parameter missing.",00:25:44.680,00:25:48.540
"Also, this vector is going
to be very high-dimensional.",00:25:48.540,00:25:51.415
"Of course, for this tiny corpus,
it's small but generally,",00:25:51.415,00:25:54.263
we'll have tens of thousands of words.,00:25:54.263,00:25:56.290
"So, it's a very high-dimensional vector.",00:25:56.290,00:25:58.070
"So, you'll have sparsity issues if you
try to train a machine learning model",00:25:58.070,00:26:01.890
"on this afterwards and that moves up in
a much less robust downstream models.",00:26:01.890,00:26:06.760
"And so, the solution to that is lets again
have the similar idea to word2vec and",00:26:07.970,00:26:12.194
"have just don't store all of the co
occurrence counts, every single number.",00:26:12.194,00:26:16.307
"But just store most of
the important information,",00:26:16.307,00:26:19.018
"the fixed small number of dimensions,
similar to word2vec,",00:26:19.018,00:26:22.360
"those will be somewhere around
25 to 1,000 dimensions.",00:26:22.360,00:26:26.420
"And then, the question is okay,
how do we now reduce the dimensionality,",00:26:26.420,00:26:29.940
"we have these very large
co-occurrence matrices here.",00:26:29.940,00:26:32.560
"In the realistic setting, we'll have
20,000 by 20,000 or even a million by",00:26:32.560,00:26:36.480
"a million, very large sparse matrix,
how do we reduce the dimensionality?",00:26:36.480,00:26:41.170
"And the answer is we'll
just use very simple SVD.",00:26:41.170,00:26:44.510
"So, who here is familiar with
singular value decomposition?",00:26:44.510,00:26:47.100
"All right, good, the majority of people,
if you're not then,",00:26:49.420,00:26:52.670
"I strongly suggest you go to the office
hours and brush up on your linear algebra.",00:26:52.670,00:26:57.230
"But, basically, we'll have here
this X hat matrix, which is",00:26:59.350,00:27:04.740
"going to be our best rank k approximation
to our original co-occurrence matrix X.",00:27:04.740,00:27:10.320
"And we'll have basically these three
simple matrices with orthonormal columns.",00:27:10.320,00:27:18.050
"U we often call also our left-singular
vectors and we have here S the diagonal",00:27:18.050,00:27:23.370
"matrix containing all the singular
values usually from largest to smallest.",00:27:23.370,00:27:28.220
"And we have our matrix V here,
our orthonormal rows.",00:27:28.220,00:27:32.763
"And so, in code,
this is also extremely simple,",00:27:32.763,00:27:36.521
"we can literally implement this
in just a few lines, if we have,",00:27:36.521,00:27:41.351
"this is our corpus here, and
this is our co-occurrence matrix X.",00:27:41.351,00:27:46.380
"Then, we can simply run SVD with
one line of Python code and",00:27:46.380,00:27:51.830
"then, we get this matrix U.",00:27:51.830,00:27:54.000
"And now, we can take the first two
columns here of U and plot them, right?",00:27:54.000,00:28:02.360
"And if we do this in the first two
dimensions here, we'll actually get",00:28:02.360,00:28:07.097
"similar kinda visualization to all this
other ones I've showed you, right?",00:28:07.097,00:28:12.430
"But this is a few lines of Python code
to create that kinda word vector.",00:28:12.430,00:28:16.630
"And now, it's kinda reading tea leaves,
none of these dimensions we can't really",00:28:18.010,00:28:23.652
"say, this dimension is noun, the verbness
of a word, or something like that.",00:28:23.652,00:28:29.648
"But as you look at these long enough,",00:28:29.648,00:28:32.420
"you'll definitely observe
some kinds of patterns.",00:28:32.420,00:28:35.600
"So for instance, I and like are very
frequent words in this corpus and",00:28:35.600,00:28:40.290
"they're a little further to the left so,
that's one.",00:28:40.290,00:28:42.560
"Like and enjoy are nearest
neighbors in this space so",00:28:42.560,00:28:46.890
"that's another observation,
they're both verbs, and so on.",00:28:46.890,00:28:50.630
"So, the things that were being liked,
flying and",00:28:50.630,00:28:54.450
"deep and other things
are closer together and so on.",00:28:54.450,00:28:58.160
"So, such a very simple method you get
a first approximation to what word",00:28:58.160,00:29:02.875
vectors can and should capture.,00:29:02.875,00:29:05.087
"Are there any questions around this SVD
method in the co-occurrence matrix?",00:29:07.155,00:29:11.704
"It's a good question,
is the window always symmetric?",00:29:18.489,00:29:20.846
"And the answer is no, we can actually
evaluate asymmetric windows and",00:29:20.846,00:29:25.231
"symmetric windows, and I'll show you
the result of that in a couple of slides.",00:29:25.231,00:29:30.226
"All right, now, once you realize, wow,
this is so simple and it works kinda well,",00:29:32.439,00:29:36.505
"and you're a researcher, you always
wanna try to improve it a little bit.",00:29:36.505,00:29:40.036
"And so, there are a lot of different hacks
that we can make to this co-occurrence",00:29:40.036,00:29:44.391
matrix.,00:29:44.391,00:29:44.920
"So, instead of taking the raw counts, for
instance, as you do this, you realize,",00:29:44.920,00:29:49.560
"well, a lot of representational power
in this word vectors is now captured",00:29:49.560,00:29:55.170
by the fact that the and he and,00:29:55.170,00:29:58.740
"has and a lot of other very, very frequent
words co-occur with almost all the nouns.",00:29:58.740,00:30:03.840
"Like the appears in the window of
pretty much every noun out there.",00:30:03.840,00:30:08.524
"And it doesn't really give us that much
information that it does over and over and",00:30:08.524,00:30:12.283
over again.,00:30:12.283,00:30:13.420
"And so, one thing we can do is
actually just cap it and say,",00:30:13.420,00:30:17.720
"all right, whatever the co-occurs
with the most, and a lot of other",00:30:17.720,00:30:21.900
"one of these function words,
we'll just maximize the count at 100.",00:30:21.900,00:30:26.620
"Or, I know some people do this also,
we just ignore a couple of the most",00:30:26.620,00:30:30.306
"frequent words cuz they really,
we have a power law distribution or",00:30:30.306,00:30:33.869
"Zipf's law where basically,
the most frequent words appear much,",00:30:33.869,00:30:37.495
"much more frequently than other words and
then, it peters out.",00:30:37.495,00:30:40.897
"And then, there's a very long tail of
words that don't appear that often but",00:30:40.897,00:30:46.867
"those very rare words often
have a lot of semantic content.",00:30:46.867,00:30:51.720
"Then, another way we can change this,",00:30:51.720,00:30:54.336
"the way we compute these counts is by
not counting all the words equally.",00:30:54.336,00:30:58.620
"So, we can say, well,",00:30:58.620,00:30:59.843
"words that appear right next to my
center word get a count of one.",00:30:59.843,00:31:03.160
"Or words that appear and
they're five steps away,",00:31:03.160,00:31:05.891
"five words away only
you get a count of 0.5.",00:31:05.891,00:31:08.377
"And so, that's another hack we can do.",00:31:08.377,00:31:10.720
"And then, instead of counts we could
compute correlations and set them to 0.",00:31:10.720,00:31:14.010
"You get the idea, you can play a little
around with this matrix of co-occurrence",00:31:14.010,00:31:19.088
"counts in a variety of different ways and
sometimes they help quite significantly.",00:31:19.088,00:31:25.010
"So, in 2005, so quite a long time ago,
people used this SVD method and",00:31:25.010,00:31:30.130
compared a lot of different,00:31:30.130,00:31:31.400
"ways of hacking the co-occurrence
matrix and modifying it.",00:31:33.510,00:31:37.580
"And basically found quite surprising and
awesome results.",00:31:38.760,00:31:42.340
"And so, this is another way
we can try to visualize",00:31:42.340,00:31:45.260
this very high dimensional space.,00:31:45.260,00:31:46.740
"Again, these vectors are usually
around 100 dimensions or so, so",00:31:46.740,00:31:50.270
it's hard to visualize it.,00:31:50.270,00:31:51.630
"And so, instead of projecting it down to
just 2D, here they just choose a couple of",00:31:51.630,00:31:55.560
"words and look at the nearest
neighbours and which word is closest To",00:31:55.560,00:32:00.440
"what other word and they find that wrist
and ankle are closest to one another.",00:32:00.440,00:32:04.780
And next closest word is shoulder.,00:32:04.780,00:32:06.810
And the next closest one is arm and so on.,00:32:06.810,00:32:09.010
"And so different extremities cluster
together, we'll see different",00:32:09.010,00:32:13.215
"cities clustering together, and American
cities are closer to one another than",00:32:13.215,00:32:18.365
"cities from other countries, and country
names are close together, and so on.",00:32:18.365,00:32:22.445
"So it's quite amazing, right?",00:32:22.445,00:32:23.895
"Even with something as simple
as SVD around these windows,",00:32:23.895,00:32:27.170
"you capture a lot of different
kinds of information.",00:32:27.170,00:32:32.040
In fact it even goes to syntactic and,00:32:32.040,00:32:35.910
"chromatical kinds of patterns that
are captured by this SVD method.",00:32:35.910,00:32:40.610
"So show, showed, shown or
take, took, taken and so",00:32:40.610,00:32:45.560
"on are all always together in
often similar kinds of patterns.",00:32:45.560,00:32:51.210
"And it goes further and
even more semantic in the verbs that",00:32:52.240,00:32:57.460
"are very similar and
related to these kinds of nouns.",00:32:58.690,00:33:04.130
"Often appear even in roughly similar
kinds of Euclidean distances.",00:33:04.130,00:33:09.295
"So, swim and swimmer, clean and janitor,
drive and driver, teach and teacher.",00:33:09.295,00:33:17.440
"They're all basically have a similar
kind of vector difference.",00:33:17.440,00:33:22.760
"And intuitively you would
think well they appear,",00:33:25.500,00:33:28.897
"they often have similar kinds
of context in which they appear.",00:33:28.897,00:33:33.089
"And there's some intuitive sense of why,
why this would happen,",00:33:33.089,00:33:36.970
"as you're trying to capture
these co-occurrence counts.",00:33:36.970,00:33:40.515
Does the language matter?,00:33:45.449,00:33:47.480
"Yes, in what way?",00:33:47.480,00:33:48.836
Great question.,00:33:52.738,00:33:53.279
So if it was German instead of English.,00:33:53.279,00:33:56.130
"So it's actually a sad truth of a lot of
natural language processing research that",00:33:56.130,00:34:01.220
the majority of it is in English.,00:34:01.220,00:34:03.040
And a few people do this.,00:34:03.040,00:34:05.950
"It turns out this works for
a lot of other languages.",00:34:05.950,00:34:08.439
"But people don't have as good
evaluation metrics often for",00:34:08.439,00:34:11.639
"these other languages and evaluation
data sets which we'll get to in a bit.",00:34:11.639,00:34:15.742
"But we would believe that it works for
pretty much all languages.",00:34:15.742,00:34:19.562
"Now there's a lot of complexity because
some languages like Finnish or German have",00:34:19.562,00:34:24.030
"potentially a lot of different words, cuz
they have much richer morphology, right?",00:34:24.030,00:34:28.540
German has compound nouns.,00:34:28.540,00:34:30.650
"And so you get more and
more rare words, and",00:34:30.650,00:34:33.819
"then the rarer the words are,
the less good counts you have of them,",00:34:33.819,00:34:38.838
"and the harder it is to use
this method in a vanilla way.",00:34:38.838,00:34:43.420
"Which eventually in the limit
will get us to character-based",00:34:43.420,00:34:46.680
"natural language processing,
which we'll get to in a couple weeks.",00:34:46.680,00:34:49.270
"But in general, this works for
pretty much any language.",00:34:49.270,00:34:51.520
Great question.,00:34:52.820,00:34:54.300
"So now, what's the problem here?",00:34:54.300,00:34:56.120
"Well SVD, while being very simple and",00:34:56.120,00:34:59.060
"one nice line of Python code, is actually
computationally not always great,",00:34:59.060,00:35:03.432
"especially as we get larger and
larger matrices.",00:35:03.432,00:35:06.040
"So we essentially have this quadratic
cost here in the smaller dimension.",00:35:07.200,00:35:12.345
"So either if it's a word by
word co-occurrence matrix or",00:35:12.345,00:35:15.301
"even a word by document,
we'd assume this gets very, very large.",00:35:15.301,00:35:18.824
"And then it also gets hard to
incorporate new words or documents into,",00:35:18.824,00:35:23.841
"into this whole model cuz you have
to rerun this whole PCA or sorry,",00:35:23.841,00:35:28.601
"the SVD, singular value decomposition.",00:35:28.601,00:35:31.515
"And then on top of that SVD, and",00:35:31.515,00:35:33.142
"how we optimize that is quite different
to a lot of the other downstream deep",00:35:33.142,00:35:37.085
"learning methods that we'll use
like neural networks and so on.",00:35:37.085,00:35:40.770
"It's a very different
kind of optimization.",00:35:40.770,00:35:42.900
"And so the word to vec objective
function is similar to SVD,",00:35:42.900,00:35:45.834
you look at one window at a time.,00:35:45.834,00:35:47.430
You make an update step.,00:35:47.430,00:35:48.390
"And that is very similar to how we
optimize most of the other models in this",00:35:48.390,00:35:52.932
lecture and in deep learning for NLP.,00:35:52.932,00:35:55.328
"And so basically what we
came with with post-doc and",00:35:55.328,00:35:59.046
"Chris' group, so
Jeffery Pennington, me and",00:35:59.046,00:36:02.428
"Chris, is a method that tries to
combine the best of both worlds.",00:36:02.428,00:36:07.376
So let's summarize what the advantages and,00:36:07.376,00:36:10.450
"disadvantages are of these two
different kinds of methods.",00:36:10.450,00:36:13.150
"Basically we have these count
based methods based on SVD and",00:36:13.150,00:36:16.150
the co-occurence matrix.,00:36:16.150,00:36:18.060
And we have the window-based or,00:36:18.060,00:36:19.320
"direct prediction methods
like the Skip-Gram model.",00:36:19.320,00:36:21.610
"The advantages of PCA is that
it's relatively fast to train,",00:36:24.310,00:36:28.920
"unless the matrix gets very,
very large but",00:36:28.920,00:36:31.710
"we're making very efficient usage of
the statistics that we have, right?",00:36:31.710,00:36:35.750
"We only have to collect the statistics
once, and we could in theory,",00:36:35.750,00:36:38.431
throw away the whole corpus.,00:36:38.431,00:36:39.599
"And then we can try a lot of different
things on just these co-occurence counts.",00:36:39.599,00:36:44.903
"Sadly, when you do this,
it captures mostly word similarity,",00:36:44.903,00:36:49.076
"and not various other patterns that
the word2vec model, captures and",00:36:49.076,00:36:53.561
"we'll show you what
those are in evaluation.",00:36:53.561,00:36:56.594
"And we give often disproportionate
importance to these large counts.",00:36:56.594,00:37:00.410
"And we can try various ways
of lowering the importance",00:37:00.410,00:37:04.410
"that these function words and
very frequent words have.",00:37:04.410,00:37:06.370
"The disadvantage of
the Skip-Gram of model is that",00:37:08.830,00:37:13.051
"it scales with a corpus size, right?",00:37:13.051,00:37:16.200
"You have to go through
every single window,",00:37:16.200,00:37:18.600
"which is not very efficient, and
henceforth you also don't really make very",00:37:18.600,00:37:23.410
"efficient usage of the statistics that
you have overall, of the data set.",00:37:23.410,00:37:28.050
"However we actually get, in may cases,",00:37:28.050,00:37:30.660
"much better performance
on downstream tasks.",00:37:30.660,00:37:32.840
"And we don't know yet,",00:37:32.840,00:37:33.610
"those downstream tasks, that's why we have
the whole lecture for this whole quarter.",00:37:33.610,00:37:36.840
"But for a variety of different
problems like an entity recognition or",00:37:36.840,00:37:40.970
part of speech tagging and so on.,00:37:40.970,00:37:42.645
"Things that you'll implement
in the problem sets,",00:37:42.645,00:37:44.475
"it turns out the Skip-Gram like models
turn out to work slightly better.",00:37:44.475,00:37:48.730
"And we can capture various
complex patterns, some of",00:37:49.950,00:37:52.900
"which are very surprising and we'll get
to in the second part of this lecture.",00:37:52.900,00:37:56.640
"And so, basically,",00:37:56.640,00:37:57.840
"what we tried to do here is combining
the best of both of these worlds.",00:37:57.840,00:38:02.040
"And the result of that was the GloVe
model, our Global Vectors model.",00:38:02.040,00:38:07.040
"So let's walk through this
objective function a little bit.",00:38:07.040,00:38:09.640
"Again, theta here will
be all our parameters.",00:38:09.640,00:38:13.080
"So in this case, again,
we have these U and these V vectors.",00:38:13.080,00:38:16.408
"But they're even more symmetric now,",00:38:16.408,00:38:18.705
"we basically just go through all pairs
of words that might ever co-occur.",00:38:18.705,00:38:23.101
"So we go through these very
large co-occurrence matrix that",00:38:23.101,00:38:26.172
"we computed in the beginning and
we call P here.",00:38:26.172,00:38:28.590
"And for
each pair of words in this entire corpus,",00:38:29.590,00:38:33.709
"we basically want to minimize
the distance between the inner",00:38:33.709,00:38:38.834
"product here, and
the log count of these two words.",00:38:38.834,00:38:43.270
"So again, this is just this kind of
matrix here that we're going over.",00:38:43.270,00:38:48.819
"We're going over all elements of
this kind of co-occurrence matrix.",00:38:48.819,00:38:53.204
"But instead of running the large SVD,",00:38:53.204,00:38:56.756
"we'll basically just optimize
one such count at a time here.",00:38:56.756,00:39:02.529
So I have the square of this distance and,00:39:02.529,00:39:05.277
"then we also have this term here,
f, which allows us to weight even",00:39:05.277,00:39:09.938
"lower some of these very
frequent kinds of co-occurrences.",00:39:09.938,00:39:14.300
"So the, for instance, will have
the maximum amount that we can weigh it",00:39:14.300,00:39:18.904
inside this overall objective function.,00:39:18.904,00:39:21.691
"All right,",00:39:24.557,00:39:25.109
"so now what this allows us to do is
essentially we can train very quickly.",00:39:25.109,00:39:28.490
"Cuz instead of saying, all right, we'll
optimize that deep and learning co-occur",00:39:28.490,00:39:32.234
"in one window, and then we'll go in a
couple windows later, they co-occur again.",00:39:32.234,00:39:35.952
"And we update again, with just one say or",00:39:35.952,00:39:38.170
"a deep learning co-occur
in this entire corpus.",00:39:38.170,00:39:40.810
"Which could now be in all of Wikipedia or
in our case, all of common crawl.",00:39:40.810,00:39:44.701
"Which is most of the Internet,
that's kind of amazing.",00:39:44.701,00:39:47.782
"It's a gigantic corpora
with billions of tokens.",00:39:47.782,00:39:50.480
"And we just say, all right, deep and",00:39:50.480,00:39:52.534
"learning in these billions of documents
co-occur 536 times or something like that.",00:39:52.534,00:39:57.524
Probably now a lot more often.,00:39:57.524,00:39:58.977
"And then we'll just optimize basically
This inner product to be closed and",00:39:58.977,00:40:03.974
"it's value to the log of
that overall account.",00:40:03.974,00:40:07.863
"And because of that,
it scales to very large corpora.",00:40:11.199,00:40:14.570
"Which is great because the rare
words appear not very often and",00:40:14.570,00:40:18.730
"just build hours to capture even rarer
like the semantics of very rare words.",00:40:18.730,00:40:23.864
"And because of the efficient usage
of the statistics, it turns out",00:40:23.864,00:40:27.955
"to also work very well on small
corpora and even smaller vector sizes.",00:40:27.955,00:40:32.280
"So now you might be confused
because individualization,",00:40:34.320,00:40:37.185
"we keep showing you a single vector but
here, we again, just like with the skip",00:40:37.185,00:40:41.068
"gram vector, we have v vector, it's the
outside vectors and the inside vectors.",00:40:41.068,00:40:45.210
And so let's get rid of that confusion and,00:40:46.810,00:40:50.620
"basically tell you that there are a lot
of different options of how you get,",00:40:50.620,00:40:54.500
"eventually, just a single vector
from having these two vectors.",00:40:54.500,00:40:57.920
You could concatenate them but,00:40:57.920,00:40:59.600
"it turns out what works best
is just to sum them up.",00:40:59.600,00:41:02.020
"They essentially both
capture co-occurence counts.",00:41:02.020,00:41:04.880
"And if we just sum them,
that turns out to work best in practice.",00:41:04.880,00:41:08.780
"And so, that also destroys
some of the intuitions of why",00:41:09.800,00:41:13.450
"certain things should happen, but it turns
out in practice this works best, yeah?",00:41:13.450,00:41:17.495
"&gt;&gt; [INAUDIBLE]
&gt;&gt; What are U and",00:41:17.495,00:41:21.620
"V again, so U here are again just
the vectors of all the words.",00:41:21.620,00:41:25.610
"And so here, just like with the skip-gram,
we had the inside and the outside vectors.",00:41:25.610,00:41:30.360
"Here, u and v are just the vectors in
the column and the vectors in the row.",00:41:30.360,00:41:34.930
"They're essentially interchangeable and
because of that,",00:41:34.930,00:41:37.750
it makes even more sense to sum them up.,00:41:37.750,00:41:39.950
"You could even say, well, why don't
you just have one set of vectors?",00:41:39.950,00:41:43.560
"But then, you'd have a more, a less
well behaved objective function here,",00:41:43.560,00:41:48.040
"because you have the inner product between
two of the same sets of parameters.",00:41:48.040,00:41:54.000
"And it turns out, in terms of the
optimization having the separate vectors",00:41:54.000,00:41:57.922
"during optimization and combining them at
the very end just was much more stable.",00:41:57.922,00:42:02.230
That's right.,00:42:07.946,00:42:08.760
"Even for skip-gram, that's the question.",00:42:08.760,00:42:10.710
"Is it common also time for
skip-gram to sum them up?",00:42:10.710,00:42:12.900
"It is.
And it's a good, it's good whenever you",00:42:12.900,00:42:15.760
"have these choices and they seem a little
arbitrary, also, for all your projects.",00:42:15.760,00:42:19.580
"The best thing to always do is like,
well, there are two things.",00:42:19.580,00:42:22.800
"You could just come to me and
say, hey what should I do?",00:42:22.800,00:42:25.250
X or Y?,00:42:25.250,00:42:26.250
"And the true answer,",00:42:26.250,00:42:27.550
"especially as you get closer to your
project and to more research and",00:42:27.550,00:42:31.350
"novel kinds of applications, the best
answer is always, try all of them.",00:42:31.350,00:42:35.670
"And then have a real metric a quantitative
of measure of how well all of them do and",00:42:37.110,00:42:42.920
"then have a nice little
table in your final projects",00:42:42.920,00:42:46.590
"description that tells you
very concretely what it is.",00:42:46.590,00:42:50.470
"And once you do that many times,
you'll gain some intuitions,",00:42:50.470,00:42:53.390
"and you'll realize alright, for the fifth
project, you just realized well summing",00:42:53.390,00:42:56.620
"them up usually works best, so
I'm just going to continue doing that.",00:42:56.620,00:43:00.540
"Especially as you get into the field,",00:43:00.540,00:43:01.930
"it's good to try a lot of these
different knobs and hyperparameters.",00:43:01.930,00:43:05.685
"&gt;&gt; [INAUDIBLE]
&gt;&gt; That's right,",00:43:05.685,00:43:11.760
they're all in the same scale here.,00:43:11.760,00:43:13.040
"Really they are quite interchangeable,
especially for the Glove model.",00:43:13.040,00:43:15.137
Is that a question?,00:43:34.349,00:43:35.495
Alright I will try to repeat it.,00:43:35.495,00:43:36.545
So in theory here you're right.,00:43:36.545,00:43:39.275
"So the question is does the magnitude
of these vectors matter?",00:43:39.275,00:43:44.735
Good paraphrase?,00:43:46.215,00:43:46.965
And so you are right.,00:43:46.965,00:43:49.325
It does.,00:43:49.325,00:43:50.015
"But in the end you will see them basically
in very similar contexts, a lot of times.",00:43:50.015,00:43:56.440
"And so in this log here,",00:43:56.440,00:43:58.820
"they will eventually have to
capture the log count, right?",00:44:00.220,00:44:03.790
"So they will have to go to a certain size
of what these log counts usually are.",00:44:03.790,00:44:09.490
"And then the model just figures
out that they are in the end",00:44:09.490,00:44:12.250
roughly in the same place.,00:44:12.250,00:44:14.450
"There's nothing in the optimization
that pushes some vectors to get really,",00:44:14.450,00:44:18.309
"really large, except of course,
the vectors of words that appear very",00:44:18.309,00:44:21.817
"frequently, and
that's why we have exactly this term here,",00:44:21.817,00:44:24.857
"to basically cap the importance
of the very frequent words.",00:44:24.857,00:44:27.851
"Yes, so the question is, and I'll just
phrase it the way it is, which is right.",00:44:45.028,00:44:51.570
"The skip-gram model tries to capture
co-occurrences one window at a time.",00:44:51.570,00:44:55.950
"And the Glove model tries to capture
the counts of the overall statistics",00:44:57.190,00:45:01.690
"of how often these words appear together,
all right.",00:45:01.690,00:45:06.250
One more question?,00:45:06.250,00:45:06.870
I think there was one.,00:45:06.870,00:45:10.450
No?,00:45:10.450,00:45:11.030
Great.,00:45:11.030,00:45:11.660
So now we can look at some fun results.,00:45:11.660,00:45:14.740
"And, basically, we found,
the nearest neighbors for",00:45:14.740,00:45:19.460
frog were all these various words.,00:45:19.460,00:45:21.400
"And we're first a little worried,
but then we looked them up.",00:45:21.400,00:45:24.020
"And realize, alright,
those are actually quite good.",00:45:24.020,00:45:25.960
"So you'll see here even for
very rare words, Glove will give you very,",00:45:25.960,00:45:30.520
very good nearest neighbors in this space.,00:45:30.520,00:45:34.080
"And so next,
we will do the evaluation, but",00:45:34.080,00:45:36.760
"before that we'll do a little
intermission with Arun.",00:45:36.760,00:45:41.490
Take it away.,00:45:42.510,00:45:43.064
"&gt;&gt; [SOUND] Cool, so
we've been talking about word vectors.",00:45:47.638,00:45:52.401
"I'm gonna take a brief detour
to talk about Polysemy.",00:45:52.401,00:45:55.930
"So far we've seen that word vectors
encode similarity, we see that",00:45:57.350,00:46:01.200
"similar concepts are even distributed
in Euclidean space near each other.",00:46:01.200,00:46:06.590
"And the question I want you to think
about is, what do we do about polysemy?",00:46:06.590,00:46:11.260
Suppose you have a word like tie.,00:46:11.260,00:46:12.630
"All right, tie could mean
something like a tie in a game.",00:46:12.630,00:46:16.220
So maybe it should be near this cluster.,00:46:16.220,00:46:19.409
"Over here.
It could be a piece of clothing, so",00:46:21.730,00:46:24.910
"maybe it should be near this cluster, or",00:46:24.910,00:46:26.840
"it could be an action like braid twist,
should be near this cluster.",00:46:26.840,00:46:31.280
Where should it lie?,00:46:31.280,00:46:33.130
So this paper by Sanjeev Arora and,00:46:33.130,00:46:37.010
"the entire group,
they seek to answer this question.",00:46:37.010,00:46:41.300
"And one of the first things
they find is that if",00:46:41.300,00:46:45.660
"you have an imaginary you could split
up tie into these polysemous vectors.",00:46:45.660,00:46:50.230
"You had tie one every time you
talk about this sport event.",00:46:50.230,00:46:53.300
"Tie two every time you talked
about the garment of clothing.",00:46:53.300,00:46:56.050
"Then, you can show that the actual
tie that is a combination of",00:46:57.450,00:47:02.110
"all of these words lies in the linear
superposition of all of these vectors.",00:47:02.110,00:47:07.770
"You might be wondering, how is this
vector close to all of them, but",00:47:07.770,00:47:11.380
"that's because we're projecting
this into a 2D plane and so",00:47:11.380,00:47:15.040
"it's actually closer to
them in other dimensions.",00:47:15.040,00:47:17.460
"Now that we know that
this tie lies near or",00:47:19.200,00:47:23.720
"in the plane of the different senses
we might be curious to find out,",00:47:23.720,00:47:29.220
"can we actually find out what
the different senses of a word are.",00:47:29.220,00:47:33.470
"Suppose we can only see this word tie,
could we computationally find out",00:47:33.470,00:47:38.540
"to some core logistics that tie had
a meaning about sport clothing etc.",00:47:38.540,00:47:43.935
"So the second thing that they're able
to show is that there's an algorithm",00:47:45.060,00:47:47.860
called sparse coding.,00:47:47.860,00:47:49.270
That is able to recover these.,00:47:49.270,00:47:51.040
"I don't have time to discuss exactly what
sparse coding how the algorithm works but",00:47:51.040,00:47:55.270
let me describe the model.,00:47:55.270,00:47:56.760
"The model says that every word
vector you have is composed as",00:47:56.760,00:48:01.823
"the sum of a small selected number
of what are called context vectors.",00:48:01.823,00:48:08.500
"So these context vectors,
there are only 2,000 that they found for",00:48:08.500,00:48:11.230
"their entire corpus,
are common across every word.",00:48:11.230,00:48:14.850
"But every word like tie is
only composed of a small",00:48:14.850,00:48:17.790
number of these context vectors.,00:48:17.790,00:48:19.640
"So, the context vector could
be something like sports, etc.",00:48:19.640,00:48:22.436
"There's some noise added in,
but that's not very important.",00:48:22.436,00:48:26.440
"And so, if you look at the type of output
that you get for something like tie,",00:48:26.440,00:48:30.110
"you see something to do with clothing,
with sports.",00:48:30.110,00:48:34.950
"Very interestingly you also
see output about music.",00:48:34.950,00:48:37.780
"Some of you might realize
that actually makes sense.",00:48:37.780,00:48:40.360
"And now,
we might wonder how this is qualitative.",00:48:41.600,00:48:45.528
"Is there a way we can quantitatively
evaluate how good the senses we",00:48:45.528,00:48:48.889
recover are?,00:48:48.889,00:48:50.405
"So it turns out, yes you can, and
here's the sort of experimental set-up.",00:48:50.405,00:48:56.315
"So, for
every word that was taken from WordNet,",00:48:56.315,00:49:00.525
"a number of about 20 sets of
related senses were picked up.",00:49:00.525,00:49:05.880
"So, a bunch of words that represent
that sense, like tie, blouse, or",00:49:05.880,00:49:09.630
"pants, or something totally unrelated,
like computer, mouse, and keyboard.",00:49:09.630,00:49:14.240
"And so now they asked a bunch of grad
students, because they're guinea pigs, to",00:49:14.240,00:49:19.589
"differentiate if they could find out which
one of these words correspond to tie.",00:49:19.589,00:49:24.954
"And they also asked the algorithm
if it could make that distinction.",00:49:24.954,00:49:28.510
"The interesting thing is that,",00:49:28.510,00:49:30.090
"the performance of this method that
I alluded to earlier, is about at",00:49:31.120,00:49:35.900
"the same level as the non-native grad
students that they had surveyed.",00:49:35.900,00:49:40.540
Which I think is interesting.,00:49:40.540,00:49:42.840
The native speakers do better on the task.,00:49:42.840,00:49:46.810
"So in summary,
word vectors can indeed capture polysemy.",00:49:46.810,00:49:50.700
"It turns out these polysemies,
the word vectors,",00:49:50.700,00:49:53.000
"are in the linear superposition
of the polysemy vectors.",00:49:53.000,00:49:56.410
"You can recover the senses that
a polysemous word has wIth sparse coding.",00:49:56.410,00:50:01.960
"And the senses that you
recover are almost as good as",00:50:01.960,00:50:05.032
that of a non-native English speaker.,00:50:05.032,00:50:07.470
Thank you.,00:50:07.470,00:50:08.630
"&gt;&gt; Awesome, thank you Arun.",00:50:08.630,00:50:09.667
"&gt;&gt; [APPLAUSE]
&gt;&gt; All right,",00:50:09.667,00:50:15.066
so now on to evaluating word vectors.,00:50:15.066,00:50:18.210
"So we've had gone through now
a bunch of new machinery.",00:50:18.210,00:50:23.160
"And you say, well,
how well does this actually work?",00:50:23.160,00:50:25.910
I have all these hyperparameters.,00:50:25.910,00:50:27.520
What's the window size?,00:50:27.520,00:50:29.340
What's the vector size?,00:50:29.340,00:50:30.690
"And we already came up
with these questions.",00:50:30.690,00:50:32.660
"How much does it matter
how do we choose them?",00:50:32.660,00:50:35.040
And these are all the answers now.,00:50:35.040,00:50:37.500
"Well, at least some of them.",00:50:37.500,00:50:39.350
"So, in a very high level, and this will be
true for a lot of your projects as well,",00:50:39.350,00:50:43.990
"you can make a high level decision of
whether you will have an intrinsic or",00:50:43.990,00:50:48.470
"an extrinsic evaluation of
whatever project you're doing.",00:50:48.470,00:50:52.680
"And in the case of word vectors,
that is no different.",00:50:52.680,00:50:56.060
"So intrinsic evaluations are usually on
some specific or intermediate subtask.",00:50:56.060,00:51:01.350
"So we might, for instance, look at how
well do these vector differences or vector",00:51:01.350,00:51:06.040
"similarities and inner products correlate
with human judgments of similarity.",00:51:06.040,00:51:10.260
"And we'll go through a couple
of these kinds of evaluations in",00:51:10.260,00:51:13.810
the next couple of slides.,00:51:13.810,00:51:15.470
"The advantage of intrinsic evaluations
is that they're going to be very fast",00:51:15.470,00:51:18.764
to compute.,00:51:18.764,00:51:19.327
"You have your vectors,",00:51:19.327,00:51:20.748
"you run them through this quick
similarity correlation study.",00:51:20.748,00:51:24.592
"And you get a number out and
you then can claim victory very quickly.",00:51:24.592,00:51:28.250
"And then or you can modify your model and
try 50,000 different little knobs and",00:51:28.250,00:51:33.819
combinations and tune this very quickly.,00:51:33.819,00:51:37.465
"It sometimes helps you really understand
very quickly how your system works, what",00:51:37.465,00:51:42.210
"kinds of hyperparameters actually have
an impact on this metric of similarity,",00:51:42.210,00:51:46.915
for instance.,00:51:46.915,00:51:48.112
"However, there's no free lunch here.",00:51:48.112,00:51:51.622
"It's not clear, sometimes,
if your intermediate or",00:51:51.622,00:51:55.062
"intrinsic evaluation and improvements
actually carry out to be a real",00:51:55.062,00:51:59.632
"improvement in some task
real people will care about.",00:51:59.632,00:52:02.810
"And real people is a little
tricky definition.",00:52:02.810,00:52:05.500
"I guess real people,",00:52:05.500,00:52:06.565
"usually we'll assume are like
normal people who want to just have",00:52:06.565,00:52:09.820
"a machine translation system or a question
answering system or something like that.",00:52:09.820,00:52:14.230
Not necessarily linguists and,00:52:14.230,00:52:15.530
"natural language processing
researchers in the field.",00:52:15.530,00:52:18.440
"And so, sometimes you actually
observe people trying to",00:52:18.440,00:52:22.533
"optimize their intrinsic
evaluations a lot.",00:52:22.533,00:52:25.867
"And they spent years of
their life optimizing them.",00:52:25.867,00:52:28.387
"And other people later find out, well,
it turns out those improvements on your",00:52:28.387,00:52:32.146
"intrinsic task, when I actually
applied your better word vectors or",00:52:32.146,00:52:35.400
"something to name entity recognition or
part of speech tagging or",00:52:35.400,00:52:38.487
"machine translation,
I don't see an improvement.",00:52:38.487,00:52:41.840
"So then the question is, well, how useful
is your intrinsic evaluation task?",00:52:41.840,00:52:45.500
"So as you go down this route, and
a lot of you will for their projects,",00:52:45.500,00:52:49.320
"you always wanna make sure you establish
some kind of correlation between these.",00:52:49.320,00:52:53.800
"Now, the extrinsic one is basically
evaluation on a real task.",00:52:53.800,00:52:56.978
"And that's really where
the rubber hits the road, or",00:52:56.978,00:53:00.130
"the proof is in the pudding, or whatever.",00:53:00.130,00:53:02.010
"The problem with that is that
it can take a very long time.",00:53:02.010,00:53:05.030
"You have your new word vectors and
you're like,",00:53:05.030,00:53:07.540
"I took the Pearson correlation instead of
the raw count of my core currents matrix.",00:53:07.540,00:53:10.815
I think that's the best thing ever.,00:53:10.815,00:53:13.000
"Now I wanna evaluate whether that
word vector really helps for",00:53:13.000,00:53:16.421
machine translation.,00:53:16.421,00:53:17.747
"And you say, all right,
now I'm gonna take my word vectors and",00:53:17.747,00:53:19.829
"plug them into this machine
translation system.",00:53:19.829,00:53:21.980
"And that turns out to
take a week to train.",00:53:21.980,00:53:23.580
"And then you have to wait a long time,
and now you have ten other knobs, and",00:53:24.760,00:53:27.600
"before you know it, the year is over.",00:53:27.600,00:53:28.730
"And you can't really just do
that every time you have a tiny,",00:53:28.730,00:53:32.330
"little improvement on your first
early word vectors, for instance.",00:53:32.330,00:53:36.910
"So that's the problem,
it takes a long time.",00:53:36.910,00:53:40.123
"And then often people will often make
the mistake of tuning a lot of different",00:53:40.123,00:53:44.165
subsystems.,00:53:44.165,00:53:44.930
"And then they put it all together
into the full system, the real task,",00:53:44.930,00:53:49.253
like machine translation.,00:53:49.253,00:53:51.028
"And something overall has improved,",00:53:51.028,00:53:53.035
"but now it's unclear which part
actually gave the improvement.",00:53:53.035,00:53:56.710
"Maybe two parts where actually, one was
really good, the other one was bad.",00:53:56.710,00:54:00.040
"They cancel each other out, and so on.",00:54:00.040,00:54:01.790
"So you wanna basically,
when you use extrinsic evaluations,",00:54:01.790,00:54:05.380
"be very certain that you only change
one thing that you came up with, or",00:54:05.380,00:54:09.880
"one aspect of your word vectors,
for instance.",00:54:09.880,00:54:12.080
"And if you then get an improvement
on your overall downstream task,",00:54:12.080,00:54:15.670
then you're really in a good place.,00:54:15.670,00:54:18.320
So let's be more explicit and,00:54:18.320,00:54:20.670
"go through some of these
intrinsic word vector evaluations.",00:54:20.670,00:54:24.060
"One that was very popular and
came out just very recently",00:54:25.430,00:54:30.251
"with the word2vec paper was
these word vector analogies.",00:54:30.251,00:54:35.392
"Where basically they found,
which was initially very surprising to",00:54:35.392,00:54:40.482
"a lot of people, that you have amazing
kinds of semantic and syntactic analogies",00:54:40.482,00:54:46.446
"that are captured through these
cosine distances in these vectors.",00:54:46.446,00:54:51.550
"So for instance, you might ask,
what is man to woman and",00:54:51.550,00:54:56.780
the relationship of king to another word?,00:54:56.780,00:54:59.360
And basically a simple analogy.,00:55:00.590,00:55:03.140
Man to woman is like king to queen.,00:55:03.140,00:55:06.590
That's right.,00:55:06.590,00:55:07.590
"And so it turns out that,
when you just take vector of woman,",00:55:07.590,00:55:11.418
"you subtract the vector of man,
and you add the vector of king.",00:55:11.418,00:55:15.411
"And then you try to find the vector
that has the largest cosine similarity.",00:55:15.411,00:55:21.940
"It turns out the vector of queen
is actually that vector that has",00:55:21.940,00:55:26.376
"the largest cosine
similarity to this term.",00:55:26.376,00:55:29.567
"And so that is quite amazing,
and it works for",00:55:31.400,00:55:34.652
"a lot of different kinds
of very intuitive patterns.",00:55:34.652,00:55:38.514
"So, let’s go through a couple of them.",00:55:38.514,00:55:40.503
"So you'd have similar things like, if
sir to madam is similar as man to woman,",00:55:40.503,00:55:45.461
"or heir to heiress, or king to queen,
or emperor to empress, and so on.",00:55:45.461,00:55:50.800
"So they all have a similar kind of
relationship that is captured very well",00:55:50.800,00:55:56.186
"by these cosine distances in this simple
Euclidean Subtractions and additions.",00:55:56.186,00:56:02.510
It goes even more specific.,00:56:05.040,00:56:06.780
"You have similar kinds of companies and
their CEO names.",00:56:06.780,00:56:11.610
"And you can take company, title,
minus CEO plus other company, and",00:56:11.610,00:56:15.990
"you get to the vector of the name
of the CEO of that other company.",00:56:15.990,00:56:19.190
"And it works not just for
semantic relationships but also for",00:56:21.200,00:56:24.440
"syntactic relationships, so slow,
slower, or slowest in these glove",00:56:24.440,00:56:29.150
"things has very similar
kind of differences and so",00:56:29.150,00:56:34.180
"on, to short, shorter, and shortest,
or strong, stronger, and strongest.",00:56:34.180,00:56:39.340
"You can have a lot of fun with this and
people did so here are some even more fun",00:56:39.340,00:56:44.070
"ones like Sushi- Japan + Germany
goes to bratwurst, and so on.",00:56:44.070,00:56:50.660
"Which as a German, I'm mildly offended by.",00:56:50.660,00:56:53.750
"And of course,
it's very intuitive in some ways.",00:56:53.750,00:56:59.350
But it's also questionable.,00:56:59.350,00:57:00.410
"Maybe it should have been [INAUDIBLE] or
whatever.",00:57:00.410,00:57:02.941
Other typical German foods.,00:57:02.941,00:57:06.474
"While this is very intuitive and for
some people, in terms of the actual",00:57:08.080,00:57:13.930
"semantics that are captured here, you
might really wonder why this has happened.",00:57:13.930,00:57:18.860
"And there is no mathematical proof
of why this has to fall out but",00:57:18.860,00:57:23.570
"intuitively you can kind of
make sense of it a little bit.",00:57:23.570,00:57:27.190
"Superlatives for instance might
appear next to certain words,",00:57:27.190,00:57:33.255
"very often, in similar kinds of ways.",00:57:33.255,00:57:37.140
"Maybe most, for instance,
appears in front of a lot of superlative.",00:57:37.140,00:57:42.035
"Or barely might appear in front of
certain words like slower or shorter.",00:57:43.850,00:57:52.520
"It's barely shorter
than this other person.",00:57:52.520,00:57:55.925
"And since in these vectors you're
capturing these core occurrence accounts,",00:57:55.925,00:58:00.682
"as you take out, basically one concurrence
you subtract that one concurrence",00:58:00.682,00:58:05.368
intuitively it's a little hand wavy.,00:58:05.368,00:58:07.717
"There's no like again here this is
not a nice mathematical proof but",00:58:07.717,00:58:11.619
"intuitively you can see how similar kinds
of words appeared and you subtract those",00:58:11.619,00:58:16.340
"counts and hence you arrive in similar
kinds of places into vector space.",00:58:16.340,00:58:20.598
"Now first you try a couple of these, and
you're surprised that this works well.",00:58:20.598,00:58:25.650
"And then you want to make it
a little more quantitative.",00:58:25.650,00:58:27.880
"All right, so",00:58:27.880,00:58:28.410
"this was a qualitative sub sample of some
words where this works incredibly well.",00:58:28.410,00:58:33.890
"It's also true that when you
really play around with it for",00:58:33.890,00:58:36.090
"a while,
you'll find something things that are like",00:58:36.090,00:58:38.720
"Audi minus German goes to some
crazy sushi term or something.",00:58:38.720,00:58:42.630
It doesn't always make sense but,00:58:42.630,00:58:44.560
"there are a lot of them where it
really is surprisingly intuitive.",00:58:44.560,00:58:48.350
"And so people essentially then came
up with a data set to try to see",00:58:48.350,00:58:53.220
"how often does it really appear and
does it really work this well?",00:58:53.220,00:58:58.230
"And so they basically collected
this Word Vector Analogies task.",00:58:58.230,00:59:02.810
And these are some examples.,00:59:02.810,00:59:04.150
"You can download all of
them on this link here.",00:59:04.150,00:59:06.260
"This is, again, the original
word2vec paper that discovered and",00:59:06.260,00:59:10.655
described these linear relationships.,00:59:10.655,00:59:13.260
"And they basically look at Chicago and
Illinois and Houston Texas.",00:59:13.260,00:59:16.500
"And you can basically come up
with a lot of different analogies",00:59:16.500,00:59:20.420
where this city appears in that state.,00:59:20.420,00:59:22.510
"Of course there are some problems and
as you optimize this metric more and",00:59:23.660,00:59:27.770
"more you will observe like well maybe
that city name actually appears in",00:59:27.770,00:59:32.360
"multiple different cities and
different states have the same name.",00:59:32.360,00:59:35.490
"And then it kind of depends on your
corpus that you're training on whether or",00:59:35.490,00:59:38.360
not this has been captured or not.,00:59:38.360,00:59:40.490
"But still, a lot of people,
it makes a lot of sense for",00:59:40.490,00:59:43.670
"most of them to optimize these
at least for a little bit.",00:59:43.670,00:59:47.021
"Here are some other examples of analogies
that are in this data set that are being",00:59:47.021,00:59:51.866
"captured, and just like the capital and
the world, of course you know as those",00:59:51.866,00:59:56.638
"change if it doesn't change in your
corpus that's also problematic.",00:59:56.638,01:00:01.013
"But in many cases the capitals of
countries don't change, and so",01:00:01.013,01:00:04.816
"it's quite intuitive and here's some
examples of syntactic relationships and",01:00:04.816,01:00:09.591
"analogies that are basically
in this data set to evaluate.",01:00:09.591,01:00:13.460
"We have several thousands
of these analogies and",01:00:13.460,01:00:16.220
"now, we compute our word vectors,
we've tuned some knob,",01:00:16.220,01:00:19.430
"we changed the hyperparameter instead of
25 dimensions, we have 50 dimensions and",01:00:19.430,01:00:23.560
"then we evaluate which one is better for
these analogies.",01:00:23.560,01:00:26.400
"And again, here is another syntactic one
with past tense kinds of relationships.",01:00:28.880,01:00:33.250
"Dancing to danced should
be like going to went.",01:00:33.250,01:00:35.500
"Now, we can basically look at a lot of
different methods, and we don't know all",01:00:37.070,01:00:41.170
"of these in the class here, but we know
the skip gram SG and the Glove model.",01:00:41.170,01:00:46.560
"And here is the first
evaluation that is quantitative",01:00:46.560,01:00:52.230
"and basically looks at the semantic and
the syntactic relationships, and",01:00:52.230,01:00:56.620
"then just average, in terms of the total.",01:00:56.620,01:00:59.060
"And just says, how often is
exactly this relationship true,",01:00:59.060,01:01:05.520
"for all these different analogies
that we have here in the data set.",01:01:05.520,01:01:09.630
"And it turns out that when both of
these papers came out in 2013 and",01:01:09.630,01:01:17.280
"14 basically GloVe was the best
at capturing these relationships.",01:01:17.280,01:01:22.530
"And so we observe a couple
of interesting things here.",01:01:22.530,01:01:24.900
"One, it turns out
sometimes more dimensions",01:01:24.900,01:01:28.990
"don't actually help in capturing
these relationships better, so",01:01:28.990,01:01:33.520
"thousand dimensional vectors work
worst than 300 dimensional vectors.",01:01:33.520,01:01:38.180
"Another interesting observation and that
is something that is somewhat sadly true",01:01:38.180,01:01:42.620
"for pretty much every deep learning model
ever is more data will work better.",01:01:42.620,01:01:48.330
"If you train your word
vectors on 42 billion tokens,",01:01:48.330,01:01:52.150
"it will work better than
on 6 billion tokens.",01:01:52.150,01:01:55.090
"By you know, 4% or so.",01:01:55.090,01:01:58.550
Here we have the same 300 dimensions.,01:01:58.550,01:02:01.250
"Again, we only want to change one thing
to understand whether that one change",01:02:01.250,01:02:05.843
actually has an impact.,01:02:05.843,01:02:07.337
And we'll see here a big gap.,01:02:07.337,01:02:10.440
"It's a good question.
How come the performance",01:02:17.664,01:02:19.354
sometimes goes down?,01:02:19.354,01:02:20.443
"It turns out it also depends on what
you're training your word vectors on.",01:02:20.443,01:02:26.620
"It turns out, Wikipedia for instance,
is really great because Wikipedia has very",01:02:26.620,01:02:30.430
"good descriptions of all these
capitals in all the world.",01:02:30.430,01:02:33.530
"But now if you take news, and let's say if
you take US news and in US news you might",01:02:33.530,01:02:38.530
"not have Abuja and
Ashgabat mentioned very often.",01:02:38.530,01:02:43.460
"Well, then the vectors for
those words will also not",01:02:43.460,01:02:46.100
"capture their semantics very well and
so you will do worse.",01:02:46.100,01:02:49.610
"And so some not, bigger is not always
better it also depends on the quality",01:02:49.610,01:02:53.780
of the data that you have.,01:02:53.780,01:02:55.090
"And Wikipedia has less misspellings
than general Internet texts and so on.",01:02:55.090,01:02:59.500
And it's actually a very good data set.,01:02:59.500,01:03:01.850
"And so here are some of
the evaluations and we have a lot of",01:03:01.850,01:03:06.800
"questions of like how do we choose this
hyperparameter the size and so on.",01:03:06.800,01:03:10.840
"This is I think a very good and careful
analysis that Geoffrey had done here three",01:03:10.840,01:03:16.520
"years ago on a variety of these different
hyperparameters that we've observed and",01:03:16.520,01:03:21.610
kind of mentioned in passing.,01:03:21.610,01:03:23.790
And so,01:03:23.790,01:03:24.580
"this is also a great sort of way that you
should try to emulate for your projects.",01:03:24.580,01:03:30.610
"Whenever I see plots like this I
get a big smile on my face and",01:03:30.610,01:03:34.360
your grades just like improve right away.,01:03:34.360,01:03:36.295
"&gt;&gt; [LAUGH]
&gt;&gt; Unless",01:03:36.295,01:03:37.620
you make certain mistakes in your plots.,01:03:37.620,01:03:39.110
But let's go through them.,01:03:39.110,01:03:41.880
"Here we look at basically the symmetric
context, the asymmetric context is",01:03:41.880,01:03:46.410
"where we only count words that have
happened after the current word.",01:03:46.410,01:03:51.010
"We ignore the things that's before but it
turns out symmetric usually works better",01:03:51.010,01:03:54.850
"and so a vector dimension here
is a good one to evaluate.",01:03:54.850,01:03:59.330
"It's pretty fundamental
how high dimensional.",01:03:59.330,01:04:01.478
Should these be.,01:04:01.478,01:04:03.810
"And we basically observe
that when they're very small",01:04:03.810,01:04:06.510
"it doesn't work as well in capturing these
analogies but then after around 200,",01:04:06.510,01:04:11.176
"300 it actually kind of peters out and
then it doesn't get much better.",01:04:11.176,01:04:15.240
"In fact, over all it's pretty
flat between 300 and 600.",01:04:15.240,01:04:21.460
And this is good.,01:04:21.460,01:04:22.687
"So, the main number we often look
at here is the overall accuracy and",01:04:22.687,01:04:26.620
that's in red here.,01:04:26.620,01:04:27.866
And that's flat.,01:04:27.866,01:04:29.190
"So, one mistake you could make
when create such a plot is you",01:04:29.190,01:04:34.210
"can prove you have some hyperparameter and
you have some kind of accuracy.",01:04:34.210,01:04:38.760
"This could be the vector size,
and you create a nice plot and",01:04:39.880,01:04:42.740
"you say look, things got better.",01:04:42.740,01:04:45.720
"And then my comment if I see
a plot like this would be,",01:04:45.720,01:04:49.390
"well why didn't you go
further in this direction?",01:04:49.390,01:04:51.740
It seems to just be going up and up.,01:04:51.740,01:04:53.920
"Like, so that is not good.",01:04:53.920,01:04:56.170
"You should find your plots until
they actually kind of peter out, and",01:04:56.170,01:04:59.860
"you say all right now, I really found
the optimum value for this hyperparameter.",01:04:59.860,01:05:05.940
"So, another important
thing to evaluate here",01:05:05.940,01:05:11.110
"is the window's size, and there
are sometimes considerations around this.",01:05:11.110,01:05:16.010
"So word vectors for instance,
maybe the 200 worked",01:05:16.010,01:05:21.030
"here slightly better than, or
300 works slightly better than 200.",01:05:21.030,01:05:25.590
"But, larger word vectors
also means more RAM, right?",01:05:25.590,01:05:28.800
"Your software now needs
to store more data.",01:05:28.800,01:05:32.850
"And you need to, you might want
to ship it to the cellphone.",01:05:32.850,01:05:36.660
"And now yes you might get 2%
improvement on this intrinsic task.",01:05:36.660,01:05:42.500
"But you also have 30%
higher RAM requirements.",01:05:42.500,01:05:46.050
"And maybe you say, well,
I don't care about those 2% or",01:05:46.050,01:05:48.400
"so improvement in accuracy
on this intrinsic task.",01:05:48.400,01:05:51.210
I still choose a smaller word vector.,01:05:51.210,01:05:53.210
"So, that's a legit argument,
but in general here,",01:05:53.210,01:05:56.530
we're just trying to optimize this metric.,01:05:56.530,01:06:00.650
"And so
we wanna look at carefully what these are.",01:06:00.650,01:06:02.630
"All right, now, window's size, again
this is how many words to the left and",01:06:02.630,01:06:06.772
"to the right of each of the center
words do we wanna predict and",01:06:06.772,01:06:12.190
compute the counts for.,01:06:12.190,01:06:14.070
"Turns out around eight or
so, you get the highest.",01:06:14.070,01:06:18.550
"But again that also increases
the complexity and the training time.",01:06:18.550,01:06:23.941
"The longer the windows are,",01:06:23.941,01:06:25.886
"the more times you have to compute
these kind of expressions.",01:06:25.886,01:06:30.108
"And then for asymmetric context,",01:06:30.108,01:06:32.574
"it's actually slightly different
windows size that works best.",01:06:32.574,01:06:37.344
"All right,
any question around these evaluations?",01:06:37.344,01:06:40.731
Great.,01:06:44.358,01:06:45.626
"Now, it's very hard actually,
to compare glove and the skip gram model,",01:06:45.626,01:06:50.275
"cuz they're very different
kinds of training regimes.",01:06:50.275,01:06:54.080
"One goes through the one window at a time,",01:06:54.080,01:06:56.425
"the other one first computes all
the counts, and then works on the counts.",01:06:56.425,01:07:01.060
"So this is kind of us
trying to do well and",01:07:01.060,01:07:05.480
"answer a reviewer question of
when you compare them directly.",01:07:05.480,01:07:09.350
"So what we did here is we
looked at the Negative Samples.",01:07:09.350,01:07:11.600
"So remember, we had that sum and
the objective function for",01:07:11.600,01:07:14.400
"the skip gram model of how many words
we want to push down the probability of",01:07:14.400,01:07:18.220
"cuz they don't appear in that window and
so",01:07:18.220,01:07:21.660
"that is one way to increase training time,
and in theory do better on that objective.",01:07:21.660,01:07:28.330
"Versus different iterations of how
often do we go over this cocurrence",01:07:28.330,01:07:33.345
"counts to optimize each pair in
the cocurrence matrix for GloVe.",01:07:33.345,01:07:38.027
And in this evaluation GloVe did better,01:07:38.027,01:07:40.948
"regardless of how many hours you
sort of trained both models.",01:07:40.948,01:07:45.559
"And this is more data helps,
that the argument already made.",01:07:45.559,01:07:52.010
Especially Wikipedia.,01:07:52.010,01:07:55.150
"So here Gigaword is I think
mostly a news corpus.",01:07:55.150,01:07:57.770
"So news, despite being more actually it
does not work quite as well, overall, and",01:07:57.770,01:08:05.220
"especially not for semantic,
relationships and analogies,",01:08:05.220,01:08:11.090
"but Common Crawl, which is a super large
data set of 42 billion tokens, works best.",01:08:11.090,01:08:15.150
"All right, so now these amazing analogies
of king minus man plus woman and",01:08:18.730,01:08:21.950
so on were very exciting.,01:08:21.950,01:08:25.070
"Before that, people used often
just correlation judgements.",01:08:25.070,01:08:30.870
"So basically they asked a bunch of people,
often grad students,",01:08:30.870,01:08:35.943
"to give on a scale of one to ten, how
similar do you think these two words are?",01:08:35.943,01:08:41.865
"So tiger and cat, when you ask three or
five humans on a scale from one to ten",01:08:41.865,01:08:45.847
"how similar they are, they might say,
one might say seven, the other eight,",01:08:45.847,01:08:50.086
"the other six or something like that and
then you average.",01:08:50.086,01:08:53.259
"And then you get basically a score
here of similarities our computer and",01:08:53.259,01:08:57.418
internet are seven.,01:08:57.418,01:08:58.760
"But stock and
CD are not very similar at all.",01:08:58.760,01:09:02.409
"So a bunch of people will say on a scale
from one to ten, it's only 1.3 on average.",01:09:02.409,01:09:06.859
"&gt;&gt; [INAUDIBLE]
&gt;&gt; And now,",01:09:06.859,01:09:10.513
we could try to basically say all right.,01:09:10.513,01:09:13.440
"We want to train word vectors such that
the vectors have a high correlation and",01:09:13.440,01:09:19.600
"their distances be it cosine similarity or
Euclidian distance,",01:09:19.600,01:09:24.580
"or you can try different distance metrics
too and look at how close they are.",01:09:24.580,01:09:29.790
And so here's one such example.,01:09:29.790,01:09:31.740
"You take the word of Sweden and
you look in terms of cosine similarity and",01:09:31.740,01:09:36.420
"you basically find lots of words
that are very, very close by or",01:09:36.420,01:09:40.670
have the largest cosine similarity and,01:09:40.670,01:09:45.140
"you basically get Norway and
Denmark to be very close by.",01:09:46.380,01:09:50.340
"And so, if you have a lot of these
kinds of data sets and this one,",01:09:50.340,01:09:55.150
"WordSim353 has basically
353 such pairs of words.",01:09:55.150,01:10:00.620
And you can look at how well,01:10:00.620,01:10:04.650
"do your vector distances correlate
with these human judgements.",01:10:04.650,01:10:09.970
"So the higher the correlation,",01:10:09.970,01:10:12.013
"the more intuitive we would think are the
distances in this large vector space.",01:10:12.013,01:10:17.097
"And again, Glove does very well
here across a whole host of",01:10:17.097,01:10:22.136
"different kinds of datasets
like the WordSim 353 and,",01:10:22.136,01:10:27.389
"again, the largest training
dataset here did best for Glove.",01:10:27.389,01:10:32.860
"Any questions on word vector
similarities and correlations?",01:10:32.860,01:10:38.315
"No, good, all right.",01:10:38.315,01:10:40.354
"Now, basically, intrinsic's evaluations
have this huge problem, right?",01:10:40.354,01:10:47.520
"We have these nice similarities,
but who knows?",01:10:47.520,01:10:49.940
"Maybe that doesn't actually improve the
real tasks that we care about in the end.",01:10:49.940,01:10:53.480
"And so the best kinds of evaluations,
but again they are very expensive,",01:10:53.480,01:10:57.910
"are those on real tasks or at least
subsequent kinds of downstream tasks.",01:10:57.910,01:11:02.450
"And so one such example is
named entity recognition.",01:11:02.450,01:11:04.850
"It's a good one cuz
it's relatively simple.",01:11:04.850,01:11:07.070
But it's actually useful enough.,01:11:07.070,01:11:09.360
"You might want to run a named entity
recognition system over a bunch of",01:11:09.360,01:11:12.740
your corporate emails.,01:11:12.740,01:11:14.090
"To understand which person is in
relationship to what company, and",01:11:14.090,01:11:17.833
"where do they live and the locations
of different people and so on.",01:11:17.833,01:11:21.520
"It's actually a useful system to have,
a named entity recognition system.",01:11:21.520,01:11:26.345
"And basically we'll go
through the actual models for",01:11:26.345,01:11:29.945
"doing a named entity recognition
in the next lecture.",01:11:29.945,01:11:34.400
"But as we plug in different
word vectors into these",01:11:34.400,01:11:37.330
"downstream models that we'll describe in
the next lecture we'll observe that for",01:11:37.330,01:11:41.770
"many of them GloVe vectors again do very,
very well on these downstream tasks.",01:11:41.770,01:11:46.550
"All right.
Any questions on extrinsic methods?",01:11:48.620,01:11:50.480
"We'll go through the actual
model that works here later.",01:11:50.480,01:11:54.060
That's right.,01:12:10.486,01:12:11.026
"Well, so you're not optimizing
anything here, you're just evaluating.",01:12:11.026,01:12:16.080
You're not training anything.,01:12:16.080,01:12:17.766
"You've trained your word vectors with your
objective function from skip-gram, and",01:12:17.766,01:12:21.900
"you fix them, and
then you just evaluate them.",01:12:21.900,01:12:24.590
"And so what you're evaluating here now
is you look at for instance Sweden and",01:12:24.590,01:12:28.730
"Norway, and they have a certain
distance between them, and",01:12:28.730,01:12:33.160
"then you want to basically
look at the human",01:12:33.160,01:12:35.460
"measure of how similar do humans
think these two words are.",01:12:37.460,01:12:41.330
"And then you want these kinds of human
judgements of similarity to correlate well",01:12:41.330,01:12:46.450
with the cosine distances of the vectors.,01:12:46.450,01:12:48.240
"And when they correlate well,
you think, the vectors are capturing",01:12:50.170,01:12:53.620
"similar kinds of intuitions that people
have, and hence they should be good.",01:12:53.620,01:12:57.056
"And again, intuitively it would
make sense that if Sweden",01:12:57.056,01:13:01.930
"has good cosine similarity and you plugged
it into some other downstream system,",01:13:01.930,01:13:05.940
"that that system will also get
better at capturing named entities.",01:13:05.940,01:13:09.940
"Because maybe at training time
it sees the vector of Sweden and",01:13:09.940,01:13:13.640
"at test time it sees
the vector of Norway and",01:13:13.640,01:13:16.285
"at training time you told that Sweden is
a location, and so a test time it might",01:13:16.285,01:13:19.800
"be more likely to correctly identify
Norway or Denmark also as a location.",01:13:19.800,01:13:24.920
"Because they're actually
close by in the vector space.",01:13:24.920,01:13:27.000
"And we'll go actually through example
of how we train word vectors and so",01:13:28.310,01:13:32.219
on in the next lecture.,01:13:32.219,01:13:33.510
Or train downstream tasks.,01:13:33.510,01:13:35.398
"So I think we have until 5:50,
so we got 8 more minutes.",01:13:35.398,01:13:40.300
"So, let's look briefly at simple,
single word classification.",01:13:40.300,01:13:47.250
"So you know we talked about these
word vectors and I basically showed",01:13:47.250,01:13:54.190
"you the difference between starting with
these very simple co-occurrence counts and",01:13:54.190,01:13:58.130
"these very sparse large vectors versus
having small dense vectors like Word2vec.",01:13:58.130,01:14:04.780
"And so the major benefits are basically
that because similar words cluster",01:14:04.780,01:14:09.820
"together, we'll be able to classify and
be more robust in classifying",01:14:09.820,01:14:16.070
"different kinds of words that we might
not see in the training data set.",01:14:17.450,01:14:22.220
"So for instance,
because countries cluster together and",01:14:22.220,01:14:25.050
"our goal is to classify location words
then we'll do better if we initialize",01:14:25.050,01:14:29.626
"all these country words to be in
a similar part of the vector space.",01:14:29.626,01:14:34.480
"It turns out later we'll actually
fine tune these vectors too.",01:14:35.660,01:14:39.080
"So right now we learned
an unsupervised objective function.",01:14:39.080,01:14:43.010
"It's unsupervised in the sense that we
don't have human labels that we assigned",01:14:43.010,01:14:47.904
"to each input, we just basically
took a large corpus of words, and",01:14:47.904,01:14:52.050
"we learned with these
unsupervised objective functions.",01:14:52.050,01:14:54.840
"But other tasks where that
doesn't actually work as well.",01:14:56.650,01:14:59.930
"So for instance sentiment analysis turns
out to not be a great downstream task for",01:14:59.930,01:15:06.110
"some word vectors because good and bad
might actually appear in similar contexts.",01:15:06.110,01:15:12.410
"I thought this movie was really good or
bad.",01:15:12.410,01:15:16.140
"And so when your downstream
task is sentiment analysis",01:15:16.140,01:15:19.260
"it turns out that maybe you can just
initialize your word vectors randomly.",01:15:19.260,01:15:22.530
"So this is kind of a bummer
after listening to us for",01:15:23.700,01:15:26.664
"many hours on how word
vectors should be trained.",01:15:26.664,01:15:30.070
"But fret not, it's in many cases word
vectors are helpful as your first step for",01:15:30.070,01:15:35.317
"your deep learning model, just not always.",01:15:35.317,01:15:38.311
"And again, that will be
something that you can evaluate.",01:15:38.311,01:15:41.430
Can I just initialize my words randomly or,01:15:41.430,01:15:43.550
"should I initialize them with
the Word2vec or the glove model.",01:15:43.550,01:15:46.500
"So as we're trying to classify words,
what we'll use is the softmax.",01:15:47.860,01:15:52.560
"And so you've seen this equation already
in the very beginning in the first slide",01:15:52.560,01:15:56.630
of the lecture.,01:15:56.630,01:15:57.532
"But we'll change the notation a little
bit because all the math that will follow",01:15:57.532,01:16:02.070
"will be easier to go through
with this kind of notation.",01:16:02.070,01:16:06.820
"So this is going to be
the softmax that we'll optimize.",01:16:06.820,01:16:11.940
"It's essentially just a different
word term for logistic regression.",01:16:11.940,01:16:15.670
"And we'll in many cases, have generally
a matrix W here for our different classes.",01:16:15.670,01:16:21.670
"So x, for instance, could be in
a simplest form, just a word vector.",01:16:22.760,01:16:27.080
"We're just trying to classify different
word vectors with no context of just like,",01:16:27.080,01:16:31.467
are these locations or not.,01:16:31.467,01:16:32.969
"It's not very useful, but just for
pedagogical reasons, let's assume x,",01:16:32.969,01:16:37.130
"our input here, is just a word vector.",01:16:37.130,01:16:39.630
"And I want to classify, is it a location,
or is it not a location.",01:16:39.630,01:16:43.120
"And then we give it basically, these
different kinds of word vectors that we",01:16:43.120,01:16:47.634
"compute it, for instance, for Sweden and
Norway, and then we want to classify is",01:16:47.634,01:16:52.499
"now Finland, Switzerland, and
also a location, yes or no.",01:16:52.499,01:16:56.686
So that's the task.,01:16:56.686,01:16:58.270
"And so our softmax here might just
have in the simplest case two,",01:16:58.270,01:17:04.380
"two doesn't really make sense so let's say
we have multiple different classes and",01:17:04.380,01:17:09.250
each class has one row vector here.,01:17:09.250,01:17:12.620
"And so this notation y is essentially
the number of rows that we have,",01:17:12.620,01:17:18.640
so the specific row that we have.,01:17:18.640,01:17:20.870
"And we have here inner product with this
rho vector times this column vector x.",01:17:20.870,01:17:26.516
"And then we normalize just
like we always do for",01:17:26.516,01:17:30.494
"logistic regression to get
an overall vector here for",01:17:30.494,01:17:35.084
all the different classes that sums to 1.,01:17:35.084,01:17:38.756
"So W in general for classification
will be a C by d dimensional matrix.",01:17:38.756,01:17:44.570
"Where d is our input and
C is the number of classes that we have.",01:17:44.570,01:17:47.660
"And again, logistic regression, just a
different term for softmax classification.",01:17:50.720,01:17:54.927
"And the nice thing about the softmax is
that it will generalize well above for",01:17:57.847,01:18:03.261
multiple different classes.,01:18:03.261,01:18:05.470
"And so, basically this is also
something we've already covered.",01:18:05.470,01:18:11.320
"So the loss function will use a similar
term for all the subsequent lectures.",01:18:11.320,01:18:15.800
"Loss function, cost function and objective
functions, we kind of use interchangeably.",01:18:15.800,01:18:19.720
"And what we'll use to optimize
the softmax is the cross entropy loss.",01:18:20.850,01:18:25.698
"And so I feel like the last minute,",01:18:25.698,01:18:29.090
"I'll just give you one extra minute,
cuz if we start now, it'll be too late.",01:18:29.090,01:18:33.190
"So that's it, thank you.",01:18:33.190,01:18:36.871
&gt;&gt; [APPLAUSE],01:18:36.871,01:18:39.380

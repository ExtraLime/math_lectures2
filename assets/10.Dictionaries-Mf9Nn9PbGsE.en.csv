text,start,stop
"The following content is
provided under a Creative",00:00:00.090,00:00:02.490
Commons license.,00:00:02.490,00:00:04.030
"Your support will help
MIT OpenCourseWare",00:00:04.030,00:00:06.360
"continue to offer high quality
educational resources for free.",00:00:06.360,00:00:10.720
"To make a donation or
view additional materials",00:00:10.720,00:00:13.320
"from hundreds of MIT courses,
visit MIT OpenCourseWare",00:00:13.320,00:00:17.280
at ocw.mit.edu.,00:00:17.280,00:00:18.450
"ERIK DEMAINE: All
right, today we're",00:00:20.940,00:00:22.440
"going to do an exciting
topic, which is hashing.",00:00:22.440,00:00:26.550
"Do it all in one
lecture, that's the plan.",00:00:26.550,00:00:28.490
See if we make it.,00:00:28.490,00:00:31.740
"You've probably
heard about hashing.",00:00:31.740,00:00:33.450
"It's probably the most
common data structure",00:00:33.450,00:00:35.970
in computer science.,00:00:35.970,00:00:37.050
"It's covered in pretty much
every algorithms class.",00:00:37.050,00:00:39.804
"But there's a lot
to say about it.",00:00:39.804,00:00:41.220
"And I want to
quickly review things",00:00:41.220,00:00:42.810
"you might know and then
quickly get to things",00:00:42.810,00:00:45.360
you shouldn't know.,00:00:45.360,00:00:47.335
"And we're going to
talk on the one hand",00:00:47.335,00:00:48.960
"about different kinds
of hash functions,",00:00:48.960,00:00:51.930
"fancy stuff like
k-wise independence,",00:00:51.930,00:00:53.790
"and a new technique that's
been analyzed a lot lately,",00:00:53.790,00:00:56.040
"simple tabulation hashing,
just in the last year.",00:00:56.040,00:00:58.819
"And then we'll look
at different ways",00:00:58.819,00:01:00.360
"to use this hash function that
actually build data structures,",00:01:00.360,00:01:03.240
"chaining is the obvious
one; perfect hashing you",00:01:03.240,00:01:05.760
"may have seen; linear probing
is another obvious one,",00:01:05.760,00:01:09.150
"but has only been
analyzed recently;",00:01:09.150,00:01:11.100
"and cuckoo hashing is a new one
that has its own fun feature.",00:01:11.100,00:01:15.690
"So that's where we're
going to go today",00:01:15.690,00:01:18.810
"Remember, the basic
idea of hashing",00:01:18.810,00:01:22.440
"is you want to reduce
a giant universe",00:01:22.440,00:01:25.350
to a reasonably small table.,00:01:25.350,00:01:28.090
"So I'm going to call
our hash function h.",00:01:28.090,00:01:30.640
"I'm going to call the universe
integer 0 up to u minus 1.",00:01:30.640,00:01:35.438
So this is the universe.,00:01:35.438,00:01:38.610
"And I'm going to denote the
universe by a capital U.",00:01:38.610,00:01:42.600
"And we have a table
that we'd like to store.",00:01:42.600,00:01:46.839
"I'm not going to draw
it yet because that's",00:01:46.839,00:01:48.630
"the second half of the lecture,
what is the table actually.",00:01:48.630,00:01:52.290
"But we'll just think of it as
indices 0 through m minus 1.",00:01:52.290,00:01:56.080
So this is the table size.,00:01:56.080,00:02:01.000
"And probably, we want
m to be about n. n",00:02:01.000,00:02:03.780
"is the number of keys we're
actually storing in the table.",00:02:03.780,00:02:06.660
"But that's not necessarily
seen at this level.",00:02:06.660,00:02:10.289
So that's a hash function.,00:02:10.289,00:02:13.110
"m is going to be
much smaller than u.",00:02:13.110,00:02:15.555
"We're just hashing
integers here,",00:02:15.555,00:02:16.930
"so if you don't have integers
you map your whatever space",00:02:16.930,00:02:19.830
of things you have to integers.,00:02:19.830,00:02:21.690
"That's pretty much
always possible.",00:02:21.690,00:02:23.790
"Now, best case scenario would
be to use a totally random hash",00:02:23.790,00:02:28.000
function.,00:02:28.000,00:02:28.500
What does totally random mean?,00:02:31.590,00:02:33.730
"The probability, if you
choose your hash function,",00:02:33.730,00:02:37.800
"that any key x maps to
any particular slot--",00:02:37.800,00:02:41.850
"these table things
are called slots--",00:02:41.850,00:02:44.700
is 1 over m.,00:02:44.700,00:02:47.640
"And this is
independent for all x.",00:02:47.640,00:02:51.925
So this would be ideal.,00:03:08.290,00:03:09.740
"You choose each h of x for
every possible key randomly,",00:03:09.740,00:03:14.900
independently.,00:03:14.900,00:03:16.120
"Then that gives you
perfect hashing.",00:03:16.120,00:03:18.450
"Not perfect in
this sense, sorry.",00:03:18.450,00:03:20.380
It gives you ideal hashing.,00:03:20.380,00:03:22.600
Perfect means no collisions.,00:03:22.600,00:03:24.910
"This actually might
have collisions,",00:03:24.910,00:03:26.410
"there's some chance that two
keys hash to the same value.",00:03:26.410,00:03:29.834
We call this totally random.,00:03:29.834,00:03:31.000
"This is sort of the
ideal thing that we're",00:03:31.000,00:03:32.410
"trying to approximate with
reasonable hash functions.",00:03:32.410,00:03:34.840
Why is this bad?,00:03:34.840,00:03:36.040
Because it's big.,00:03:36.040,00:03:37.990
"The number of bits
of information",00:03:37.990,00:03:39.820
"you'd need if you actually
could flip all these coins,",00:03:39.820,00:03:43.480
"you'd need to write
down, I guess U times",00:03:43.480,00:03:47.840
log m bits of information.,00:03:47.840,00:03:50.790
Which is generally way too big.,00:03:55.800,00:03:57.330
"We can't afford U.
The whole point is",00:03:57.330,00:03:59.220
"we want to store n items much
smaller than U. Surprisingly,",00:03:59.220,00:04:03.880
"this concept will
still be useful.",00:04:03.880,00:04:05.389
So we'll get there.,00:04:05.389,00:04:06.180
"Another system you've probably
seen is universal hashing.",00:04:11.920,00:04:18.870
"This is a constraint
on hash function.",00:04:18.870,00:04:27.380
"So this would be
ideally you'd choose h",00:04:27.380,00:04:29.690
"uniformly at random
from all hash functions.",00:04:29.690,00:04:31.700
"That would give you
this probability.",00:04:31.700,00:04:34.090
"We're going to make a
smaller set of hash functions",00:04:34.090,00:04:36.440
whose size is much smaller.,00:04:36.440,00:04:38.100
"And so you can encode the hash
function in many fewer bits.",00:04:38.100,00:04:40.850
"And the property we want
from that hash family",00:04:45.050,00:04:47.960
"is that if you look
at the probability",00:04:47.960,00:04:52.160
"that two keys collide,
you get roughly what",00:04:52.160,00:04:56.300
you expect from totally random.,00:04:56.300,00:05:00.200
You would hope for 1 over m.,00:05:00.200,00:05:02.832
"Once you pick one
key, the probability",00:05:02.832,00:05:04.415
"the other key would hit
it would be 1 over m.",00:05:04.415,00:05:08.110
But we'll allow constant factor.,00:05:08.110,00:05:11.030
And also allow it to be smaller.,00:05:11.030,00:05:13.390
It gives us some slop.,00:05:13.390,00:05:15.077
You don't have to do this.,00:05:15.077,00:05:16.160
"If you don't do this you it's
called strongly universal.",00:05:16.160,00:05:18.500
That's universal.,00:05:21.445,00:05:23.420
"And universal is enough
for a lot of things",00:05:23.420,00:05:25.460
"that you've probably seen, but
not enough for other things.",00:05:25.460,00:05:29.130
"So here are some examples
of hash functions",00:05:29.130,00:05:31.040
"that are universal, which
again, you may have seen.",00:05:31.040,00:05:34.655
"You can take a random
integer a, multiply it",00:05:49.880,00:05:53.030
by x integer multiplication.,00:05:53.030,00:05:54.680
"You could also do this as
a vector wise dot product.",00:05:54.680,00:05:57.140
"But here I'm doing
it as multiplication.",00:05:57.140,00:05:59.840
Modulo a prime.,00:05:59.840,00:06:01.460
"Prime has to be bigger than U,
maybe bigger or equal is fine.",00:06:01.460,00:06:07.630
Universe.,00:06:13.220,00:06:14.690
"And then you take the
whole thing modulo em,",00:06:14.690,00:06:17.270
"Now, this is universal but
it loses a factor of 2 here,",00:06:17.270,00:06:20.690
"I believe, in general.",00:06:20.690,00:06:21.950
"Because you take things
modulo prime and then",00:06:21.950,00:06:26.060
"you take things modulo
whatever your table size is.",00:06:26.060,00:06:28.310
"If you set your table
size to p that's great.",00:06:28.310,00:06:30.970
I think you get a factor of 1.,00:06:30.970,00:06:32.750
"If you don't, you're essentially
losing possibly half the slots,",00:06:32.750,00:06:39.170
"depending on how m and p
are related to each other.",00:06:39.170,00:06:43.040
"So it's OK, but not great.",00:06:43.040,00:06:46.190
"It's also considered
expensive because you",00:06:48.610,00:06:50.360
"have do all this division,
which people don't like to do.",00:06:50.360,00:06:54.290
"So there's a
fancier method which",00:06:54.290,00:06:56.900
"is a times x shifted right
by log u minus log m.",00:06:56.900,00:07:08.810
"This is when m and u are
powers of 2, which is",00:07:12.180,00:07:18.809
the case we kind of care about.,00:07:18.809,00:07:20.100
"Usually your
universe is of size 2",00:07:20.100,00:07:24.380
"to the word size of your
machine, 2 to the 32,",00:07:24.380,00:07:26.910
"2 to the 64, however
bigger integers are.",00:07:26.910,00:07:30.202
So it's usually a power of 2.,00:07:30.202,00:07:31.410
"It's fine to make your
table a power of 2.",00:07:31.410,00:07:33.180
"We're probably going
to use table doubling.",00:07:33.180,00:07:36.000
"So you just multiply and then
take the high order bits,",00:07:36.000,00:07:40.680
that's what this is saying.,00:07:40.680,00:07:42.760
"So this is a method
more recent, 1997.",00:07:42.760,00:07:46.200
"Whereas this one
goes back to 1979.",00:07:46.200,00:07:49.500
"So '79, '97.",00:07:49.500,00:07:53.580
And it's also universal.,00:07:53.580,00:07:57.071
"There's a lot of
universal hash functions.",00:07:57.071,00:07:58.820
I'm not going to list them all.,00:07:58.820,00:08:00.540
"I'd rather get to stronger
properties than universality.",00:08:00.540,00:08:05.460
"So the next one is called
k-wise independence.",00:08:05.460,00:08:08.330
This is harder to obtain.,00:08:14.400,00:08:17.780
And it implies universality.,00:08:17.780,00:08:21.600
"So we want a family of
hash functions such that--",00:08:21.600,00:08:27.270
"Maybe let's start with
just pairwise independence,",00:08:54.600,00:08:56.720
k equals 2.,00:08:56.720,00:08:58.349
"Then what this is saying
is the probability",00:08:58.349,00:09:00.140
"of your choice of hash function,
that the first key maps",00:09:00.140,00:09:03.740
"to this slot, t1, and
the second key maps",00:09:03.740,00:09:07.250
"to some other slot, t2.",00:09:07.250,00:09:09.920
For any two keys x1 and xk.,00:09:09.920,00:09:13.140
"If your function was
random each of those",00:09:16.070,00:09:19.580
"happens with probability 1
over m, they're independent.",00:09:19.580,00:09:22.070
"So you get 1 over m to the
k, or 1 over m squared for k",00:09:22.070,00:09:26.025
equals 2.,00:09:26.025,00:09:28.340
"Even in that situation
that's different from saying",00:09:28.340,00:09:30.590
"the probability of 2 keys
being equal is 1 over m.",00:09:30.590,00:09:34.460
This would imply that.,00:09:34.460,00:09:36.380
"But here there could still
be some co-dependence",00:09:36.380,00:09:38.810
between x and y.,00:09:38.810,00:09:40.320
Here there essentially can't be.,00:09:40.320,00:09:42.590
"I mean, other than
this constant factor.",00:09:42.590,00:09:46.160
"Pairwise independence means
every two guys are independent,",00:09:46.160,00:09:49.130
"k-wise means every k
guys are independent",00:09:49.130,00:09:51.640
up to the constant factor.,00:09:51.640,00:09:55.010
So this is for distinct xi's.,00:09:55.010,00:10:00.740
"Obviously if two
of them are equal",00:10:03.770,00:10:05.260
"they're very likely to
hash to the same slot.",00:10:05.260,00:10:08.690
So you've got to forbid that.,00:10:08.690,00:10:11.830
"OK, so an example of
such a hash function.",00:10:11.830,00:10:14.510
Here we just took a product.,00:10:19.130,00:10:22.820
"In general you can take a
polynomial of degree k minus 1.",00:10:22.820,00:10:35.180
Evaluate that mod p.,00:10:35.180,00:10:36.630
"And then if you want some,
modulo that to your table size.",00:10:39.200,00:10:47.279
"So in particular if k
equals 2, we actually",00:10:47.279,00:10:49.070
have to do some work.,00:10:49.070,00:10:49.945
"This function is not pairwise
independent, it is universal.",00:10:49.945,00:10:55.740
"If you make it ax plus
b for random a and b,",00:10:55.740,00:10:58.790
"then this becomes
pairwise independent.",00:10:58.790,00:11:00.890
"In general, you want
three wise independent,",00:11:00.890,00:11:04.340
"triple wise independent, you
need ax squared plus bx plus c",00:11:04.340,00:11:10.040
"for random a, b's, and c's.",00:11:10.040,00:11:13.250
"So these are arbitrary numbers
between 0 and p, I guess.",00:11:13.250,00:11:19.713
OK.,00:11:25.140,00:11:26.350
"This is also old, 1981.",00:11:26.350,00:11:29.870
"Wegman and Carter
introduced these two notions",00:11:29.870,00:11:33.300
in a couple of different papers.,00:11:33.300,00:11:34.830
This is an old idea.,00:11:34.830,00:11:37.620
"This is, of course, expensive
in that we pay order k time",00:11:37.620,00:11:41.670
to evaluate it.,00:11:41.670,00:11:43.500
"Also, there's a lot
of multiplications",00:11:43.500,00:11:46.050
"and you have to do
everything modulo p.",00:11:46.050,00:11:47.820
"So a lot of people have
worked on more efficient ways",00:11:47.820,00:11:50.028
to do k-wise independence.,00:11:50.028,00:11:51.780
"And there two main
results on this.",00:11:51.780,00:11:54.780
"Both of them achieve m to the
epsilon space, is not great.",00:11:54.780,00:11:59.830
"One of them the query
time depends on k,",00:12:11.840,00:12:16.580
"and it's uniform, and
reasonably practical",00:12:16.580,00:12:24.255
[? with ?] experiments.,00:12:24.255,00:12:26.450
"The other one is constant query
for a logarithmic independence.",00:12:26.450,00:12:38.136
So this one is actually later.,00:12:47.860,00:12:49.840
"It's by Thorpe and [? Tsang.
?] And this is by Siegel.",00:12:49.840,00:12:54.610
"Both 2004, so fairly recent.",00:12:54.610,00:12:59.800
It takes a fair amount of space.,00:12:59.800,00:13:01.960
"This paper proves that
to get constant query",00:13:01.960,00:13:04.630
"time for logarithmic
independence",00:13:04.630,00:13:06.100
"you'd need quite a bit of space
to store your hash function.",00:13:06.100,00:13:09.640
"Keep in mind, these hash
functions only take--",00:13:09.640,00:13:13.330
"well this is like k
log in bits to store.",00:13:13.330,00:13:16.660
So this is words of space.,00:13:16.660,00:13:18.920
"So here we're only
spending k words",00:13:18.920,00:13:22.574
to store this hash function.,00:13:22.574,00:13:23.740
It's very small.,00:13:23.740,00:13:24.360
"Here you need
something depending",00:13:24.360,00:13:25.420
"on n, which is kind of
annoying, especially",00:13:25.420,00:13:27.211
if you want to be dynamic.,00:13:27.211,00:13:29.290
"But statically you can get
constant query logarithmic wise",00:13:29.290,00:13:33.920
"independence, but you have
to pay a lot in space.",00:13:33.920,00:13:36.205
There's more practical methods.,00:13:39.999,00:13:41.290
"This is especially
practical for k",00:13:41.290,00:13:43.010
"equals 5, which is a case
that we'll see is of interest.",00:13:43.010,00:13:47.320
Cool.,00:13:50.010,00:13:52.030
"So this much space is
necessary if you want.",00:13:52.030,00:13:55.000
"We'll see log wise independence
is the most we'll ever",00:13:55.000,00:13:57.940
require in this class.,00:13:57.940,00:14:01.060
"And as far as I know,
in hashing in general.",00:14:04.987,00:14:06.820
"So you don't need
to worry about more",00:14:06.820,00:14:08.361
than log wise independence.,00:14:08.361,00:14:11.982
"All right, one more
hashing scheme.",00:14:11.982,00:14:17.555
"It's called simple
tabulation hashing.",00:14:17.555,00:14:21.120
This is a simple idea.,00:14:32.730,00:14:35.790
"It goes also back
to '81 but it's just",00:14:35.790,00:14:39.660
been analyzed last year.,00:14:39.660,00:14:42.740
"So there's a lot of
results to report on it.",00:14:42.740,00:14:47.000
"The idea is just
take your integer,",00:14:47.000,00:14:49.500
"split it up into some base
so that there's exactly",00:14:49.500,00:14:55.720
c characters.,00:14:55.720,00:14:58.720
c is going to be a constant.,00:14:58.720,00:15:01.050
"Then build a totally
random hash table.",00:15:01.050,00:15:08.070
"This is the thing that
we couldn't afford.",00:15:08.070,00:15:11.020
"But we're just going to
do it on each character.",00:15:11.020,00:15:13.020
"So there's going to be
c of these hash tables.",00:15:21.269,00:15:23.185
"And each of them is going to
have size u to the 1 over c.",00:15:26.290,00:15:33.130
"So essentially we're getting u
to the epsilon space, which is",00:15:33.130,00:15:37.960
similar to these space bounds.,00:15:37.960,00:15:40.030
"So again, not great, but it's
a really simple hash function.",00:15:40.030,00:15:45.100
"Hash function is just going to
be you take your first table,",00:15:47.920,00:15:52.780
"apply it to the first
character, x over that,",00:15:52.780,00:15:56.110
"with the second table applied
to the second character,",00:15:56.110,00:16:00.220
"and so on through
all the characters.",00:16:00.220,00:16:02.980
"So the nice thing about
this is it's super simple.",00:16:06.260,00:16:08.994
"You can imagine this
being done probably",00:16:08.994,00:16:10.660
"in one instruction
on a fancy CPU.",00:16:10.660,00:16:14.800
"if you convince people this
is a cool enough instruction",00:16:14.800,00:16:17.470
to have.,00:16:17.470,00:16:17.970
"It's very simple to
implement circuit wide.",00:16:17.970,00:16:21.940
"But in our model you have
to do all these operations",00:16:21.940,00:16:25.270
separately.,00:16:25.270,00:16:26.100
"You're going to take
orders c time to compute.",00:16:26.100,00:16:30.700
"And one thing that's
known about it",00:16:30.700,00:16:32.770
"is that it's three independent,
three wise independent.",00:16:32.770,00:16:39.340
"So it does kind of
fit in this model.",00:16:39.340,00:16:41.310
"But three wise independence
is not very impressive.",00:16:41.310,00:16:44.140
"A lot of the results we'll see
require log n independence.",00:16:44.140,00:16:47.740
"But the cool thing is, roughly
speaking simple tabulation",00:16:47.740,00:16:51.970
"hashing is almost as good
as log n wise independence",00:16:51.970,00:16:54.830
"in all the hashing schemes
that we care about.",00:16:54.830,00:16:58.280
"And so we'll get there,
exactly what that means.",00:16:58.280,00:17:01.990
"So that was my overview of some
hash functions, these two guys.",00:17:04.950,00:17:09.810
"Next we're going to
look at basic chaining.",00:17:09.810,00:17:13.609
Perfect hashing.,00:17:16.800,00:17:17.569
"How many people
have seen perfect",00:17:20.437,00:17:21.910
hashing just to get a sense?,00:17:21.910,00:17:25.000
More than half.,00:17:25.000,00:17:26.980
Maybe 2/3.,00:17:26.980,00:17:29.020
"All right, I should
do this really fast.",00:17:29.020,00:17:32.440
"Chaining, this is the first
kind of hashing you usually see.",00:17:32.440,00:17:38.314
"You have your hash
function, which",00:17:38.314,00:17:39.730
is mapping keys into slots.,00:17:39.730,00:17:42.700
"If you have two keys
that go to the same slot",00:17:42.700,00:17:45.010
you store them as a linked list.,00:17:45.010,00:17:47.960
"OK, if you don't have anything
in this slot, it's blank.",00:17:47.960,00:17:51.290
This is very easy.,00:17:51.290,00:17:53.410
"If you look at a
particular slot t and call",00:17:53.410,00:17:58.120
"the length of the chain
that you get there Ct.",00:17:58.120,00:18:02.200
"You can look at the expected
length of that chain.",00:18:02.200,00:18:05.860
"In general, it's just going
to be sum of the probability",00:18:05.860,00:18:11.260
that the keys map to that slot.,00:18:11.260,00:18:19.410
And then you sum over all keys.,00:18:19.410,00:18:21.780
"This is just writing this
as a sum of indicator",00:18:21.780,00:18:23.900
"random variables, and then
each linearity of expectation,",00:18:23.900,00:18:27.660
"expectation of each of
the indicator variables",00:18:27.660,00:18:30.300
is probability.,00:18:30.300,00:18:31.770
So that is the expected number.,00:18:31.770,00:18:33.240
"Here we just need to
compute the probability",00:18:39.770,00:18:41.610
each guy goes to each slot.,00:18:41.610,00:18:43.290
"As long as your hash
function is uniform,",00:18:43.290,00:18:46.810
"meaning that each of these
guys is equally likely",00:18:46.810,00:18:50.070
to be hashed to.,00:18:50.070,00:18:52.097
"Well actually, we're looking
at a particular slot.",00:18:52.097,00:18:54.180
"So we're essentially
using universality here.",00:18:54.180,00:18:57.990
"Once we fix one slot
that we care about,",00:18:57.990,00:19:00.490
"so let t be some h of
y that we care about,",00:19:00.490,00:19:04.520
then this is universality.,00:19:04.520,00:19:07.230
"By universality we
know this is 1 over m.",00:19:07.230,00:19:10.160
"And so this is 1 over n over m,
usually called the load factor.",00:19:10.160,00:19:16.890
"And what we care about
is this is constant for m",00:19:16.890,00:19:22.170
equal theta n.,00:19:22.170,00:19:23.220
"And so you use table
doubling to keep m theta n.",00:19:23.220,00:19:26.010
"Boom, you've got expected
chain length constant.",00:19:26.010,00:19:30.580
"But in the theory world
expected is a very weak bound.",00:19:30.580,00:19:36.090
"What we want are high
probability bounds.",00:19:36.090,00:19:38.007
"So let me tell you a little bit
about high probability bounds.",00:19:38.007,00:19:40.590
"This, you may not
have seen as much.",00:19:40.590,00:19:42.390
"Let's start with if your hash
function is totally random,",00:19:50.520,00:19:55.390
"then your chain lengths will be
order log m over a log log n,",00:19:55.390,00:20:01.914
with high probability.,00:20:01.914,00:20:02.830
They are not constant.,00:20:02.830,00:20:05.050
"In fact, you expect
the maximum chain",00:20:05.050,00:20:07.680
"to be at least log
n over log log n.",00:20:07.680,00:20:09.920
I won't prove that here.,00:20:09.920,00:20:11.370
"Instead, I'll prove
the upper bound.",00:20:11.370,00:20:13.670
"So the claim is that while
in expectation each of them",00:20:13.670,00:20:20.240
"is constant, variance
is essentially high.",00:20:20.240,00:20:23.360
"Actually let's talk about
variance a little bit.",00:20:32.610,00:20:34.700
"Sorry, I'm getting distracted.",00:20:34.700,00:20:36.779
"So you might say, oh
OK, expectation is nice.",00:20:41.470,00:20:44.052
Let's look at variance.,00:20:44.052,00:20:45.010
"Turns out variance is
constant for these chains.",00:20:45.010,00:20:49.905
"There are various
definitions of variance.",00:20:49.905,00:20:51.655
"But in particular the formula
I want to use is this one.",00:20:55.010,00:20:58.210
"It writes it as
some expectations.",00:21:01.472,00:21:05.140
"Now, this expected chain
length we know is constant.",00:21:05.140,00:21:08.057
"So you square it,
it's still constant.",00:21:08.057,00:21:09.640
So that's sort of irrelevant.,00:21:09.640,00:21:11.520
"The interesting part is what
is the expected squared chain",00:21:11.520,00:21:15.995
length.,00:21:15.995,00:21:17.230
"Now this is going to depend
exactly on your hash function.",00:21:17.230,00:21:20.320
"Let's analyze it
for totally random.",00:21:20.320,00:21:22.570
"In general, we just need a
certain kind of symmetry here.",00:21:22.570,00:21:25.900
"You can write I will look at the
expected squared chain lengths.",00:21:25.900,00:21:35.890
"And instead, what I'd like to
do is just sum over all of them.",00:21:40.000,00:21:45.770
"This is going to be
easier to analyze.",00:21:45.770,00:21:48.540
"So this is expected
squared chain lengths.",00:21:48.540,00:21:50.620
"If I sum over all chains and
then divide, take the average,",00:21:50.620,00:21:55.520
"then I'll probably
get the expected chain",00:21:55.520,00:21:57.259
length of any individual.,00:21:57.259,00:21:58.300
"As long as your hash
function is symmetric",00:21:58.300,00:22:00.049
"all the keys are sort
of equally likely.",00:22:00.049,00:22:02.020
This will be true.,00:22:02.020,00:22:02.770
"And then you could
just basically",00:22:05.926,00:22:07.300
"apply a random
permutation to your keys",00:22:07.300,00:22:08.860
"to make this true
if it isn't already.",00:22:08.860,00:22:10.443
"Now this thing is
just the number",00:22:13.360,00:22:16.750
of pairs of keys that collide.,00:22:16.750,00:22:20.240
"So you can forget about
slots, this is just the sum",00:22:20.240,00:22:25.850
"overall pairs of keys
ij of the probability",00:22:25.850,00:22:32.390
"that xi hashes to
the same spot as xj.",00:22:32.390,00:22:37.400
"And that's something we
know by universality.",00:22:37.400,00:22:41.330
This is 1 over m.,00:22:41.330,00:22:43.790
Big O.,00:22:43.790,00:22:46.940
"The number of
pairs is m squared.",00:22:46.940,00:22:49.940
"So we get m squared times
1 over m, times 1 over m.",00:22:49.940,00:22:53.110
This is constant.,00:22:53.110,00:22:53.840
"So the variance
is actually small.",00:22:58.620,00:23:00.230
"It's not a good indicator of
how big our chains can get.",00:23:00.230,00:23:03.240
"Because still, with time
probability one of the chains",00:23:03.240,00:23:05.960
will be log n over log log n.,00:23:05.960,00:23:07.340
It's just typical one won't be.,00:23:07.340,00:23:10.900
Let's prove the upper bound.,00:23:10.900,00:23:12.740
This uses Chernoff bounds.,00:23:12.740,00:23:14.840
"This is a tail
bound, essentially.",00:23:18.090,00:23:21.289
"I haven't probably defined
with high probability.",00:23:21.289,00:23:23.330
"It's probably good to
remember, review this.",00:23:23.330,00:23:25.365
"This means probability
at least 1 minus 1",00:23:25.365,00:23:28.640
"over n to c where I get
to choose any constant c.",00:23:28.640,00:23:34.370
"So high probability means
polynomially small failure",00:23:34.370,00:23:38.990
probability.,00:23:38.990,00:23:40.250
"This is good because if you do
this polynomially many times",00:23:40.250,00:23:43.310
this property remains true.,00:23:43.310,00:23:46.280
"You just up your constant
by however many times",00:23:46.280,00:23:49.280
you're going to use it.,00:23:49.280,00:23:51.840
"So we prove these kinds of
bounds using Chernov, which",00:23:51.840,00:23:56.330
looks something like this.,00:23:56.330,00:23:58.460
"e to the c minus 1
mu over c mu c mu.",00:23:58.460,00:24:07.608
So mu here is the mean.,00:24:07.608,00:24:10.240
"The mean we've already
computed is constant.",00:24:10.240,00:24:12.820
"The expectation of the
ct variable is constant.",00:24:12.820,00:24:16.830
"So we want it to be not
much larger than that.",00:24:16.830,00:24:20.340
"So say the probability that
it's some factor larger--",00:24:20.340,00:24:23.980
"c doesn't have to be constant
here, sorry, maybe not",00:24:23.980,00:24:26.480
great terminology.,00:24:26.480,00:24:29.900
"So the probability of ct is at
least some c times the mean,",00:24:29.900,00:24:34.839
"is going to be at
most this exponential.",00:24:34.839,00:24:36.505
"Which is a bit
annoying, or a bit ugly.",00:24:36.505,00:24:39.980
"But in particular, if we plug in
c equals log n over log log n,",00:24:39.980,00:24:45.626
"use that as our factor, which is
what we're concerned about here",00:24:45.626,00:24:50.840
then.,00:24:50.840,00:24:51.860
"We get that this probability
is essentially dominated",00:24:51.860,00:24:58.070
by the bottom term here.,00:24:58.070,00:25:00.170
"And this becomes
log n over log log n",00:25:00.170,00:25:07.100
"to the power log
n over log log n.",00:25:07.100,00:25:09.800
"So essentially, get 1 over that.",00:25:12.620,00:25:16.720
"And if you take this bottom part
and put it into the exponent,",00:25:16.720,00:25:21.470
you get essentially log log n.,00:25:21.470,00:25:23.560
"So this is something
like 1 over 2",00:25:23.560,00:25:27.035
"to the log n over log
log n times log log n.",00:25:27.035,00:25:33.780
And the log log n's cancel.,00:25:33.780,00:25:35.210
"And so this is
basically 1 over n.",00:25:35.210,00:25:38.300
"And if you put a
constant in here",00:25:38.300,00:25:40.850
"you can get a constant
in the exponent here.",00:25:40.850,00:25:43.920
"So you can get failure
probability 1 over n to the c.",00:25:43.920,00:25:47.366
"So get this with
high probability",00:25:47.366,00:25:48.740
"bound as long as
you go up to a chain",00:25:48.740,00:25:53.690
length of log n over log log n.,00:25:53.690,00:25:55.310
It's not true otherwise.,00:25:55.310,00:25:56.960
So this is kind of depressing.,00:25:56.960,00:25:58.370
"It's one reason we will
turn to perfect hashing,",00:25:58.370,00:26:01.070
some of the chains are long.,00:26:01.070,00:26:02.780
"But there is a sense in
which this is not so bad.",00:26:02.780,00:26:05.540
So let me go to that.,00:26:08.910,00:26:12.200
I kind of want all these.,00:26:21.020,00:26:23.010
AUDIENCE: [INAUDIBLE],00:26:29.432,00:26:32.841
ERIK DEMAINE: What's that?,00:26:33.611,00:26:34.694
AUDIENCE: [INAUDIBLE],00:26:34.694,00:26:36.880
"ERIK DEMAINE: Since
when is log n long.",00:26:36.880,00:26:38.590
Well--,00:26:38.590,00:26:39.345
AUDIENCE: [INAUDIBLE],00:26:39.345,00:26:40.960
"ERIK DEMAINE: Right, so
I mean, in some sense",00:26:40.960,00:26:43.100
"the name of the
game here is we want",00:26:43.100,00:26:44.599
to beat binary search trees.,00:26:44.599,00:26:46.300
"I didn't even mention what
problem we're solving.",00:26:46.300,00:26:48.349
"We're solving the
dictionary problem, which",00:26:48.349,00:26:50.140
"is sort of bunch of
keys, insert delete,",00:26:50.140,00:26:52.840
"and search is now
just exact search.",00:26:52.840,00:26:54.520
"I want to know is
this key in there?",00:26:54.520,00:26:56.020
"If so, find some data
associated with it.",00:26:56.020,00:26:59.660
"Which is something binary search
trees could do, n log n time.",00:26:59.660,00:27:02.920
"And we've seen
various fancy ways",00:27:02.920,00:27:04.750
to try to make that better.,00:27:04.750,00:27:06.420
"But in the worst case,
you need log n time",00:27:06.420,00:27:08.170
to do binary search trees.,00:27:08.170,00:27:09.253
"We want to get to constant
as much as possible.",00:27:09.253,00:27:12.940
"We want the hash function to be
evaluatable in constant time.",00:27:12.940,00:27:17.470
"We want the queries to
be done in constant time.",00:27:17.470,00:27:21.280
"If you have a long chain, you've
got to search the whole chain",00:27:21.280,00:27:24.550
"and I don't want to spend
log n over log log n.",00:27:24.550,00:27:29.290
Because I said so.,00:27:29.290,00:27:31.250
"Admittedly, log n over
log log n is not that big.",00:27:31.250,00:27:34.220
"And furthermore,
the following holds.",00:27:34.220,00:27:36.470
"This is a sense in
which it's not really",00:27:36.470,00:27:39.230
log n over log log n.,00:27:39.230,00:27:41.450
"If we change the
model briefly and say,",00:27:41.450,00:27:46.230
"well, suppose I have a cache
of the last log n items",00:27:46.230,00:27:50.300
"that I searched for
in the hash table.",00:27:50.300,00:27:53.420
"Then if you're
totally random, which",00:27:56.630,00:28:00.039
"is something we
assumed here in order",00:28:00.039,00:28:01.580
"to apply the Chernoff bound,
we needed that everything",00:28:01.580,00:28:04.320
was completely random.,00:28:04.320,00:28:06.750
"Then you get a constant
amortized bound per operation.",00:28:06.750,00:28:12.050
So this is kind of funny.,00:28:15.110,00:28:16.610
"In fact, all it's saying
this is easy to prove.",00:28:16.610,00:28:20.550
And it's not yet in any paper.,00:28:20.550,00:28:22.260
"It's on Mihai Petrescu's
blog from 2011.",00:28:22.260,00:28:30.440
"All right, we're here.",00:28:30.440,00:28:31.514
"We're looking at
different chains.",00:28:31.514,00:28:32.930
"So you access some chain,
then you access another chain,",00:28:32.930,00:28:35.420
then you access another chain.,00:28:35.420,00:28:37.640
"If you're unlucky,
you'll hit the big chain",00:28:37.640,00:28:39.740
"which cost log n
over over log log n",00:28:39.740,00:28:41.930
"to touch, which is expensive.",00:28:41.930,00:28:46.190
"But you could then put
all those guys into cache,",00:28:46.190,00:28:48.410
"and if you happen to
keep probing there",00:28:48.410,00:28:52.320
you know it should be faster.,00:28:52.320,00:28:55.070
"In general, you do
a bunch of searches.",00:28:55.070,00:28:58.370
"OK, first I search for x1, then
I search for x2, x3, so on.",00:28:58.370,00:29:06.020
"Cluster those into
groups theta log n.",00:29:06.020,00:29:10.706
"OK, let's look at
the first log n",00:29:10.706,00:29:12.080
"searches, then the next log
n searches, and analyze those",00:29:12.080,00:29:15.160
separately.,00:29:15.160,00:29:16.140
"We're going to amortize
over that period of log n.",00:29:16.140,00:29:20.130
So if we look at theta log n--,00:29:20.130,00:29:25.290
"actually, this is
written in a funny way.",00:29:25.290,00:29:27.290
"You've got the data, just log n.",00:29:34.342,00:29:36.200
"So I'm going to look at a
batch of log n operations.",00:29:42.590,00:29:45.500
"And I claim that
the number of keys",00:29:45.500,00:29:48.590
"that collide with them is theta
log n, with high probability.",00:29:48.590,00:29:56.126
"If this is true, then
it's constant each.",00:29:59.530,00:30:06.460
"If I can do log n operations
by visiting order log n total",00:30:06.460,00:30:11.380
"chain items with
high probability,",00:30:11.380,00:30:13.810
then I just charge one each.,00:30:13.810,00:30:15.550
"And so amortized over
this little log n window",00:30:15.550,00:30:18.075
of sort of smoothing the cost.,00:30:18.075,00:30:20.470
"With high probability
now, not just expectation,",00:30:20.470,00:30:23.410
"I get constant
amortized per operation.",00:30:23.410,00:30:26.560
"So I should have said,
with high probability.",00:30:26.560,00:30:29.050
Why is this true?,00:30:33.040,00:30:35.050
"It's essentially
the same argument.",00:30:35.050,00:30:36.580
"Here this is normally called
a balls and bin argument.",00:30:36.580,00:30:40.000
"So you're throwing balls,
which are your keys, randomly",00:30:40.000,00:30:42.910
"into your bins,
which are your slots.",00:30:42.910,00:30:46.600
"And the expectation is constant
probability to any one of them,",00:30:46.600,00:30:51.574
"I mean any one of
them could go up",00:30:51.574,00:30:52.990
"to log n over log log
n, high probability.",00:30:52.990,00:30:56.590
"Over here, we're looking
at log n different slots",00:30:56.590,00:30:59.950
"and taking the sum of balls that
fall into each of the slots.",00:30:59.950,00:31:04.990
"And in expectation that's log
n, because it's constant each.",00:31:04.990,00:31:08.412
"An expectation is
linear if you're taking",00:31:08.412,00:31:10.120
the sum over these log n bins.,00:31:10.120,00:31:12.760
So the expectation is log n.,00:31:12.760,00:31:14.620
So you apply Chernoff again.,00:31:14.620,00:31:16.390
Except now the mean is log n.,00:31:16.390,00:31:19.210
"And then it suffices
to put c equals 2.",00:31:19.210,00:31:22.080
We can run through this.,00:31:22.080,00:31:23.410
"So here we get the
mean is theta log n.",00:31:23.410,00:31:27.130
"We expect there to
be log n items that",00:31:27.130,00:31:30.550
fall into these log n bins.,00:31:30.550,00:31:33.190
"And so you just plug in c equals
2 to the Chernoff bound and you",00:31:33.190,00:31:36.730
"get e to the log n--
which is kind of weird--",00:31:36.730,00:31:41.860
over 2 log n to the 2 log n.,00:31:41.860,00:31:49.420
"So this thing is like
n to the log log n 2.",00:31:49.420,00:31:58.060
"So this is big, way
bigger than this.",00:31:58.060,00:32:00.250
So this essentially disappears.,00:32:00.250,00:32:02.850
"And in particular, this is
bigger than 1 over n to the c,",00:32:02.850,00:32:06.880
for any c.,00:32:06.880,00:32:07.945
"So log log n is bigger
than any constant.",00:32:07.945,00:32:11.900
So you're done.,00:32:11.900,00:32:12.929
"So that's just saying
the probability",00:32:12.929,00:32:14.470
"that you're more than twice
the mean is very, very small.",00:32:14.470,00:32:18.700
"So with high probability
there's only log n items",00:32:18.700,00:32:21.951
that fall in these log n bins.,00:32:21.951,00:32:23.200
"So you just amortize,
boom, constant.",00:32:23.200,00:32:25.510
This is kind of a weird notion.,00:32:25.510,00:32:27.010
"I've never actually seen
amortized with high probability",00:32:27.010,00:32:29.410
ever in a paper.,00:32:29.410,00:32:31.070
"This is the first time it
seems like a useful concept.",00:32:31.070,00:32:35.330
"So if you think log n
over log log n is bad,",00:32:35.330,00:32:40.160
"this is a sense
in which it's OK.",00:32:40.160,00:32:42.880
Don't worry about it.,00:32:42.880,00:32:45.820
"All right, but if you did worry
about it, next thing you do",00:32:45.820,00:32:49.390
is perfect hashing.,00:32:49.390,00:32:53.790
"So, perfect hashing is
really just an embellishment.",00:33:12.266,00:33:17.260
This is also called FKS hashing.,00:33:17.260,00:33:21.000
"From the authors, Friedman,
Komlosh, and [? Samaretti. ?]",00:33:21.000,00:33:27.800
"This is from 1984, so old idea.",00:33:27.800,00:33:30.350
"You just take
chaining, but instead",00:33:30.350,00:33:33.170
"of storing your chains
in a linked list,",00:33:33.170,00:33:34.880
store them in a hash table.,00:33:34.880,00:33:36.590
Simple idea.,00:33:36.590,00:33:38.550
There's one clever trick.,00:33:38.550,00:33:39.920
"You store it in
a big hash table.",00:33:44.036,00:33:45.410
"Hash table of size
theta ct squared.",00:33:48.410,00:33:53.175
"Now this looks like a
problem because that's",00:33:55.960,00:33:58.761
"going to be quadratic space,
in the worst case, if everybody",00:33:58.761,00:34:01.260
hashes to the same chain.,00:34:01.260,00:34:02.844
"But we know that
chains are pretty",00:34:02.844,00:34:04.260
small with high probability.,00:34:04.260,00:34:07.030
So turns out this is OK.,00:34:07.030,00:34:09.989
The space is sum of ct squared.,00:34:09.989,00:34:15.454
"And that's something we
actually computed already,",00:34:19.250,00:34:21.760
except I erased it.,00:34:21.760,00:34:23.236
How convenient of me.,00:34:23.236,00:34:24.110
It was right here.,00:34:24.110,00:34:25.179
I can still barely read it.,00:34:25.179,00:34:26.780
When we computed the variance.,00:34:26.780,00:34:29.330
"We can do it again it's
not really that hard.",00:34:29.330,00:34:31.219
"This is the number of
pairs of keys that collide.",00:34:31.219,00:34:34.770
"And so there's n squared
pairs and each of them",00:34:34.770,00:34:39.980
"has a probability 1 over
me of colliding if you",00:34:39.980,00:34:43.070
have a universal hash function.,00:34:43.070,00:34:45.300
"So this is n squared
over m, which",00:34:45.300,00:34:49.550
"if m is within a constant
factor of n, is linear.",00:34:49.550,00:34:52.580
So linear space in expectation.,00:34:59.890,00:35:02.440
"Expected amount of
space is linear.",00:35:04.990,00:35:07.210
"I won't try to do with high
probability bound here.",00:35:07.210,00:35:10.580
What else can I say?,00:35:10.580,00:35:11.620
"You have to play a similar
trick when you're actually",00:35:16.190,00:35:18.470
building these hash tables.,00:35:18.470,00:35:19.700
"All right, so why
do we use n squared?",00:35:19.700,00:35:22.850
Because of the birthday paradox.,00:35:22.850,00:35:24.980
"So if you have a hash table of
size n squared, essentially,",00:35:24.980,00:35:29.870
"or ct squared with
high probability",00:35:29.870,00:35:34.610
"or constant probability you
don't get any collisions.",00:35:34.610,00:35:37.560
Why?,00:35:37.560,00:35:38.150
"Because then they expected
number of collisions in ct.",00:35:38.150,00:35:47.200
"Well, there's ct pairs
or ct squared pairs.",00:35:47.200,00:35:50.570
"Each of them, if you're
using universal hashing,",00:35:50.570,00:35:52.970
"had a probability of 1 over
ct squared of happening.",00:35:52.970,00:35:58.190
"Because that's the
table size, 1 over m.",00:35:58.190,00:36:00.752
So this is constant.,00:36:00.752,00:36:04.004
"And if we set the
constants right,",00:36:04.004,00:36:05.420
"I get to set this theta
to be whatever I want.",00:36:05.420,00:36:08.610
I get this to be less than 1/2.,00:36:08.610,00:36:12.960
"If the expected number of
collisions is less than 1/2,",00:36:12.960,00:36:15.740
"then the probability that
the number of collisions is 0",00:36:15.740,00:36:25.100
is at least a 1/2.,00:36:25.100,00:36:26.960
"This is Markov's
inequality, in particular.",00:36:26.960,00:36:31.960
"The probability number of
collisions is at least 1",00:36:31.960,00:36:34.100
"is at most the expectation
over 1, so which is 1/2.",00:36:34.100,00:36:43.990
So you try to build this table.,00:36:43.990,00:36:46.510
"If you have 0
collisions you're happy.",00:36:46.510,00:36:48.907
You go on to the next one.,00:36:48.907,00:36:49.990
"If you don't have 0
collisions, just try again.",00:36:49.990,00:36:52.090
"So in an expected
constant number of trials",00:36:52.090,00:36:54.065
"you're flipping
a coin each time.",00:36:54.065,00:36:55.440
Eventually you'll get heads.,00:36:55.440,00:36:56.920
Then you can build this table.,00:36:56.920,00:36:58.880
And then you have 0 collisions.,00:36:58.880,00:37:00.430
"So we always want them
to be collision free.",00:37:00.430,00:37:03.040
"So in an expected linear
time you can build this table",00:37:07.930,00:37:12.680
"and it will have
expected linear space.",00:37:12.680,00:37:14.540
"In fact, if it doesn't
have linear space",00:37:14.540,00:37:15.820
"you can just try the
whole thing over again.",00:37:15.820,00:37:17.653
"So in expected
linear time you'll",00:37:17.653,00:37:19.300
"build a guaranteed
linear space structure.",00:37:19.300,00:37:21.280
"The nice thing about
perfect hashing",00:37:21.280,00:37:22.870
"is you're doing two hash
de-references and that's it.",00:37:22.870,00:37:26.650
"So the query is
constant deterministic.",00:37:26.650,00:37:34.050
"Queries are now deterministic,
only updates are randomized.",00:37:34.050,00:37:38.350
I didn't talk about updates.,00:37:38.350,00:37:39.760
I talked about building.,00:37:39.760,00:37:41.760
"The construction
here is randomized,",00:37:41.760,00:37:43.900
"queries are constant
deterministic.",00:37:43.900,00:37:45.820
"Now, you can make this dynamic
in pretty much the obvious way",00:37:45.820,00:37:50.020
"you say, OK, I want to insert.",00:37:50.020,00:37:52.700
"So I compute which of the, it's
essentially two level hashing.",00:37:52.700,00:37:59.410
"So first you figure out where
it fits in the big hash table,",00:37:59.410,00:38:02.710
"then you find the corresponding
chain, which is now hash table,",00:38:02.710,00:38:05.920
"and you insert into
that hash table.",00:38:05.920,00:38:07.900
So it's the obvious thing.,00:38:07.900,00:38:09.160
"The trouble is you might get a
collision in that hash table.",00:38:09.160,00:38:12.190
"If you get a collision, you
rebuild that hash table.",00:38:12.190,00:38:16.180
"Probability of the collision
happening is essentially small.",00:38:16.180,00:38:20.740
"It's going to remain small
because of this argument.",00:38:20.740,00:38:25.659
"Because we know the expected
number of collisions",00:38:25.659,00:38:27.700
"remains small unless that
chain gets really big.",00:38:27.700,00:38:30.820
"So if the chain grows
by a factor of 2,",00:38:30.820,00:38:33.760
"then generally you have
to rebuild the table.",00:38:33.760,00:38:37.710
"But if your chain length
grows by a factor of 2,",00:38:37.710,00:38:43.120
"then you rebuild your table
to have factor 4 size larger",00:38:43.120,00:38:46.720
because it's ct squared.,00:38:46.720,00:38:50.410
"And in general, you
maintain that the table",00:38:50.410,00:38:52.300
"is sized for a chain of roughly
the correct chain length",00:38:52.300,00:38:56.590
within a constant factor.,00:38:56.590,00:38:58.090
"And you do doubling and halving
in the usual way, like b-tree.",00:38:58.090,00:39:01.510
"Or, I guess it's
table doubling really.",00:39:01.510,00:39:04.840
"And it will be
constant amortized",00:39:04.840,00:39:07.420
expected per operation.,00:39:07.420,00:39:11.320
"And there's a fancy way
to make this constant",00:39:11.320,00:39:14.650
"with high probability
per insert and delete,",00:39:14.650,00:39:17.410
which I have not read.,00:39:17.410,00:39:19.622
"But it's by [? Desfil ?]
Binger and [? Meyer ?]",00:39:19.622,00:39:21.580
"[? Ofterheight, ?] 1990.",00:39:21.580,00:39:23.890
"So, easy to make this
expected amortized.",00:39:23.890,00:39:28.180
"With more effort
you could make it",00:39:28.180,00:39:29.770
"with high probability
per operation.",00:39:29.770,00:39:32.630
That is trickier.,00:39:32.630,00:39:35.980
Cool.,00:39:35.980,00:39:37.966
"I actually skipped one
thing with chaining,",00:39:37.966,00:39:42.560
which I wanted to talk about.,00:39:42.560,00:39:43.780
"So this analysis was fine,
it just used universality.",00:39:43.780,00:39:49.180
"This cache analysis,
I said totally random.",00:39:49.180,00:39:52.060
"This analysis, I
said totally random.",00:39:52.060,00:39:53.920
What about real hash functions?,00:39:53.920,00:39:55.580
We can't use totally random.,00:39:55.580,00:39:57.070
"What about universal k-wise
independent simple tabulation",00:39:57.070,00:40:00.760
"hashing, just for chaining?",00:40:00.760,00:40:04.600
"OK, and similar things hold
for perfect hashing, I think.",00:40:04.600,00:40:08.380
"I'm not sure if
they're all known.",00:40:08.380,00:40:11.610
"Oh, sorry, perfect hashing.",00:40:11.610,00:40:13.410
"In expectation everything's
fine, just with universal.",00:40:13.410,00:40:16.540
"So we've already done
that with universal.",00:40:16.540,00:40:18.370
What about chaining?,00:40:18.370,00:40:20.310
How big can the chains get?,00:40:20.310,00:40:21.670
"I said log n over log log
with a high probability,",00:40:21.670,00:40:23.849
"but our analysis
used Chernoff bound.",00:40:23.849,00:40:25.390
"That's only true for
Bernoulli trials.",00:40:25.390,00:40:27.450
"It was only true for totally
random hash functions.",00:40:27.450,00:40:29.980
"It turns out same
is true if you have",00:40:29.980,00:40:34.030
"a log n over log log n wise
independent hash function.",00:40:34.030,00:40:44.890
So this is kind of annoying.,00:40:44.890,00:40:46.240
"If you want this to be true
you need a lot of independence.",00:40:46.240,00:40:49.270
"And it's hard to get
log n independence.",00:40:49.270,00:40:51.340
"There is a way to get constant,
but it needed a lot of space.",00:40:51.340,00:40:55.060
"That was this one, which
is not so thrilling.",00:40:55.060,00:41:02.800
It's also kind of complicated.,00:41:02.800,00:41:05.320
"So if you don't mind
the space but you just",00:41:05.320,00:41:11.050
"want it to be simpler, you can
use simple tabulation hashing.",00:41:11.050,00:41:14.640
"Both of these, the same chain
analysis turns out to work.",00:41:18.520,00:41:23.980
So this is fairly old.,00:41:23.980,00:41:25.710
This is from 1995.,00:41:25.710,00:41:28.670
This is from last year.,00:41:28.670,00:41:31.370
"So if you just use this
simple tabulation hashing,",00:41:31.370,00:41:33.670
"still has a lot of
space, e to the epsilon.",00:41:33.670,00:41:37.540
But very simple to implement.,00:41:37.540,00:41:40.210
"Then still, the chain lengths
are as you expect them to be.",00:41:40.210,00:41:43.780
"And I believe that carries
over to this caching argument,",00:41:43.780,00:41:46.579
but I haven't checked it.,00:41:46.579,00:41:47.620
All right.,00:41:50.120,00:41:50.620
"Great, I think we're now happy.",00:41:53.290,00:41:56.490
"We've talked about
real hash functions",00:41:56.490,00:41:58.150
"for chaining and
perfect hashing.",00:41:58.150,00:42:02.200
"Next thing we're going to
talk about is linear probing.",00:42:02.200,00:42:05.202
"I mean, in some sense we have
good theoretical answers now.",00:42:05.202,00:42:07.660
"We can do constant
expected amortized even",00:42:07.660,00:42:10.510
"with constant
deterministic queries.",00:42:10.510,00:42:15.070
"But we're greedy, or people
like to implement all sorts",00:42:15.070,00:42:21.260
of different hashing schemes.,00:42:21.260,00:42:23.000
"Perfect hashing is
pretty rare in practice.",00:42:23.000,00:42:25.070
Why?,00:42:25.070,00:42:25.869
"I guess, because you have to
hash twice instead of once",00:42:25.869,00:42:28.160
and that's just more expensive.,00:42:28.160,00:42:29.470
"So what about the
simpler hashing schemes?",00:42:29.470,00:42:31.220
"Simple tabulation hashing
is nice and simple,",00:42:31.220,00:42:34.130
but what about linear probing?,00:42:34.130,00:42:39.380
That's really simple.,00:42:42.590,00:42:43.850
"Linear probing is
either the first",00:42:59.130,00:43:00.870
"or the second hashing
scheme you learned.",00:43:00.870,00:43:04.519
You store things in a table.,00:43:04.519,00:43:05.685
"The hash function
tells you where to go.",00:43:09.554,00:43:11.220
"If that's full, you just
go to the next spot.",00:43:11.220,00:43:13.094
"If that's full, you
go to the next spot",00:43:13.094,00:43:14.910
"till you find an empty slot
and then you put x there.",00:43:14.910,00:43:18.000
"So if there's some y
and z here, that that's",00:43:18.000,00:43:21.600
where you end up putting it.,00:43:21.600,00:43:24.840
"Everyone knows
linear probing is bad",00:43:24.840,00:43:27.450
because the rich get richer.,00:43:27.450,00:43:29.070
"It's like the
parking lot problem.",00:43:29.070,00:43:30.700
"If you get big runs of elements
they're more likely to get hit,",00:43:30.700,00:43:35.940
"so they're going to grow
even faster and get worse.",00:43:35.940,00:43:39.840
"So you should never
use linear probing.",00:43:39.840,00:43:43.070
Has everyone learned that?,00:43:43.070,00:43:46.290
"It's all false, however.",00:43:46.290,00:43:49.500
"Linear probing is
actually really good.",00:43:49.500,00:43:51.900
"And first indication is it's
really good in practice.",00:43:51.900,00:43:54.780
"There's this small
experiment by Mihai Petrescu,",00:43:54.780,00:43:59.200
"who was an undergrad
and PhD student here.",00:43:59.200,00:44:02.370
He's working on AT&amp;T now.,00:44:02.370,00:44:04.110
"And he was doing
some experiments",00:44:04.110,00:44:05.670
"and he found that in
practice on a network router",00:44:05.670,00:44:08.540
"linear probing costs 10% more
time than a memory access.",00:44:08.540,00:44:14.250
"So, basically free.",00:44:14.250,00:44:18.180
Why?,00:44:18.180,00:44:20.820
"You just set m to be 2 times
n, or 1 plus epsilon times",00:44:20.820,00:44:25.370
"n, whatever.",00:44:25.370,00:44:27.570
It actually works really well.,00:44:27.570,00:44:28.880
"And I'd like to convince you
that it works really well.",00:44:28.880,00:44:31.920
"Now, first let me
tell you some things.",00:44:31.920,00:44:37.680
"The idea that it works
really well is old.",00:44:37.680,00:44:42.120
"For a totally
random hash function",00:44:42.120,00:44:56.420
"you require constant
time per operation.",00:44:56.420,00:44:58.790
"And Knuth actually
showed this first in 1962",00:44:58.790,00:45:01.490
in a technical report.,00:45:01.490,00:45:03.020
"The answer ends up being
1 over epsilon squared.",00:45:03.020,00:45:05.496
"Now you might say,
1 over epsilon",00:45:05.496,00:45:06.870
"squared, oh that's really bad.",00:45:06.870,00:45:08.630
"And there are other
schemes that achieve 1",00:45:08.630,00:45:10.380
"over epsilon, which is better.",00:45:10.380,00:45:11.780
"But what's a little
bit of space, right?",00:45:11.780,00:45:14.180
"I mean, just set epsilon
to 1, you're done.",00:45:14.180,00:45:16.550
"So I think linear probing
was bad when we really",00:45:19.320,00:45:22.460
were tight on space.,00:45:22.460,00:45:23.390
"But when you can afford a factor
of 2, linear probing is great.",00:45:23.390,00:45:26.212
That's the bottom line.,00:45:26.212,00:45:27.170
"Now, this is totally
random, not so useful.",00:45:29.760,00:45:32.450
"What about all these
other hash functions?",00:45:32.450,00:45:34.580
"Like universal,
turns out universal",00:45:34.580,00:45:36.620
"with the universal
hash function,",00:45:36.620,00:45:38.330
"linear probing is
really, really bad.",00:45:38.330,00:45:40.580
"And that's why it
gets a bad rap.",00:45:40.580,00:45:42.350
But some good news.,00:45:42.350,00:45:44.720
"OK, first result was
log n wise independence.",00:45:44.720,00:45:49.580
"This is extremely
strong but it also",00:45:49.580,00:45:52.220
"implies constant
expected per operation.",00:45:52.220,00:45:56.780
Not very exciting.,00:45:56.780,00:45:59.390
"The big breakthrough was in
2007 that five-wise independence",00:45:59.390,00:46:04.790
is enough.,00:46:04.790,00:46:06.260
"And this is why
this paper, Thorpe",00:46:06.260,00:46:14.310
"was focusing in particular
on the case of k equals 4.",00:46:14.310,00:46:17.179
"Actually, they were
doing k equals 4,",00:46:17.179,00:46:18.720
"but they solved 5
at the same time.",00:46:18.720,00:46:22.340
"And so this was a very
highly optimized, practical,",00:46:22.340,00:46:26.080
all that good stuff.,00:46:26.080,00:46:27.180
"Get five-wise independence,
admittedly with some space.",00:46:27.180,00:46:30.750
But it's pretty cool.,00:46:30.750,00:46:33.610
"So this is enough to
get constant expected.",00:46:33.610,00:46:40.490
"I shouldn't write order
1, because I'm not writing",00:46:40.490,00:46:43.900
the dependence on epsilon here.,00:46:43.900,00:46:46.190
I don't know exactly what it is.,00:46:46.190,00:46:47.950
"But it's some constant
depending on epsilon.",00:46:47.950,00:46:51.870
"And then this turns
out to be tight.",00:46:51.870,00:46:56.280
"There are four-wise
independent hash functions,",00:46:56.280,00:47:01.200
"including I think the
polynomial ones that we did.",00:47:01.200,00:47:06.120
These guys that are really bad.,00:47:06.120,00:47:09.090
You can get really bad.,00:47:09.090,00:47:11.380
"They're as bad as
binary search trees.",00:47:11.380,00:47:13.275
You can get constant expected.,00:47:13.275,00:47:14.670
"So you really need
five-wise independence.",00:47:17.220,00:47:19.320
"It's kind of weird,
but it's true.",00:47:19.320,00:47:22.540
"And the other fun fact is that
simple tabulation hashing also",00:47:22.540,00:47:29.460
achieves constant.,00:47:29.460,00:47:30.940
"And here it's known that it's
also 1 over epsilon squared.",00:47:30.940,00:47:34.200
"So it's just as good as totally
random simple tabulation",00:47:34.200,00:47:37.180
hashing.,00:47:37.180,00:47:37.680
"Which is nice because
again, this is simple.",00:47:37.680,00:47:39.930
"Takes a bit of space but both
of these have that property.",00:47:39.930,00:47:46.350
"And so these are
good ways to use",00:47:46.350,00:47:48.600
linear probing in particular.,00:47:48.600,00:47:50.026
"So you really need
a good hash function",00:47:50.026,00:47:51.650
for linear probing to work out.,00:47:51.650,00:47:53.100
"If you use the universal
hash function like a times",00:47:53.100,00:47:56.100
x mod p mod m it will fail.,00:47:56.100,00:48:00.270
"But if you use a good hash
function, which we're now",00:48:00.270,00:48:03.014
getting to the point--,00:48:03.014,00:48:03.930
"I mean, this is super
simple to implement.",00:48:03.930,00:48:07.101
It should work fine.,00:48:07.101,00:48:08.130
"I think would be a
neat project to take",00:48:08.130,00:48:09.755
"a Python or something that had
hash tables deep inside it,",00:48:09.755,00:48:12.930
replace--,00:48:12.930,00:48:13.580
"I think they use quadratic
probing and universal hash",00:48:13.580,00:48:17.250
functions.,00:48:17.250,00:48:18.750
"If you instead use linear
probing and simple tabulation",00:48:18.750,00:48:21.810
"hashing, might do the same,
might do better, I don't know.",00:48:21.810,00:48:25.812
It's interesting.,00:48:25.812,00:48:26.520
"It would be a
project to try out.",00:48:26.520,00:48:28.240
Cool.,00:48:31.690,00:48:33.930
"Well, I just quoted results.",00:48:33.930,00:48:35.110
"What I'd like to do is prove
something like this to you.",00:48:35.110,00:48:39.790
"Totally random hash functions
imply some constant expected.",00:48:39.790,00:48:43.000
"I won't try to work out
the dependence on epsilon",00:48:43.000,00:48:45.800
"because it's actually a pretty
clean proof, it looks nice.",00:48:45.800,00:48:48.790
Very data structures-y.,00:48:51.750,00:48:53.030
"I'm not going to
cover Knut's proof.",00:48:59.810,00:49:01.440
"I'm essentially
covering this proof.",00:49:01.440,00:49:07.650
"In this paper
five-wise independence",00:49:07.650,00:49:09.360
implies constant expected.,00:49:09.360,00:49:10.830
"They re-prove the
totally random case",00:49:10.830,00:49:13.320
"and strengthen it, analyze
the independence they need.",00:49:13.320,00:49:15.880
"Let's just do totally
random unbiased constant",00:49:18.690,00:49:26.400
expected for linear probing.,00:49:26.400,00:49:29.940
"We obviously know how to do
constant expected already",00:49:29.940,00:49:32.520
with other fancy techniques.,00:49:32.520,00:49:33.840
"But linear probing
seems really bad.",00:49:33.840,00:49:37.230
"Yet I claim, not so much.",00:49:37.230,00:49:41.580
"And we're going to assume
m is at least 3 times n.",00:49:41.580,00:49:47.822
"That will just make
the analysis cleaner.",00:49:47.822,00:49:49.530
"But it does hold
for 1 plus epsilon.",00:49:49.530,00:49:52.440
"OK, so here's the idea.",00:49:52.440,00:49:53.760
"We're going to take our array,
our hash table, it's an array.",00:49:53.760,00:49:58.890
"And build a binary tree
on it because that's",00:50:02.700,00:50:04.740
what we like to do.,00:50:04.740,00:50:06.390
"We do this every
lecture pretty much.",00:50:06.390,00:50:09.900
"This is kind of like ordered
file maintenance, I guess.",00:50:09.900,00:50:12.300
This is just a conceptual tree.,00:50:12.300,00:50:14.400
"I mean, you're not even
defining an algorithm",00:50:14.400,00:50:16.380
"based on this because the
algorithm is linear probing.",00:50:16.380,00:50:18.630
You go into somewhere.,00:50:18.630,00:50:19.850
"You hop, hop, hop, hop until
you find a blank space.",00:50:19.850,00:50:22.140
You put your item there.,00:50:22.140,00:50:23.710
"OK, but each of these
nodes defines an interval",00:50:23.710,00:50:25.770
"in the array, as we know.",00:50:25.770,00:50:28.320
"So I'm going to call
a node dangerous,",00:50:28.320,00:50:38.880
"essentially if its
density is at least 2/3.",00:50:38.880,00:50:43.350
"But not in the literal sense
because there's a little bit",00:50:43.350,00:50:46.920
of a subtlety here.,00:50:46.920,00:50:48.240
"There's the location
where a key wants to live,",00:50:48.240,00:50:50.970
which is h of that key.,00:50:50.970,00:50:52.740
"And there's the location
that it ended up living.",00:50:52.740,00:50:56.790
"I care more about the
first one because that's",00:50:56.790,00:50:59.400
what I understand.,00:50:59.400,00:51:00.690
"h of x, that's going to be nice.",00:51:00.690,00:51:03.820
It's totally random.,00:51:03.820,00:51:04.770
"So h of x is random
independent of everything else.",00:51:04.770,00:51:07.860
Great.,00:51:07.860,00:51:09.210
"Where x ends up being,
that depends on other keys",00:51:09.210,00:51:12.190
"and it depends on
this linear thing",00:51:12.190,00:51:13.830
which I'm trying to understand.,00:51:13.830,00:51:15.510
"So I just want to talk
about the number of keys",00:51:15.510,00:51:22.140
"that hash via h to the interval
if that is at least 2/3 times",00:51:22.140,00:51:34.935
the length of the interval.,00:51:34.935,00:51:36.060
"This is the number of slots
that are actually there.",00:51:38.544,00:51:40.710
"We expect the number of keys
that hash via h to the interval",00:51:44.520,00:51:47.100
to be 1/2.,00:51:47.100,00:51:48.090
"So the expectation would be
1/3 the length the interval.",00:51:48.090,00:51:51.240
"If it happens to be
2/3 it could happen",00:51:51.240,00:51:53.850
"because of high
probability, whatever.",00:51:53.850,00:51:55.770
That's a dangerous node.,00:51:55.770,00:51:56.770
That's the definition.,00:51:56.770,00:51:59.100
"Those ones we worry
will be very expensive.",00:51:59.100,00:52:01.752
"And we worry that we're
going to get super clustering",00:52:01.752,00:52:03.960
"and then get these
giant runs, and so on.",00:52:03.960,00:52:06.560
"So, one thing I
want to compute was",00:52:30.100,00:52:33.050
"what's the probability
of this happening.",00:52:33.050,00:52:37.010
"Probability of a
node being dangerous.",00:52:37.010,00:52:40.550
"Well, we can again use
Chernoff bounds here",00:52:40.550,00:52:42.800
"because we're in a
totally random situation.",00:52:42.800,00:52:45.170
"So this is the probability
that the number",00:52:45.170,00:52:47.420
"of things that went
there was bigger",00:52:47.420,00:52:49.220
than twice the expectation.,00:52:49.220,00:52:51.520
"The expectation is 1/2,
2/3 is twice of 1/3.",00:52:51.520,00:52:55.370
"So this is the probability that
you're at least twice the mean,",00:52:55.370,00:52:59.580
which by Chernoff is small.,00:52:59.580,00:53:03.830
"It comes out to e to
the mu over 2 to 2 mu.",00:53:03.830,00:53:10.825
So this is e over 4 to the mu.,00:53:16.460,00:53:19.550
You can check e.,00:53:19.550,00:53:20.820
It's 2.71828.,00:53:20.820,00:53:24.230
"So this is less than 1,
kind of roughly a half-ish.",00:53:24.230,00:53:31.580
So this is good.,00:53:31.580,00:53:33.350
"This is something like
1 over 2 to the mu.",00:53:33.350,00:53:36.635
What's mu?,00:53:36.635,00:53:37.340
"mu is 1/3 2 to the h
for a height h node.",00:53:45.260,00:53:53.340
It depends on how high you are.,00:53:53.340,00:53:55.130
"If you're at a leaf h is 0, so
you expect 1/3 of an element",00:53:55.130,00:53:58.980
there.,00:53:58.980,00:54:00.300
"As you go up you
expect more elements",00:54:00.300,00:54:01.860
"to hash there, of course.",00:54:01.860,00:54:04.380
"OK, so this gives
us some measure",00:54:04.380,00:54:06.450
"in terms of this h
of what's going on.",00:54:06.450,00:54:09.180
"But it's actually
doubly exponential in h.",00:54:09.180,00:54:11.550
"So this is a very
small probability.",00:54:11.550,00:54:13.340
You go up a few levels.,00:54:13.340,00:54:15.030
"Like, after log
log n levels it's",00:54:15.030,00:54:16.670
"a polynomially small
probability of happening.",00:54:16.670,00:54:20.040
"Because then 2 to the
log log n is log n.",00:54:20.040,00:54:22.700
"And then e over 4 to
the log n is about n.",00:54:22.700,00:54:26.670
OK.,00:54:26.670,00:54:27.170
"But at small levels this
may happen, near the leaves.",00:54:29.930,00:54:35.100
"All right, so now I want to
look at a run in the table.",00:54:35.100,00:54:41.260
"These are the things I
have trouble thinking about",00:54:41.260,00:54:45.270
"because runs tend to get
bigger, and we worry about them.",00:54:45.270,00:54:49.500
"This is now as items are
actually stored at the table,",00:54:49.500,00:54:52.620
"when do I have a bunch of
consecutive items in there",00:54:52.620,00:54:55.950
"that happen to end up
in consecutive slots?",00:54:55.950,00:54:59.745
"So I'm worried about
how long that run is.",00:55:02.280,00:55:05.640
"So let's look at its
logarithm and round",00:55:05.640,00:55:12.300
to the nearest power of 2.,00:55:12.300,00:55:13.530
"So let's say it has
length about 2 to the l.",00:55:13.530,00:55:15.860
"Sorry, plus 1.",00:55:15.860,00:55:17.740
"All right, between 2 to the
l and 2 to the l plus 1.",00:55:20.270,00:55:23.890
"OK, look at that.",00:55:23.890,00:55:28.830
"And it's spanned by some
number of nodes of height h",00:55:28.830,00:55:44.160
equals l minus 3.,00:55:44.160,00:55:48.100
"OK, so there's some interval
that happens to be a run,",00:55:48.100,00:55:51.240
"meaning all of these
slots are occupied.",00:55:51.240,00:55:54.810
"And that's 2 to the
2, I guess, since I",00:55:54.810,00:55:58.830
got to level negative 1.,00:55:58.830,00:56:00.550
"A little hard to do
in a small picture.",00:56:00.550,00:56:02.910
"But we're worried about
when this is really",00:56:02.910,00:56:04.800
big more than some constant.,00:56:04.800,00:56:08.020
"OK, so let's suppose I
was looking at this level.",00:56:08.020,00:56:11.460
"Then this interval is
spanned, in particular,",00:56:11.460,00:56:14.520
by these two nodes.,00:56:14.520,00:56:15.870
"Now it's a little
sloppy because this node",00:56:15.870,00:56:18.210
"contains some non-interval,
non-run stuff,",00:56:18.210,00:56:20.700
and so does this one.,00:56:20.700,00:56:22.860
"At the next level down it would
be this way one, this one,",00:56:22.860,00:56:26.750
"and this one, which is
a little more precise.",00:56:26.750,00:56:28.730
"But it's never going
to be quite perfect.",00:56:28.730,00:56:31.680
"But just take all
the nodes you need",00:56:31.680,00:56:34.350
to completely cover the run.,00:56:34.350,00:56:37.050
"Then this will be at least eight
nodes because the length is",00:56:37.050,00:56:43.630
1 to the l.,00:56:43.630,00:56:44.190
"We went three levels
down, 2 to the 3 is 8.",00:56:44.190,00:56:47.640
"So if it's perfectly aligned
it will be exactly 8 nodes.",00:56:47.640,00:56:51.840
"In the worst case, it
could be as much as 17.",00:56:51.840,00:56:56.700
"Because potentially,
we're 2 to the l plus 1,",00:56:56.700,00:57:00.750
"which means we have 16 nodes
if we're perfectly aligned.",00:57:00.750,00:57:03.170
"But then if you shift
if over it might be",00:57:03.170,00:57:05.100
one more because of the slot.,00:57:05.100,00:57:07.750
"OK, but some constant
number of nodes.",00:57:07.750,00:57:10.630
"It's important that
it's at least eight.",00:57:10.630,00:57:13.780
That's what we need.,00:57:13.780,00:57:15.592
"Actually, we just need
that's it at least five,",00:57:15.592,00:57:17.550
"but eight is the nearest
power of two rounding up.",00:57:17.550,00:57:22.860
Cool.,00:57:22.860,00:57:23.700
"So, there they are.",00:57:23.700,00:57:25.500
"Now, I want to look at
the first four nodes",00:57:25.500,00:57:30.750
of these eight to 12 nodes.,00:57:30.750,00:57:32.850
So first meaning leftmost.,00:57:32.850,00:57:35.220
Earliest in the run.,00:57:35.220,00:57:38.107
"So if you think about
them, so there's",00:57:38.107,00:57:39.690
"some four nodes each
of them spans some--",00:57:39.690,00:57:43.325
I should draw these properly.,00:57:43.325,00:57:45.645
"What we know is that these guys
are entirely filled with items.",00:57:51.090,00:57:56.630
The run occupies here.,00:57:56.630,00:57:58.410
"It's got to be at least one item
into here, but the rest of this",00:57:58.410,00:58:01.740
could be empty.,00:58:01.740,00:58:02.682
"And the interval keeps
going to the right",00:58:02.682,00:58:04.390
"so we know that all of
these are completely",00:58:04.390,00:58:06.600
filled with items somehow.,00:58:06.600,00:58:09.360
"So let's start with how
many there are, I guess.",00:58:09.360,00:58:13.500
"They span more than three times
2 to the h slots of the run.",00:58:13.500,00:58:26.940
So somehow 3 times 2 to the h--,00:58:26.940,00:58:29.815
"because there's three of them
that are completely filled,",00:58:29.815,00:58:32.190
otherwise it would be four.,00:58:32.190,00:58:35.490
"Somehow three times two
to the h items ended here.",00:58:35.490,00:58:38.490
"Now, how did they end up here?",00:58:38.490,00:58:40.030
"Notice there's a blank
space right here.",00:58:40.030,00:58:44.910
"By definition this was
the beginning of a run.",00:58:44.910,00:58:46.860
"Meaning the previous
slot is empty.",00:58:46.860,00:58:50.380
"Which means all of the
keys that wanted to live",00:58:50.380,00:58:53.580
from here to the left got to.,00:58:53.580,00:58:57.480
"So if we're just thinking
about the keys that ended up",00:58:57.480,00:58:59.730
"in this interval, they had to
initially hash to somewhere",00:58:59.730,00:59:03.840
in here.,00:59:03.840,00:59:05.160
"h put them somewhere
in this interval",00:59:05.160,00:59:08.010
"and then they may have
moved to the right,",00:59:08.010,00:59:10.290
"but they never move to
the left in linear hashing",00:59:10.290,00:59:12.820
if you're not completely full.,00:59:12.820,00:59:14.490
"So because there was a blank
spot here none of these keys",00:59:14.490,00:59:17.370
"could have fallen over
to here, no deletions.",00:59:17.370,00:59:23.310
So you're doing insertions.,00:59:23.310,00:59:25.925
"They may have just
spread it out,",00:59:25.925,00:59:27.300
"and they made sconces have
gone farther to the right,",00:59:27.300,00:59:28.950
"or they may filled in
gaps, whatever, but h",00:59:28.950,00:59:31.170
put them in this interval.,00:59:31.170,00:59:33.780
"Now, I claim that in fact,
at least one of these nodes",00:59:33.780,00:59:40.350
must be dangerous.,00:59:40.350,00:59:42.470
"Now dangerous is tricky,
because dangerous is talking",00:59:42.470,00:59:45.360
about where h puts nodes.,00:59:45.360,00:59:46.980
"But we just said, got to be at
least three times two to the h",00:59:46.980,00:59:51.180
"keys, where h put them
within these four nodes,",00:59:51.180,00:59:55.620
"otherwise they wouldn't
have filled in here.",00:59:55.620,00:59:58.170
"Now, if none of those
nodes were dangerous,",00:59:58.170,01:00:11.520
then we'll get a contradiction.,01:00:11.520,01:00:15.120
"Because none of
them were dangerous",01:00:15.120,01:00:17.610
"this means at most
4 times 2/3 times 2",01:00:17.610,01:00:25.460
"to the h keys hash
via h to them.",01:00:25.460,01:00:32.430
Why?,01:00:37.991,01:00:38.490
"Because there's
four of the nodes.",01:00:38.490,01:00:40.350
"Each of them, if it's not
dangerous, has at most 2/3",01:00:40.350,01:00:45.210
of its size keys hashing there.,01:00:45.210,01:00:50.130
"4 times 2/3 is 8/3, which is
less than 9/3, which is 3.",01:00:50.130,01:00:58.670
"OK, so this would
be a contradiction",01:00:58.670,01:01:01.620
"because we just argued that at
least 3 times 2 to the h nodes",01:01:01.620,01:01:05.070
"have to hash via h to
somewhere in these nodes.",01:01:05.070,01:01:10.350
"They might hash here and
then fallen over to here.",01:01:10.350,01:01:12.434
"So there is this kind of,
things can move to the right,",01:01:12.434,01:01:14.724
we've got to worry about it.,01:01:14.724,01:01:15.900
"But just look three
levels up and it's OK.",01:01:15.900,01:01:19.650
"So one of these nodes, not
necessarily all of them,",01:01:22.146,01:01:24.270
are dangerous.,01:01:24.270,01:01:25.314
"And we can use that to
finish our analysis.",01:01:33.380,01:01:36.570
"This is good news
because it says",01:01:54.690,01:01:58.230
"that if we have a run,
which is something that's",01:01:58.230,01:02:00.230
"hard to think about because
nodes are moving around",01:02:00.230,01:02:02.500
"to form a run, so keys are
moving around to form a run,",01:02:02.500,01:02:05.540
"we can charge it to
a dangerous node.",01:02:05.540,01:02:08.101
"Which is easy to think
about because that's just",01:02:08.101,01:02:10.100
"talking about where keys hash
via h, and h is totally random.",01:02:10.100,01:02:16.190
"There's a loss of a
factor of 17, potentially.",01:02:16.190,01:02:19.760
"But it's a constant
factor, no big deal.",01:02:19.760,01:02:23.350
"If we look at the probability
that the length of a run,",01:02:23.350,01:02:28.520
"say containing some key x,
has length between 2 to the l",01:02:28.520,01:02:37.130
"and to the l plus
1, this is going",01:02:37.130,01:02:41.420
"to be at most 17 times the
probability of a node at height",01:02:41.420,01:02:51.335
l minus 3 is dangerous.,01:02:51.335,01:02:54.650
"Because we know one of them
is, and so just to be sloppy",01:02:58.970,01:03:02.790
"it's at most the sum of the
probabilities that any of them",01:03:02.790,01:03:05.881
is.,01:03:05.881,01:03:06.380
"Then potentially there's
a run of that length.",01:03:06.380,01:03:08.930
"And so union bound it's at
most 17 times probability",01:03:08.930,01:03:12.020
of this happening.,01:03:12.020,01:03:12.830
"Now all nodes look the
same because we have",01:03:12.830,01:03:14.750
a totally random hash function.,01:03:14.750,01:03:16.940
"So we just say any node
at height l minus 3.",01:03:16.940,01:03:20.260
"We already computed
that probability.",01:03:20.260,01:03:22.820
That was this.,01:03:22.820,01:03:23.630
"Probability of being dangerous
was e over 4 to the 1/3 2",01:03:23.630,01:03:27.980
to the h.,01:03:27.980,01:03:29.810
"So this is going to be at most
17 times e over 4 to the 2",01:03:29.810,01:03:39.250
to the l minus 3 power.,01:03:39.250,01:03:42.150
"Again, doubly exponential in l.",01:03:42.150,01:03:48.410
"So if we want to compute
the expected run length",01:03:48.410,01:03:57.890
"we can just expand
out the definition.",01:03:57.890,01:04:01.280
"Well, let's round
it to powers of 2.",01:04:01.280,01:04:05.765
"It could be the run
length is about 2",01:04:05.765,01:04:07.850
"to the l within a constant
factor of 2 to the l.",01:04:07.850,01:04:11.360
"So it's going to be that
times this probability.",01:04:11.360,01:04:16.474
"But this thing is basically
1 over 2 to the 2 to the l.",01:04:21.420,01:04:25.640
"And so the whole
thing is constant.",01:04:25.640,01:04:27.425
This is l.,01:04:31.640,01:04:32.579
"I mean, l could go to infinity.",01:04:32.579,01:04:33.870
I don't really care.,01:04:33.870,01:04:34.703
"I mean, this gets dwarfed
by the double exponential.",01:04:37.320,01:04:40.210
This is super geometric.,01:04:40.210,01:04:42.090
"So a very low probability
of getting long runs.",01:04:42.090,01:04:46.470
"As we said, after
a log log n size--",01:04:46.470,01:04:51.210
"yeah, it's very unlikely
to run longer than log n.",01:04:51.210,01:04:54.690
We proved that in particular.,01:04:54.690,01:04:57.619
"But in particular, you compute
the expected run length,",01:04:57.619,01:04:59.910
it's constant.,01:04:59.910,01:05:02.800
"OK, now this of course
assumed totally random.",01:05:02.800,01:05:07.010
It's harder to prove--,01:05:07.010,01:05:09.300
where were we.,01:05:09.300,01:05:11.790
Somewhere.,01:05:11.790,01:05:12.660
Linear probing.,01:05:12.660,01:05:14.420
"It's harder to prove five-wise
independence is enough,",01:05:14.420,01:05:16.680
but it's true.,01:05:16.680,01:05:17.810
"And it's much harder to
prove simple tabulation",01:05:17.810,01:05:21.090
"hashing works, but it's true.",01:05:21.090,01:05:22.840
So we can use them.,01:05:22.840,01:05:24.210
"This gives you some intuition
for why it's really not",01:05:24.210,01:05:26.610
that bad.,01:05:26.610,01:05:28.129
"And similar proof
techniques are used",01:05:28.129,01:05:29.670
for the five-wise independence.,01:05:29.670,01:05:33.570
Other fun facts.,01:05:33.570,01:05:34.680
"You can do similar caching
trick that we did before.",01:05:34.680,01:05:40.540
"Again, the worst run is going
to be log, or log over log log.",01:05:40.540,01:05:45.510
I don't have it written here.,01:05:45.510,01:05:48.030
But if you cache the last--,01:05:48.030,01:05:52.680
"it's not quite enough
to do the last log n.",01:05:52.680,01:05:55.320
"But if you cache the last log
to the 1 plus epsilon n queries.",01:05:55.320,01:06:06.852
It's a little bit more.,01:06:06.852,01:06:09.390
"Then you can generalize
this argument.",01:06:09.390,01:06:11.440
"And so at least for totally
random hash functions",01:06:11.440,01:06:16.230
"you get constant amortize
with high probability.",01:06:16.230,01:06:20.230
"This weird thing that
I've never seen before.",01:06:26.120,01:06:29.690
"But it's comforting because
it's expected bounds are not",01:06:29.690,01:06:34.280
"so great, but you get it
with high probability bound",01:06:34.280,01:06:36.590
"as long as you're willing
to average over log to the 1",01:06:36.590,01:06:40.040
plus epsilon different queries.,01:06:40.040,01:06:41.785
"As long as you
can remember them.",01:06:41.785,01:06:43.160
"And the proof is
basically the same.",01:06:46.880,01:06:49.730
"Except now instead of looking
at the length of a run",01:06:49.730,01:06:52.610
"containing x, you're looking
at the length of the run",01:06:52.610,01:06:55.220
"containing one of these log
to the 1 plus epsilon n nodes.",01:06:55.220,01:06:59.810
That's your batch.,01:06:59.810,01:07:01.520
And you do the same thing.,01:07:01.520,01:07:03.890
"But now do it with high
probability analysis.",01:07:03.890,01:07:07.140
"But again, because
the expectation is now",01:07:07.140,01:07:09.860
"bigger than log,
expect there to be",01:07:09.860,01:07:13.250
a lot of fairly long runs here.,01:07:13.250,01:07:15.470
"But that's OK, because
on average is good.",01:07:15.470,01:07:18.440
"You expect to pay log to
the 1 plus epsilon for log",01:07:21.696,01:07:23.820
to the 1 plus epsilon queries.,01:07:23.820,01:07:25.790
"And so then you divide and
amortize and you're done.",01:07:25.790,01:07:30.860
"It's a little bit more
details in the notes about",01:07:30.860,01:07:33.490
that if you want to read.,01:07:33.490,01:07:36.020
"I want to do one more
topic, unless there are",01:07:36.020,01:07:40.390
questions about linear probing.,01:07:40.390,01:07:43.700
"So, yeah?",01:07:43.700,01:07:44.957
"AUDIENCE: So, could you motivate
why the [INAUDIBLE] value of mu",01:07:44.957,01:07:49.727
"is the mean for
whatever quantity?",01:07:49.727,01:07:52.600
"ERIK DEMAINE: So mu is defined
to be the mean of whatever",01:07:52.600,01:07:55.250
quantity we're analyzing.,01:07:55.250,01:07:56.780
"And the Chernoff bounds
says, probability",01:07:56.780,01:08:00.410
"that you're at least
something times the mean is",01:08:00.410,01:08:03.320
the formula we wrote last time.,01:08:03.320,01:08:05.250
"Now here, we're measuring--",01:08:05.250,01:08:09.270
"I didn't write what
the left-hand side was.",01:08:09.270,01:08:11.652
"But here we're measuring
what's the probability",01:08:11.652,01:08:13.610
"that the number of keys that
hash via h to the interval",01:08:13.610,01:08:16.370
"is at least 2/3 the
length of the interval.",01:08:16.370,01:08:19.050
"Now, let's say m equals 3m then
the expected number of keys",01:08:19.050,01:08:25.760
"that hash via h to interval
is 1/3 times the length",01:08:25.760,01:08:28.590
of the interval.,01:08:28.590,01:08:29.899
"Because we have a
totally random thing,",01:08:29.899,01:08:32.149
"and we have a density
of 1/3 overall.",01:08:32.149,01:08:35.420
"So you expect there
to be 1/3 and so",01:08:35.420,01:08:38.270
"dangerous is when you're
more than twice that.",01:08:38.270,01:08:42.380
And so it's twice mu.,01:08:42.380,01:08:44.000
"Mu is, in this case, 1/3
the length the interval.",01:08:44.000,01:08:46.470
And that's why I wrote that.,01:08:46.470,01:08:48.482
"AUDIENCE: So this comes
from the m squared.",01:08:48.482,01:08:50.294
[INAUDIBLE],01:08:50.294,01:08:50.750
"ERIK DEMAINE: Yeah,
it comes from m",01:08:50.750,01:08:52.208
equals 3m and totally random.,01:08:52.208,01:08:54.482
AUDIENCE: [INAUDIBLE],01:08:54.482,01:08:58.180
"ERIK DEMAINE: Yeah, OK
let's make this equal.",01:08:58.180,01:09:00.720
Make this more formal.,01:09:00.720,01:09:03.600
"It's an assumption, anyway,
to simplify the proof.",01:09:03.600,01:09:07.359
Good.,01:09:07.359,01:09:08.481
Change that in the notes too.,01:09:08.481,01:09:09.689
Cool.,01:09:16.090,01:09:16.590
"So then the
expectation is exactly",01:09:16.590,01:09:18.300
1/3 instead of at most 1/3.,01:09:18.300,01:09:19.979
So it's all a little cleaner.,01:09:19.979,01:09:21.330
"Of course, this all works
when m is at least 1",01:09:21.330,01:09:24.420
"plus epsilon times
n, but then you",01:09:24.420,01:09:26.580
get a dependence on epsilon.,01:09:26.580,01:09:28.590
Other questions?,01:09:28.590,01:09:32.189
"So bottom line is linear
probing is actually good.",01:09:32.189,01:09:36.806
"Quadratic probing, double
hashing, all those fancy things",01:09:36.806,01:09:39.180
are also good.,01:09:39.180,01:09:40.859
"But they're really
tuned for the case",01:09:40.859,01:09:42.990
when your table is almost full.,01:09:42.990,01:09:44.399
"They get a better
dependence on epsilon,",01:09:44.399,01:09:46.210
"which is how close
to the bound you are.",01:09:46.210,01:09:49.800
"And so if you're constant
factor away from space bound,",01:09:49.800,01:09:53.040
linear probing is just fine.,01:09:53.040,01:09:54.660
"As long as you have enough
independence, admittedly.",01:09:54.660,01:09:57.600
"Double hashing, I
believe, gets around that.",01:09:57.600,01:10:01.080
"It does not need so
much independence.",01:10:01.080,01:10:07.030
OK.,01:10:07.030,01:10:08.640
"Instead of going
to double hashing,",01:10:08.640,01:10:10.481
"I'm going to go to something
kind of related double hashing,",01:10:10.481,01:10:12.980
which is cuckoo hashing.,01:10:12.980,01:10:13.980
Cuckoo hashing is a weird idea.,01:10:25.340,01:10:29.070
"It's kind of a more extreme
form of perfect hashing.",01:10:29.070,01:10:32.870
"It says, look, perfect
hashing did two hash queries.",01:10:32.870,01:10:41.420
"So I did one hash evaluation
and another hash evaluation",01:10:41.420,01:10:45.620
"followed it, which is OK.",01:10:45.620,01:10:48.680
"But again, I want my queries to
only do two things, two probes.",01:10:51.770,01:10:57.560
"So it's going to take
that concept of just two",01:10:57.560,01:11:07.090
"and actually use
two hash tables.",01:11:07.090,01:11:09.460
"So you've got B over here,
I've got A over here.",01:11:09.460,01:11:14.080
"And if you have a
key x, you hash it",01:11:19.270,01:11:23.740
"to a particular spot in
A via g, and you hash it",01:11:23.740,01:11:27.300
"to a particular spot in B via
H. So you have two hash tables,",01:11:27.300,01:11:31.630
two hash functions.,01:11:31.630,01:11:32.680
"To do a query you
look at A of g of x,",01:11:42.990,01:11:50.880
and you look at B of h of x.,01:11:50.880,01:11:55.890
"Oh sorry, I forgot to mention.",01:11:55.890,01:11:57.860
"The other great thing
about linear probing",01:11:57.860,01:11:59.610
"is that it's cache
performance is so great.",01:11:59.610,01:12:01.740
"This is why it runs
so fast in practice.",01:12:01.740,01:12:04.440
"Why it's only 10% slower
than a memory access.",01:12:04.440,01:12:06.510
"Because once you
access a single slot,",01:12:06.510,01:12:09.810
"whole you get B slots in
a cache with block size B.",01:12:09.810,01:12:13.230
"So most of the time, because
your runs are very short,",01:12:13.230,01:12:17.990
"you will find your
answer immediately.",01:12:17.990,01:12:20.310
"So that's why we kind
of prefer linear probing",01:12:20.310,01:12:22.300
"in practice over all
the other schemes I'm",01:12:22.300,01:12:24.050
going to talk about.,01:12:24.050,01:12:26.010
"Well, cuckoo
hashing is all right",01:12:26.010,01:12:28.230
"because it's only going to look
at two places and that's it.",01:12:28.230,01:12:31.740
Doesn't go anywhere else.,01:12:31.740,01:12:33.330
"I guess with perfect hashing
the thing is you have",01:12:36.980,01:12:41.172
more than two hash functions.,01:12:41.172,01:12:42.380
"You have the first
hash function which",01:12:42.380,01:12:43.730
sends you to the first table.,01:12:43.730,01:12:44.938
"Then you look up a
second hash function.",01:12:44.938,01:12:46.940
"Using that hash function
you rehash your value x.",01:12:46.940,01:12:51.012
"Downside of that is you
can't compare those two",01:12:51.012,01:12:52.970
hash functions in parallel.,01:12:52.970,01:12:54.810
"So if you're like
two cores, you could",01:12:54.810,01:12:57.020
"compute these two in
parallel, look them",01:12:57.020,01:12:59.030
both up simultaneously.,01:12:59.030,01:13:00.500
"So in that sense you
save a factor of 2",01:13:00.500,01:13:02.720
with some parallelism.,01:13:02.720,01:13:03.931
"Now, the weird thing is
the way we do an insertion.",01:13:07.160,01:13:12.380
"You try to put it in the
A slot, or the B slot.",01:13:12.380,01:13:22.950
"If either of them is
empty you're golden.",01:13:22.950,01:13:26.510
"If neither of them
are empty, you've",01:13:26.510,01:13:28.010
got to kick out whoever's there.,01:13:28.010,01:13:31.400
"So let's say if you kicked
out y from it's A slot.",01:13:31.400,01:13:41.030
"So we ended up
putting x in this one,",01:13:44.360,01:13:47.660
"so we end up kicking y
from wherever it belonged.",01:13:47.660,01:13:52.160
Then you move it to B of h of y.,01:13:52.160,01:13:59.750
"There's only one other
place that that item can go,",01:13:59.750,01:14:02.210
so you put it there instead.,01:14:02.210,01:14:05.060
"In general, I think about A key
it has two places it can go.",01:14:05.060,01:14:11.900
"There's some slot in
A, some slot in B.",01:14:11.900,01:14:13.670
"You can think of this as an
edge in a bipartite graph.",01:14:13.670,01:14:17.040
"So make vertices
for the A slots,",01:14:17.040,01:14:19.760
vertices for the B slots.,01:14:19.760,01:14:22.190
Each edge is an item on a key.,01:14:22.190,01:14:25.732
"Key Can only live one
spot in A, one spot in B",01:14:25.732,01:14:28.880
for this query to work.,01:14:28.880,01:14:31.890
"So what's happening
is if both of these",01:14:31.890,01:14:34.820
"are full you take
whoever is currently here",01:14:34.820,01:14:37.880
"and put them over in
their corresponding slot",01:14:37.880,01:14:41.510
"over in B. Now, that
one might be full,",01:14:41.510,01:14:43.360
"which means you've got to
kick that guy to wherever",01:14:43.360,01:14:45.484
"he belongs in A, and so on.",01:14:45.484,01:14:47.930
"If eventually you find an
empty slot, great, you're done.",01:14:47.930,01:14:51.380
"Just chain reaction
of cuckoo steps",01:14:51.380,01:14:55.040
"where the bird's going
from in and out, or from A",01:14:55.040,01:14:57.680
"to B, vice a versa.",01:14:57.680,01:15:01.010
"If it terminates, you're happy.",01:15:01.010,01:15:03.020
"It doesn't terminate,
you're in trouble",01:15:03.020,01:15:05.570
"because you might get a cycle,
or a few failure situations.",01:15:05.570,01:15:09.890
In that case you're screwed.,01:15:09.890,01:15:11.240
"There is no cuckoo
hash table that",01:15:11.240,01:15:13.100
works for your set of keys.,01:15:13.100,01:15:14.690
"In that case, you pick
another hash function,",01:15:14.690,01:15:16.820
rebuild from scratch.,01:15:16.820,01:15:19.160
"So it's kind of a
weird hashing scheme",01:15:19.160,01:15:20.840
"because it can
fail catastrophic.",01:15:20.840,01:15:24.470
"Fortunately, it doesn't
happen too often.",01:15:24.470,01:15:26.510
It still rubs me a funny way.,01:15:33.640,01:15:35.810
"I don't know what
to say about it.",01:15:35.810,01:15:37.910
"OK, so you lose a
factor of 2 in space.",01:15:41.450,01:15:44.390
"2 deterministic
probes for a query.",01:15:47.910,01:15:50.360
That's good news.,01:15:50.360,01:15:53.225
"All right, now we get
to, what about updates?",01:15:58.850,01:16:01.760
"So if it's fully random
or log n-wise independent,",01:16:01.760,01:16:15.510
"then you get a constant expected
update, which is what we want.",01:16:15.510,01:16:22.110
Even with the rebuilding cost.,01:16:22.110,01:16:23.520
"So you'll have to rebuild about
every n squared insertions",01:16:23.520,01:16:28.290
you do.,01:16:28.290,01:16:30.150
"The way they say this
is there's a 1 over n",01:16:30.150,01:16:35.670
build failure probability.,01:16:35.670,01:16:37.883
"There's a 1 over n chance
that your key set will",01:16:42.050,01:16:44.530
be completely unsustainable.,01:16:44.530,01:16:47.856
"If you want to put all n keys
into this table there's a 1",01:16:47.856,01:16:50.230
"over n chance that
it will be impossible",01:16:50.230,01:16:52.750
and then you have to start over.,01:16:52.750,01:16:54.430
"So amortize per insertion,
that's about 1 over n squared.",01:16:54.430,01:16:58.360
"Insertions you can do before
the whole thing falls apart",01:16:58.360,01:17:00.960
and you have to rebuild.,01:17:00.960,01:17:03.130
"So it's definitely
going to be this should",01:17:03.130,01:17:04.990
"be amortize expected, I guess.",01:17:04.990,01:17:08.120
"However you want
to think about it.",01:17:08.120,01:17:10.780
"But it's another way to do
constant amortized expected.",01:17:10.780,01:17:14.920
Cool.,01:17:14.920,01:17:15.940
"The other thing that's known
is that six-wise independence",01:17:15.940,01:17:22.000
is not enough.,01:17:22.000,01:17:24.190
"This was actually a
project in this class,",01:17:24.190,01:17:26.650
"I believe the first time
it was offered in 2003.",01:17:26.650,01:17:30.270
"Six-wise independence
is not sufficient to get",01:17:30.270,01:17:33.850
constant expected bound.,01:17:33.850,01:17:35.560
"It will actually fail
with high probability",01:17:38.110,01:17:41.770
"if you only have
six-wise independence.",01:17:41.770,01:17:43.510
"What's not known is, do you
need constant Independence?",01:17:43.510,01:17:46.780
Or log n independence?,01:17:46.780,01:17:47.850
"With log n, very low
failure probability.",01:17:47.850,01:17:50.830
"With six-wise, high
probability you fail.",01:17:50.830,01:17:54.250
"Like, you fail with
probability 1 minus 1 over n.",01:17:54.250,01:17:58.711
Not so good.,01:17:58.711,01:17:59.210
"Some good news is simple
tabulation hashing.",01:18:04.530,01:18:07.610
"Means you will fail to build
with probability not 1 over n,",01:18:13.230,01:18:23.060
but 1 over n to the 1/3 power.,01:18:23.060,01:18:26.000
And this is theta.,01:18:30.340,01:18:31.190
This is tight.,01:18:31.190,01:18:33.470
It's almost as good as this.,01:18:33.470,01:18:34.850
"We really only
need constant here.",01:18:34.850,01:18:36.470
"This is to build
the entire table.",01:18:36.470,01:18:38.940
"So in this case you can
insert like n to the 4",01:18:38.940,01:18:41.400
"those items before your
table self-destructs.",01:18:41.400,01:18:43.910
"So simple tabulation hashing
is, again, pretty good.",01:18:43.910,01:18:47.940
"That's I think
the hardest result",01:18:47.940,01:18:51.310
in this paper from last year.,01:18:51.310,01:18:53.000
"So I do have a
proof of this one.",01:18:59.930,01:19:03.170
Something like that.,01:19:07.107,01:19:07.940
Or part of a proof.,01:19:07.940,01:19:09.710
"So me give you a rough
idea how this works.",01:19:09.710,01:19:13.130
"So if you're a fully
random hash function.",01:19:13.130,01:19:18.620
"The main concern is that what
if this path is really long.",01:19:18.620,01:19:23.630
"I claim that if an insert
follows a path of length k,",01:19:23.630,01:19:36.760
"or the probability
of this happening,",01:19:36.760,01:19:41.840
"is actually at most
1 over 2 to the k.",01:19:41.840,01:19:44.320
It's very small.,01:19:44.320,01:19:45.400
Exponentially small in k.,01:19:45.400,01:19:47.130
"I just want to sketch how
this works because it's",01:19:50.590,01:19:53.170
"a cool argument that's actually
in this simple tabulation",01:19:53.170,01:19:58.630
paper.,01:19:58.630,01:19:59.802
So the idea is the following.,01:19:59.802,01:20:01.010
You have some really long path.,01:20:01.010,01:20:04.760
"What I'm going to
give you is a way",01:20:04.760,01:20:07.870
to encode the hash functions.,01:20:07.870,01:20:13.450
There's hash functions g and h.,01:20:13.450,01:20:16.300
Each of them has n values.,01:20:16.300,01:20:20.380
"Each of those values
is log m bits.",01:20:20.380,01:20:24.730
"So if I just wrote them
down the obvious way,",01:20:24.730,01:20:27.300
"it's 2 n log m bits to write
down those hash functions.",01:20:27.300,01:20:31.840
"Now we're assuming these
are totally random hash",01:20:31.840,01:20:34.300
"functions, which means
you need this many bits.",01:20:34.300,01:20:36.955
"But I claim that if you
follow a path of length k,",01:20:36.955,01:20:41.020
"I can find a new
encoding scheme, a way",01:20:41.020,01:20:43.690
"to write down g and h
that is basically minus k.",01:20:43.690,01:20:48.950
This many bits minus k.,01:20:48.950,01:20:50.920
I get to save k bits.,01:20:50.920,01:20:52.750
"Now, it turns out
that can happen",01:20:52.750,01:20:54.730
"but it happens only with
probability 1 over 2 to the k.",01:20:54.730,01:20:57.480
"This is an information
theoretic argument.",01:20:57.480,01:20:59.920
You might get lucky.,01:20:59.920,01:21:01.480
"And the g's and h's
you're trying to encode",01:21:01.480,01:21:04.690
"can be done with fewer
bits, k fewer bits.",01:21:04.690,01:21:07.851
"But that will only happen with
probability 1 over 2 to the k",01:21:07.851,01:21:10.350
if g and h are totally random.,01:21:10.350,01:21:12.580
So how do you do it?,01:21:12.580,01:21:14.040
"Basically, I want
to encode the things",01:21:14.040,01:21:17.740
on the path slightly cheaper.,01:21:17.740,01:21:20.200
"I'm going to save one
bit per node on the path.",01:21:20.200,01:21:24.350
So what do I need to do?,01:21:24.350,01:21:26.450
"Well, the idea is OK, I
will start by writing down",01:21:26.450,01:21:33.220
this hash value.,01:21:33.220,01:21:34.730
"This takes log m bits to
write down that hash value.",01:21:34.730,01:21:39.549
"Then I'll write down
this hash value.",01:21:39.549,01:21:41.090
"That takes a log
m bit, log m bits.",01:21:41.090,01:21:42.690
"Generally there's going to be
roughly k log n to write down",01:21:42.690,01:21:46.150
all of the node hash values.,01:21:46.150,01:21:48.520
"Then I need to say
that it's actually",01:21:48.520,01:21:50.490
"x, this particular key that
corresponds to this edge.",01:21:50.490,01:21:54.940
So I've got to write that down.,01:21:54.940,01:21:56.680
"That's going to
take a log n bits",01:21:56.680,01:21:59.170
"to say that x is the
guy for the first edge,",01:21:59.170,01:22:01.390
"then y is the key that
corresponds to the second edge",01:22:01.390,01:22:04.215
"of the path, then z, then w.",01:22:04.215,01:22:06.190
"But nicely, things
are ordered here.",01:22:06.190,01:22:08.680
"So it only takes
me to log n, k log",01:22:08.680,01:22:11.800
n to write down all these guys.,01:22:11.800,01:22:14.420
"So I get k times
log m plus log n.",01:22:14.420,01:22:19.540
"Now if m is 2 times n, this
is k times 2 log m minus 1.",01:22:19.540,01:22:37.525
"So I get one bit of savings
per k, per thing in the path.",01:22:37.525,01:22:43.130
"Essentially because
it's easier for me",01:22:43.130,01:22:46.460
"to write down these
labels to say,",01:22:46.460,01:22:48.090
"oh, it's the key x
that's going here.",01:22:48.090,01:22:50.777
"Instead of having to write down
slot names all the time, which",01:22:50.777,01:22:53.360
"cost log m bits, writing
down key names only",01:22:53.360,01:22:56.360
"takes log n bits,
which is a savings",01:22:56.360,01:22:58.310
of 1 bit per thing on the path.,01:22:58.310,01:23:02.000
"And so that was a quick
sketch of how this proof goes.",01:23:02.000,01:23:05.120
"It's kind of neat,
information theoretic argument",01:23:05.120,01:23:07.600
why the paths can't get long.,01:23:07.600,01:23:08.960
"You then have to worry
about cycles and things that",01:23:08.960,01:23:12.350
look like this.,01:23:12.350,01:23:14.170
That's kind of messy.,01:23:14.170,01:23:15.500
"But same kind of
argument generalizes.",01:23:15.500,01:23:18.170
"So that was your quick overview
of lots of hashing stuff.",01:23:18.170,01:23:23.110

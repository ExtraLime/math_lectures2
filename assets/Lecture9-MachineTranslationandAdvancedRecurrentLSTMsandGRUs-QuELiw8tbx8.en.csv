text,start,stop
[MUSIC],00:00:00.000,00:00:04.797
Stanford University.,00:00:04.797,00:00:05.844
"&gt;&gt; All right, hello, everybody.",00:00:05.844,00:00:09.458
And welcome to lecture number nine.,00:00:09.458,00:00:11.770
"Today, we'll do a brief recap,
some organizational stuff, and",00:00:11.770,00:00:14.738
"then we'll talk about, I love when we call
them fancy, recurrent neural networks.",00:00:14.738,00:00:18.554
"But those are the most important
deep learning models of the day,",00:00:18.554,00:00:23.491
LSTMs and GRU type models.,00:00:23.491,00:00:26.290
"They're very exciting and really form
the base model for pretty much every",00:00:26.290,00:00:31.140
"deep learning paper or almost all the deep
learning papers you see out there.",00:00:31.140,00:00:34.840
"So after today, you'll really have in your
hands the kind of tool that is the default",00:00:34.840,00:00:40.720
"tool for a lot of different deep learning
final P applications, so super exciting.",00:00:40.720,00:00:45.040
"And the best part is, you kind of know
most of the important math already of it,",00:00:45.040,00:00:49.031
so we can just define the model.,00:00:49.031,00:00:50.673
"And everything else will kind of
follow through with these basic, and",00:00:50.673,00:00:54.157
"sometimes painful building blocks
that we went through before.",00:00:54.157,00:00:58.320
"All right, before we jump in,
some organizational stuff.",00:00:58.320,00:01:00.640
"We have a new office hour schedule and
places.",00:01:00.640,00:01:02.770
"Today, we're continuously trying to
optimize the whole process based on",00:01:02.770,00:01:07.042
your feedback.,00:01:07.042,00:01:08.460
Thanks for that.,00:01:08.460,00:01:09.760
"So I'll have office hours every day,
multiple times.",00:01:09.760,00:01:14.229
"I hope that will allow us to kind of
distribute the load a little bit,",00:01:14.229,00:01:19.214
"cuz I know sometimes lots of people
come to one office hour, and",00:01:19.214,00:01:23.849
then there's a long wait there.,00:01:23.849,00:01:26.552
"Also, it's important that you register for",00:01:26.552,00:01:28.470
"your GPU teams by the end of today,
or ideally before.",00:01:28.470,00:01:32.740
"So that we can make
sure you all get a GPU.",00:01:32.740,00:01:36.390
"Ideally, we also encourage people
to have pairs for Problem set 3 and",00:01:36.390,00:01:42.565
"4 the project, at least pairs,
cuz we only have 300 or",00:01:42.565,00:01:47.692
"so GPUs and
almost 700 students in the class.",00:01:47.692,00:01:52.000
"So try to form teams, but do make sure
that you don't just have your partner or",00:01:52.000,00:01:55.735
work on and,00:01:55.735,00:01:56.252
"implement all the GPU stuff, and you do
all the other parts of the problem set.",00:01:56.252,00:02:00.110
"Cuz then you really miss out on
a very important valuable skill for",00:02:00.110,00:02:04.763
"both research and applied deep learning,
if you don't know how to use a GPU.",00:02:04.763,00:02:10.276
"And sadly,
I have to get back to some work event.",00:02:10.276,00:02:14.250
"So I'll have a pretty
short office hour today.",00:02:14.250,00:02:16.660
"But then I know we have the deadline for
project proposals on Thursdays.",00:02:16.660,00:02:21.340
"So on Thursday,
I'm gonna have an unlimited office hour.",00:02:21.340,00:02:25.022
"I'm gonna start after class, and
will end when queue status is empty.",00:02:25.022,00:02:28.690
"So if you come half an hour late,
prepare to talk to me three hours later.",00:02:28.690,00:02:33.460
"So [LAUGH] the project is the coolest
part, so I don't wanna discourage",00:02:33.460,00:02:38.461
"people from doing the project
because we don't have enough.",00:02:38.461,00:02:42.887
So it's gonna be great.,00:02:42.887,00:02:45.330
"I'll bring food.
You should bring food too,",00:02:45.330,00:02:48.016
"and-
&gt;&gt; [LAUGH]",00:02:48.016,00:02:48.585
&gt;&gt; [LAUGH],00:02:48.585,00:02:49.089
"&gt;&gt; It's kind of good fun, all right.",00:02:49.089,00:02:51.160
"If by any chance even after midnight,
people, we still have the queue status",00:02:52.190,00:02:57.049
"is still full, which I doubt at
that point, I think, I hope,",00:02:57.049,00:03:00.932
"then we can push the proposals out for
those, or you can submit the proposal.",00:03:00.932,00:03:05.819
"And then we'll figure out the mentor
situation, very soon thereafter.",00:03:05.819,00:03:09.650
"So all right, any questions
around any class organization?",00:03:09.650,00:03:12.740
"All right, then lets dive right in.",00:03:21.032,00:03:23.470
"So basically today,
we'll have a a very advanced,",00:03:23.470,00:03:26.927
"cutting edge blast from the past, because
while pedagogically, it'll make sense for",00:03:26.927,00:03:32.745
"us to first talk about a model from 2014,
from just three years ago.",00:03:32.745,00:03:37.563
"The main model we'll end up with,
the long-short-term-memories is actually",00:03:37.563,00:03:41.616
"a very old model, from 97, and
has kind of been dormant for a while.",00:03:41.616,00:03:45.092
"As very powerful model,
you need a lot of training data for it,",00:03:45.092,00:03:48.469
you need fast machines for it.,00:03:48.469,00:03:50.460
"But now, that we have those two things,
this is a very powerful model for NLP.",00:03:50.460,00:03:54.110
"And if you ask one of the inventors,",00:03:54.110,00:03:57.010
"the second model is really just
a special case of the LSTM.",00:03:57.010,00:04:00.320
"But I think, pedagogically,
it makes sense to sort of first talk about",00:04:00.320,00:04:03.760
"the so-called Gated Recurrent Unit,
which is slightly simpler version.",00:04:03.760,00:04:06.842
"&gt;&gt; And we'll use machine translation which
is one of the sort of most useful tasks",00:04:06.842,00:04:11.669
"you might argue of NLP,
sort of a real life task.",00:04:11.669,00:04:15.280
"Something that actual people
outside academia, outside research,",00:04:15.280,00:04:18.971
outside linguistics really care about.,00:04:18.971,00:04:21.167
"And by the end, you'll actually have the
skills to build one of the best machine",00:04:21.167,00:04:27.052
"translation models out there, modulo
a lot of time and some extra effort.",00:04:27.052,00:04:32.640
"But the biggest parts of 90% of
the top MT systems out there,",00:04:32.640,00:04:36.365
"you'll be able to understand at least and
probably build also,",00:04:36.365,00:04:40.161
"if you have the GPU
skills after this class.",00:04:40.161,00:04:43.480
"All right, so I'm not gonna go through
too many of the details, but in just in",00:04:43.480,00:04:46.930
"preparation to mentally make you also
think about the midterm that's coming up.",00:04:46.930,00:04:51.560
"Next lecture, we'll have midterm review.",00:04:51.560,00:04:53.610
"But ideally, these kinds of
equations that I'm throwing up here,",00:04:53.610,00:04:57.227
you're pretty familiar with.,00:04:57.227,00:04:58.945
"At this point, you're like yeah,",00:04:58.945,00:05:00.728
"I just do some negative
sampling here from my Word2Vec.",00:05:00.728,00:05:03.420
"And I have my inside and
my outside vectors in the window.",00:05:04.450,00:05:06.981
"And similarly for glove, I have two
sets of vectors, you optimize this.",00:05:06.981,00:05:12.450
"You have a function here, dead limits,",00:05:12.450,00:05:14.441
"how important very frequent pairs
are in your concurrence matrix.",00:05:14.441,00:05:17.791
"You understand the max-margin
objective function.",00:05:17.791,00:05:22.130
"You have scores of good windows
from the large training corpus and",00:05:22.130,00:05:25.688
corrupted windows.,00:05:25.688,00:05:26.857
So all of these should be familiar.,00:05:26.857,00:05:28.924
"And if not, then you really should also
start thinking about sort of studying",00:05:28.924,00:05:33.764
again for the midterm.,00:05:33.764,00:05:35.257
"The most basic definition of neural net,",00:05:36.310,00:05:39.076
"where we just have some score
at the end or some soft max.",00:05:39.076,00:05:42.798
"And really being comfortable
with these two final equations,",00:05:42.798,00:05:46.706
"that if you understand those, all
the rest of the models will basically be,",00:05:46.706,00:05:51.423
"in many cases, sort of fancy versions or
adapted versions of these two equations.",00:05:51.423,00:05:56.770
So that's important.,00:05:56.770,00:05:59.424
"And then we'll have our standard recurrent
neural network that we already went",00:05:59.424,00:06:03.676
"through, and we kind of assume you
should know for the midterm as well.",00:06:03.676,00:06:07.493
"And our grade of Cross Entropy Error,
as one of the main loss or",00:06:07.493,00:06:12.392
objective functions that we optimize.,00:06:12.392,00:06:16.440
"And when we optimize,
we usually use the Mini-batched SGD.",00:06:16.440,00:06:20.177
We don't go through single example.,00:06:20.177,00:06:21.080
"We don't go through the entire
batch of our trained data, but",00:06:21.080,00:06:25.136
"we take 100 or so
of examples each time we train.",00:06:25.136,00:06:28.496
"So all those concepts, you should
feel reasonably comfortable now.",00:06:28.496,00:06:33.590
"And if not, then definitely come
back to the office hours, and",00:06:33.590,00:06:37.154
start sort of studying for the midterm.,00:06:37.154,00:06:39.450
"All right, and we'll go over more
midterm details in the next lecture.",00:06:39.450,00:06:44.151
"All right, now, onto the main topic
of today, machine translation.",00:06:44.151,00:06:48.500
"So you might think for some NLP tasks
that you can get away with thinking of",00:06:48.500,00:06:53.273
"all the rules that, for
instance, sentiment analysis.",00:06:53.273,00:06:57.118
"A sentence might come up positive or
negative, right?",00:06:57.118,00:07:00.138
"You say, I have a list of all the positive
words, most of all the negative words.",00:07:00.138,00:07:03.540
"And I can think of the ways you can negate
positive words and things like that.",00:07:03.540,00:07:06.936
"And you could maybe conceive of
creating a sentiment analysis system",00:07:06.936,00:07:10.996
"of just all your intuitions
about linguistics and sentiment.",00:07:10.996,00:07:15.260
"That kind of approach is completely
ridiculous for machine translation.",00:07:15.260,00:07:18.860
"There's no way you would ever,",00:07:18.860,00:07:20.250
"nobody will ever be able to think of all
the different rules and exceptions for",00:07:20.250,00:07:24.010
"translating all possible sentences
of one language to another.",00:07:24.010,00:07:28.410
"So basically, the baseline that's
pretty well established is that",00:07:28.410,00:07:32.920
"all machine translation systems
are somewhat statistical in nature.",00:07:32.920,00:07:36.790
"We will always try to
take a very large corpus.",00:07:36.790,00:07:38.670
"In fact, we'll have so
called parallel copra,",00:07:38.670,00:07:41.137
"where we have a lot of the sentences or
paragraphs in one language.",00:07:41.137,00:07:44.520
"And we know that this paragraph in this
language translates to that paragraph",00:07:44.520,00:07:48.845
in another language.,00:07:48.845,00:07:50.119
"One of the popular parallel copra
of All of, for a long time,",00:07:50.119,00:07:55.040
"for the last couple thousand
years is the Bible, for instance.",00:07:55.040,00:07:57.060
You'll have Bible translated.,00:07:57.060,00:07:58.451
It has nice paragraphs.,00:07:58.451,00:07:59.789
"And each paragraph is translated
in different languages.",00:07:59.789,00:08:02.243
"That would be one of
the first parallel corpora.",00:08:02.243,00:08:05.614
"The very first is actually
the Rosetta Stone.",00:08:05.614,00:08:07.629
"Which allowed people to have
at least some understanding",00:08:07.629,00:08:12.547
of ancient Egyptian hieroglyphs.,00:08:12.547,00:08:15.626
"And it's pretty exciting if you're
into historical linguistics.",00:08:15.626,00:08:22.872
"And it allows basically to translate
those to the Demotic script and",00:08:22.872,00:08:27.742
"the ancient Greek also,
which we still know.",00:08:27.742,00:08:31.081
"And so we can gain some intuition about
what's going on in the other two.",00:08:31.081,00:08:35.490
"Now, in the next couple of slides,",00:08:35.490,00:08:37.743
"I will basically try to bring across
to you that traditional statistical",00:08:37.743,00:08:42.330
"machine translation systems are very,
very complex beasts.",00:08:42.330,00:08:46.956
"And it wouldn't have been impossible for
me to say at the end of the lecture,",00:08:46.956,00:08:50.140
"all right, now you could implement
this whole thing yourself,",00:08:50.140,00:08:52.770
"after just one lecture, going over MT cuz
there are a lot of different moving parts.",00:08:52.770,00:08:57.380
So let's walk through this.,00:08:57.380,00:08:59.910
"You won't have to actually implement
traditional statistical MT system in",00:08:59.910,00:09:04.390
this class.,00:09:04.390,00:09:05.570
"But I want you to appreciate
a little bit the history.",00:09:05.570,00:09:07.978
"And why deep learning is so impactful and
amazing for machine translation.",00:09:07.978,00:09:13.495
"Cuz it's replacing a lot of different
submodules in these very complex models.",00:09:13.495,00:09:18.210
"And sometimes it uses still
ideas from this, but not very.",00:09:20.570,00:09:24.060
"Most of them we don't need any more for
neural machine translation systems.",00:09:24.060,00:09:27.974
"All right, so let's set the stage.",00:09:27.974,00:09:30.342
We have generally a source language.,00:09:30.342,00:09:32.810
"Let's call that f, such as French.",00:09:32.810,00:09:35.340
"And we have a target language, e,
in our case, let's say it's English.",00:09:35.340,00:09:38.599
"So we wanna translate from the source
French to the target language of English.",00:09:38.599,00:09:44.031
"And we'll usually describe
this here with a simple Bayes",00:09:44.031,00:09:49.205
"rule where we basically try to find the,
Target sentence,",00:09:49.205,00:09:56.070
"usually e here we assume is the whole
sentence in the target language.",00:09:56.070,00:10:00.720
"That gives us the largest conditional
probability conditioned on f.",00:10:00.720,00:10:04.910
So this is an abstract formulation.,00:10:04.910,00:10:07.343
"We'll try to fill in how to actually
compute these probabilities",00:10:07.343,00:10:10.496
"in traditional and then later in
neural machine translation systems.",00:10:10.496,00:10:13.960
So now we can use Bayes rule.,00:10:13.960,00:10:15.510
"Posterior equals its prior times
likelihood divided by marginal evidence.",00:10:15.510,00:10:19.320
"Marginal evidence here would just be for
the source language.",00:10:19.320,00:10:21.740
So that doesn't change.,00:10:21.740,00:10:22.371
"So we can drop that,
argmax would not change from that.",00:10:22.371,00:10:25.340
"So basically, we'll try to
compute these two factors here.",00:10:25.340,00:10:31.470
"The probability of the French
sentence given, or",00:10:31.470,00:10:36.210
"the source language given the target,
times the probability of just the target.",00:10:36.210,00:10:42.940
"And now, we'll basically call
these two elements here.",00:10:44.760,00:10:47.500
One is our translation model.,00:10:47.500,00:10:49.410
And the other one is our language model.,00:10:49.410,00:10:51.140
"Remember language modeling where we
tried to get the probability of a longer",00:10:51.140,00:10:54.443
sequence.,00:10:54.443,00:10:54.967
This is a great use case for it.,00:10:54.967,00:10:58.360
"Basically, you can think of this
as you get some French sentence.",00:10:58.360,00:11:02.075
Your translation model will try to find.,00:11:02.075,00:11:04.689
"Maybe this phrase,
I can translate into that.",00:11:04.689,00:11:06.830
"And this phrase,
I can translate into this.",00:11:06.830,00:11:09.220
"And then you have a bunch
of pieces of English.",00:11:09.220,00:11:11.550
"And then your language model will
essentially in the decoder be combined",00:11:11.550,00:11:16.620
"to try to get a single,
smooth sentence in the target language.",00:11:16.620,00:11:21.312
"So it'll help us to take all these pieces
that we have from the translation model.",00:11:21.312,00:11:26.362
"And make it into one sentence that
actually sounds reasonable and flows and",00:11:26.362,00:11:30.112
is grammatical and all that.,00:11:30.112,00:11:31.830
"So the language model helps us to
weight grammatical sentences better.",00:11:31.830,00:11:36.150
"So, for instance, I go home will
sound better than I go house, right?",00:11:36.150,00:11:40.767
"Because I go home will have a more
likely higher probability, so",00:11:40.767,00:11:44.965
"more likely English
sentence to be uttered.",00:11:44.965,00:11:47.930
"Now, how do we actually train
all these different pieces?",00:11:47.930,00:11:51.711
And how would you go about doing this?,00:11:51.711,00:11:53.170
"Well, if you wanted to translate,
do this translation model here.",00:11:53.170,00:11:56.820
"Then the first thing you'd have to do
is you'd find so called alignments.",00:11:56.820,00:12:01.067
"Which is basically, the goal of the
alignment step is to know which word or",00:12:01.067,00:12:05.249
"phrase in the source language would
translate to the other word or",00:12:05.249,00:12:08.959
phrase in the target language.,00:12:08.959,00:12:10.790
And that sub problem already.,00:12:10.790,00:12:13.660
"And now, again,
we have these three different systems.",00:12:13.660,00:12:18.030
"And now we're zooming in to
the step one of that system.",00:12:18.030,00:12:21.420
"Now that one is already hard
because alignment is non-trivial.",00:12:21.420,00:12:27.093
"These are actually some cool examples from
previous incarnation from Chris's class,",00:12:27.093,00:12:31.908
"224, and from previous years.",00:12:31.908,00:12:33.871
"Here are some examples of why
alignment is already hard.",00:12:33.871,00:12:37.020
"And this is for a language pair
that is actually quite similar.",00:12:37.020,00:12:43.010
"English and French share a lot
of common history, and so on,",00:12:43.010,00:12:47.130
and they're more similar.,00:12:47.130,00:12:49.240
"But even if we have these two sentences
here, like Japan shaken by two new quakes.",00:12:49.240,00:12:54.290
"Or Le Japon secoue par
deux nouveaux seismes.",00:12:54.290,00:12:58.990
"Then we'll basically have
here a spurious word.",00:12:58.990,00:13:03.730
"So Le was actually not
translated to anything.",00:13:03.730,00:13:06.850
And we would skip it in our alignment.,00:13:06.850,00:13:10.110
So you see here this alignment matrix.,00:13:10.110,00:13:13.750
"And you'll notice that Le
just wasn't translated.",00:13:13.750,00:13:16.400
"We don't say the Japan, or
a Japan, or something like that.",00:13:16.400,00:13:23.195
"So it gets trickier, though.",00:13:23.195,00:13:24.740
Cuz there are also so,00:13:24.740,00:13:25.880
"called zero fertility words
that are not translated at all.",00:13:25.880,00:13:29.190
"So we start in a source and
we just drop them.",00:13:29.190,00:13:31.317
"And, for some reason, the translators,
or for grammatical reasons and",00:13:31.317,00:13:36.977
"so on, they don't actually have any
equivalent in the target language.",00:13:36.977,00:13:42.647
"And to make it even more complex,
we can also have one-to-many alignments.",00:13:42.647,00:13:46.160
"So implemented in English is actually
mis en application in French.",00:13:46.160,00:13:51.630
"So made into an application
of sorts is just the word and",00:13:51.630,00:13:57.000
the verb implemented here.,00:13:57.000,00:13:58.122
So then we'll have to try to find.,00:13:58.122,00:13:59.997
"And now, as you try to think through
algorithms that might do this",00:13:59.997,00:14:02.786
alignment for you.,00:14:02.786,00:14:03.656
"You'll have to think, so this word could
go to either this one word or no word.",00:14:03.656,00:14:08.450
Or these three words together.,00:14:08.450,00:14:09.551
Or maybe these two words together.,00:14:09.551,00:14:11.210
"And you can see how that would create, if
you tried to go through all the statistics",00:14:11.210,00:14:14.980
"and collect all of these probabilities,
of which phrase would go to what phrase.",00:14:14.980,00:14:18.610
"It'll get pretty hard to
actually combine them all.",00:14:18.610,00:14:22.667
"And language is just incredible and
very complex.",00:14:22.667,00:14:27.090
And you also have many-to-one alignments.,00:14:27.090,00:14:28.972
"So aboriginal people are just
autochtones in French.",00:14:28.972,00:14:35.618
"So similar actually in German, [FOREIGN].",00:14:35.618,00:14:38.403
So you'd have two words in German.,00:14:38.403,00:14:40.260
"And so, you have many-to-one
alignments making the combinatorial",00:14:40.260,00:14:45.218
"explosion even harder if you
try to find good alignments.",00:14:45.218,00:14:49.341
"And lastly,
you'll also have many-to-many alignments.",00:14:49.341,00:14:52.039
"You have certain phrases
like don't have any money.",00:14:52.039,00:14:56.114
This just goes to sont demunis in French.,00:14:56.114,00:14:59.340
"And so it's a very, very complex
problem that has combinatorial",00:15:00.520,00:15:04.838
"explosion of all potential
combinations and it's tricky.",00:15:04.838,00:15:08.853
"All right, so now, really,
if you were to take a traditional class,",00:15:08.853,00:15:14.315
"you could have several lectures,
or at least an entire lecture,",00:15:14.315,00:15:19.407
"just on the various ways you could
implement cleverly an alignment model.",00:15:19.407,00:15:26.100
"And sometimes,
people use just single words.",00:15:26.100,00:15:30.010
"And other times, they actually use
parses like the one you're now familiar,",00:15:30.010,00:15:33.327
syntactic parses.,00:15:33.327,00:15:34.188
"And try to find which,
no, not just words, but",00:15:34.188,00:15:37.691
"phrases from a parse would
map to the other language.",00:15:37.691,00:15:41.640
"And then, of course, it's not just that.",00:15:43.450,00:15:45.720
"And not usually are sentences and
languages nicely aligned, but",00:15:45.720,00:15:50.250
you can also have complete reorderings.,00:15:50.250,00:15:53.660
"So German sometimes, for sub Clauses
actually has the verb at the end,",00:15:53.660,00:15:58.700
"so you flip a lot of the words, and you
can't just have this vocality assumption",00:15:58.700,00:16:03.390
"that words rough in this area will
translate to roughly a similar area,",00:16:03.390,00:16:07.590
"in terms of the sequence of
words in the other language.",00:16:07.590,00:16:10.890
"So yeah, ja nicht here,
ja is technically just yes in German,",00:16:13.717,00:16:17.567
also not translated at all.,00:16:17.567,00:16:19.730
"And then actually going over there and
going, moving also.",00:16:19.730,00:16:22.870
"All right, now let's say we have
all these potential alignments, and",00:16:24.320,00:16:28.480
"now as we start from the source
language we say, all right.",00:16:28.480,00:16:31.690
"Let's say the source here is this German
sentence, geht ja nicht nach hause.",00:16:31.690,00:16:37.040
"Now could be translated
into many different words.",00:16:37.040,00:16:42.880
"So German it's technically just the he
of he, she, it, as the es in German.",00:16:42.880,00:16:48.446
"But sometimes English as
you do your alignment",00:16:48.446,00:16:53.400
"when not unreasonable one is just it or
comma he or",00:16:53.400,00:16:57.950
"he will be, cuz those were dropped
before in the alignment and so on.",00:16:57.950,00:17:01.422
"So you now have lots of candidates for
each possible word and for",00:17:01.422,00:17:05.610
"each possible phrase that you
might want to combine now in",00:17:05.610,00:17:10.420
"some principled way to
the final target translation.",00:17:10.420,00:17:16.310
"So you have again here a combinatorial
explosion of lots of potential ways you",00:17:16.310,00:17:20.140
"could translate each of the words or
phrases of various lengths.",00:17:20.140,00:17:24.230
"And so basically what that means is
you'll have a very hard search problem",00:17:24.230,00:17:27.710
"that also includes having to
have a good language model.",00:17:29.180,00:17:33.010
"So that as you put all these pieces
together, you essentially try to keep",00:17:33.010,00:17:37.880
"saying or combining phrases that
are grammatically plausible or",00:17:37.880,00:17:42.060
sound reasonable to native speakers.,00:17:42.060,00:17:44.010
"And this often ends up being
so-called beam search,",00:17:45.710,00:17:48.707
"where you try to keep around a couple of
candidates as you go from left to right",00:17:48.707,00:17:52.969
"and you try to put all of these
different pieces together.",00:17:52.969,00:17:56.253
"Now again, this is totally not
doing traditional MT justice.",00:17:56.253,00:17:59.685
"Right, we just went in five minutes over
what could have been an entire lecture on",00:17:59.685,00:18:04.085
"statistical machine translation, or
maybe even many multiple lectures.",00:18:04.085,00:18:07.725
"So there are lots of important
details we skipped over.",00:18:07.725,00:18:11.110
"But the main gist here
is that there's a lot of",00:18:11.110,00:18:14.060
"human feature engineering that's required
and involved in all of these different",00:18:14.060,00:18:18.550
"pieces that used to require building
a machine translation system.",00:18:18.550,00:18:21.550
"And it also meant that there were whole
companies that you could form just for",00:18:21.550,00:18:26.020
"machine translation because nobody
could go through all that work and",00:18:26.020,00:18:29.720
really build out a good system.,00:18:29.720,00:18:31.510
"Whereas now you have companies that have
worked for decades in this and they start",00:18:31.510,00:18:36.233
"using an open-source machine translation
system that anybody can download.",00:18:36.233,00:18:40.900
"And now a normal student, a PhD
student can spend a couple months and",00:18:40.900,00:18:43.898
"then he has like one of
the best MT systems.",00:18:43.898,00:18:45.808
"Which just completely would have been
completely impossible in their large",00:18:45.808,00:18:50.035
"groups that all work together in very
large systems before, in academia.",00:18:50.035,00:18:54.214
"So one of the main problems
of this kind of approach,",00:18:54.214,00:18:57.120
"is actually that not only is
it a very complex system, but",00:18:57.120,00:19:00.291
"it's also a system of independently
trained machine learning models.",00:19:00.291,00:19:04.348
"And, if there's one thing that I think
that I like most, when property of deep",00:19:04.348,00:19:09.400
"learning models, not just for MT, but
in all of NLP and maybe in all of AI.",00:19:09.400,00:19:14.540
"Is that we're usually in deep learning
try to have end to end trainable models",00:19:14.540,00:19:18.420
"where you have your final objective
function that you care about and",00:19:18.420,00:19:22.260
"everything is learned
jointly in one model.",00:19:22.260,00:19:24.390
"And this MT system is kind
of the opposite of that.",00:19:25.420,00:19:27.850
"You have an alignment model
you optimize for that, and",00:19:27.850,00:19:30.050
"then you have a reordering model maybe,
and then you have the language model.",00:19:30.050,00:19:35.630
"And they're all separate systems and you
couldn't jointly train all of it together.",00:19:35.630,00:19:41.152
"So that's kind of the very quick summary
for traditional machine transaction.",00:19:41.152,00:19:46.408
"Any high level questions
around traditional MT?",00:19:46.408,00:19:47.625
"All right, so now deep learning to",00:19:49.976,00:19:55.267
"the rescue, maybe, probably.",00:19:55.267,00:20:00.379
"So let's go through a sequence of
models and see if they would suffice.",00:20:00.379,00:20:07.710
"So the simplest one that we could
possibly do is kind of an encoder and",00:20:07.710,00:20:12.628
decoder model that looks like this.,00:20:12.628,00:20:16.220
"Where we literally just have
a single recurrent neural network,",00:20:16.220,00:20:20.200
"where we have our word vectors so
let's say here",00:20:20.200,00:20:22.910
"we translate from German to English
Echt Kiste is awesome sauce in English.",00:20:22.910,00:20:28.390
"And we now have our word vectors
here we learned them in German, and",00:20:28.390,00:20:32.690
we have our soft max classifier here.,00:20:32.690,00:20:35.160
"And we just have a single recurrent neural
network and once it sees the end of German",00:20:35.160,00:20:38.820
"sentence and there's no input left we'll
just try to output the translation.",00:20:38.820,00:20:44.681
"Not totally unreasonable,
it's an end-to-end trainable model.",00:20:46.923,00:20:50.290
"We'll have our standard cross entry
pair here that tries to just predict",00:20:50.290,00:20:53.626
the next word.,00:20:53.626,00:20:54.342
"But the next word actually has
to be in a different language.",00:20:54.342,00:20:59.336
"Now, basically this last vector here,
if this was our main model,",00:20:59.336,00:21:02.898
"this last vector would have to
capture the entirety of the phrase.",00:21:02.898,00:21:06.410
"And sadly, I've already told you
that usually five or six words or so",00:21:06.410,00:21:10.511
"can be captured and
after that, we don't really,",00:21:10.511,00:21:13.552
"we can't memorize the entire
context of the sentence before.",00:21:13.552,00:21:17.320
"So this might work for like,
very short sentenced but maybe not.",00:21:17.320,00:21:23.820
"But let's define what this model
would be in its most basic form,",00:21:23.820,00:21:27.720
cuz we'll work on top of this afterwards.,00:21:27.720,00:21:30.900
"So we have here our standard recurrent
neural network from the last lecture.",00:21:30.900,00:21:34.480
"Where we have our next hidden state,
it's just basically a linear",00:21:34.480,00:21:39.000
"network here followed by
non-element wise linearities.",00:21:40.080,00:21:43.330
"And we sum here the matrix
vector product with the vector,",00:21:43.330,00:21:46.970
"the previous hidden state in
our current word vector xt.",00:21:46.970,00:21:49.710
"And that's our encoder and
then in our decoder in the simplest form,",00:21:49.710,00:21:55.470
"again not the final model,
in the simplest form we could just",00:21:55.470,00:21:58.130
"drop this cuz the decoder doesn't
have an input at that time.",00:21:58.130,00:22:01.550
"Right, it's just we wanna now
translate and just generate an output.",00:22:01.550,00:22:05.320
"So during the decoder we drop
this matrix vector product and",00:22:05.320,00:22:09.400
we just go each time step.,00:22:09.400,00:22:10.780
"It's just basically moving along based
on the previous hidden time step.",00:22:10.780,00:22:15.810
"And we'll have our final softmax output
here at each time step of the decoder.",00:22:15.810,00:22:20.470
"Now I also introduced this phi notation
here, and basically whenever you have,",00:22:22.750,00:22:26.780
"we'll see this only in
the next couple of slides.",00:22:26.780,00:22:29.280
"But whenever I write phi of two vectors,",00:22:29.280,00:22:32.180
"that means we'll have two separate W
matrices for each of these vectors.",00:22:32.180,00:22:37.420
"This is the little, shorter notation, and
then the default here would be well, just",00:22:37.420,00:22:42.210
"like I said, minimize the cross entropy
error for all the target words conditioned",00:22:42.210,00:22:46.350
"on all the source words that we hoped
would be captured in that hidden state.",00:22:46.350,00:22:49.740
"All right, any questions, concerns,
thoughts about how this model would do?",00:22:51.130,00:22:57.140
"So, the comment or question is that
neither are the traditional model or",00:23:14.673,00:23:19.250
this model account for grammar.,00:23:19.250,00:23:21.940
"And in some ways, that's not true.",00:23:21.940,00:23:23.048
"So there are actually a lot of traditional
models that work on top of syntactic",00:23:23.048,00:23:28.035
grammatical tree structures.,00:23:28.035,00:23:30.120
"And they do this alignment based
on the syntactic structure of",00:23:30.120,00:23:34.373
prefer potentially the alignment step.,00:23:34.373,00:23:37.290
"But also for the generation and
the encoding step and",00:23:37.290,00:23:39.400
all these different steps.,00:23:39.400,00:23:40.128
"So there are several ways you can infuse
grammar and chromatical sort of priors",00:23:40.128,00:23:45.860
"into neuro machine translation systems or
so syntactic machine translation systems.",00:23:45.860,00:23:52.010
"It turns out it's questionable
if that actually helps.",00:23:52.010,00:23:54.480
"In many cases for machine translation,
you have such a broad range of sentences.",00:23:54.480,00:23:58.440
"You actually might have un-grammatical
sentences sometimes, and",00:23:58.440,00:24:01.575
you still want them to be translated.,00:24:01.575,00:24:03.870
"You have very short,",00:24:03.870,00:24:05.390
"complex ambiguous kinds of
sentences like headlines and so on.",00:24:05.390,00:24:08.505
"So it's tricky, the jury was sort of out.",00:24:08.505,00:24:12.452
"And some tactic models were battling
it out with non-tactic models until",00:24:12.452,00:24:16.068
neural machine translation came.,00:24:16.068,00:24:17.825
"And now, it's not as important
of a question anymore.",00:24:17.825,00:24:21.019
"Now, for neural systems, we would assume
and hope that our hidden state actually",00:24:21.019,00:24:26.029
"captures some grammatical structures and
some grammatical intuitions that we have.",00:24:26.029,00:24:31.340
"But we don't explicitly give
that to the algorithm anymore.",00:24:31.340,00:24:34.880
"Which some people who are very good
at giving those kinds of features,",00:24:34.880,00:24:40.503
your algorithms might think is sad.,00:24:40.503,00:24:43.473
"But at the same time,
it's good if we don't have to, right?",00:24:43.473,00:24:46.040
"It's less work for us, putting more
artificial back into artificial",00:24:46.040,00:24:49.880
"intelligence, less human
intelligence on designing grammars.",00:24:49.880,00:24:55.400
"Anyways, so any other questions?",00:24:55.400,00:24:57.511
Yeah.,00:24:57.511,00:24:58.401
"Good question, so sometimes, the number of
input words is different to the numbers",00:25:08.690,00:25:12.579
"of output words, and that's very true.",00:25:12.579,00:25:15.000
"So one modification we would have to
make to this kind of model for sure,",00:25:15.000,00:25:19.364
"is actually say, have the last output word
here, BA, stop out putting up words work.",00:25:19.364,00:25:27.090
"Like a special token that says, I'm done.",00:25:27.090,00:25:29.860
"And one, you add that to your softmax
classifier sort of the last row.",00:25:29.860,00:25:35.250
"And then you hope that when it
predicts that token, it just stops.",00:25:35.250,00:25:38.480
"And that is good enough and
not uncommon actually for",00:25:40.000,00:25:45.226
all these neural machine translations.,00:25:45.226,00:25:49.406
"The superscript S is just again,",00:25:49.406,00:25:52.592
"to distinguish the different
W matrices that we have for",00:25:52.592,00:25:57.950
"hidden connections, visible or
hidden inputs, and softmax W.",00:25:57.950,00:26:04.222
"All right, now sadly,
while neural MT is pretty cool, and",00:26:04.222,00:26:07.206
"it is simpler than traditional systems,
it's not quite that simple.",00:26:07.206,00:26:11.400
So we'll have to be a little more clever.,00:26:11.400,00:26:14.340
"And so let's go through a series of
extensions to this model where in the end,",00:26:14.340,00:26:19.640
"we'll have a very big
powerful LSTM type model.",00:26:19.640,00:26:23.830
"So step one, is we'll actually have
different recurrent neural network weights",00:26:23.830,00:26:27.211
for encoding and decoding.,00:26:27.211,00:26:28.396
"So instead of having the same W here,
we actually should have a different set",00:26:28.396,00:26:33.265
"of parameters, a different W for
the decoding step.",00:26:33.265,00:26:37.320
That's still relatively similar.,00:26:37.320,00:26:40.533
"All right, so again, remember this
notation here of fi where every",00:26:40.533,00:26:45.167
"input has its own matrix
W associated with it.",00:26:45.167,00:26:48.356
"The second modification is
that the previous hidden state",00:26:48.356,00:26:53.388
"is kind of the standard that you
have as input for during decoding.",00:26:53.388,00:26:59.149
"But instead of just having
the previous hidden state,",00:26:59.149,00:27:02.298
"we'll actually also add the last
hidden vector of the encoding.",00:27:02.298,00:27:06.880
"So we call this c here,
but it's essentially ht.",00:27:06.880,00:27:10.360
"So at this input here, we don't just
have the previous hidden state,",00:27:10.360,00:27:16.127
"but we always take the last hidden
state from the encoding step.",00:27:16.127,00:27:21.428
"And we have, again,
a separate matrix for that.",00:27:21.428,00:27:25.158
"And then on top of that, we will also add,
and that's actually, if you think about",00:27:25.158,00:27:28.620
"it, it's a lot of parameters, we'll add
the previous predicted output word.",00:27:28.620,00:27:32.000
"So as we translate,",00:27:33.190,00:27:35.280
"we have three inputs for each hidden
state during the decoding step.",00:27:35.280,00:27:40.670
"We'll have the previous hidden state
as a standard recurrent neural network.",00:27:40.670,00:27:43.453
"We have the last hidden
state of the encoder.",00:27:43.453,00:27:45.800
"And we have the actual output word
we predicted just before that.",00:27:46.800,00:27:50.660
"And this will essentially help the model
to know that it just output a word, and",00:27:50.660,00:27:55.413
"it'll prevent it from
outputting that word again.",00:27:55.413,00:27:58.596
"Cuz it'll learn to
transform the hidden state,",00:27:58.596,00:28:02.071
"based on having just upload
a specific word before.",00:28:02.071,00:28:05.809
Yeah?,00:28:05.809,00:28:06.495
"That's right, that's right, yeah.",00:28:14.213,00:28:15.637
"So whenever you have fi of xyz here,",00:28:15.637,00:28:19.543
"it'll just f of w times
x + u of y + v of z.",00:28:19.543,00:28:23.980
"So you just,
I don't wanna define all the matrices.",00:28:25.980,00:28:28.730
That's a great question.,00:28:39.760,00:28:40.760
"So why do we need to make y,
t minus one a parameter,",00:28:40.760,00:28:45.344
"if we actually had computed yt
minus one from ht minus one, right?",00:28:45.344,00:28:51.331
"So two answers, one, it will allow
us to have the softmax weights",00:28:51.331,00:28:56.363
"also modify a little bit how that
hidden state behaves at test time.",00:28:56.363,00:29:02.250
"And two,
we actually can choose usually yt, and",00:29:02.250,00:29:05.295
there are different ways you can do this.,00:29:05.295,00:29:07.905
"You could take the actual probability, the
multinomial distribution from the softmax.",00:29:07.905,00:29:13.380
"But here,
we'll actually make a hard choice, and",00:29:13.380,00:29:15.620
"we'll actually tell the model
we chose exactly this one.",00:29:15.620,00:29:19.120
"So instead of having the distribution,
we'll make a hard choice.",00:29:19.120,00:29:21.780
"And we say, this is the one word, the
highest probability that had the highest",00:29:21.780,00:29:25.108
"probability, we predicted that one,
and that's the one we give us input.",00:29:25.108,00:29:28.800
"So it turns out in practice,",00:29:28.800,00:29:30.150
"that helps to prevent the model
from repeating words many times.",00:29:30.150,00:29:35.520
"And again, it incorporates the softmax
weights in that computation indirectly.",00:29:35.520,00:29:40.197
Yeah.,00:29:40.197,00:29:40.930
That is a good catch.,00:29:53.361,00:29:55.210
That is not how we define the model.,00:29:55.210,00:29:57.050
Ignore those errors.,00:29:57.050,00:29:58.601
"Yeah, well done.",00:29:58.601,00:29:59.730
"In theory, again, so I didn't define
it but you can also, you can do",00:30:00.810,00:30:05.410
"the same thing with the softmax, and
this is what the picture actually shows.",00:30:05.410,00:30:09.360
"So instead of having a softmax of just W,",00:30:09.360,00:30:14.035
ht for the probability of yt.,00:30:14.035,00:30:17.480
"You can also concatenate here your c,
and that's what the picture said.",00:30:17.480,00:30:22.410
"But I wanted to skip over the details so
you caught it, well done.",00:30:22.410,00:30:25.291
"So this model usually,",00:30:40.302,00:30:41.555
"so the question is, do we have
kind of a look ahead type thing?",00:30:41.555,00:30:44.820
Or does the model output blanks?,00:30:44.820,00:30:46.679
"And the model basically has to
output the words in the right order.",00:30:46.679,00:30:52.019
"And it doesn't not have the ability
to do this whole reordering step or",00:30:52.019,00:30:57.415
look ahead kind of thing.,00:30:57.415,00:30:59.431
"Or there's no sort of post processing
of reordering at the end, so",00:30:59.431,00:31:03.748
"this model isn't able to output
the verb at the right time stamp.",00:31:03.748,00:31:08.028
"It's over, okay, here we go.",00:31:08.028,00:31:11.672
"Now, of course,",00:31:11.672,00:31:12.476
"once it works well, everybody will try to
see if they can kind of improve it, and",00:31:12.476,00:31:16.071
"eventually you can do beam searches
too for these kinds of models.",00:31:16.071,00:31:19.091
"But surprisingly, in many cases, you
don't have to get a reasonable MT system.",00:31:19.091,00:31:23.610
"All right, now, I want you to
become more and more familiar,",00:31:28.489,00:31:31.869
to be able to read the literature.,00:31:31.869,00:31:33.760
So the same picture that we had here and,00:31:33.760,00:31:36.290
"the same equations we defined,
here's another way off looking at this.",00:31:36.290,00:31:40.090
"So with the exception that this
one doesn't have the c connection",00:31:42.390,00:31:46.268
that you caught.,00:31:46.268,00:31:47.336
"So, Yeah, it's similar.",00:31:47.336,00:31:48.963
"It's the same exact model,
just a different way to look at it, and",00:31:48.963,00:31:53.031
it's kind of good to see.,00:31:53.031,00:31:54.890
Sometimes people explicitly write,00:31:54.890,00:31:56.960
"that you start out with a discreet
one of k and coding of the words.",00:31:58.200,00:32:03.160
"It's just like you want one-hot
vectors that we defined, and",00:32:03.160,00:32:05.770
"then you embed it into
continuous word vector space.",00:32:05.770,00:32:09.030
"You give those as input, you compute
your recurrent neural network, ht steps.",00:32:09.030,00:32:13.714
"And now,
you give those as input to the decoder.",00:32:13.714,00:32:17.762
"And that each time stamp of decoder, you
get the one word sample that you actually",00:32:17.762,00:32:22.454
"took as input, the previous hidden state
and to see vector, we defined before.",00:32:22.454,00:32:28.070
"So all these three already
are the inputs for",00:32:28.070,00:32:30.860
"each node in this
recurrent neural network.",00:32:30.860,00:32:33.733
So just a different picture for,00:32:33.733,00:32:37.547
"the same model we just defined, so",00:32:37.547,00:32:41.800
"you learn picture in variances first,
model semantics.",00:32:41.800,00:32:49.310
"Now, it gets more powerful.",00:32:49.310,00:32:50.095
"It needs to get more powerful cuz
even with those two assumptions here,",00:32:50.095,00:32:53.594
"we have a very simple recurrent
neural network with just one layer,",00:32:53.594,00:32:56.918
that's not going to cut it.,00:32:56.918,00:32:58.770
"So we'll use some of the extensions
we discussed in the last lecture,",00:32:58.770,00:33:03.490
"we'll actually have stacked
deep recurrent neural networks",00:33:03.490,00:33:07.740
where we have multiple layers.,00:33:07.740,00:33:09.630
"And then we'll also have,
in some cases, this is not as common,",00:33:09.630,00:33:15.140
"but sometimes it's used,
we have a bidirectional encoder.",00:33:15.140,00:33:19.652
"Where you go from left to right,
and then we give both of,",00:33:19.652,00:33:23.920
"last hidden states of both directions
as input to every step of the decoder.",00:33:23.920,00:33:31.674
"And then this is kind
of almost an XOR here.",00:33:31.674,00:33:34.181
"If you don't do this, than another way
to improve your system slightly is by",00:33:34.181,00:33:38.981
"training the input
sequence in reverse order,",00:33:38.981,00:33:41.981
"because then you have a simpler
optimization problem.",00:33:41.981,00:33:45.510
"So especially for languages that align
reasonably well like English and French.",00:33:45.510,00:33:51.230
"You might instead of saying A,
B, C, the other word's A,",00:33:51.230,00:33:55.485
"the word B, or C goes to in the different
language the words X and Y.",00:33:55.485,00:34:00.264
"You'll say, C B A goes to X Y,
because as they align,",00:34:00.264,00:34:04.272
"A is more likely to translate to X,
and B is more like to Y.",00:34:04.272,00:34:08.563
"And as you have longer sequences,",00:34:08.563,00:34:10.208
"you basically bring the words that are
actually being translated closer together.",00:34:10.208,00:34:14.250
"And hence, you have less of a vanishing
gradient problems and so on, because where",00:34:15.520,00:34:20.560
"you want the work to be predicted, it's
closer to where it came in to the encoder.",00:34:20.560,00:34:25.532
Yeah?,00:34:25.532,00:34:26.032
"That's right, but yeah,
it's still an average force.",00:34:31.362,00:34:36.055
So how does reversing not mess it up?,00:34:44.907,00:34:46.582
"Cuz this sentence doesn't
make grammatical sense.",00:34:46.582,00:34:48.990
"So we never gave this model
an explicit grammar for",00:34:48.990,00:34:54.600
"the source language, or
the target language, right?",00:34:54.600,00:34:56.330
"It's essentially trying, in some really
deep, clever, continuous function,",00:34:56.330,00:35:01.549
"general function approximation kind of
way, just correlation, basically, right?",00:35:01.549,00:35:07.220
"And it doesn't have to know the grammar,
but as long as you're consistent and",00:35:07.220,00:35:12.211
"you just reverse every sequence,
the same way.",00:35:12.211,00:35:15.323
"It's still grammatical if you
read it from the other side.",00:35:15.323,00:35:17.361
"And the model reads it from
potentially both sides, and so on.",00:35:17.361,00:35:21.760
"So it doesn't really matter
to these learning models,",00:35:21.760,00:35:24.750
"as long as your transformation of the
input is consistent across training and",00:35:24.750,00:35:29.580
"testing times, and so on.",00:35:29.580,00:35:30.390
"So the question is,
he understands the argument, but",00:35:42.130,00:35:44.986
it could still change the meaning.,00:35:44.986,00:35:47.330
"And it doesn't change the meaning if
you assume the model will always go",00:35:47.330,00:35:51.620
from one direction to the other.,00:35:51.620,00:35:53.060
"If you start to sometimes do it and
sometimes not,",00:35:53.060,00:35:55.390
then it will totally mess up the system.,00:35:55.390,00:35:57.250
"But as long as it's
a consistent transformation,",00:35:57.250,00:36:00.060
"it is still the same order and
so you're good.",00:36:00.060,00:36:02.420
"So why is reversing the order
a simpler optimization problem?",00:36:08.889,00:36:11.760
"Imagine, you had a very
long sequence here.",00:36:11.760,00:36:14.100
"And again, this is only the case
if the languages align well.",00:36:14.100,00:36:18.800
"As in usually,
the first capital words in one",00:36:18.800,00:36:21.410
"of the source language translated to first
capital words in the target language.",00:36:21.410,00:36:24.810
"Now, If you have a long sequence and
you try to translate",00:36:24.810,00:36:29.892
"it to another long sequence, and
say there are a lot of them here.",00:36:29.892,00:36:35.856
"Now, what that would mean is that this
word here is very far away from that word,",00:36:35.856,00:36:41.149
"cuz it has to go through
this entire transformation.",00:36:41.149,00:36:45.410
"And likewise,
these words are also very far away.",00:36:45.410,00:36:47.960
"So everything is far away from
everything in terms of the number",00:36:47.960,00:36:54.019
"of non-linear function applications
before you get to the actual output.",00:36:54.019,00:37:01.320
"Now, if you just reverse this one,
then this word, so",00:37:01.320,00:37:06.295
"let's call this a, b, c, d, e, f.",00:37:06.295,00:37:09.768
"Now, this is now f,",00:37:09.768,00:37:14.684
"e, d, c, b, a.",00:37:14.684,00:37:19.030
"Now, this word, it's here now.",00:37:19.030,00:37:21.405
"And now, this word translates
directly to that word, right?",00:37:21.405,00:37:24.570
So in your decoder.,00:37:24.570,00:37:25.680
"So now, these two are very,
very close to one another.",00:37:25.680,00:37:29.090
"And so as you do back propagation and we
learn about the vanishing creating problem",00:37:29.090,00:37:33.323
"in the last lecture you have much
less of a vanishing creating problem.",00:37:33.323,00:37:37.020
"So at least in the beginning,
it'll be much better at translating those.",00:37:37.020,00:37:40.420
"So, how does this check work for
languages with different morphology?",00:37:51.243,00:37:56.270
"It doesn't actually matter, but
the sad truth is also that very few",00:37:56.270,00:38:00.040
"MT researchers work on languages
with super complex morphology.",00:38:00.040,00:38:04.470
"So like Finnish doesn't have
very large parallel corpora",00:38:04.470,00:38:08.990
of tons of other languages.,00:38:08.990,00:38:10.600
"And so you don't sadly see
as many people work on that.",00:38:10.600,00:38:13.618
German does work.,00:38:13.618,00:38:14.657
"And for German actually,
a lot of other tricks that we'll get to.",00:38:14.657,00:38:17.380
"And really these tricks are not as
important as the one as trick number six.",00:38:17.380,00:38:22.860
"But before that,
we'll have a research highlight.",00:38:22.860,00:38:24.539
"&gt;&gt; [LAUGH]
&gt;&gt; Give you a bit of a break, all right.",00:38:24.539,00:38:30.140
"Allen, take it away.",00:38:30.140,00:38:33.876
"&gt;&gt; This?
&gt;&gt; Yes.",00:38:33.876,00:38:34.744
"&gt;&gt; Okay.
Hi, everyone.",00:38:34.744,00:38:35.941
My name is Allen.,00:38:35.941,00:38:36.751
"So I'm gonna talk about Building Towards
a Better Language Modeling.",00:38:36.751,00:38:41.950
"So as we've learned last week,",00:38:41.950,00:38:43.449
"language modeling is one of
the most canonical task in NLP.",00:38:43.449,00:38:46.288
"And there are three different ways
we can make it a little bit better.",00:38:46.288,00:38:49.520
We can have better input representation.,00:38:49.520,00:38:51.294
"We can have better regularization or
preprocessing.",00:38:51.294,00:38:54.590
"And eventually,
we can have a better model.",00:38:54.590,00:38:57.760
"So for input, I know you guys
have all played with Glove, and",00:38:57.760,00:39:01.162
that's a word level representation.,00:39:01.162,00:39:03.530
And I heard morphemes.,00:39:03.530,00:39:05.070
From you guys who are down there.,00:39:05.070,00:39:06.310
"So in fact,
you can code the word at a subword level.",00:39:06.310,00:39:09.790
You can do morpheme encoding.,00:39:09.790,00:39:11.702
You can do BPE.,00:39:11.702,00:39:13.111
"You can eventually do
character level embedding.",00:39:13.111,00:39:14.540
"What it does is that it drastically
reduce the size of your vocabulary,",00:39:14.540,00:39:18.110
make the model prediction much easier.,00:39:18.110,00:39:22.010
"So as you can see, Tomas Mikolov in 2012,
and Yoon Kim in 2015,",00:39:22.010,00:39:26.220
"explored this route and got better results
compared to just plain word-based models.",00:39:26.220,00:39:32.350
"So another way to improve your model
is that one of the bigger problems for",00:39:33.880,00:39:38.270
language modelling is over-fitting.,00:39:38.270,00:39:40.220
"And we know that we need to apply
regularization techniques when the model",00:39:40.220,00:39:44.050
is over-fitting.,00:39:44.050,00:39:45.111
"So there are a bunch of them, but today,",00:39:45.111,00:39:46.752
"I'm gonna focus on preprocessing
because it's a little bit newer.",00:39:46.752,00:39:50.378
"What preprocessing does is
that we know that we're",00:39:50.378,00:39:52.840
never gonna have unlimited training data.,00:39:54.950,00:39:57.390
"So in order to have our corpus look
more like the true distribution",00:39:57.390,00:40:02.180
"of the English language, what we can do is
quite similar to computer vision we can",00:40:02.180,00:40:07.710
"do this type of data augmentation
technique where we try to replace",00:40:07.710,00:40:12.150
"some words in our corpus
with some other words.",00:40:12.150,00:40:15.410
"So for example,",00:40:15.410,00:40:16.180
"your model during the first pass
you can see a word called New York,",00:40:16.180,00:40:19.740
"the next pass you can see New Zealand,
the next pass you can see New England.",00:40:19.740,00:40:23.890
"So by doing that, you're basically
generating this data by yourself and",00:40:23.890,00:40:28.380
"eventually you achieve
a smoothed out distribution.",00:40:28.380,00:40:32.090
"The reason this happens is
that more frequent word by",00:40:32.090,00:40:35.250
replacing by dropping them.,00:40:35.250,00:40:37.300
"They appear less often and
rarer words by making them appear.",00:40:37.300,00:40:41.340
They appear more often.,00:40:41.340,00:40:43.010
"So a smooth distribution allow us to
learn a better language model and",00:40:43.010,00:40:47.810
"the result is on the, I think is on
the right hand side of you guys.",00:40:47.810,00:40:51.880
"And the left hand side is what happen when
we apply better regularization techniques.",00:40:51.880,00:40:56.900
"So at last we can, wait,
that's it okay, awesome thank you guys.",00:40:58.160,00:41:03.630
"&gt;&gt; All right, now what you'll also see
in these tables is that the default for",00:41:09.101,00:41:14.487
"all these models is an LSTM and that's
exactly what we'll end up very soon with.",00:41:14.487,00:41:21.160
"Which is basically a better
type of recurrent unit.",00:41:21.160,00:41:24.800
"And so, we'll start with gated
recurrent units that were introduced",00:41:25.850,00:41:30.980
by Cho just three years ago.,00:41:30.980,00:41:33.460
"And the main idea is that,
we wanna basically keep around",00:41:33.460,00:41:37.430
"memories that capture long
distance dependencies and",00:41:37.430,00:41:40.470
"you wanna have the model learn when and
how to do that.",00:41:40.470,00:41:44.110
"And with that,
you also allow your error messages to flow",00:41:44.110,00:41:48.030
"differently at different strengths,
depending on the input.",00:41:48.030,00:41:50.250
"So, how does this work?",00:41:51.260,00:41:52.900
What is a GRU as our step to the LSDM?,00:41:52.900,00:41:56.330
"And sometimes you don't need
to go all the way to the LSDM.",00:41:56.330,00:41:58.300
The GRU is a really good model by itself.,00:41:58.300,00:42:00.640
In many cases already in its simpler.,00:42:00.640,00:42:03.030
"So let's start with our standard
recurrent neural network,",00:42:03.030,00:42:06.560
"which basically computes our hidden
layer at the next time step directly.",00:42:06.560,00:42:11.780
"So we just have again previous hidden
state recurring to our vector that's it.",00:42:11.780,00:42:16.990
Now instead what we'll do for,00:42:16.990,00:42:19.180
"gated recurring units or GRUs,
is we'll compute to gates first.",00:42:19.180,00:42:24.300
"These gates are also just like ht,
continuous vectors of the same",00:42:24.300,00:42:30.340
"length as the hidden state, and
they are computed exactly the same way.",00:42:30.340,00:42:36.040
"And here, it's important to note that
the superscripts that's just basically",00:42:36.040,00:42:40.930
"are lined with the kind of
gate that you're computing.",00:42:40.930,00:42:44.210
"So we'll compute a so
called update gate and a reset gate.",00:42:44.210,00:42:48.400
"Now the inside here is
the exact same thing but",00:42:49.830,00:42:52.450
"is important to note that we
have here a sigmoid function.",00:42:52.450,00:42:55.780
"So we'll have elements of this vector
are exactly between zero and one.",00:42:55.780,00:42:59.860
"And we could interpret them as
probabilities if we want to.",00:42:59.860,00:43:03.010
"And it's also important to note that
the super scripts here are different.",00:43:04.610,00:43:07.060
"So the update gate of course,",00:43:07.060,00:43:09.150
"uses a different set of
weights to the reset gate.",00:43:09.150,00:43:13.220
"Now why are they called update and
reset gates, and how do we use them?",00:43:13.220,00:43:16.230
It's relatively straight forward.,00:43:16.230,00:43:18.630
"We just introduced one new function
here just the element wise product.",00:43:18.630,00:43:25.250
We've remember it from back propagation.,00:43:25.250,00:43:27.280
"We also call it the Hadamard
product sometimes.",00:43:27.280,00:43:29.020
"Where we just element wise multiply
this vector here from the reset",00:43:29.020,00:43:33.930
"gate with this,
which would be our new memory content.",00:43:33.930,00:43:39.056
"We call it ht,
this is our intermediate memory content,",00:43:39.056,00:43:43.290
"it has the standard tanh that
we also know as a [INAUDIBLE].",00:43:43.290,00:43:46.700
"This part here is exactly the same,
we just have to input our word vector and",00:43:46.700,00:43:51.250
then transformed with a W.,00:43:51.250,00:43:54.040
But what's going on in here?,00:43:54.040,00:43:56.347
"So intuitively right, this is just a long
vector of numbers between zero and one.",00:43:56.347,00:44:01.500
"Now intuitively,
if this reset gate at a certain unit,",00:44:02.530,00:44:07.920
"is around zero,
then we essentially ignore all the past.",00:44:07.920,00:44:12.950
"We ignore that entire computation
of the past, and we're just going",00:44:12.950,00:44:17.430
"to define that element where our zero,
with the current word vector.",00:44:17.430,00:44:22.540
Now why would we want to do that?,00:44:22.540,00:44:23.790
What's the intuition here?,00:44:23.790,00:44:25.270
"Let's take the task of sentiment analysis
cuz it's very simple and intuitive.",00:44:25.270,00:44:28.960
"If you were to say, you're talking
about a plot of a movie review.",00:44:30.040,00:44:35.750
"And you talk about the plot and
you know some girl falls in love for",00:44:35.750,00:44:38.750
"some guy who falls in love with her but
then they can't meet, blah, blah, blah.",00:44:38.750,00:44:41.850
"That's a long plot and in the end you say,
but the movie was really boring.",00:44:41.850,00:44:46.205
"Then really doesn't matter that
you keep around that whole plot.",00:44:47.900,00:44:51.060
"You wanna say boring as a really
negative strong word for sentiments, and",00:44:51.060,00:44:55.870
"you wanna basically be able to allow the
model to ignore the previous plot summary.",00:44:55.870,00:45:02.322
"Cuz for the task of sentiments
analysis it's irrelevant.",00:45:02.322,00:45:06.930
"Now this is essentially what
the reset gate will let you do, but",00:45:06.930,00:45:10.090
"of course not in this global fashion,",00:45:10.090,00:45:11.700
"where you update the entire hidden state,
but in a more subtle way, where you learn",00:45:11.700,00:45:17.290
"which of the units you actually will reset
and which ones you will keep around.",00:45:17.290,00:45:21.610
"So this will allow some
of the units to say,",00:45:22.860,00:45:25.020
"well maybe I want to be a plot unit and
I will keep around the plot.",00:45:25.020,00:45:28.410
"But other units learn, well if I see one
of the sentiment words, I will definitely",00:45:28.410,00:45:33.330
"set that reset gate to zero and I will
now make sure that I don't wash out,",00:45:33.330,00:45:37.892
"the content with previous stuff
by summing these two, right?",00:45:37.892,00:45:44.310
"You're sort of like, not quite
averaging but you're summing the two.",00:45:44.310,00:45:47.070
"So you wash out the content
from this word and",00:45:47.070,00:45:50.380
"instead it will set that to zero and take
only the content from that current word.",00:45:50.380,00:45:54.410
"Now the final memory it will compute,
we'll combine this with the update gate.",00:45:57.820,00:46:03.592
"And the update gate now,
there's something similar but",00:46:03.592,00:46:06.860
"basically allows us to keep around
only the past and not the future.",00:46:08.090,00:46:13.060
Or not the current time steps.,00:46:13.060,00:46:14.400
"So intuitively here when you look at Z,
if Z is a vector of all ones,",00:46:14.400,00:46:20.698
"then what we would do is
essentially do ht = ht-1",00:46:20.698,00:46:25.424
"+ 1-1 is 0, so this term just falls away.",00:46:25.424,00:46:30.800
"Basically if zt was all ones we could
just copy over our previous time step.",00:46:30.800,00:46:37.170
"Super powerful,
if you copied over the previous time step",00:46:37.170,00:46:40.060
"you have no vanishing gradient problem,
right.",00:46:40.060,00:46:42.530
Your vector just gets a bunch of ones.,00:46:42.530,00:46:45.100
"Nothing changes in your
gradient computation.",00:46:45.100,00:46:47.720
"So that's very powerful and intuitively
you can use that same sentiment example.",00:46:47.720,00:46:52.590
"But you say in the beginning man,
I love this movie so much,",00:46:52.590,00:46:55.410
here's this beautiful love story.,00:46:55.410,00:46:57.960
"And now you go through the love story,
and really what's important for",00:46:57.960,00:47:01.330
"sentiment is not about the love story,
but it's about the person saying,",00:47:01.330,00:47:05.190
I love this movie a lot.,00:47:05.190,00:47:06.630
"And you wanna make sure you
don't lose that information.",00:47:06.630,00:47:09.780
"And with the standard
recurring neural network,",00:47:09.780,00:47:11.580
"we update our hidden state,
every time, every word.",00:47:11.580,00:47:14.740
"No matter how unimportant a word is,
we're gonna sum up those two vectors,",00:47:14.740,00:47:18.350
"washing out the content as we
move further and further along.",00:47:18.350,00:47:21.890
"Here we can decide, and what's even
more amazing, you don't have to decide.",00:47:21.890,00:47:25.430
"You can say, this word is positive, so
I'm gonna set my reset gate manually.",00:47:25.430,00:47:28.870
"No, the model will learn when to reset and
when to update.",00:47:28.870,00:47:32.190
"So this is a very simple kind of
modification but extremely powerful.",00:47:33.650,00:47:38.786
"Now, we're gonna go through it and
explain it a couple more times.",00:47:41.920,00:47:44.980
And we'll try to.,00:47:44.980,00:47:46.510
"Have an attempt here at
a clean illustration.",00:47:46.510,00:47:49.320
"Honestly, personally, I feel the equations
here are still straight forward, and",00:47:49.320,00:47:52.930
"very intuitive, that I don't know
if these illustrations always help,",00:47:52.930,00:47:55.900
"but some people like
them more than others.",00:47:55.900,00:47:59.950
"So intuitively here, you basically
see that only the final memory,",00:47:59.950,00:48:05.300
"that you computed is the one that's
actually used as input to the next step.",00:48:05.300,00:48:08.880
"So all of these are only
modifying through the final state.",00:48:08.880,00:48:14.360
"And now this one gets as input to
our reset gate or update gate,",00:48:14.360,00:48:18.270
"the intermediate state and
the final state of the memory.",00:48:18.270,00:48:22.330
"And so does our x vector the word vector
here also gets its input through the reset",00:48:22.330,00:48:27.199
"gate, the update gate, and
our intermediate memory state.",00:48:27.199,00:48:30.989
"And then, I tried to use this,
so the dotted line here,",00:48:30.989,00:48:35.674
"as basically gates that modify
how these two interact.",00:48:35.674,00:48:40.380
"All right, so I've said, I think,
most of these things already, but",00:48:43.300,00:48:48.297
"again, reset gate here is close to 0.",00:48:48.297,00:48:51.490
We ignore our previous state.,00:48:51.490,00:48:53.757
"And that again, allows the model,
in general, to drop information that is",00:48:53.757,00:48:57.562
"irrelevant for the future
predictions that it wants to make.",00:48:57.562,00:49:01.460
"And if we update the gate z controls,",00:49:01.460,00:49:04.670
"how much of the past state should
matter at the current time stamp?",00:49:04.670,00:49:07.730
"And again, this is a huge improvement for
the vanishing gradient problem,",00:49:09.240,00:49:12.040
"which allows us to actually train these
models on nontrivial, long sequences.",00:49:12.040,00:49:15.850
Any questions around the GRU?,00:49:19.524,00:49:21.198
Yep?,00:49:21.198,00:49:21.698
"Does it matter if you reset first or
update first?",00:49:25.309,00:49:27.300
"Well, so you can't compute
h until you have h tilled.",00:49:27.300,00:49:31.830
So the order of these two doesn't matter.,00:49:31.830,00:49:34.210
"You can compute that in peril, but
you first have to compute h tilled",00:49:34.210,00:49:39.050
"with the reset gate before
you can compute that one.",00:49:39.050,00:49:40.880
"So the question is,
does it matter to switch and",00:49:54.749,00:49:57.679
"use an equation like this first,
and then an equation like that?",00:49:57.679,00:50:01.727
I guess it's just a different model.,00:50:01.727,00:50:06.040
"It's not one that I know
of people having tried.",00:50:06.040,00:50:09.470
"It's not super unreasonable,
I don't see a sort of reason why",00:50:10.790,00:50:15.420
"it would be illogical to ever to that,
but yeah, just not the GRU model.",00:50:15.420,00:50:20.350
"You will actually see,",00:50:22.560,00:50:24.026
"in [INAUDIBLE] she has a paper on a Search
Space Odyssey type paper where there",00:50:24.026,00:50:28.647
"are a thousand modifications you can
make to the next model, the LSTM.",00:50:28.647,00:50:33.070
"And people have tried a lot of them,
and it's not trivial.",00:50:33.070,00:50:36.315
There are a lot of modifications.,00:50:36.315,00:50:37.974
"And a lot of times they
seem kind of intuitive, but",00:50:37.974,00:50:41.256
"don't actually change performance that
much across a bunch of different tasks.",00:50:41.256,00:50:46.660
"But sometimes, one modification improves
things a tiny bit on one of the tasks.",00:50:46.660,00:50:50.988
"It turns out the final model of GRU here
and the LSTM, are actually incredibly",00:50:50.988,00:50:54.798
"stable, they give good performance
across a lot of different tasks.",00:50:54.798,00:50:58.270
"But it can't ever hurt to, if you have
some intuition of why you want to have,",00:51:00.080,00:51:04.110
"make something different,
it can't hurt to try.",00:51:04.110,00:51:06.300
"So the question is,
is it important of how they're computed?",00:51:20.126,00:51:24.005
"I think there are some people who have
tried once to have a two layer neural",00:51:24.005,00:51:26.878
network to compute.,00:51:26.878,00:51:28.125
"These a z and update, z and r.",00:51:28.125,00:51:30.185
"In general, it matters of course
a lot of how they're computed, but",00:51:30.185,00:51:34.049
"not in the sense that you have to
modify them manually or something.",00:51:34.049,00:51:37.860
"It just the model learns when to
update and when not to update.",00:51:37.860,00:51:41.070
That's a good question.,00:52:01.876,00:52:03.125
So what do I mean when I say unit.,00:52:03.125,00:52:05.395
"So in general, what you'll observe in
a slide that's coming up very soon is",00:52:05.395,00:52:10.800
"that we will kind of abstract away from
the details of what these equations are.",00:52:10.800,00:52:16.470
And we're going to write that just ht,00:52:16.470,00:52:22.243
equals GRU of xt and ht minus 1.,00:52:22.243,00:52:27.280
"And then we'll just say that GRU
abbreviation means all these other things,",00:52:27.280,00:52:32.530
"all these equations, and
we're going to abstract away from that.",00:52:32.530,00:52:35.620
"And that's something that you'll see even
more in subsequent lectures where you",00:52:35.620,00:52:40.210
"just say a whole recurrent
network with a five layer GRU and",00:52:40.210,00:52:44.980
"combine lots of different
ways is just one block.",00:52:44.980,00:52:48.520
"We often see this in computer vision too
where CNNs are now just like the CNN",00:52:48.520,00:52:51.683
"block, and you assume you've got
a feature vector out at the end.",00:52:51.683,00:52:54.555
"And people will start abstracting
away more and more from that.",00:52:54.555,00:52:58.100
"But yeah,
you'll always have to remember that, yes,",00:52:58.100,00:53:00.520
"there's a lot of complexity
inside that unit.",00:53:00.520,00:53:04.230
"Here's another attempt at an illustration
which I'm even less of a fan of,",00:53:04.230,00:53:09.240
then the one I tried to come up with.,00:53:09.240,00:53:11.090
"Basically, how you have your z gate
that kind of can jump back and forth.",00:53:12.160,00:53:17.020
"But of course,
it's usually a continuous type thing.",00:53:17.020,00:53:19.650
"It's not a zero one type thing, so I'm not
a big fan of this kind of illustration.",00:53:19.650,00:53:27.036
"And so in terms of derivatives,",00:53:27.036,00:53:28.877
"we couldn't theory asks you to
derive all the details of the GRU.",00:53:28.877,00:53:33.250
"And the only change here is that we now
have the derivative of these element",00:53:33.250,00:53:38.120
"wise multiplications,
both of which I have parameters or inside.",00:53:38.120,00:53:43.060
"And we all should know what
derivative of this is, and",00:53:43.060,00:53:47.357
"the rest is again,
the same kind of chain rule.",00:53:47.357,00:53:51.190
"But again, now you're sort of realizing
why we wanna modularized this more and",00:53:51.190,00:53:55.965
"more, and abstract a way from actually
manually taking these instead having",00:53:55.965,00:54:00.600
error messages and deltas sent around.,00:54:00.600,00:54:03.270
Yeah?,00:54:03.270,00:54:03.770
Explain why we have both update and reset.,00:54:08.872,00:54:12.130
"So basically, it helps the model
to have different mechanisms for",00:54:12.130,00:54:17.570
"when to memorize something and
keep it around, versus when to update it.",00:54:17.570,00:54:22.650
"You're right, in theory, you could try to
put both of those into one thing, right?",00:54:22.650,00:54:27.660
"In theory, you'd say, well,
if this was just my previous ht here,",00:54:27.660,00:54:35.921
"then this could say, well, I wanna keep
it around, or I wanna update it here.",00:54:35.921,00:54:40.730
"But now, this update here,",00:54:40.730,00:54:42.500
"if you just had an equation like this it
would be still be a sum of two things.",00:54:42.500,00:54:46.610
"So that means that xt here
does not have complete control",00:54:46.610,00:54:52.030
"over modifying the current
hidden state in its entirety.",00:54:52.030,00:54:57.040
"It would still be summed
up with something else,",00:54:57.040,00:54:59.200
"and that happens at
every single time stamp.",00:54:59.200,00:55:01.270
"So its only once you have
this reset gates are here.",00:55:01.270,00:55:04.165
"These reset gates are here,
that you would allow h",00:55:04.165,00:55:09.850
"to be completely dominated by the current
word vector, if the model so chooses.",00:55:09.850,00:55:14.020
"If the reset gates are all,
Okay, so if these are all ones,",00:55:21.850,00:55:29.857
"then you have here basically a standard
recurrent neural network type equation.",00:55:29.857,00:55:36.220
"And then if you just have zs, all 0s,",00:55:36.220,00:55:39.220
"then you take that exact equation and
you're right.",00:55:39.220,00:55:41.780
Then you just have a standard RNN.,00:55:41.780,00:55:43.610
"It's also beautiful,
it's always nice to say my model",00:55:43.610,00:55:46.120
"Is a more general form of your model or-
&gt;&gt; [LAUGH]",00:55:46.120,00:55:48.903
"&gt;&gt; An opposite,",00:55:48.903,00:55:49.438
you're model's a special case of my model.,00:55:49.438,00:55:51.139
"It was actually a couple years ago
that you could by and say that.",00:55:51.139,00:55:56.865
"&gt;&gt; [LAUGH]
&gt;&gt; It's good machine learning banter.",00:55:56.865,00:55:59.670
"So yeah, it's always good.",00:56:01.490,00:56:02.590
"And likewise, the inventor of this model
made exactly that statement about the GRU.",00:56:02.590,00:56:08.860
"Not knowing why anybody
had to publish a new paper",00:56:08.860,00:56:12.040
"about this instead of just referring to
this and the special cases of the LSTM.",00:56:12.040,00:56:17.300
"So if we have one more
question about the GRU, yeah?",00:56:17.300,00:56:19.860
&gt;&gt; Is there a reason.,00:56:19.860,00:56:21.741
&gt;&gt; Good question.,00:56:25.464,00:56:26.960
Why tanh and sigmoid?,00:56:26.960,00:56:27.622
"So in theory, you could say the tan h
here could be a rectified linear unit or",00:56:27.622,00:56:32.353
other kind of unit.,00:56:32.353,00:56:33.740
"In practice, you do want sigmoids here
because you have this plus 1 minus that.",00:56:33.740,00:56:39.459
"And so if they're all over the place then
everything will kind of be modified and",00:56:39.459,00:56:43.663
"it's less intuitive that you kind of have
a hard reset in sort of a hard sort of,",00:56:43.663,00:56:47.802
"yeah, hard reset or a hard update.",00:56:47.802,00:56:49.710
And if this wasn't 10h and,00:56:50.910,00:56:53.121
"was rectified linear unit then these
two might be all over the place too and",00:56:53.121,00:56:58.834
"it might be kind of easy to potentially
have the sum also the not very synthecal.",00:56:58.834,00:57:05.025
"But at the same time,",00:57:05.025,00:57:06.375
"it's not unreasonable to try having
a rectified learning unit here.",00:57:06.375,00:57:11.050
"And maybe, if you combine it with
proper regularization and so on,",00:57:11.050,00:57:13.980
"you could get away with other kinds
of other kinds of linearities.",00:57:13.980,00:57:17.645
"That's unlike probabilistic
graphical models for",00:57:17.645,00:57:20.093
certain things just make no sense.,00:57:20.093,00:57:21.766
"And you can't do them, deep learning
you can often try some things and",00:57:21.766,00:57:25.996
"sometimes even nonsensical
things surprisingly work.",00:57:25.996,00:57:29.450
"And then other people try to analyse why
that was the case in the first place.",00:57:29.450,00:57:34.343
"But yeah, there's no mathematical reasons
why you couldn't at all have a rectified",00:57:34.343,00:57:37.871
linear unit here.,00:57:37.871,00:57:38.660
"All right, now on to a even more
complex sort of overall recurrent unit.",00:57:42.117,00:57:48.638
"Namely the long-short-term-memories or
LSTMs.",00:57:48.638,00:57:53.022
"So now this is the hippest
model of the day, and",00:57:53.022,00:57:56.922
it's pretty important to know it well.,00:57:56.922,00:58:00.710
"Fortunately, it's again very similar
to the kinds of basic building blocks.",00:58:00.710,00:58:03.766
"But now we allow each of
the different steps to have again,",00:58:03.766,00:58:08.481
we separate them out even more.,00:58:08.481,00:58:11.840
So how do we separate them out?,00:58:11.840,00:58:13.130
"Basically, this is what's
going on at each time step.",00:58:13.130,00:58:15.925
"We will have an input gate,
forget gate, output gate, memory cell,",00:58:15.925,00:58:20.095
"final memory, and a final hidden state.",00:58:20.095,00:58:22.583
"Now let's gain a little
bit of intuition and",00:58:22.583,00:58:24.814
"there is good intuition of
why we want any of them.",00:58:24.814,00:58:27.358
"So the input gate will
basically determine how much we",00:58:27.358,00:58:31.621
will care about the current vector at all.,00:58:31.621,00:58:35.043
"So how much does the current cell or
the current input word vector matter?",00:58:35.043,00:58:39.726
"The forget gate is a separate mechanism
that just says maybe I should forget,",00:58:39.726,00:58:44.194
maybe I don't.,00:58:44.194,00:58:45.166
"In this case here, just kind of
counterintuitive sometimes and",00:58:45.166,00:58:48.218
"they're actually different
models in the literatures.",00:58:48.218,00:58:51.020
"Some have the one minus there and
others don't.",00:58:51.020,00:58:53.543
"But in general here,
we'll define our forget gate.",00:58:53.543,00:58:55.980
If it's 0 then we're forgetting the past.,00:58:55.980,00:58:58.220
"Then we have an output gate,
basically when you have this output gate,",00:59:00.010,00:59:04.560
"you will separate out what
matters to a certain prediction",00:59:04.560,00:59:09.775
"versus what matters to being kept around
over the current recurrent time steps.",00:59:09.775,00:59:16.030
"So you might say at
this current time step,",00:59:16.030,00:59:19.054
"this particular cell is not important,
but it will become important later.",00:59:19.054,00:59:24.515
"And so I'm not going to output it,
to my final softmax for instance, but",00:59:24.515,00:59:28.580
I'm still gonna keep it around.,00:59:28.580,00:59:30.460
"So it's yet another separate
mechanism to learn when to do that.",00:59:31.470,00:59:35.020
"And then we have our new memory cell here,
which is similar to what we had before.",00:59:36.250,00:59:41.241
"So in fact all these four here
have the same equation inside and",00:59:41.241,00:59:45.323
"just three sigmoid non linearity and
one tan h non linearity.",00:59:45.323,00:59:49.420
"So these are all just four
single layer neural nets.",00:59:51.050,00:59:54.770
"Now we'll put all of these gates together
when we compute the memory cell and",00:59:56.710,01:00:01.660
the final hidden state.,01:00:01.660,01:00:02.620
"So the final memory cell now
basically separated out the input and",01:00:02.620,01:00:06.260
the forget gate.,01:00:06.260,01:00:07.680
"Instead of just c and 1 minus c,
we have two separate mechanisms",01:00:07.680,01:00:10.920
"that can be trained and
learn slightly different things.",01:00:10.920,01:00:14.110
"And actually become also in some ways
counter intuitive like you say, I don't",01:00:14.110,01:00:18.817
"wanna forget but you do wanna forget,
but you also input something right now.",01:00:18.817,01:00:23.546
But the model turns out to work very well.,01:00:23.546,01:00:28.771
"So basically here we have final hidden
state is just to forget gate how to",01:00:28.771,01:00:33.571
"mark product with the previous hidden
states final memory cell ct-1.",01:00:33.571,01:00:38.464
"So this again will determine,
how much do you wanna keep this around or",01:00:38.464,01:00:42.433
how much do we wanna forget from the past?,01:00:42.433,01:00:45.600
"And then the new memory cell here,
this has a standard recurrent neural net.",01:00:45.600,01:00:50.250
"If i is all 1s,
then we really keep the input around.",01:00:50.250,01:00:55.720
"And if the input gate says no,
this one doesn't matter,",01:00:55.720,01:00:59.211
"then you just basically ignore
the current word back there.",01:00:59.211,01:01:02.946
"So in that sense,
this equation is quite intuitive, right?",01:01:06.475,01:01:09.500
"Forget the past or not, take the input or
not, that's basically it, yeah?",01:01:09.500,01:01:16.694
"So the secret question,",01:01:19.751,01:01:20.469
"once you forget the past does it mean
you forget grammar or something else?",01:01:20.469,01:01:23.996
"And the truth is we can think of these
forget gates as sort of absolutes.",01:01:23.996,01:01:30.020
"They're all vectors, and",01:01:30.020,01:01:31.843
"they will all forget only certain
elements of a long hidden unit.",01:01:31.843,01:01:36.208
"And so really, I can eventually show
you what these hidden states look like.",01:01:36.208,01:01:42.930
"And sometimes they're actually
more intuitive than others.",01:01:42.930,01:01:46.030
"But it's rare that you would find this
particular unit when it was turned off or",01:01:46.030,01:01:50.280
"on actually had like this
perfect interpretation that",01:01:50.280,01:01:53.410
"we as humans find intuitive and
think of as grammar.",01:01:53.410,01:01:56.542
"And also of course grammar is
a very complex kind of beast.",01:01:56.542,01:02:00.760
"And so it's hard to say any single unit
would capture any particular like entirety",01:02:00.760,01:02:05.327
"of a grammar,
it might only capture certain things.",01:02:05.327,01:02:08.320
"So it's not implausible to think
of these three cells together",01:02:08.320,01:02:12.530
"suggest that the next noun should be
a plural noun or something like that.",01:02:12.530,01:02:16.240
"But that's the most we could hope for
in many cases.",01:02:16.240,01:02:18.422
"All right, and then here,
the final hidden state again,",01:02:21.624,01:02:24.662
"we can keep these cs around, right?",01:02:24.662,01:02:26.589
"And cs will compute our
computer from other cs.",01:02:26.589,01:02:30.370
"But we might not want to expose
the content of this memory cell",01:02:30.370,01:02:34.840
"in order to compute the final
hidden state, ht minus 1.",01:02:34.840,01:02:38.460
"All right, now yeah,
this is it, this is the LSTM.",01:02:43.175,01:02:46.878
"It's a really powerful model, are there
any questions around the equations?",01:02:46.878,01:02:50.240
"We're gonna attempt at some illustrations,
but",01:02:50.240,01:02:54.150
"again I think the equations
are sometimes more intuitive.",01:02:54.150,01:02:59.283
"Does the LSTM and GRU completely liviate
or just help with an engine came problem?",01:03:03.853,01:03:09.870
"And the truth is they helped with it a
lot, but they don't completely obviate it.",01:03:09.870,01:03:13.741
"You do multiply here a bunch of
numbers that are often smaller than 1.",01:03:13.741,01:03:18.420
"And over time even if it would
have to be a perfect one,",01:03:18.420,01:03:23.620
"but that would mean that, that unit
is really, really strongly active.",01:03:23.620,01:03:26.680
"And then it's hard to sort of dies,
it's like the gradient,",01:03:26.680,01:03:31.467
"when you have unit that's really, really
active and looks something like this.",01:03:31.467,01:03:37.837
"Now the input is really large
to that unit and it's here,",01:03:37.837,01:03:41.314
"then grade in around here,
It's pretty much 0.",01:03:41.314,01:03:44.170
So that unit's kind of dead.,01:03:44.170,01:03:45.510
"And then the model can't do
anything with it anymore.",01:03:45.510,01:03:47.380
"And so it happens, there are,
when you want to train these,",01:03:47.380,01:03:50.610
"you'll observe some units just sort
of die after training after awhile.",01:03:50.610,01:03:53.830
"And you'll just sort of keep around stuff,
or delete stuff at each time step.",01:03:53.830,01:03:58.340
"But in general most of the units
are somewhat small than 1, and",01:03:58.340,01:04:03.560
"so you still have a bit of a vanishing
creating problem but much less so.",01:04:03.560,01:04:10.280
"And intuitively you can
come up with final P for",01:04:10.280,01:04:13.030
"a lot of good ways to
think about this right?",01:04:13.030,01:04:17.170
"Maybe you want to predict different
things at different time steps.",01:04:17.170,01:04:19.960
"But you wanna keep around knowledge
through the memory cells but",01:04:19.960,01:04:25.150
not expose it at a given prediction.,01:04:25.150,01:04:27.290
Yeah.,01:04:27.290,01:04:27.790
"What is the point of the exposure gate
when it already had the forget gate?",01:04:32.166,01:04:34.470
"So basically, you want to,",01:04:34.470,01:04:37.970
"sort of forget gate will tell you whether
you keep something around or not.",01:04:37.970,01:04:40.770
"But exposure gate, will mean, does it
matter to this current time step or not.",01:04:41.780,01:04:45.990
So you might not wanna forget something.,01:04:47.000,01:04:48.968
"But you also might not wanna
show it to the current output,",01:04:48.968,01:04:51.783
because it's irrelevant for that output.,01:04:51.783,01:04:53.920
"And it would just confuse the Softmax
classifier at that output.",01:04:53.920,01:04:57.453
Yeah?,01:05:00.749,01:05:01.485
"Does the exposure gate help you, or
do you mean the output gate here, right?",01:05:11.171,01:05:16.620
"So does the output gate,
does it help you to what exactly?",01:05:16.620,01:05:19.615
To not have to forget everything forever.,01:05:22.221,01:05:24.404
"So, in some ways, yes.",01:05:28.493,01:05:29.880
"You can basically,
this model could decide that,",01:05:29.880,01:05:33.710
"while it doesn't wanna give as
output something for a long time.",01:05:33.710,01:05:38.170
"And hence it's basically
a temporal forgetting, right?",01:05:38.170,01:05:42.660
"It will only be forgotten at that time
set but actually be kept around in.",01:05:42.660,01:05:46.240
"I don't wanna use,
like anthropomorphize the models, but",01:05:46.240,01:05:49.660
"like the subconsciousness of this model or
whatever, right?",01:05:49.660,01:05:52.280
Keeps it around but doesn't expose it.,01:05:52.280,01:05:55.080
Don't quote me on that.,01:05:55.080,01:05:55.828
"All right, one last question, yeah?",01:06:00.521,01:06:02.025
"The initialization to all these models
matters, it matters quite significantly.",01:06:07.252,01:06:10.750
"So, if you initialize all your weights,
for instance such that",01:06:10.750,01:06:13.770
"whatever you do in the beginning,
all of the weights are super large.",01:06:13.770,01:06:16.760
"Then your gradients are zero and
you're stuck in the optimization.",01:06:16.760,01:06:19.450
"So you always have to
initialize them properly.",01:06:19.450,01:06:23.490
"In most cases, as long as they're
relatively small, you can't go too wrong.",01:06:23.490,01:06:27.330
"Eventually, it might slow down
your eventual convergence, but",01:06:27.330,01:06:30.100
"as long as all your parameters, W here,",01:06:30.100,01:06:31.980
"and your word vectors and so
on are initialized to very small numbers.",01:06:31.980,01:06:35.310
"It will usually eventually
do it pretty well.",01:06:35.310,01:06:37.370
"Yes you could use lots of different
strategies for initialization.",01:06:40.421,01:06:43.190
"All right, now, some visualizations.",01:06:44.270,01:06:46.390
"I like this one from Chris Olah on
his blog from not too long ago.",01:06:46.390,01:06:50.800
"But again, I don't know.",01:06:50.800,01:06:53.260
"I feel like the equations speak mostly for
themselves.",01:06:53.260,01:06:55.890
You can think of these.,01:06:55.890,01:06:57.000
"I have four different neural network
layers, and then you combine them in",01:06:57.000,01:07:00.840
"various ways with pointwise operations,
such as multiplication or addition.",01:07:00.840,01:07:05.890
"And sometimes you know multiplication and
then addition,",01:07:05.890,01:07:08.080
and concatenation and copies and so on.,01:07:08.080,01:07:11.340
"But, In the end you often observe,",01:07:11.340,01:07:13.500
"this kind of thing where we'll
just write LSTM in this block.",01:07:13.500,01:07:17.590
"And has an X and an H, and",01:07:17.590,01:07:19.354
"we don't really look into too many
details of what's going on there.",01:07:19.354,01:07:24.578
"And here's some, I think, even less
helpful [LAUGH] illustrations that,",01:07:26.498,01:07:31.699
"yeah, I think are mostly
confusing to a lot of people.",01:07:31.699,01:07:35.740
"I have the forget gates here,
output gates, input gates, and so on.",01:07:35.740,01:07:39.180
"But and your memory cells as
they try to modify each other.",01:07:39.180,01:07:45.530
This one is a little cleaner.,01:07:45.530,01:07:47.200
"You know you have some inputs, your gates,",01:07:47.200,01:07:49.000
"you have your forget gates on top
of your memory cell and so on.",01:07:49.000,01:07:54.100
"But in general I think the equations
are actually quite intuitive, right?",01:07:54.100,01:07:57.510
"If you think of your extremes,",01:07:57.510,01:07:58.910
"if this is zero, one, then this
input matters more to the output.",01:07:58.910,01:08:03.320
"All right, now as I said,
LSTMs, currently super hip.",01:08:05.890,01:08:09.820
"The en vogue model are for pretty
much all sequence labeling tasks and",01:08:09.820,01:08:14.450
"sequence to sequence tasks
like machine translation.",01:08:14.450,01:08:17.430
"Super powerful in many cases, you will
actually observe that we'll stack them.",01:08:17.430,01:08:21.740
"So just like the other RNN architectures,
we'll have a whole LSTM block and",01:08:21.740,01:08:26.510
"we put another LSTM block with different
sets of parameters on top of it.",01:08:26.510,01:08:30.660
"And then the parameters
are shared over time, but",01:08:30.660,01:08:33.740
"are different as you
have a very deep model.",01:08:33.740,01:08:36.240
"And, of course, with all these
parameters here, we have essentially",01:08:37.360,01:08:42.250
"many more parameters then the standard
recurrent neural network.",01:08:42.250,01:08:45.300
"Where we only have two such parameters and
we update every time.",01:08:45.300,01:08:49.210
"You wanna have more data especially
if you stack you now have",01:08:49.210,01:08:53.050
"10x the parameters of standard RNN,
we wanna train this on a lot of data.",01:08:53.050,01:08:57.730
"And in terms of amount of training
data available machine translation is",01:08:57.730,01:09:01.600
actually one of the best tasks for that.,01:09:01.600,01:09:04.310
"And is also the one where these
model sort of shine the most.",01:09:04.310,01:09:10.920
"And so in 2015, I think the first time I
gave the deep learning for NLP lecture,",01:09:10.920,01:09:15.647
the jury was still a little bit out.,01:09:15.647,01:09:17.790
"The neural network models
came up fairly quickly.",01:09:17.790,01:09:21.330
"But some different, more traditional
machine translation systems",01:09:21.330,01:09:26.580
"were still slightly better,
like by half a BLEU point.",01:09:26.580,01:09:30.640
We haven't defined BLEU scores yet.,01:09:30.640,01:09:33.490
"You can essentially think
of it as an engram overlap.",01:09:33.490,01:09:36.430
"The more your translation overlaps
in terms of unigrams and bigrams and",01:09:36.430,01:09:40.840
"trigrams, the better it likely is, period.",01:09:40.840,01:09:45.870
"So you have this reference translation,
sometimes multiple reference translations.",01:09:45.870,01:09:49.610
"You have your translation, you look
at engram overlap between the two.",01:09:49.610,01:09:52.730
So the higher the better.,01:09:52.730,01:09:53.540
"And basically the neural network
models were often also just use it for",01:09:54.580,01:09:58.350
rescoring traditional MT model.,01:09:58.350,01:10:02.280
"Now, just one year later, last year,",01:10:02.280,01:10:04.980
"really a couple months ago,
the story was completely different.",01:10:04.980,01:10:08.990
"So this is WMT, the worldwide
competition for machine translation.",01:10:08.990,01:10:15.008
"And you have different universities,",01:10:15.008,01:10:18.210
"and different companies and
so on, submit their systems.",01:10:18.210,01:10:22.550
"And the top three systems were all
neural machine translation systems.",01:10:22.550,01:10:28.320
The jury is now basically not out anymore.,01:10:28.320,01:10:31.580
"It's clear neural machine
translation is the most accurate",01:10:31.580,01:10:35.334
machine translation model in the world.,01:10:35.334,01:10:37.917
"Yeah that number two was us, yeah.",01:10:41.427,01:10:43.064
"&gt;&gt; [LAUGH]
&gt;&gt; James Bradbury and me worked on that.",01:10:43.064,01:10:46.819
"James Bradbury was actually a linguistics
undergrad while he was doing that, but",01:10:49.795,01:10:53.576
now he's full-time.,01:10:53.576,01:10:54.684
"So, yeah, basically we haven't talked
that much about ensembling and",01:10:55.880,01:10:59.990
ensembles of different models.,01:10:59.990,01:11:01.610
"But you can also train
five of these monsters and",01:11:01.610,01:11:04.610
"then average all the probabilities and
you'll usually get a little better.",01:11:04.610,01:11:08.580
"We just, as general thing,",01:11:08.580,01:11:09.960
"you'll observe for every competition
machine learning competition out there.",01:11:09.960,01:11:12.990
"If you go on Kaggle, other machine
learning competitions usually train",01:11:12.990,01:11:16.800
even the same kind of model five times.,01:11:16.800,01:11:18.510
"You end up in slightly different
local optimum average,",01:11:18.510,01:11:20.690
and you still do pretty well.,01:11:20.690,01:11:23.050
"What's cool also though,",01:11:23.050,01:11:24.660
"is that while we might not be able
to exactly recover grammar, or",01:11:24.660,01:11:30.200
"have specific units be explicitly sort
of capturing very intuitive things.",01:11:30.200,01:11:37.110
"As we project this down
similar to the word vectors,",01:11:37.110,01:11:39.641
"we actually do observe some
pretty interesting regularities.",01:11:39.641,01:11:42.760
"So this is a paper from Sutskever in 2014,",01:11:42.760,01:11:47.855
they projected different sentences.,01:11:47.855,01:11:52.175
"They were trained basically with
a machine translation task and",01:11:52.175,01:11:57.079
"basically observe quite
interesting regularities.",01:11:57.079,01:12:01.270
"So John admires Mary is close
to John is in love with Mary and",01:12:01.270,01:12:05.419
to John respects Mary.,01:12:05.419,01:12:07.120
"Now of course,",01:12:07.120,01:12:07.695
"we have to be a little carefull here
to not over interpret the amazingness.",01:12:07.695,01:12:10.494
"It's amazing, but
we also have a selection vice here, right?",01:12:10.494,01:12:14.260
"Maybe if we just had
John did admire Mary or",01:12:14.260,01:12:20.080
"something, it might also be close to it,
right?",01:12:20.080,01:12:21.830
"And it might be closer too, but",01:12:21.830,01:12:23.310
"if you just project these six particular
sentences into lower dimensional space.",01:12:23.310,01:12:29.030
"Then you do see very nicely that whenever
John has some positive feelings for",01:12:29.030,01:12:33.790
"Mary, all those sentences are in here.",01:12:33.790,01:12:36.790
"And all the ones that are on this area
of the first two item vectors, Mary",01:12:36.790,01:12:42.410
"admires John, Mary admires John, Mary is
in love with John, and Mary respects John.",01:12:42.410,01:12:46.810
"They're all closer together,",01:12:48.150,01:12:49.150
"which is kind of amazing cuz
some people are also worried.",01:12:49.150,01:12:52.670
"Well it's a sequence model, so",01:12:52.670,01:12:54.160
"how could it ever capture
that the word order changes?",01:12:54.160,01:12:58.370
"And so this is a particularly
cool example of that.",01:12:58.370,01:13:01.288
"So here we have,",01:13:01.288,01:13:02.361
"she was given a card by me in the garden
versus in the garden I gave her a card.",01:13:02.361,01:13:07.202
"And I gave her a card in the garden, and",01:13:07.202,01:13:09.514
"despite the word order being
actually flipped, right?",01:13:09.514,01:13:13.000
"In the garden is in the beginning here,
and in the end here.",01:13:13.000,01:13:16.737
"These are still closer together
than the different ones where,",01:13:16.737,01:13:20.740
"in the garden basically she gave me
a card verses I gave her a card.",01:13:20.740,01:13:25.630
"So that shows that the semantics here
turn out to be more important than",01:13:25.630,01:13:29.481
the word order.,01:13:29.481,01:13:30.405
"Despite the model just
going from left to right or",01:13:30.405,01:13:32.801
"this one was still the trick where we
reversed the order of the input sentence.",01:13:32.801,01:13:37.450
"But it choses that its
incredibly invariant and",01:13:37.450,01:13:41.800
"variance is a pretty important concept,
right?",01:13:41.800,01:13:43.480
"We want this model to be
invariant to simple syntactic",01:13:43.480,01:13:48.100
"changes when the semantics
are actually kept the same.",01:13:48.100,01:13:51.940
"It's pretty incredible, that it does that.",01:13:51.940,01:13:55.313
"So this is also the power
I think of some of these.",01:13:55.313,01:13:58.253
"This is a very deep LSTM model where
you have five different LSTM stacked in",01:13:58.253,01:14:02.578
the encoder and several in the decoder.,01:14:02.578,01:14:04.965
"And they're all connected
in multiple places too.",01:14:04.965,01:14:07.780
"All right, any questions around
those visualizations and LSTMs?",01:14:10.380,01:14:14.109
"All right, you now have knowledge under
you belt that is super powerful and",01:14:19.392,01:14:23.911
very interesting.,01:14:23.911,01:14:25.670
"I expected to maybe have
five minutes more of time.",01:14:25.670,01:14:27.945
"So I'm going to talk to you about a recent
improvement, two recurrent neural networks",01:14:27.945,01:14:31.746
"that I think is also very
applicable to machine translation.",01:14:31.746,01:14:34.467
"But nobody has actually yet
applied it to machine translation.",01:14:34.467,01:14:37.625
"And that is a general problem
with all softmax classification",01:14:37.625,01:14:42.245
"that we do in all the models I've so
far described to you.",01:14:42.245,01:14:44.755
"And really up until two or
three months ago,",01:14:44.755,01:14:46.905
"that everybody in NLP
had as a major problem.",01:14:46.905,01:14:50.170
"And that is you can only ever predict
answers if you saw that exact word at",01:14:50.170,01:14:55.131
training time.,01:14:55.131,01:14:56.295
"And you have your cross entropy error
saying I wanna predict this word.",01:14:56.295,01:15:00.090
"And if you've never predicted that word,
no matter how obvious it is for",01:15:00.090,01:15:03.612
"the translation system it will
not be able to do it, right?",01:15:03.612,01:15:05.950
"So we have some kind of translation,
and let's us say we have a new word,",01:15:05.950,01:15:13.650
"like a new name or something that
we've never seen at training time.",01:15:13.650,01:15:18.120
"And it is very obvious that this word
here should go at this location.",01:15:18.120,01:15:23.223
"This is like Mrs. and
then maybe the new word is like yelling or",01:15:23.223,01:15:26.782
"something like that,
it could be any other word.",01:15:26.782,01:15:29.668
"And now let's say at training time,
we've never seen the word yelling.",01:15:29.668,01:15:32.766
"But now, it's like vowel, German misses,",01:15:32.766,01:15:37.046
"miss in, yeah,
German translation for this.",01:15:37.046,01:15:41.562
"And now it's very obvious to
everybody that after this word,",01:15:41.562,01:15:44.930
"it should be the next one,
the name of the of the miss.",01:15:44.930,01:15:47.865
"And so these models would never
be able to do that, right?",01:15:47.865,01:15:53.050
"And so one way to fix that is to think
about character meant translation models,",01:15:53.050,01:15:58.213
"where the model's actually surprisingly
similar to what we described here.",01:15:58.213,01:16:03.317
"Well many times it have to go, but instead
of having words we just have characters.",01:16:03.317,01:16:09.560
"So that's one way, but
now we have very long sequences.",01:16:09.560,01:16:13.170
"And at every character you have
a lot of matrix multiplications.",01:16:13.170,01:16:18.310
"And these matrix multiplications
that we have in here are not",01:16:18.310,01:16:22.480
"50 dimensional for really powerful MT
models, they're a 1,000 dimensional.",01:16:23.720,01:16:27.420
"And now you have several thousand
by a thousand matrices here",01:16:28.490,01:16:32.644
"multiplying with thousand
dimensional vectors.",01:16:32.644,01:16:36.151
"And you stack them, so
you have to do it five times.",01:16:36.151,01:16:38.999
"Doing that for every single character
actually gets really, really expensive.",01:16:38.999,01:16:42.249
"So at the same time,
it's very intuitive that",01:16:43.540,01:16:47.950
"after we see a new word at test time
we wanna be able to predict it.",01:16:47.950,01:16:51.366
"And also in general when we have
the softmax, even for words that we do",01:16:51.366,01:16:55.022
"see once or twice, it's hard for
the model to then still predict them.",01:16:55.022,01:16:58.817
"It's this skewed data set
distribution problem.",01:16:58.817,01:17:02.230
"But you have very rare, very infrequent
classes, our words are hard to predict for",01:17:02.230,01:17:07.012
the models.,01:17:07.012,01:17:07.773
"So this is one attempt at fixing that,
which is essentially a mixture",01:17:07.773,01:17:13.225
"model of using standard softmax and
what we call a pointer.",01:17:13.225,01:17:18.880
"So what's a pointer?
It's essentially a mechanism to",01:17:18.880,01:17:21.610
"say well maybe my next word is one of
the previous words in the context.",01:17:21.610,01:17:26.484
"You say 100 words in the past,
and every time step you say,",01:17:26.484,01:17:30.675
"maybe I just wanna copy a word
over from the last 100 words.",01:17:30.675,01:17:35.370
"And if not, then I will use my
standard softmax for the rest.",01:17:35.370,01:17:40.998
"So this is kind of this
sentinel idea here.",01:17:40.998,01:17:43.021
"This is a paper by Stephen Merity and
some other folks.",01:17:43.021,01:17:48.570
"And basically, we now have
a mixture model, where we combine",01:17:48.570,01:17:52.870
"the probabilities from the standard
vocabulary and from this pointer.",01:17:52.870,01:17:57.520
And now how do we compute this pointer?,01:17:57.520,01:17:58.835
"It's very straightforward,
we basically have a query.",01:17:58.835,01:18:03.498
"This query is just a modification of
the last hidden layer that we have here.",01:18:03.498,01:18:09.380
"And we pipe that through a standard
single layer neural network",01:18:09.380,01:18:12.260
"to compute another hidden layer,
which we'll call q.",01:18:12.260,01:18:14.910
"And then we'll do an inter
product between this q and",01:18:14.910,01:18:18.328
"all the previous hidden states
of the last 100 timed steps.",01:18:18.328,01:18:22.457
"And that will give us,
basically, the single number for",01:18:22.457,01:18:25.241
each of these interproducts.,01:18:25.241,01:18:26.800
"And then we'll apply
a softmax on top of that.",01:18:26.800,01:18:30.496
"And this gives us essentially,
a probability for",01:18:30.496,01:18:33.582
"how likely do we wanna point
to each of these words.",01:18:33.582,01:18:37.490
"Or the very last one is we
don't point to anything,",01:18:37.490,01:18:41.810
we just take the standard softmax.,01:18:41.810,01:18:43.910
"So we keep one unit
around where we do this.",01:18:43.910,01:18:47.035
"And now of course in the context,
the same word might appear multiple times.",01:18:47.035,01:18:51.412
"And so you just sum up all
the probabilities for specific words.",01:18:51.412,01:18:55.410
"If they appear multiple times,
you just sum them up.",01:18:55.410,01:18:58.980
"With this simple modification, we now
have the ability to predict unseen words.",01:18:58.980,01:19:05.242
"We can predict based on the pattern
of how rare words appear much more",01:19:05.242,01:19:10.112
similar things.,01:19:10.112,01:19:11.490
"For instance, Fed, Chair, Janet,
Yellen, raised, rates and so on, Ms.",01:19:11.490,01:19:16.408
"is very obvious that this is the same Ms.
that we're referring to here.",01:19:16.408,01:19:20.970
"And you can base or
you combine this in this mixture model.",01:19:22.150,01:19:25.540
"And now over many, many years for
language modeling.",01:19:25.540,01:19:28.772
"The perplexity that we defined before
was sort of stock actually around 80.",01:19:28.772,01:19:35.203
"And then in 2015,",01:19:35.203,01:19:36.514
"we have a bunch of modifications
to LSTMs that were very powerful.",01:19:36.514,01:19:40.820
"And lower this, and
now were down to the lowest 70s.",01:19:40.820,01:19:46.060
"And was some modifications
will cover another class,",01:19:46.060,01:19:49.790
were actually down on the 60s now.,01:19:49.790,01:19:51.430
"So it really had to told for
several years, and",01:19:51.430,01:19:53.859
"now perplexity numbers
are really dropping in.",01:19:53.859,01:19:56.429
"And this models are getting better and
bettered capturing, more and",01:19:56.429,01:20:00.805
"more the semantics and
the syntax of language.",01:20:00.805,01:20:03.831
"All right, so let's summarize.",01:20:03.831,01:20:05.180
"Recurrent Neural Networks, super powerful.",01:20:05.180,01:20:07.190
"You now know the best ones in
that family to use in LSTMs.",01:20:07.190,01:20:11.654
"This is a pretty advanced lecture,
I hope you gained some of the intuition.",01:20:11.654,01:20:15.110
"Again, most of the math falls out from the
same basic building blocks we had before.",01:20:15.110,01:20:20.659
"And next week or no next Thursday,
we'll do midterm review.",01:20:20.659,01:20:25.582
"All right, thank you.",01:20:25.582,01:20:26.520

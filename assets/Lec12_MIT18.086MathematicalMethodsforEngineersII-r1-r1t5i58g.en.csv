text,start,stop
"NARRATOR: The
following content is",00:00:00.000,00:00:01.715
"provided by MIT OpenCourseWare
under a Creative Commons",00:00:01.715,00:00:04.690
license.,00:00:04.690,00:00:06.090
"Additional information
about our license",00:00:06.090,00:00:08.230
"and MIT OpenCourseWare
in general",00:00:08.230,00:00:10.490
is available at ocw.mit.edu.,00:00:10.490,00:00:11.930
"PROFESSOR: Finite difference
methods for initial value",00:00:15.480,00:00:17.730
"problems that we're
coming to the end of,",00:00:17.730,00:00:20.770
"and the solving large
systems that we're",00:00:20.770,00:00:24.810
"coming to the beginning of, was
to talk today about matrices,",00:00:24.810,00:00:33.200
"because that language is
just useful for everything.",00:00:33.200,00:00:38.230
"So about the homeworks, Mr.
Cho put in a long weekend",00:00:40.830,00:00:46.410
"grading the homeworks
that were turned in Friday",00:00:46.410,00:00:49.590
"and preparing code
and output to go up",00:00:49.590,00:00:55.420
"on the web page, probably
late tonight or tomorrow.",00:00:55.420,00:00:58.980
"So have a look to see and I
hope that those codes will",00:00:58.980,00:01:06.110
be useful for the future too.,00:01:06.110,00:01:08.470
Right.,00:01:08.470,00:01:09.740
"So we'll maybe say
more about the outputs.",00:01:09.740,00:01:13.430
"And about the projects,
which by the way,",00:01:13.430,00:01:18.110
"could grow out of
that homework or could",00:01:18.110,00:01:21.030
"go in a totally
different direction,",00:01:21.030,00:01:23.250
"I'm thinking that the right
time to say projects due",00:01:23.250,00:01:28.510
would be after spring break.,00:01:28.510,00:01:31.200
"So pretty much nearly
immediately after the spring",00:01:31.200,00:01:36.100
"break would be -- and
we'll talk about it more,",00:01:36.100,00:01:38.440
"but just so you have
an idea of what's --",00:01:38.440,00:01:40.780
what timetable I had in mind.,00:01:40.780,00:01:44.120
"So, matrices then.",00:01:44.120,00:01:45.260
"In particular, finite
difference matrices.",00:01:49.010,00:01:52.780
"So that's the second
difference matrix, K,",00:01:52.780,00:01:58.770
"and I'll frequently use that
letter K as I did in 18.085",00:01:58.770,00:02:03.340
"for that very, very
important and useful matrix.",00:02:03.340,00:02:07.630
So what are its properties?,00:02:07.630,00:02:09.940
It's tridiagonal.,00:02:09.940,00:02:10.730
"That's a very important
property, which we'll see,",00:02:13.360,00:02:16.200
"because that means
that computations",00:02:16.200,00:02:19.410
"solving linear systems are very
fast with a tridiagonal matrix.",00:02:19.410,00:02:25.420
It's symmetric.,00:02:25.420,00:02:27.740
"All its eigenvalues
are positive,",00:02:27.740,00:02:30.660
"so I would say it's
symmetric, positive definite.",00:02:30.660,00:02:34.200
"And those eigenvalues
and eigenvectors --",00:02:34.200,00:02:37.030
"so the eigenvectors for this
matrix turn out to be discrete",00:02:37.030,00:02:42.065
"-- not quite discrete
exponentials, discrete sines,",00:02:42.065,00:02:47.510
discrete sine function.,00:02:47.510,00:02:49.670
"If I change the
boundary conditions,",00:02:49.670,00:02:51.650
"I can get discrete cosines
as the eigenvectors.",00:02:51.650,00:02:54.950
"Or I could get
discrete exponentials",00:02:54.950,00:02:57.100
"as the eigenvectors
by making it periodic.",00:02:57.100,00:03:00.620
"Maybe I'll mention how
to make it periodic",00:03:00.620,00:03:02.710
and then I'll erase it again.,00:03:02.710,00:03:05.390
"To make it periodic means that
this second different centered",00:03:05.390,00:03:09.550
"at point 1 should
look ahead to point 2",00:03:09.550,00:03:13.640
"and look behind to point 0, but
that'll be the same as point n,",00:03:13.640,00:03:17.540
"so I would put a minus 1
in that corner and this one",00:03:17.540,00:03:21.750
"similarly, it looks ahead,
which really brings it around",00:03:21.750,00:03:27.360
"again, since we're
sort of on a circle,",00:03:27.360,00:03:30.080
brings it around again here.,00:03:30.080,00:03:32.470
"So that matrix now, with
minus 1's added in the corner,",00:03:32.470,00:03:36.890
"I would call C, a circular.",00:03:36.890,00:03:39.970
"So I'll leave the
letter K there,",00:03:39.970,00:03:44.390
"but the right letter is C
while these minus 1's are here",00:03:44.390,00:03:49.290
to make it periodic.,00:03:49.290,00:03:50.920
"By the way, they mess
up that the tridiagonal.",00:03:50.920,00:03:54.370
It's no longer tridiagonal.,00:03:54.370,00:03:56.690
"Also, it certainly got
-- it's very sparse.",00:03:56.690,00:03:59.880
"Mentioning sparse
reminds me, if you're",00:04:03.390,00:04:07.640
"coding large matrices
that are sparse,",00:04:07.640,00:04:11.400
"you should let MATLAB
know that they're sparse.",00:04:11.400,00:04:14.640
"So MATLAB has a whole -- it
carries out operations --",00:04:14.640,00:04:19.690
"if you tell it it's
a sparse matrix,",00:04:19.690,00:04:23.050
"then it only operates where the
non-zeros are located and it",00:04:23.050,00:04:29.510
"doesn't waste its time looking
at these 0's, these 0's through",00:04:29.510,00:04:34.640
all the matrix steps.,00:04:34.640,00:04:36.840
"So using sparse MATLAB is
important thing to know about.",00:04:42.300,00:04:47.380
"It's just typically
an s or an sp",00:04:47.380,00:04:49.970
will appear in MATLAB commands.,00:04:49.970,00:04:53.070
"For example, I'll just
maybe fill it in here.",00:04:53.070,00:04:56.700
"What's the sparse
identity matrix?",00:04:56.700,00:04:59.050
"The normal identity
matrix would be",00:04:59.050,00:05:01.300
"eye of n and the sparse identity
matrix is sp, speye of n.",00:05:01.300,00:05:09.620
"Similarly, we would create K --
we could use 2 times speye of n",00:05:09.620,00:05:16.290
"as the diagonal
of K and the two,",00:05:16.290,00:05:20.450
"the upper and lower diagonals --
and we could tell it these two",00:05:20.450,00:05:27.140
entries.,00:05:27.140,00:05:28.670
"Another thing to say -- what
I said I wasn't going to do,",00:05:28.670,00:05:33.730
"I'll do because I hate to see
a K up there while it's not",00:05:33.730,00:05:37.451
right.,00:05:37.451,00:05:37.950
"Two more points
about this matrix.",00:05:41.510,00:05:46.460
It's singular now.,00:05:46.460,00:05:49.060
The determinant is 0.,00:05:49.060,00:05:52.070
"Now, I'm never going to take the
determinant of a giant matrix.",00:05:52.070,00:05:57.200
That's a bad thing to do.,00:05:57.200,00:05:59.810
"Much better to recognize that
there's a vector, x, let's say,",00:05:59.810,00:06:08.620
and it's the vector of all 1's.,00:06:08.620,00:06:11.010
It's the vector of n 1's.,00:06:11.010,00:06:13.040
"If you imagine multiplying this
matrix by the vector of n 1's,",00:06:13.040,00:06:17.130
what do you get?,00:06:17.130,00:06:19.720
You get all 0's.,00:06:19.720,00:06:22.760
"So that vector of all
1's is in the null space,",00:06:22.760,00:06:26.770
"I would say, of a matrix.",00:06:26.770,00:06:28.030
"Null space is just
the vectors that",00:06:28.030,00:06:30.510
get wiped out by the matrix.,00:06:30.510,00:06:33.440
C*x is all 0's.,00:06:33.440,00:06:36.080
"So that vector -- this matrix
has some nonzero vectors in its",00:06:36.080,00:06:42.550
null space.,00:06:42.550,00:06:44.040
"I know right away then,
its determinant is 0.",00:06:44.040,00:06:47.290
"So the determinant
of that is 0 and now",00:06:47.290,00:06:51.470
"that would tell me something
about the eigenvalues",00:06:51.470,00:06:54.880
of the matrix.,00:06:54.880,00:06:56.020
"It tells me about
one eigenvalue.",00:06:56.020,00:06:59.400
It's 0.,00:06:59.400,00:06:59.900
"A matrix, like C, that
has C*x equals 0, well,",00:07:02.410,00:07:10.170
"I could also say that
that's C*x equals 0*x.",00:07:10.170,00:07:14.400
"That would actually be better,
so that both sides are vectors.",00:07:14.400,00:07:19.230
"So I'm realizing that that
vector x in the null space",00:07:19.230,00:07:24.630
"is an eigenvector and the
corresponding eigenvalue is 0.",00:07:24.630,00:07:28.990
"Otherwise, the eigenvalues
will all still be positive.",00:07:36.590,00:07:40.140
"So this would be a
positive semidefinite.",00:07:40.140,00:07:45.250
"I would call that matrix
positive semidefinite,",00:07:45.250,00:07:51.580
"the semi telling me that it
isn't quite definite, that it",00:07:51.580,00:07:56.150
"gets down and has 0,
the matrix is singular.",00:07:56.150,00:08:04.190
"But still, it's good to know
where all the other n minus 1",00:08:04.190,00:08:08.580
eigenvalues are.,00:08:08.580,00:08:09.510
They're all positive.,00:08:09.510,00:08:10.750
"About the bandwidth --
let me go back to K now.",00:08:13.660,00:08:19.380
"Because the bandwidth, strictly
speaking, the bandwidth of C",00:08:19.380,00:08:23.960
is very large.,00:08:23.960,00:08:25.770
"The bandwidth is, if I'm
looking for only one number,",00:08:25.770,00:08:32.040
"that number tells me
how many diagonals",00:08:32.040,00:08:35.210
"I have to grow until I
reach the last nonzero.",00:08:35.210,00:08:44.100
So the bandwidth here is large.,00:08:44.100,00:08:48.800
"And in general, the
operation count,",00:08:48.800,00:08:56.070
"the amount of work
to do, is going",00:08:56.070,00:08:59.930
to grow with the bandwidth.,00:08:59.930,00:09:02.100
"Of course, that
having full bandwidth",00:09:02.100,00:09:05.110
"isn't quite the full
story for this matrix,",00:09:05.110,00:09:08.310
because it's so sparse.,00:09:08.310,00:09:10.080
"I've got hundreds
of zero diagonals",00:09:10.080,00:09:14.460
in between and just this one.,00:09:14.460,00:09:16.380
"Anyway, this is a matrix
with a large bandwidth,",00:09:16.380,00:09:20.800
but a little deceptive.,00:09:20.800,00:09:23.070
"Now here's a matrix with
a -- back to K again.",00:09:23.070,00:09:29.120
"I call the bandwidth
just 1 here.",00:09:29.120,00:09:34.030
"Really, maybe half bandwidth
would be a better word.",00:09:34.030,00:09:37.820
"The bandwidth is the number
of diagonals above or below --",00:09:37.820,00:09:46.280
"take the maximum of the count
above and the count below",00:09:46.280,00:09:50.105
"and in this case,
both counts are 1.",00:09:50.105,00:09:53.210
"One diagonal above, one
diagonal below, so really,",00:09:53.210,00:09:56.390
"half bandwidth, I
would say, is 1.",00:09:56.390,00:10:01.550
"Just some convention
is needed there.",00:10:01.550,00:10:04.620
"The crucial point is that
the bandwidth measures",00:10:04.620,00:10:11.110
"the amount of work to do
when you do elimination,",00:10:11.110,00:10:17.590
"as MATLAB will do, of course.",00:10:17.590,00:10:21.170
"One other thing about MATLAB
-- so I'm often referring",00:10:21.170,00:10:26.390
"to MATLAB and I'm thinking
of its backslash command.",00:10:26.390,00:10:29.430
"The backslash, which solves A*x
equal b by just A backslash b.",00:10:29.430,00:10:38.990
"So if it doesn't know
these matrices are sparse,",00:10:42.140,00:10:49.540
"it will go through
all the steps,",00:10:49.540,00:10:52.620
"not taking advantage of the fact
that we've got all these 0's",00:10:52.620,00:10:55.940
here.,00:10:55.940,00:10:56.750
"If it does know
that they're sparse,",00:10:56.750,00:10:58.850
then it's tremendously fast.,00:10:58.850,00:11:01.500
Let me come back to that point.,00:11:01.500,00:11:04.240
"Maybe actually backslash
is smart enough",00:11:04.240,00:11:08.690
"to look to see
whether the matrix,",00:11:08.690,00:11:15.160
whether sparseness is available.,00:11:15.160,00:11:17.050
"So I shouldn't have said --
I think maybe there's a lot",00:11:17.050,00:11:19.950
engineered into backslash.,00:11:19.950,00:11:22.600
"Actually, backslash,
also engineered in there",00:11:22.600,00:11:26.020
is the least squares solution.,00:11:26.020,00:11:29.120
"If you give it a
rectangular problem",00:11:29.120,00:11:32.580
"and, say, too many equations,
so that you can't expect",00:11:32.580,00:11:41.870
"to have an exact
solution, backslash",00:11:41.870,00:11:45.590
"will pick the least
squares solution.",00:11:45.590,00:11:48.880
And much more.,00:11:48.880,00:11:49.840
"That's probably the
most used operation.",00:11:49.840,00:11:55.010
"So I said something about
the eigenvalues and everybody",00:11:55.010,00:11:58.750
"sees that if I multiply this
matrix by the values of u",00:11:58.750,00:12:06.380
"at successive mesh points, I'll
get the second difference that",00:12:06.380,00:12:10.870
corresponds to u_xx.,00:12:10.870,00:12:12.980
"Now this would correspond
-- this matrix --",00:12:12.980,00:12:17.360
"so that tells me I'm
taking a finite difference.",00:12:17.360,00:12:21.350
"That plus tells me I'm going
in the forward direction,",00:12:21.350,00:12:24.640
"so that's 1 at the mesh
value to the right minus 1",00:12:24.640,00:12:31.890
"of the mesh value
at the center point.",00:12:31.890,00:12:34.740
"Of course, this has
to be divided by delta",00:12:34.740,00:12:36.750
x and that by delta x squared.,00:12:36.750,00:12:39.260
"So these are matrices, along
with the backward difference",00:12:39.260,00:12:46.940
"and the centered
difference, out of which",00:12:46.940,00:12:48.870
"you build the basic
finite difference",00:12:48.870,00:12:54.350
"equation, as you've done.",00:12:54.350,00:12:57.540
"I want to make a comment
here though, now,",00:12:57.540,00:13:01.640
on this topic of eigenvalues.,00:13:01.640,00:13:06.900
"Eigenvalues for
K really tell you",00:13:06.900,00:13:10.980
"the truth about the
matrix K. The eigenvalues",00:13:10.980,00:13:14.650
"of that matrix K start
just a little above 0",00:13:14.650,00:13:20.970
"and they go to a
little before 4.",00:13:20.970,00:13:25.270
"So the eigenvalues for K
-- shall I put that here?",00:13:25.270,00:13:31.920
"The eigenvalues of K
are between 0 and 4.",00:13:31.920,00:13:36.880
"They come very close
to 4 and quite close",00:13:43.180,00:13:45.130
"to 0, depending on the size
of the matrix of course.",00:13:45.130,00:13:51.840
"Let me just do -- if I
took the one by one case,",00:13:51.840,00:13:56.030
its eigenvalue is 2.,00:13:56.030,00:13:57.950
"If I took the two by two case,
its eigenvalues are 1 and 3.",00:13:57.950,00:14:04.270
Notice nice properties there.,00:14:04.270,00:14:08.490
"The eigenvalues 1 and 3
are positive, as we said.",00:14:08.490,00:14:13.340
"This matrix K has positive
eigenvalues, whatever size.",00:14:13.340,00:14:17.860
"What's more, the 1 and 3
kind of interlace the 2.",00:14:17.860,00:14:25.550
"So what I'm saying is,
the eigenvalue for that",00:14:25.550,00:14:28.320
"is in between the eigenvalue
for the two by two,",00:14:28.320,00:14:34.130
"and the two
eigenvalues 1 and 3 are",00:14:34.130,00:14:37.160
"in between the three eigenvalues
that I would get for the three",00:14:37.160,00:14:41.150
by three case.,00:14:41.150,00:14:43.460
"Maybe I'll just
write those down.",00:14:43.460,00:14:44.960
They are useful numbers.,00:14:44.960,00:14:46.700
So K_2 has lambda equal 1 and 3.,00:14:46.700,00:14:52.890
"The three by three one,
I think the eigenvalues",00:14:52.890,00:14:56.090
"are 2 minus root 2,
which is smaller than 1;",00:14:56.090,00:15:01.060
"2, which is in between;
and 2 plus root 2, which",00:15:01.060,00:15:06.150
is larger than 3.,00:15:06.150,00:15:07.350
"And of course, they're
all between 0 and 4.",00:15:07.350,00:15:12.300
Just a comment.,00:15:12.300,00:15:13.130
"How do I know they're
between 0 and 4?",00:15:13.130,00:15:15.980
"There's a somewhat
handy little rule",00:15:18.820,00:15:24.190
"for getting the location
of eigenvalues, that's",00:15:24.190,00:15:28.350
"just worth knowing as a
sort of general principle,",00:15:28.350,00:15:31.130
"but of course it can't tell
you exactly where they are.",00:15:31.130,00:15:35.440
"First of all, the fact that
the matrix is symmetric",00:15:35.440,00:15:39.570
"tells us what about
the eigenvalues?",00:15:39.570,00:15:42.750
"So we learn a very,
very important fact",00:15:42.750,00:15:45.170
"about the eigenvalues from
just looking at the matrix",00:15:45.170,00:15:49.310
"and observing that
it's symmetric.",00:15:49.310,00:15:51.700
"That tells us that the
eigenvalues are real.",00:15:51.700,00:15:55.690
They're real numbers.,00:15:55.690,00:15:56.670
"Actually, it tells us
something equally important",00:15:59.460,00:16:01.520
about the eigenvectors.,00:16:01.520,00:16:03.420
"The eigenvectors of the
matrix are orthogonal.",00:16:03.420,00:16:07.710
"The symmetric matrix
has real eigenvalues,",00:16:07.710,00:16:10.670
orthogonal eigenvectors.,00:16:10.670,00:16:12.010
"That's a little bit of
linear algebra to know.",00:16:12.010,00:16:15.510
"Now, why between
0 and 4, though?",00:16:15.510,00:16:19.420
"There's there's this
-- what's his name?",00:16:22.030,00:16:25.210
Gershgorin.,00:16:25.210,00:16:27.510
"Gershgorin pointed out --
and it's a two-line proof --",00:16:27.510,00:16:32.620
"that every eigenvalue
is in one of his --",00:16:32.620,00:16:35.900
one or more of his circles.,00:16:35.900,00:16:38.390
"So where are the
Gershgorin circles?",00:16:38.390,00:16:41.660
"A typical Gershgorin circle is
centered at the number -- here,",00:16:41.660,00:16:47.310
"2 -- that's on the diagonal,
and its radius is the sum off",00:16:47.310,00:16:54.120
"the diagonal, but you
take absolute values.",00:16:54.120,00:16:57.350
"Gershgorin wasn't doing
any of the fine points that",00:16:57.350,00:17:01.150
really locate the eigenvalue.,00:17:01.150,00:17:03.150
"So in this matrix,
all the centers",00:17:03.150,00:17:06.830
"are at 2, the
diagonals, and the radii",00:17:06.830,00:17:11.330
"are 2, also 2, because that and
that in absolute value make 2.",00:17:11.330,00:17:16.970
"This first row makes a 1 and
actually, that's what -- that,",00:17:19.630,00:17:24.670
"by a little careful argument,
is what gets the eigenvalues not",00:17:24.670,00:17:32.180
"actually touching
0 or touching 4.",00:17:32.180,00:17:36.050
It takes a little patience.,00:17:36.050,00:17:37.570
"All you could say from here --
all Gershgorin could say would",00:17:37.570,00:17:42.470
"be, the eigenvalues are
in a circle centered at 2,",00:17:42.470,00:17:46.660
"its radius is 2, so
they're between 0 and 4.",00:17:46.660,00:17:51.770
"Then it's that first
and last row --",00:17:51.770,00:17:55.420
"either the first or the last
row would do to say that",00:17:55.420,00:18:01.660
they're strictly positive.,00:18:01.660,00:18:05.240
"Of course, the true
second difference,",00:18:05.240,00:18:09.890
"I should divide K
by delta x squared.",00:18:09.890,00:18:13.640
"So the eigenvalues will be
divided by delta x squared,",00:18:13.640,00:18:17.450
"divided by a small number,
so they will really",00:18:17.450,00:18:19.610
be much larger.,00:18:19.610,00:18:22.790
"The ratio of the
biggest to the smallest",00:18:22.790,00:18:25.900
"isn't affected by
the delta x squared",00:18:25.900,00:18:28.350
and that's a key number.,00:18:28.350,00:18:31.020
"Let me put that maybe on
the next board, for K.",00:18:31.020,00:18:38.530
So lambda_max is about 4.,00:18:38.530,00:18:43.890
"Lambda_min, the smallest
eigenvalue, is close to 0.",00:18:43.890,00:18:53.170
How close?,00:18:53.170,00:18:55.340
"It's of the order of some
constant over n squared.",00:18:55.340,00:18:59.450
"Now I use the word
condition number and that --",00:19:03.380,00:19:06.830
so let me write that down.,00:19:06.830,00:19:08.670
"Condition number -- say c
of K for condition number --",00:19:08.670,00:19:15.050
"is the ratio lambda_max
over lambda_min.",00:19:15.050,00:19:19.120
"Of course, I'm
using here the fact",00:19:19.120,00:19:21.030
that K is a symmetric matrix.,00:19:21.030,00:19:23.410
"Symmetric matrices
are so beautiful",00:19:23.410,00:19:25.810
"that their eigenvalues give
you a reliable story here.",00:19:25.810,00:19:31.220
"So 4 divided by this -- the
main point is that it's O of --",00:19:31.220,00:19:37.890
it's of order 1 over n -- sorry.,00:19:37.890,00:19:40.270
"When I divide by
lambda_min, that",00:19:40.270,00:19:42.140
"puts the n squared
up in the numerator.",00:19:42.140,00:19:45.390
"It's O of n squared,
growing like n squared.",00:19:45.390,00:19:52.210
"And that condition
number, somehow it",00:19:52.210,00:19:58.170
"measures how close the
matrix is to being singular,",00:19:58.170,00:20:03.990
"because it involves
this lambda_min, which",00:20:03.990,00:20:10.810
"is the smallest eigenvalue and
would be 0 if it were singular,",00:20:10.810,00:20:15.690
and it's scaled by lambda_max.,00:20:15.690,00:20:18.000
"So if I'm multiply
the matrix by 100,",00:20:18.000,00:20:21.570
"what happens to the
condition number?",00:20:21.570,00:20:23.830
No change.,00:20:23.830,00:20:25.300
"No change, because I'm doing
lambda_max over lambda_min.",00:20:25.300,00:20:29.180
"Both lambda_max and lambda_min
would be multiplied by 100,",00:20:29.180,00:20:32.620
"but the ratio
wouldn't be different",00:20:32.620,00:20:35.230
So this is a useful measure.,00:20:35.230,00:20:38.500
"And O of n squared, that's
a big number if n is large.",00:20:38.500,00:20:48.220
A big numbers if n is large.,00:20:48.220,00:20:50.000
"And that condition number, where
does it come in elimination?",00:20:50.000,00:20:54.850
"In elimination, the
round-off error -- roughly,",00:20:58.860,00:21:09.360
"the rule of thumb
is that you would --",00:21:09.360,00:21:14.130
"if the condition number
is 10 to the eighth,",00:21:14.130,00:21:18.620
"you might lose eight significant
bits in the back slide.",00:21:18.620,00:21:26.850
You could.,00:21:26.850,00:21:28.690
"So this condition number
measures how sensitive",00:21:28.690,00:21:34.580
your matrix is to round-off.,00:21:34.580,00:21:38.250
"So that's a few
thoughts about matrices",00:21:38.250,00:21:44.540
and that matrix K in particular.,00:21:44.540,00:21:47.770
"Now what about the other
matrix, the first difference?",00:21:47.770,00:21:55.960
"The point I want to
make about that matrix",00:21:55.960,00:21:59.550
"is, what about its eigenvalues?",00:21:59.550,00:22:04.600
"What are the eigenvalues of
that upper triangular matrix?",00:22:04.600,00:22:10.470
"They are, if you remember
linear algebra --",00:22:10.470,00:22:15.440
"but I can just tell you
quickly the main point --",00:22:15.440,00:22:18.310
"for a triangular matrix,
the values are sitting",00:22:18.310,00:22:23.020
on the diagonal.,00:22:23.020,00:22:25.840
"So this matrix
has the eigenvalue",00:22:25.840,00:22:30.110
minus 1 repeated n times.,00:22:30.110,00:22:32.150
"That true fact is totally
misleading, totally misleading.",00:22:37.800,00:22:43.340
"The eigenvalues for
this triangular matrix",00:22:47.110,00:22:49.770
"don't even notice what
I've got above the diagonal",00:22:49.770,00:22:54.090
"and somehow they can't give
a reasonable picture of what",00:22:54.090,00:23:00.530
the matrix is actually doing.,00:23:00.530,00:23:02.880
"So maybe that's my
warning here, that",00:23:02.880,00:23:06.900
"for a matrix which is
absolutely not symmetric, right?",00:23:06.900,00:23:11.750
"I mean, not at all symmetric.",00:23:11.750,00:23:13.120
"For the centered
difference, which is --",00:23:15.640,00:23:20.039
what's the centered difference?,00:23:20.039,00:23:21.330
"I was going to say symmetric,
but it's the opposite.",00:23:21.330,00:23:24.110
"Centered difference would be --
let's put delta_centered down",00:23:24.110,00:23:30.050
here.,00:23:30.050,00:23:31.320
"Centered difference
would be, I'd have a 1.",00:23:31.320,00:23:34.050
"0's would go on the point
that -- for the central value.",00:23:34.050,00:23:41.130
"1 would multiply the
forward value and minus 1",00:23:41.130,00:23:45.210
"would multiply that,
and then I'd have 1/2",00:23:45.210,00:23:48.350
"and then I'd probably
have a delta x.",00:23:48.350,00:23:50.250
"But the main point is, my matrix
would look something like this.",00:23:52.990,00:24:00.680
"Minus 1's and 1's
on two diagonals.",00:24:00.680,00:24:04.670
"Now we could find the
eigenvalues of that matrix.",00:24:04.670,00:24:09.110
"Do you know anything
about the eigenvalues?",00:24:09.110,00:24:14.820
"This is a chance
for me just to speak",00:24:14.820,00:24:17.640
"for a few minutes
about useful facts",00:24:17.640,00:24:22.100
"that you can tell about a
matrix just by looking at it.",00:24:22.100,00:24:26.410
"So what do I see when
I look at that matrix?",00:24:26.410,00:24:30.450
Is it symmetric?,00:24:30.450,00:24:33.540
"It's the opposite of symmetric,
because the symmetric matrix",00:24:33.540,00:24:37.220
"up there, if I transpose
it, it doesn't change.",00:24:37.220,00:24:42.020
"If I transpose this
one, I'll get some kind",00:24:42.020,00:24:46.330
of a backward difference.,00:24:46.330,00:24:48.500
"If I transpose this one, then
the 1's and the minus 1's will",00:24:48.500,00:24:53.840
reverse.,00:24:53.840,00:24:54.630
I'll get the negative.,00:24:54.630,00:24:56.170
"So the rule for this centered
difference is -- so shall I --",00:24:56.170,00:25:01.520
"how am I going to call
centered difference?",00:25:01.520,00:25:03.540
"Del centered, for the moment.",00:25:03.540,00:25:06.840
"The transpose of
that is minus itself.",00:25:06.840,00:25:12.930
So what does that tell me?,00:25:16.077,00:25:17.160
"First, that matrix is -- it's
the opposite of symmetric,",00:25:21.330,00:25:32.330
but it's actually OK.,00:25:32.330,00:25:35.130
"What I mean by OK is,
its eigenvectors --",00:25:35.130,00:25:39.910
"we're back to
orthogonal eigenvectors.",00:25:39.910,00:25:42.870
"I didn't say anything about
the eigenvectors of del plus,",00:25:42.870,00:25:46.090
"but actually, that was
the biggest problem.",00:25:46.090,00:25:49.990
"This matrix del plus has one
eigenvalue repeated n times,",00:25:49.990,00:25:55.010
"and it has only one
eigenvector, not --",00:25:55.010,00:26:00.360
"it doesn't even have a
full set of eigenvectors,",00:26:00.360,00:26:02.730
much less orthogonal ones.,00:26:02.730,00:26:06.060
"So that matrix is like --
you don't want to trust",00:26:06.060,00:26:11.660
"the eigenvalue picture that you
get from a matrix like that.",00:26:11.660,00:26:15.380
"Here this anti-symmetric
matrix can be trusted.",00:26:15.380,00:26:20.210
"Its eigenvalue
picture is reliable.",00:26:20.210,00:26:22.860
"It does tell you
what's going on.",00:26:22.860,00:26:25.130
The eigenvectors are orthogonal.,00:26:25.130,00:26:29.000
"They're complex, actually.",00:26:29.000,00:26:31.990
"Actually, they'll look a lot
like our e to the i*k*x's So we",00:26:31.990,00:26:38.010
"don't panic when we see
complex eigenvectors.",00:26:38.010,00:26:42.430
"The eigenvalues are -- do you
know what the eigenvalues looks",00:26:42.430,00:26:46.260
"like for an
anti-symmetric matrix?",00:26:46.260,00:26:50.470
"They're pure imaginary, just
the way that when we took second",00:26:50.470,00:26:56.560
"differences -- maybe I'll
just put here the centered",00:26:56.560,00:27:05.450
"difference, the centered
first difference,",00:27:05.450,00:27:12.900
"when we applied it to -- I want
apply to e to the i*k*x to find",00:27:12.900,00:27:20.280
what factor comes out.,00:27:20.280,00:27:22.320
"So I get e to the i k plus 1
delta x from the plus side.",00:27:22.320,00:27:32.740
"This is the matrix I'm
doing, with 1 and minus 1.",00:27:35.990,00:27:39.990
"So the minus 1 will give me
minus e to the i k minus 1",00:27:39.990,00:27:46.130
"delta x and of course, I
factor out of that the e",00:27:46.130,00:27:51.150
"to the i*k*x's so I'm
left with e to the --",00:27:51.150,00:27:55.010
"the thing that factors out
is e to the i delta x minus e",00:27:55.010,00:27:59.425
"to the minus i delta
x -- and what's that?",00:27:59.425,00:28:03.380
"That multiplies the e to
the i*k*x, the eigenvector.",00:28:03.380,00:28:08.350
"This is like the
eigenvalue, and what",00:28:08.350,00:28:11.260
do I say about that quantity?,00:28:11.260,00:28:12.850
"Of course, it's 2*i sine
delta x, pure imaginary.",00:28:15.390,00:28:24.740
"And I should have
divided by the 2, which",00:28:24.740,00:28:31.200
would take away that 2.,00:28:31.200,00:28:34.830
So it's pure imaginary.,00:28:34.830,00:28:36.190
"In reality and in
this Fourier analysis,",00:28:40.130,00:28:43.930
"both are giving this
understanding of what i is.",00:28:43.930,00:28:49.080
"So that when we did this sort
of operation, von Neumann's",00:28:49.080,00:28:58.400
"rule of following
the exponential,",00:28:58.400,00:29:01.310
we got something reasonable.,00:29:01.310,00:29:05.280
"When we do it with
this one, it's",00:29:05.280,00:29:10.480
"von Neumann that's reliable
and the eigenvalues",00:29:10.480,00:29:13.370
that are not reliable.,00:29:13.370,00:29:15.040
"So the eigenvalues of this
being all minus 1's is nonsense,",00:29:15.040,00:29:19.230
"doesn't tell us what the forward
difference operator is really",00:29:19.230,00:29:23.520
doing.,00:29:23.520,00:29:25.020
"But von Neumann tells us
what is truly going on.",00:29:25.020,00:29:30.276
"Of course, that would
be the same thing",00:29:30.276,00:29:31.900
"in which this minus
1 wouldn't appear",00:29:31.900,00:29:35.670
and the 2 wouldn't appear.,00:29:35.670,00:29:38.050
"So what would we get
out of von Neumann?",00:29:38.050,00:29:40.790
So this is for delta plus.,00:29:40.790,00:29:44.230
"I would factor out an e
to the i delta x minus 1.",00:29:44.230,00:29:49.500
"That's what would multiply
it e to the i*k*x.",00:29:52.250,00:29:54.910
"Oh, e to the i -- sorry.",00:29:54.910,00:29:59.530
"Yes, that's right.",00:29:59.530,00:30:01.230
"That would multiple
e to the i*k delta x.",00:30:01.230,00:30:05.570
"Sorry, should've
had delta x there,",00:30:05.570,00:30:09.850
"but I wasn't paying
attention to that.",00:30:09.850,00:30:11.990
"Here I was paying attention to
this and it was pure imaginary.",00:30:11.990,00:30:16.230
"Here I'm paying
attention -- here,",00:30:16.230,00:30:18.390
"von Neumann at least is paying
attention to this and what's he",00:30:18.390,00:30:22.480
seeing?,00:30:22.480,00:30:25.590
"Not pure imaginary
or purely real,",00:30:25.590,00:30:29.950
"off in the complex
plane and that's",00:30:29.950,00:30:34.260
"really what the right growth
factor or the right number",00:30:34.260,00:30:47.850
"to associate with frequency k
for this forward difference.",00:30:47.850,00:30:54.690
"So I guess I'm saying
that von Neumann does --",00:30:54.690,00:30:58.710
"did the right thing to
come up with these pictures",00:30:58.710,00:31:03.490
for the growth factors.,00:31:03.490,00:31:06.880
"Eigenvalues confirm that
and really pin it down",00:31:06.880,00:31:11.550
when they're reliable.,00:31:11.550,00:31:13.200
"Eigenvectors and -- eigenvalues
are reliable when eigenvector",00:31:13.200,00:31:17.590
"are orthogonal and that's for
matrices that are symmetric",00:31:17.590,00:31:22.360
"or anti-symmetric or -- there's
a little bit larger class",00:31:22.360,00:31:27.650
"of matrices that includes
orthogonal matrices,",00:31:27.650,00:31:30.980
"but beyond that --
actually, there's been a lot",00:31:30.980,00:31:36.770
"of discussion over many years
of eigenvalues and the --",00:31:36.770,00:31:51.590
"for problems that are not
controlled by symmetric",00:31:51.590,00:31:54.740
or anti-symmetric matrices.,00:31:54.740,00:31:56.440
"The alternative, the
more refined idea",00:32:00.460,00:32:03.440
"of pseudo-eigenvalues is now
appearing in an important book",00:32:03.440,00:32:11.850
"by Trefethen and Embree,
with many examples -- OK,",00:32:11.850,00:32:17.030
I won't pursue that.,00:32:17.030,00:32:18.330
Right.,00:32:18.330,00:32:19.030
"So this is some basic fact about
those matrices, all of which",00:32:19.030,00:32:28.510
are one-dimensional.,00:32:28.510,00:32:30.680
"Now this board prepares the
way to get into 2D and 3D.",00:32:30.680,00:32:37.500
"So I just want to
ask, what does the --",00:32:37.500,00:32:42.910
"we didn't really do the heat
equation or the wave equation",00:32:42.910,00:32:46.900
"in 2D, but we could have.",00:32:46.900,00:32:52.170
"The von Neumann test
would be straightforward,",00:32:52.170,00:32:58.510
"but now I want to think
about the matrices.",00:32:58.510,00:33:01.580
"What does the two-dimensional
second difference matrix",00:33:01.580,00:33:07.920
look like?,00:33:07.920,00:33:11.350
"What I'm going to do,
just to look ahead,",00:33:11.350,00:33:14.330
"I'm going to use MATLAB's
operation called kron,",00:33:14.330,00:33:18.470
"short for Kronecker, to
create a 2D matrix out",00:33:18.470,00:33:24.070
of this 1D centered difference.,00:33:24.070,00:33:27.580
"So if I think now about
centered differences,",00:33:27.580,00:33:31.020
"second differences -- so I'm
approximating u_xx plus u_yy --",00:33:31.020,00:33:38.150
"or really, I'm approximating
minus u_xx minus u_yy,",00:33:38.150,00:33:42.910
"because that K approximates
minus the second difference.",00:33:42.910,00:33:47.620
"What do I -- what do
my matrices look like?",00:33:47.620,00:33:54.430
What's their bandwidth?,00:33:54.430,00:33:56.590
"How expensive is
it to invert them?",00:33:56.590,00:33:58.540
These are the key questions.,00:33:58.540,00:34:00.390
"What's the matrix K2D
that corresponds to --",00:34:00.390,00:34:08.460
"gives me second differences
in the x direction plus second",00:34:08.460,00:34:12.460
differences in the y direction.,00:34:12.460,00:34:15.120
"So I'll write K2D
for that matrix.",00:34:15.120,00:34:18.630
Let's get some picture of it.,00:34:18.630,00:34:20.060
"First of all, let
me imagine that I'm",00:34:23.300,00:34:27.040
on a square with a square grid.,00:34:27.040,00:34:30.990
Delta x in both directions.,00:34:30.990,00:34:33.170
"Square grid, let me say
N mesh points each way.",00:34:41.770,00:34:46.700
"So N, I don't know whether
I'm counting -- right now,",00:34:46.700,00:34:49.960
"I won't worry whether I'm
counting the boundary ones",00:34:49.960,00:34:52.540
or not.,00:34:52.540,00:34:53.070
Probably not.,00:34:53.070,00:34:54.350
"So N -- I'll say N point, point
N, N points -- so N delta x,",00:34:54.350,00:35:03.270
and in this direction N delta x.,00:35:03.270,00:35:08.630
So my matrix is a border.,00:35:08.630,00:35:10.740
"It's of size N squared,
the number of unknowns.",00:35:10.740,00:35:18.950
"Now I will be a
little more careful.",00:35:18.950,00:35:21.080
"Here, let me take the
boundary values as given.",00:35:21.080,00:35:25.000
They're not unknown.,00:35:25.000,00:35:26.480
So N in this picture is 4.,00:35:26.480,00:35:29.380
"One, two, three, four unknowns
there on a typical row.",00:35:29.380,00:35:35.980
"Now I have to give them a new
number -- five, six, seven,",00:35:35.980,00:35:39.330
"eight, nine, 10, 11, 12, 13,
14, 15, 16 -- and N being 4,",00:35:39.330,00:35:48.620
"N squared is 16 for
that particular square.",00:35:48.620,00:35:54.600
So my matrix is 16 by 16.,00:35:54.600,00:35:56.800
"But somehow I want to be able
to create it out of 4 by 4",00:36:00.970,00:36:06.900
"matrices like K. So K1D, which
I'm just going to call K --",00:36:06.900,00:36:15.430
K_N will be the 4 by 4 one.,00:36:15.430,00:36:18.110
"2 minus 1 -- so that's the
matrix that gives me second",00:36:18.110,00:36:28.160
"differences along a typical
row or down a typical column.",00:36:28.160,00:36:35.610
But now what am I looking for?,00:36:35.610,00:36:37.540
"I'm looking to do both --
second differences in a row",00:36:37.540,00:36:41.180
and a column.,00:36:41.180,00:36:42.430
"So if I pick a typical
mesh point, like number 11.",00:36:42.430,00:36:47.320
"Mesh point 11 -- let me
blow up this picture here.",00:36:50.100,00:36:54.740
"It's going to be influenced,
mesh point 11, by 10 and 12,",00:36:54.740,00:36:59.270
"the second differences in the
x direction, and by 7 and 15 --",00:36:59.270,00:37:03.550
"notice those are not
so close to 10 or 11 --",00:37:03.550,00:37:08.120
"the second differences
in the y direction.",00:37:08.120,00:37:10.460
So let me blow that up.,00:37:10.460,00:37:12.590
"So here's mesh point number 11
corresponding to row 11 of K2D.",00:37:12.590,00:37:20.500
"So I guess I'm asking what
row 11 of K2D will look like.",00:37:25.540,00:37:30.530
"So here's mesh points 10 and 12,
so I have a second difference",00:37:30.530,00:37:36.710
"in the x direction from
u_xx, for minus u_xx, that",00:37:36.710,00:37:41.950
"means I can put a minus 1 there,
a 2 there and a minus 1 there.",00:37:41.950,00:37:47.900
"Now I have the same in the
y direction with 15 and 7,",00:37:47.900,00:37:53.980
"so I have a minus 1 in column
15, a minus 1 in column 7,",00:37:53.980,00:38:03.220
"so I have four minus 1's and
then 2 more for the center",00:38:03.220,00:38:10.060
gives me a 4.,00:38:10.060,00:38:11.420
"So a typical row will have --
it'll be sparse, of course --",00:38:11.420,00:38:17.750
"it'll have a minus 1 in position
7, a minus 1 in position 10,",00:38:17.750,00:38:24.220
"a 4, a minus 1, and a minus
1 over there in position 15.",00:38:24.220,00:38:31.160
That's a typical row of K2D.,00:38:31.160,00:38:34.530
It adds to 0.,00:38:34.530,00:38:35.670
"If Gershgorin got his
hands on this matrix,",00:38:39.130,00:38:44.310
"he would say that since all
the rows -- the 4 will be on --",00:38:44.310,00:38:51.200
will the 4 be on the diagonal?,00:38:51.200,00:38:52.820
"Yes, the 4 will be on
the diagonal all the way.",00:38:52.820,00:38:58.180
"I guess -- let's see a
little bit more clearly what",00:38:58.180,00:39:02.040
the matrix K2D looks like.,00:39:02.040,00:39:04.110
"This is probably the
most studied matrix",00:39:06.800,00:39:09.670
"in numerical analysis because
it's a model of what stays nice",00:39:09.670,00:39:16.960
"and what gets more difficult as
you move into two dimensions.",00:39:16.960,00:39:22.090
"So some things
certainly stay nice.",00:39:22.090,00:39:24.180
"Symmetries -- so properties
of K2D, K2D will be --",00:39:24.180,00:39:31.790
it'll be symmetric again.,00:39:31.790,00:39:33.250
"It'll be positive
definite again.",00:39:35.940,00:39:37.610
"So what does K2D -- it's 16 by
16 and a typical row looks like",00:39:42.930,00:39:52.050
that.,00:39:52.050,00:39:53.020
That's a typical interior row.,00:39:53.020,00:39:56.180
"What does maybe -- if
I took the first row,",00:39:56.180,00:39:59.970
"what does the very
first row look like?",00:39:59.970,00:40:02.520
If I put row one above it.,00:40:02.520,00:40:03.810
Let me draw row one.,00:40:07.410,00:40:09.960
"So what's the
difference with row one?",00:40:09.960,00:40:12.730
It's these are boundary values.,00:40:12.730,00:40:15.070
"These are not -- these are going
to show up on the right-hand",00:40:15.070,00:40:18.660
side of the equation.,00:40:18.660,00:40:20.360
They're known numbers.,00:40:20.360,00:40:21.380
"They're not -- they don't
involve unknown things,",00:40:21.380,00:40:23.500
"so the only neighbors
are 2 and 5.",00:40:23.500,00:40:26.460
"So I'll still have the second
difference, minus 1, 2,",00:40:26.460,00:40:29.170
"minus 1, and minus
1, 2, minus 1 --",00:40:29.170,00:40:31.990
"still this five-point
molecule with 4 at the center,",00:40:31.990,00:40:37.670
"but there won't be
so many neighbors.",00:40:37.670,00:40:40.060
"There'll be the neighbor at
the -- just to the right,",00:40:40.060,00:40:45.320
"neighbor number two, and
there'll be neighbor number",00:40:45.320,00:40:47.990
"five a little further
along and that's it.",00:40:47.990,00:40:50.270
"It's like the 2
minus 1 boundary row.",00:40:52.820,00:40:58.060
"It's a boundary row
and a boundary row",00:40:58.060,00:41:01.330
"hasn't got as many minus
1's because it hasn't got",00:41:01.330,00:41:05.780
as many neighboring unknowns.,00:41:05.780,00:41:10.020
"That boundary row would
have three neighbors.",00:41:10.020,00:41:12.940
"Row two would have a 4,
minus 1, minus 1, but not --",00:41:12.940,00:41:17.380
nobody there.,00:41:17.380,00:41:18.720
"So now I'm going
to try to draw K2D.",00:41:18.720,00:41:23.710
Let me try to draw K2D.,00:41:23.710,00:41:26.140
I can do it.,00:41:26.140,00:41:27.940
"K2D will have, from the u_xx,
the second differences along",00:41:27.940,00:41:38.290
"the rows, it will have --
I'll have a row, row one,",00:41:38.290,00:41:43.710
it'll have another K on row two.,00:41:43.710,00:41:46.350
"It'll have a K on
-- a K for each row.",00:41:46.350,00:41:54.280
These are blocks now.,00:41:54.280,00:41:56.180
"That's of size N by
N. All of those are,",00:41:56.180,00:41:59.360
"so the whole thing is
N squared by N squared.",00:41:59.360,00:42:02.780
"Actually, I'll stop here.",00:42:02.780,00:42:06.180
"If I wanted to create that
matrix with the same K on --",00:42:06.180,00:42:13.570
"it's somehow the identity is
in there and the matrix K is",00:42:13.570,00:42:18.610
"in there and that's
-- let's see.",00:42:18.610,00:42:23.590
It's one or the other of those.,00:42:23.590,00:42:26.530
I guess it's the first one.,00:42:26.530,00:42:29.810
"So what's this
Kronecker product?",00:42:29.810,00:42:32.000
"Now I'm saying what
this construction is.",00:42:32.000,00:42:35.770
"It's very valuable, because it
allows you to create matrices.",00:42:35.770,00:42:41.270
"If you created K as
a sparse matrix and I",00:42:41.270,00:42:44.820
"as a sparse matrix, then
the Kronecker product",00:42:44.820,00:42:47.670
"would be automatically dealt
with as a sparse matrix.",00:42:47.670,00:42:51.950
"So what's the rule for
a Kronecker product?",00:42:51.950,00:42:54.740
"You take the first matrix,
which is the identity.",00:42:54.740,00:43:00.090
And the rest 0's.,00:43:00.090,00:43:04.960
"So that's I. Then
-- so that's 1D --",00:43:08.740,00:43:15.330
"and then you multiple each --
each entry in I becomes a block",00:43:15.330,00:43:22.810
"with entry a_(i, j) times this
guy, K. This'll be the 0 block,",00:43:22.810,00:43:28.550
"this'll be the K block,
this'll be the K block.",00:43:28.550,00:43:31.490
"I take those 1's and multiply
K and those 0's and multiple K",00:43:31.490,00:43:35.810
and that's what I get.,00:43:35.810,00:43:38.450
"So you see now, the Kronecker
product is a bigger guy.",00:43:38.450,00:43:43.290
"If matrix A was p by p
and matrix B was q by q,",00:43:43.290,00:43:52.220
"then the Kronecker product
would be p times q by p times q.",00:43:52.220,00:43:57.570
It would be square again.,00:43:57.570,00:43:59.000
"It would be symmetric if
A and B are symmetric.",00:44:02.360,00:44:06.160
"Actually, it would have
various nice properties.",00:44:06.160,00:44:09.290
"Its eigenvalues
would be the products",00:44:09.290,00:44:11.810
"of the eigenvalues of A
times the eigenvalues of B.",00:44:11.810,00:44:16.910
"It's a very handy
construction and here we",00:44:16.910,00:44:20.430
"saw it in a pretty easy case
where A was only the identity.",00:44:20.430,00:44:26.000
"Well, let me do this case.",00:44:26.000,00:44:28.150
"What does the Kronecker
rule produce here?",00:44:28.150,00:44:31.960
"So I take this
first matrix and I",00:44:31.960,00:44:34.350
"put it in here, 2, minus 1,
minus 1, 2, minus 1, minus 1,",00:44:34.350,00:44:43.430
2.,00:44:43.430,00:44:44.190
"So that's the matrix K. And
now, each of those numbers",00:44:47.890,00:44:52.200
"multiplies the second thing,
which here is I. So it's 2I,",00:44:52.200,00:44:58.400
"minus I, minus I, 2I,
minus I, minus I, minus I,",00:44:58.400,00:45:04.210
"minus I, 2I, and 2I.",00:45:04.210,00:45:06.340
"Now that is the second
difference matrix",00:45:10.280,00:45:13.250
that does all the columns.,00:45:13.250,00:45:14.590
"When I add those -- so now
I'm going to add that to that.",00:45:17.650,00:45:21.060
They're both size n squared.,00:45:23.840,00:45:26.530
They give me K2D.,00:45:26.530,00:45:28.450
"So the neat construction of
K2D is Kronecker product kron",00:45:28.450,00:45:34.470
"of I, K plus kron of K, I.",00:45:34.470,00:45:37.810
"That's the matrix and let's look
at what it actually looks like.",00:45:37.810,00:45:45.120
We're seeing it block-wise here.,00:45:45.120,00:45:48.920
We saw it row-wise here.,00:45:48.920,00:45:53.370
"And maybe now we can take one
more look at it, assemble it.",00:45:53.370,00:45:59.200
"So K2D -- I plan to add that
matrix to that matrix to get",00:45:59.200,00:46:08.290
K2D.,00:46:08.290,00:46:10.260
"So it will have -- since they
both have 2's on the diagonal,",00:46:10.260,00:46:15.010
"it'll have 4's on the
diagonal, so 4's all the way,",00:46:15.010,00:46:19.330
but it'll be block-wise.,00:46:19.330,00:46:21.460
"Up here, this
block is K plus 2I.",00:46:24.060,00:46:28.420
"So that block is
-- goes down to 4.",00:46:28.420,00:46:32.280
"Now, the K contributed minus
1's next to the diagonal,",00:46:36.870,00:46:44.500
"and I guess that's it, right?",00:46:44.500,00:46:45.870
"K plus 2I has that block as
-- that's the one, one block.",00:46:45.870,00:46:54.120
"Why is it -- so we're seeing
-- did I get it right?",00:46:54.120,00:47:00.660
"Yes, so we're seeing the
neighbors to the right",00:47:00.660,00:47:05.860
"and left, but now
let me bring in --",00:47:05.860,00:47:11.670
"only now comes the neighbor
above or below and that comes",00:47:11.670,00:47:15.990
"from this off-diagonal
block, minus I,",00:47:15.990,00:47:18.930
"which is then minus
1's to minus 1's.",00:47:18.930,00:47:22.520
These are all 0 blocks.,00:47:26.800,00:47:29.260
"Here will be a minus
the identity block.",00:47:29.260,00:47:35.390
"Then another block,
the diagonal blocks",00:47:35.390,00:47:39.490
"are all the same,
and another one,",00:47:39.490,00:47:43.190
another minus the identity.,00:47:43.190,00:47:44.600
"Now we're getting to
interior mesh points,",00:47:47.780,00:47:53.040
where we see typical rows.,00:47:53.040,00:47:55.010
"So a typical row has
the 4's on the diagonal,",00:47:55.010,00:47:58.170
"the minus 1's left and right,
and the minus 1's far left",00:47:58.170,00:48:02.750
"and far right -- and of course,
what I'm going to ask you is,",00:48:02.750,00:48:06.705
what's the bandwidth?,00:48:06.705,00:48:07.580
"We're coming to
the key point now.",00:48:14.700,00:48:17.230
"What's the bandwidth
of this matrix?",00:48:17.230,00:48:20.290
"I only have two nonzeros above
the diagonal on a typical row,",00:48:20.290,00:48:26.750
"but I have to wait n diagonals
before I get to the second one.",00:48:26.750,00:48:35.540
"So the bandwidth is n, because
I have to wait that long.",00:48:35.540,00:48:39.470
"Then the operation count -- if
I just do ordinary elimination",00:48:39.470,00:48:47.890
"on this matrix, the operation
will be the size of the matrix",00:48:47.890,00:48:54.690
times the bandwidth squared.,00:48:54.690,00:48:56.530
"We can easily check that that's
the right count of operations",00:48:59.680,00:49:05.340
for a banded matrix.,00:49:05.340,00:49:06.670
"This is operations
on a banded matrix.",00:49:06.670,00:49:09.070
So what do we get?,00:49:12.390,00:49:13.940
"The size of the
matrix is N squared.",00:49:13.940,00:49:18.110
"The bandwidth is N,
and it gets squared,",00:49:18.110,00:49:22.550
so we get N to the fourth.,00:49:22.550,00:49:23.870
It's getting up there.,00:49:26.810,00:49:29.360
"If N is 1,000, we've got a
serious-sized matrix here.",00:49:29.360,00:49:34.040
"Still a very sparse matrix,
so it's not like give up,",00:49:34.040,00:49:40.020
"but the question is, does
the matrix stay sparse",00:49:40.020,00:49:45.400
as we do elimination?,00:49:45.400,00:49:47.360
"That's the center
of the next lecture.",00:49:47.360,00:49:52.090
"Can we organize elimination
-- how closely can we organize",00:49:52.090,00:49:55.850
"elimination to preserve all
these zillions of 0's that are",00:49:55.850,00:50:01.170
between here?,00:50:01.170,00:50:02.640
"They're easy to preserve
down here, way up there,",00:50:02.640,00:50:05.650
"but in this intermediate, those
diagonals tend to fill in,",00:50:05.650,00:50:13.880
"and that's not a
happy experience.",00:50:13.880,00:50:17.490
And it's even less happy in 3D.,00:50:17.490,00:50:20.590
"So let me just do this
same calculation for 3D",00:50:20.590,00:50:24.420
and then we're done for today.,00:50:24.420,00:50:25.900
"So K3D -- you might like to
think how K3D could be created",00:50:25.900,00:50:33.430
by the kron operation.,00:50:33.430,00:50:35.780
Let me just imagine it.,00:50:35.780,00:50:37.030
So what's K3D looking like?,00:50:37.030,00:50:39.690
"I've got three directions,
so I have a 6 in the center",00:50:39.690,00:50:44.320
"and six minus 1's
beside it and so 6 goes",00:50:44.320,00:50:49.990
on the diagonal of K3D.,00:50:49.990,00:50:52.800
"Six minus 1's go
on a typical row",00:50:52.800,00:50:55.070
"and how long do I have to wait
until I reach the last one?",00:50:55.070,00:50:58.360
"So I'm again going to do
the size -- which is --",00:50:58.360,00:51:02.450
"the matrix size is
going to be like N cube,",00:51:02.450,00:51:05.610
"and what's the bandwidth
going to be like?",00:51:05.610,00:51:11.680
Maybe I'll ask you now.,00:51:11.680,00:51:13.590
"What do you think
for the bandwidth?",00:51:13.590,00:51:15.720
"How long -- if I just number
it in the simplest way --",00:51:15.720,00:51:20.240
"and that'll be
the key next time,",00:51:20.240,00:51:21.900
is there a better numbering?,00:51:21.900,00:51:23.600
"But if I just number along rows
until the rows fill up a plane",00:51:23.600,00:51:29.870
"of rows and then I
move up to the next,",00:51:29.870,00:51:33.050
"in the z direction to the plane
above, then I have to wait --",00:51:33.050,00:51:42.790
"this was the x and y, so this
gave me a bandwidth of N,",00:51:42.790,00:51:47.390
"but that's not the bandwidth
because I have to wait until I",00:51:47.390,00:51:50.840
"finish the whole plane, until
I go up to the next plane",00:51:50.840,00:51:54.290
"and catch this chap, so
the bandwidth is N squared.",00:51:54.290,00:52:00.360
"Then the operations, which are
size times bandwidth squared,",00:52:00.360,00:52:10.250
"we're up to N^7 and that is
a really horrifying exponent",00:52:10.250,00:52:16.730
"to see, N to the seventh.",00:52:16.730,00:52:19.410
"That means that even
for a moderate --",00:52:19.410,00:52:22.770
"a problem in 3D with
a moderate number,",00:52:22.770,00:52:26.450
"say 1,000 unknowns at
each direction, we have --",00:52:26.450,00:52:32.320
"we're looking, in theory, at
a cost that we can't afford.",00:52:32.320,00:52:42.050
"Of course, there's
a lot to do here.",00:52:42.050,00:52:47.160
"So there's a lot to do if
I stay with direct methods",00:52:47.160,00:52:49.670
"and that's what I'll do for
the next couple of lectures.",00:52:49.670,00:52:54.270
"Then it's really in 3D
that iterative methods",00:52:54.270,00:53:00.940
become essential.,00:53:00.940,00:53:02.540
"I mean, this one, I could
do those by direct methods,",00:53:02.540,00:53:05.850
"2D, by a smarter direct method
than any I've tried here,",00:53:05.850,00:53:12.510
"but in 3D, even
smarter elimination",00:53:12.510,00:53:17.670
"is facing a serious exponent
and loses to iterative methods.",00:53:17.670,00:53:27.370
"So that's what's
coming, actually.",00:53:27.370,00:53:29.020
"So today's lecture, you
see, with the simplest",00:53:29.020,00:53:32.920
"possible matrices, what
the central questions are.",00:53:32.920,00:53:37.890
"Thanks and I've got homeworks
coming back from Mr. Cho",00:53:37.890,00:53:45.400
"and I'll collect any that are
ready to come in and see you",00:53:45.400,00:53:49.430
Wednesday.,00:53:49.430,00:53:51.010

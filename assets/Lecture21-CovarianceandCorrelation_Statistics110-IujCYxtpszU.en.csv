text,start,stop
So today is covariance day.,00:00:00.090,00:00:02.860
"Covariant, is a long
awaited moment that will",00:00:02.860,00:00:05.900
"let us finally deal with
the variance of a sum.",00:00:05.900,00:00:08.350
"For one thing, we said variance is
not linear, unlike expectation.",00:00:08.350,00:00:12.110
"That doesn't mean we don't need ways
to deal with the variance of a sum,",00:00:12.110,00:00:16.010
"it just means we need to think harder,
rather than falsely applying linearity.",00:00:16.010,00:00:20.590
"So on the one hand covariance is what we
need to deal with variance of the sum,",00:00:20.590,00:00:23.740
on the other hand it's what we need when,00:00:23.740,00:00:26.640
"we wanna study two random
variables together instead of one.",00:00:26.640,00:00:29.950
"So it's like variance
except two of them and",00:00:29.950,00:00:32.050
"that's why it's called covariance,
so let's define it.",00:00:32.050,00:00:37.400
"Do some properties, do some examples.",00:00:37.400,00:00:39.370
So at first start with the definition.,00:00:40.490,00:00:43.070
"It's analagous to how we define variance,
except now we have an X and a Y.",00:00:44.220,00:00:48.360
Cuz we're looking at joint distributions.,00:00:48.360,00:00:50.260
"So we have X, we have Y,
we want their covariance.",00:00:50.260,00:00:54.320
"And we define it like this,
covariance of X and Y.",00:00:54.320,00:00:59.490
"X and Y are any two random
variables on the same space.",00:00:59.490,00:01:02.808
"Covariance X,Y equals expected",00:01:02.808,00:01:08.467
"value of X minus it's mean,",00:01:08.467,00:01:13.315
times Y minus its mean.,00:01:13.315,00:01:17.366
"That's just the definitions, so you can't
really argue with it too much but let's",00:01:23.414,00:01:28.702
"stare at it intuitively for a bit and just
see where might this thing have come from?",00:01:28.702,00:01:34.230
"Why define it this way
instead of any other way?",00:01:34.230,00:01:36.540
"Well, first of all, it's a product,
something times something.",00:01:38.300,00:01:45.850
"So we've brought the X stuff and
the Y stuff together into one thing,",00:01:45.850,00:01:48.930
"cuz we're trying to see
how they vary together.",00:01:48.930,00:01:51.410
"And just, obviously,",00:01:51.410,00:01:53.640
"we all know that a positive number
times a positive number is positive,",00:01:53.640,00:01:56.670
"negative times negative is positive,
positive times negative is negative.",00:01:56.670,00:02:00.480
"So if it happens to be true that,",00:02:00.480,00:02:04.627
"This is X relative to its mean,
Y relative to its mean.",00:02:07.006,00:02:11.790
"So now imagine drawing a random sample,
suppose we had a lot of i.i.d",00:02:11.790,00:02:16.540
"pairs X, Y but, the pairs are i.i.d, but",00:02:16.540,00:02:21.530
"within each pair Xi, [INAUDIBLE] Yi,
they have some joint distribution.",00:02:21.530,00:02:25.680
They may not be independent.,00:02:25.680,00:02:27.390
"By the way, we did show before
that if they're independent,",00:02:27.390,00:02:30.750
"then you can write this is just
E of this times E of this.",00:02:30.750,00:02:33.490
"So this is you know, we're interested in
what happens if they are not independent.",00:02:33.490,00:02:38.340
"Well if in that random sample we drew,",00:02:38.340,00:02:41.480
"if most of the time when X is above it's
mean, then also Y is above it's mean.",00:02:41.480,00:02:46.360
"Then you're getting
positive times positive.",00:02:46.360,00:02:49.210
"And if X is below it's mean,
tends to imply that Y is below it's mean,",00:02:49.210,00:02:53.870
"you get negative times
negative is positive.",00:02:53.870,00:02:57.020
So if X being above it's mean,00:02:58.090,00:03:02.540
"tends to imply that Y is above it's mean,
and being below, being below.",00:03:02.540,00:03:06.310
"Then we would say that they're
positively correlated.",00:03:06.310,00:03:08.700
And vice versus.,00:03:10.010,00:03:10.810
"It's be negatively coordinated
if X is above it's mean.",00:03:10.810,00:03:13.410
"It doesn't imply that Y
is below it's mean but",00:03:13.410,00:03:15.610
"it has more of a tendency
that Y is below it's mean.",00:03:15.610,00:03:18.050
"Then we would say that they're
negatively correlated.",00:03:18.050,00:03:21.870
So this is just a measure of that.,00:03:21.870,00:03:25.040
"We'll actually define
correlation in a little while.",00:03:25.040,00:03:27.900
"But correlation is a very familiar term to
everyone cuz people talk about correlation",00:03:27.900,00:03:32.410
all the time.,00:03:32.410,00:03:33.000
"But mathematically, what is correlation?",00:03:33.000,00:03:35.500
It's defined in times of covariance.,00:03:35.500,00:03:37.110
So we'll get to that soon.,00:03:37.110,00:03:39.020
That's just the definition.,00:03:39.020,00:03:40.560
"But just like, you know how for variance,
we had two different ways to write it.",00:03:40.560,00:03:46.610
"We define variance as, notice",00:03:46.610,00:03:49.110
"the way we define variance was expect
the value of X to minus its mean squared.",00:03:49.110,00:03:53.830
"So, if we let X equal Y,
that is just the variance.",00:03:53.830,00:03:58.400
"So we've just proved the theorem already,
so I'll just call this properties.",00:03:58.400,00:04:03.620
The first property to keep in mind,00:04:03.620,00:04:05.320
"is that covariance of X with
itself is the variance.",00:04:06.340,00:04:10.570
"Proof is just let X equal Y,
well that's the definition of variance.",00:04:12.650,00:04:15.900
"But that's a very useful
fact to keep in mind.",00:04:17.260,00:04:19.720
"And secondly, it's symmetric.",00:04:21.630,00:04:23.902
"Covariance X, Y equals covariance Y, X.",00:04:23.902,00:04:30.476
"And that's, again, something you can just
see immediately just swap the X and Y,",00:04:30.476,00:04:35.285
"but it's the same thing, so
it's immediately true that it's symmetric.",00:04:35.285,00:04:40.540
That's also a useful fact.,00:04:40.540,00:04:41.890
"I don't even want to
group it into this list.",00:04:45.290,00:04:48.650
"Right here, what's the alternative
way to write covariance?",00:04:48.650,00:04:53.190
"This is completely analogous to how,",00:04:53.190,00:04:54.670
"where we defined variance as this thing,
this part squared without that part.",00:04:54.670,00:05:00.360
"But then we quickly showed that we could
also write it as E of X squared minus E of",00:05:00.360,00:05:03.876
"X squared,
you know parenthesize the other way.",00:05:03.876,00:05:06.640
"The analog of that formula
which is generalization,",00:05:06.640,00:05:12.051
so this is E of XY minus E of X E of Y.,00:05:12.051,00:05:15.426
"So in general these two
things are not equal.",00:05:17.710,00:05:19.940
"We proved that they are equal if X and
Y are independent, but",00:05:19.940,00:05:24.410
in general they're not equal.,00:05:24.410,00:05:25.430
"Notice that if we let X equal Y,
like in property one here,",00:05:27.420,00:05:31.540
"that's just E of X squared,
minus E of X squared the other way.",00:05:31.540,00:05:34.200
So that is just a version of that formula.,00:05:34.200,00:05:36.540
"And the proof of this is just to
multiply this out and use linearity.",00:05:37.790,00:05:44.410
"We'll just quickly do that
over here just for practice.",00:05:45.460,00:05:49.680
"And we'll just have four
terms use linearity,",00:05:49.680,00:05:51.780
so it should be very straightforward.,00:05:51.780,00:05:53.550
"We're doing this times this,
this times this, and so on.",00:05:53.550,00:05:56.500
So we have E of X.,00:05:56.500,00:05:58.380
I'm just gonna use linearity.,00:05:58.380,00:06:01.070
"The first term, X times Y, E of XY.",00:06:02.210,00:06:04.330
"And then minus, and then we do
this times this, but notice we're",00:06:04.330,00:06:10.370
"doing E of X times this, this thing is
a constant, you can take out the constant.",00:06:10.370,00:06:15.170
"So, that term would just be E of X, E of
Y, and then we have another cross term.",00:06:15.170,00:06:22.950
This one times this one.,00:06:22.950,00:06:24.940
"E of X is just a constant, that comes out.",00:06:24.940,00:06:27.680
"So, that's minus another one that
looks the same, E of X E of Y, and",00:06:27.680,00:06:32.605
then the last term is this times this.,00:06:32.605,00:06:35.710
"Again, that's just a constant,
E of a constant is a constant, so",00:06:35.710,00:06:38.750
"it's plus that thing again, and so",00:06:38.750,00:06:41.610
"that's all it is, minus 2 of them plus
1 of them, so it's the same thing.",00:06:41.610,00:06:47.260
"All right, so that's just an easy
application of linearity of expectation.",00:06:47.260,00:06:51.570
"So most of the time this way is
a little bit easier than this for",00:06:53.420,00:06:59.140
computing covariance.,00:06:59.140,00:07:00.950
"But, like with variance,",00:07:00.950,00:07:02.410
"this one has a little bit more intuitive
appeal because it's just saying",00:07:02.410,00:07:06.120
"X relative to its mean Y relative to
its mean, but it's the same thing.",00:07:06.120,00:07:09.900
"So well we already have two properties,
well,",00:07:11.770,00:07:14.420
"let's get some more
properties of covariance.",00:07:14.420,00:07:17.200
"What if we have a covariance
of X with a constant?",00:07:17.200,00:07:20.922
So I'm letting Y equal a constant C.,00:07:25.773,00:07:28.060
So here Y is C.,00:07:29.790,00:07:32.540
"The expected value of constant C is C,
that's just 0.",00:07:32.540,00:07:36.390
"So it's immediately just 0 just from
the definition if C is a constant.",00:07:36.390,00:07:40.450
"Similarly by symmetry we could
have covariance of C with X.",00:07:41.770,00:07:46.060
"I just happened to write it on
this side but it's symmetric.",00:07:46.060,00:07:48.220
So if C is a constant.,00:07:49.570,00:07:51.220
"Okay, now what if we multiplied by",00:07:54.060,00:07:58.000
"a constant instead of just
having a constant there?",00:07:58.000,00:08:01.370
"So if we have,
let's say the covariance of CX with Y.",00:08:01.370,00:08:09.301
And let's just use this one.,00:08:09.301,00:08:11.470
"To compute this, all we have to
do is replace X by C times X.",00:08:11.470,00:08:15.690
"C comes out, C comes out.",00:08:15.690,00:08:17.470
So C just comes out of the whole thing.,00:08:17.470,00:08:19.430
So constants come out.,00:08:19.430,00:08:20.820
"Okay, so we just prove that just
by plugging in cX in for X,",00:08:23.740,00:08:26.470
and then it's just immediate.,00:08:26.470,00:08:27.530
"Okay, again, c is any constant here.",00:08:29.050,00:08:31.341
"Similarly they could have constant here,
a constant here,",00:08:31.341,00:08:33.652
and just take them both out.,00:08:33.652,00:08:35.020
"Very, very straightforward.",00:08:35.020,00:08:36.090
"All right, and now we want something
that looks kind of like linearity.",00:08:37.120,00:08:41.008
"What happens if we have
the covariance of x with y plus z?",00:08:43.448,00:08:47.957
"So if we take the covariance
of x with y plus z,",00:08:51.945,00:08:56.675
"then what that says to do is to
replace y by y plus z here, okay?",00:08:56.675,00:09:03.590
"And just as a quick
little scratch work for",00:09:03.590,00:09:07.650
"seeing what's going on,
I'm taking xy, replace y by y plus z.",00:09:07.650,00:09:12.520
"Well, of course, that's just xy plus xz,
and now we expect a value of that.",00:09:12.520,00:09:21.180
"So we use linearity, so
it's E of this plus E of that.",00:09:21.180,00:09:24.560
"Similarly, we replace this Y by Y+Z,
so again use linearity, E(Y) + E(Z).",00:09:24.560,00:09:31.700
"And so those terms you get are simply
the sum of the two covariances.",00:09:31.700,00:09:38.113
"So Cov(X,Y) + Cov(X,Z).",00:09:38.113,00:09:43.397
"Just write down the four terms you get and
you've just added the two covariances.",00:09:47.026,00:09:50.490
"So again, all of these things
are basically immediate.",00:09:50.490,00:09:53.670
"I'm not writing out long proofs for these
because all of these things are immediate",00:09:53.670,00:09:57.810
from plugging into the definition.,00:09:57.810,00:10:00.160
"Either this definition or
this equivalent, plug into either one and",00:10:00.160,00:10:04.240
"use linearity of expectation and
all of these follow immediately.",00:10:04.240,00:10:07.380
"So these two together are especially
useful, and they're called,",00:10:09.270,00:10:13.895
"it's not linearity, but
it's called bilinearity.",00:10:13.895,00:10:17.576
"Bilinearity is just
a fancy term that means,",00:10:20.459,00:10:23.134
"If you imagine treating one
coordinate as just kind of fixed and",00:10:25.165,00:10:28.651
"you're working with the other coordinate,
it looks like linearity, right?",00:10:28.651,00:10:33.610
"So like here, notice the Y just stayed
as Y, and what happened to the cX?",00:10:33.610,00:10:38.860
"Well, I took out the constant
just like linearity.",00:10:38.860,00:10:42.023
And what happened here?,00:10:42.023,00:10:42.970
X just stayed x throughout.,00:10:45.450,00:10:47.300
"But if you just look at the y + z part,
we split it out into the y and a z.",00:10:47.300,00:10:52.050
"So it looks like linearity if you're
going one coordinate at a time.",00:10:52.050,00:10:56.020
"I just happened to write it this way, but
obviously, I could have done x + y, z and",00:10:56.020,00:11:00.230
it would be analogous.,00:11:00.230,00:11:01.400
"I could have put the constant over there,
or",00:11:01.400,00:11:03.190
"a constant here, a constant there, okay.",00:11:03.190,00:11:06.960
"So those are really useful
properties that kind of",00:11:06.960,00:11:12.100
"if you use these properties, you can avoid
a lot of ugly calculations that is you can",00:11:12.100,00:11:17.210
"just like apply this rather than always
having to go back to the definition.",00:11:17.210,00:11:21.190
"Just like linearity is incredibly useful,",00:11:21.190,00:11:23.560
"bilinearity is incredible useful,
when working with covariances.",00:11:23.560,00:11:28.210
"So, and kind of an easy kind of
way to remember this is it kind",00:11:29.400,00:11:34.627
"of looks this distributive property,
here this is just",00:11:34.627,00:11:39.542
"the distributive property x times
y plus z is x, y plus x, z.",00:11:39.542,00:11:44.902
"It kind of looks like that
as if I'm doing code expect,",00:11:44.902,00:11:47.502
"it's not literally multiplication,
it's covariance, but",00:11:47.502,00:11:50.500
"I'm doing covariance of this and
this and this and this.",00:11:50.500,00:11:53.520
"Right, so if I wanted to extend that to
what happens if we have more of them.",00:11:53.520,00:11:58.564
Let's say we had covariance of X plus Y.,00:12:02.307,00:12:06.530
"I mean this doesn't really need to be
listed separately but for practice,",00:12:06.530,00:12:10.410
let's just do it.,00:12:10.410,00:12:11.320
Just apply that property five repeatedly.,00:12:13.300,00:12:15.840
"And we're gonna get the covariance of
this and this, this and this, this and",00:12:15.840,00:12:18.700
"this, that.",00:12:18.700,00:12:19.430
"It's just like multiplying
two polynomials,",00:12:19.430,00:12:22.530
or however you usually do that thing.,00:12:22.530,00:12:24.250
"So, So we can immediately just
write this down as four terms.",00:12:24.250,00:12:32.573
"Cov(X,Z) + Cov(X,W) + Cov(Y,Z) + Cov(Y,W).",00:12:32.573,00:12:39.260
"And that follows immediately just by
using that property 5 repeatedly.",00:12:39.260,00:12:43.381
"And more generally than that,",00:12:46.177,00:12:47.828
"let's just write what happens if we have
a covariance of one sum with another sum.",00:12:47.828,00:12:52.970
"I don't wanna write out nine terms,",00:12:52.970,00:12:55.060
"let's just write the general
thing once and for all.",00:12:55.060,00:12:57.260
"So we have a covariance
of one sum of terms.",00:12:57.260,00:13:00.400
Let's say we have a sum over i of,00:13:00.400,00:13:04.140
"AiXi, where Ai is a constants.",00:13:06.740,00:13:10.650
"so this is linear combination
of random variables.",00:13:10.650,00:13:14.150
"And then, let's say i goes from 1 to m.",00:13:14.150,00:13:16.110
"And then we have another one,
let's say j=1 to n of bjYj.",00:13:16.110,00:13:22.310
"So, we want the covariance, so
it looks like this complicated thing.",00:13:22.310,00:13:28.350
"Okay, but as soon as you think about
what's the structure of the problem,",00:13:28.350,00:13:31.880
"it's just the covariance of
one sum with another sum.",00:13:31.880,00:13:35.380
"So if you apply that property
five over and over and",00:13:35.380,00:13:38.220
"over again,
we don't literally have to do that.",00:13:38.220,00:13:40.760
"But conceptually we're just using that
property over and over and over again, and",00:13:40.760,00:13:44.490
just think about what you're gonna get.,00:13:44.490,00:13:46.340
"And also use property four
to take out the constants.",00:13:46.340,00:13:49.710
"Well, it just means you're
gonna get a sum over all ij",00:13:49.710,00:13:52.420
"of the covariance of individual terms,
right?",00:13:53.590,00:13:56.150
"Cuz it's just saying,",00:13:56.150,00:13:56.830
"you know, take one term here and
co-vary it with one term here.",00:13:56.830,00:14:00.780
For all possible pairs.,00:14:00.780,00:14:03.490
So the sum over all ij of,00:14:03.490,00:14:07.899
aibj covariance Xi Yj.,00:14:07.899,00:14:12.550
"So that's just a very, I know this
looks complicated but it's no different",00:14:12.550,00:14:17.639
"from property five that just means we
used it a lot of times instead of once.",00:14:17.639,00:14:22.663
"So a lot of times it'd be easier to use
this kind of thing rather than going back",00:14:22.663,00:14:26.967
"to the definition and multiplying
everything out in terms of expectation.",00:14:26.967,00:14:31.870
"It's often easier to be able to
work directly with covariance.",00:14:31.870,00:14:34.560
"All right, so that shows us how,",00:14:36.350,00:14:41.460
"property one says how covariance
is related to variance, but",00:14:41.460,00:14:45.190
"it doesn't show us how it would be
useful in actually computing a variance,",00:14:45.190,00:14:49.600
the variance of a sum that is.,00:14:50.790,00:14:54.120
"Okay, so one of the main reasons
we want covariance is so",00:14:54.120,00:14:57.490
that we can deal with sums.,00:14:57.490,00:14:59.530
"So let's just work out
the variants of a sum.",00:15:00.530,00:15:03.530
"Let's say we have the variance of x1 + x2
to start with, but then we can generalize",00:15:05.840,00:15:11.730
"that to a sum of any number of terms
just by using this one repeatedly, okay?",00:15:11.730,00:15:16.660
"Well, we already know how to do this,
because by property 1,",00:15:16.660,00:15:21.080
"that's the covariance of (x1+x2),
with itself.",00:15:21.080,00:15:26.250
"But by property five, or
whichever property six,",00:15:26.250,00:15:30.649
"what's the covariance
of x1 + x2 with itself?",00:15:30.649,00:15:34.960
"Well, we just have those four terms.",00:15:34.960,00:15:36.380
"We have the covariance of x1 with
itself but that's just the variance.",00:15:36.380,00:15:40.890
"And we have covariance of x2 with itself,
that's just the variance of x2.",00:15:40.890,00:15:46.163
"And then we have two cross terms,
we have the covariance of x1 and x2.",00:15:46.163,00:15:51.152
And we have the covariance of x2 and x1.,00:15:51.152,00:15:53.559
"But by the symmetry property,
those are the same thing.",00:15:53.559,00:15:56.010
"So it's simpler to just write it as
2 times the covariance of x1 and x2.",00:15:56.010,00:16:01.010
"In particular this says that
if the covariance is 0,",00:16:05.310,00:16:09.496
"then the variance of the sum
is the sum of the variances.",00:16:09.496,00:16:14.120
And that's an if and only if statement.,00:16:14.120,00:16:16.770
"So one case were that's true
is if they're independent,",00:16:16.770,00:16:20.190
"we showed that before that if they're
independent then the covariance is 0.",00:16:20.190,00:16:24.390
"So if they're independent, this is gone.",00:16:24.390,00:16:26.980
"And we'll also see examples where
they're not independent, but",00:16:26.980,00:16:29.290
"this term is still zero and
so, so then it's true.",00:16:29.290,00:16:32.370
"Okay, but in general, you can't say
the variance of the sum of the sum",00:16:32.370,00:16:35.770
"of the variances,
because you have these covariance terms.",00:16:35.770,00:16:38.358
"Yeah, question?",00:16:38.358,00:16:39.729
"&gt;&gt; [INAUDIBLE]
&gt;&gt; That's if and only if the covariance",00:16:39.729,00:16:44.880
"is 0, that the variance of the sum
will be the sum of the variance.",00:16:44.880,00:16:51.030
"So let's write what would happen
if there's more than two of them,",00:16:51.030,00:16:54.498
"variance x1 + blah, blah, blah + xn.",00:16:54.498,00:16:58.000
"Just applying, so that's the covariance
of this sum with itself, so",00:16:59.560,00:17:03.510
we can just apply this result.,00:17:03.510,00:17:06.240
"So again, it's gonna be the sum
of all the variances, and",00:17:07.270,00:17:09.870
"then we're gonna have
all these covariances.",00:17:09.870,00:17:11.820
"So add up all the variances, and
then add up all the covariances.",00:17:14.090,00:17:18.431
"And so, you're gonna have a covariance
of x1 and x2, x2 and x1, x1 and x3,",00:17:18.431,00:17:23.307
"x3 and x1, all those things.",00:17:23.307,00:17:25.229
"I think it's easiest if
we write it as 2 times",00:17:25.229,00:17:30.207
"the sum over i less than j covariance x i,
x j,",00:17:30.207,00:17:35.185
it's easy to forget the 2 here.,00:17:35.185,00:17:38.870
"I could have also written it to
i not equal to j in this case I",00:17:38.870,00:17:42.951
would have not put the 2.,00:17:42.951,00:17:44.830
"It's simply the question of
are you going to list cov(x1,x2)",00:17:44.830,00:17:49.488
"separately from cov(x2,
x1) or group them together.",00:17:49.488,00:17:53.660
"Seems a little simpler to
group them together, but",00:17:53.660,00:17:56.659
then we need to remember to put the 2.,00:17:56.659,00:17:58.900
"Since I specified less than j,
then I have cov(x1, x2) listed here, but",00:17:58.900,00:18:03.660
"not cov(x2, x1), cuz I included that here.",00:18:03.660,00:18:06.750
"All right, so that's the general
way to get the variance of a sum,",00:18:07.860,00:18:11.556
"and we'll there are some examples
of that in a few minutes.",00:18:11.556,00:18:14.860
"First I just wanna make sure
that the connection with",00:18:14.860,00:18:19.162
"independence is clear and
we also need to define correlation.",00:18:19.162,00:18:24.361
So theorem says that if x and,00:18:24.361,00:18:28.755
"y are independent,
Then they are uncorrelated.",00:18:28.755,00:18:36.766
"The definition of uncorrelated is
just that the covariance is 0.",00:18:39.383,00:18:42.603
That's just definition.,00:18:42.603,00:18:44.390
"I.e, cov(x,y) = 0.",00:18:46.110,00:18:49.130
"And we actually proved this last time when
we just didn't have the terminology yet.",00:18:50.920,00:18:59.640
"At least we proved in the continuous case,
but the discreet case is analogous.",00:18:59.640,00:19:04.060
"So we proved this using
the 2 dimensional lotus",00:19:04.060,00:19:08.310
"thing that we did e of x times y,
all right?",00:19:08.310,00:19:12.320
"Equals e of x, e of y in the independent
case, so we showed that before.",00:19:12.320,00:19:16.510
Converse is false.,00:19:18.330,00:19:19.850
"And that's a common mistake is
to show the covariance is 0, and",00:19:23.630,00:19:28.902
"then just leap to the conclusion
that they're independent.",00:19:28.902,00:19:34.650
"If the covariance is 0, and that's all we
know, they may or may not be independent.",00:19:34.650,00:19:39.570
"So just to give a simple counter example
showing why this doesn't imply this.",00:19:41.260,00:19:47.290
"Let's just consider an example
with normal random variables.",00:19:49.390,00:19:54.340
"So let's let z be standard normal,
and let n and we'll let x = z.",00:19:54.340,00:20:01.160
"Slightly redundant notation but
I'm just in the habit of using z for",00:20:02.300,00:20:07.430
standard normals and y = z squared.,00:20:07.430,00:20:10.590
"So we're looking at a normal and
it's square, okay?",00:20:10.590,00:20:16.140
"So now let's compute the covariance for
this example.",00:20:16.140,00:20:22.095
"Cov(X, Y) = E(X, Y)- E(X)E(Y).",00:20:22.095,00:20:29.160
"In terms of Z,
that's E(Z cubed) -E(Z)E(Z squared),",00:20:32.905,00:20:38.841
"but both terms are just 0,
because we saw before",00:20:38.841,00:20:43.433
"that the odd moments of
a standard normal are 0.",00:20:43.433,00:20:48.060
"That's an odd moment and
that's an odd moment so it's just 0- 0.",00:20:48.060,00:20:52.605
"So they're uncorrelated, but
they're clearly not independent.",00:20:52.605,00:20:58.720
"In fact, they are very non-independent,
I should say, very dependent.",00:21:00.690,00:21:06.070
Avoid too many double negatives.,00:21:07.270,00:21:09.060
"So they're very dependent, in fact, y is",00:21:10.240,00:21:15.250
"a function of x, so
that they're extremely dependent.",00:21:15.250,00:21:20.140
"If you know x, you know y,
complete information.",00:21:20.140,00:21:23.620
So y is actually a function of x.,00:21:25.500,00:21:27.450
"Dependent just means there's
some information, right?",00:21:27.450,00:21:32.600
"It doesn't have to be
complete information.",00:21:32.600,00:21:33.970
"In this case, if we know x,
we have complete information about y,",00:21:33.970,00:21:39.325
y is a function of x.,00:21:39.325,00:21:45.000
"And if we go the other way around,
if we know y, well, we don't know x, but",00:21:45.000,00:21:49.030
"we do know its magnitude, all right?",00:21:49.030,00:21:51.830
"If we know z squared,
then we can take the square root and",00:21:51.830,00:21:54.520
"we'll get the absolute value, so
we know it up to a plus or minus.",00:21:54.520,00:21:57.679
"So that also shows it's dependent going
the other, which we didn't need to do but",00:21:59.080,00:22:03.370
it's just nice to think.,00:22:03.370,00:22:04.750
"If you know this, okay, we know this,
if we know this, then what do we know?",00:22:04.750,00:22:08.290
"Well, we know it up to a sign.",00:22:08.290,00:22:09.800
"So I would just say y also determines x,
at least it determines",00:22:11.600,00:22:17.607
"it up to a sign, so
it determines the magnitude of x.",00:22:17.607,00:22:22.700
"So, okay, so that's just an example
that shows the converse is false.",00:22:26.592,00:22:30.570
"But it's kind of a handy counter example
to keep in mind for a lot of things.",00:22:30.570,00:22:36.360
"So kind of intuitively
what's going wrong here,",00:22:36.360,00:22:39.670
"I mean, there's nothing wrong with this.",00:22:39.670,00:22:41.230
"But why the definition
doesn't capture this is part",00:22:41.230,00:22:46.050
"of the intuition of correlation is it's
kind of a measure of linear association.",00:22:46.050,00:22:50.470
"And those of you who
have taken Stat 100 or",00:22:50.470,00:22:52.980
104 see a lot of things a lot like that.,00:22:52.980,00:22:55.220
"Where you actually have a data set,
and if it kind of looks like it's",00:22:55.220,00:22:58.260
"sloping upwards generally you
have this cloud of points.",00:22:58.260,00:23:01.360
"And as they kind of go upwards or
downwards, that kind of thing.",00:23:01.360,00:23:05.510
"It's measuring linear
trends in some sense.",00:23:05.510,00:23:08.040
"There's a theorem that we're not gonna
prove that says, if every function of x is",00:23:09.190,00:23:13.810
"uncorrelated every function of y,
then they're independent.",00:23:13.810,00:23:17.830
But just having the linear things be,00:23:17.830,00:23:21.310
"uncorrelated is not enough
as this example shows.",00:23:21.310,00:23:24.290
"Okay, here they have this
quadratic relationship",00:23:26.420,00:23:30.064
"there is no linear relationship
that the kind of intuition on that.",00:23:30.064,00:23:34.901
"All right, so
let's also define correlation and",00:23:34.901,00:23:38.324
"then I will do some examples of how to
use this to compute the variance of",00:23:38.324,00:23:43.133
"the things that we did not
already know the variance.",00:23:43.133,00:23:46.819
"Okay, so once we have covariance, which
we do, correlation is easy to define.",00:23:48.230,00:23:52.650
"And I'll tell you some of the intuition
as well as what's the math.",00:23:52.650,00:23:58.190
So here's the definition of correlation.,00:23:59.200,00:24:01.500
"You can think of it as just
a standardized version of covariance.",00:24:03.678,00:24:07.095
"So correlation, which you either write
as Cor, or usually I write it as Corr.",00:24:10.093,00:24:16.653
"Just because R's tend to look like V's
sometimes if you're writing too fast.",00:24:16.653,00:24:20.821
"Corr(X, Y), usually it's defined this way,",00:24:22.462,00:24:28.190
"as the covariance, and then we divide by
the product of the standard deviations.",00:24:28.190,00:24:34.830
"Remember, standard deviation's
just the square root of variance.",00:24:37.688,00:24:40.287
"So take the covariance,",00:24:40.287,00:24:42.192
"divide by the square root of
the product of the variances.",00:24:42.192,00:24:46.645
But that's the usual definition.,00:24:46.645,00:24:49.020
"I actually would prefer to define it
a different way, and I'll show you why,",00:24:49.020,00:24:52.900
that these are equivalent.,00:24:52.900,00:24:54.020
"I would prefer to define
it as the covariance of X,",00:24:54.020,00:25:00.791
remember standardization?,00:25:00.791,00:25:04.840
"If we have any normal, we subtract the
mean, divide by the standard deviation,",00:25:04.840,00:25:09.130
that gives us standard normal.,00:25:09.130,00:25:10.390
So that's called standardization.,00:25:10.390,00:25:11.630
"Now here,
I'm not assuming anything is normal, but",00:25:11.630,00:25:14.537
that the same standardization makes sense.,00:25:14.537,00:25:17.000
"That we take X, we subtract its mean,
we divide by its",00:25:17.000,00:25:22.062
"standard deviation, and
then we do the same thing with Y.",00:25:22.062,00:25:27.840
"So we've standardized both X and
Y, And we take their covariance.",00:25:27.840,00:25:34.090
"So correlation means standardize them
first, then take the covariance.",00:25:34.090,00:25:40.845
The reason that this is a useful thing to,00:25:40.845,00:25:44.128
"do is that covariance kinda
has an annoying property,",00:25:44.128,00:25:48.708
"as far as interpretation in terms
of units and things like that.",00:25:48.708,00:25:54.830
"If you imagine X and Y are distances,
right, they're random variables but",00:25:54.830,00:26:00.984
"they're representing a distance quantity,
okay?",00:26:00.984,00:26:05.327
"And if you measured X and Y in nanometers,",00:26:05.327,00:26:09.292
"and then someone else
working on the same problem",00:26:09.292,00:26:13.939
"measures them in light years
instead of nanometers,",00:26:13.939,00:26:19.039
"you're gonna get extremely
different answers.",00:26:19.039,00:26:24.510
"So if I just tell you,
the covariance between my X and",00:26:24.510,00:26:29.246
"Y is 42, what does that tell you?",00:26:29.246,00:26:32.221
"You have to think really hard about
what are the units, what's going on,",00:26:32.221,00:26:37.119
"is 42 a big number or
a small number, right?",00:26:37.119,00:26:40.310
"I mean, it's the answer to life,
the universe and everything, but",00:26:40.310,00:26:43.007
is it a big number or a small number?,00:26:43.007,00:26:44.416
"I don't know, because the units thing.",00:26:44.416,00:26:47.448
"This is a dimensionless quantity,",00:26:47.448,00:26:49.525
"dimensionless just
basically means unitless.",00:26:49.525,00:26:52.660
"So if X is measured in nanometers,
and you're subtracting off nanometers,",00:26:52.660,00:26:56.900
that's still nanometers.,00:26:56.900,00:26:58.270
"Remember, that's why we define
standard deviation, also.",00:26:59.400,00:27:03.570
"Standard deviation has
a square root in it, so",00:27:03.570,00:27:05.370
"mathematically, it's pretty annoying
to deal with these square roots.",00:27:05.370,00:27:08.460
"Mathematically, it's nicer
to work with variance,",00:27:08.460,00:27:10.470
"but intuitively, the variance
would be in nanometers squared.",00:27:10.470,00:27:14.660
"Now we're back to nanometers,
divide nanometers by nanometers,",00:27:14.660,00:27:17.690
we'll get a dimensionless quantity.,00:27:17.690,00:27:20.000
So that's a major advantage of this.,00:27:20.000,00:27:23.020
"And I guess I should tell you briefly,
why is this thing the same as this?",00:27:23.020,00:27:25.910
"Well, you should kind of just
think about those properties,",00:27:25.910,00:27:29.450
I'll just say this kind of quickly.,00:27:29.450,00:27:31.270
"First of all, subtracting the mean,
that's just adding a constant,",00:27:31.270,00:27:36.110
"that's not gonna affect
the covariance at all.",00:27:36.110,00:27:39.430
"So I could have left this out, but it's
just useful to think of standardizing.",00:27:39.430,00:27:43.937
"Cuz this standardization, what it does is
takes X, which could have any mean and",00:27:43.937,00:27:48.291
"any variance, and
makes it have mean 0 and variance 1.",00:27:48.291,00:27:51.390
That's why it's called standardization.,00:27:51.390,00:27:54.078
"The part that's affecting what's
going on is the standard deviation.",00:27:54.078,00:27:58.360
"But from one of those
properties that we wrote,",00:27:58.360,00:28:00.241
"we can just pull out the standard
deviations, and we get exactly that.",00:28:00.241,00:28:03.076
"So they're exactly the same thing,",00:28:03.076,00:28:04.861
"I just think this one's a little
more intuitive to think about.",00:28:04.861,00:28:07.980
"Okay, so
one quick theorem about correlation.",00:28:09.180,00:28:11.926
Correlation can never equal 42.,00:28:15.505,00:28:17.480
"More generally,
correlation is always between -1 and 1.",00:28:17.480,00:28:22.212
"So not only is it something more
interpretable in the sense that it doesn't",00:28:31.590,00:28:35.314
depend on what system of units you used.,00:28:35.314,00:28:37.336
"It's also more interpretable in that,
if I say a correlation is 0.9?",00:28:37.336,00:28:42.096
"That's a pretty high correlation,",00:28:42.096,00:28:44.717
"cuz I know the largest it can be is 1,
okay, so that's very useful.",00:28:44.717,00:28:50.280
"And kind of an interesting fact about this
inequality is that it's essentially just",00:28:50.280,00:28:53.845
Cauchy-Schwartz.,00:28:53.845,00:28:54.755
"For those of you who have seen
the Cauchy-Schwarz inequality in linear",00:28:54.755,00:28:59.586
algebra or elsewhere.,00:28:59.586,00:29:01.207
"The Cauchy-Schwarz is one of the most
important inequalities in all of",00:29:01.207,00:29:04.855
mathematics.,00:29:04.855,00:29:05.657
"And if you put this, if you rewrite this
statement in a linear algebra setting,",00:29:05.657,00:29:09.995
"you can show that it's
essentially Cauchy-Schwarz.",00:29:09.995,00:29:13.080
"If you haven't seen Cauchy-Schwartz yet,
we'll come back",00:29:13.080,00:29:15.960
"to it later in the semester, and you
don't need to worry about it right now.",00:29:15.960,00:29:18.490
"But for those of you who have,
I wanted to make the connection right now.",00:29:18.490,00:29:22.571
So let's prove this fact.,00:29:22.571,00:29:24.390
"So one proof would just be to put it
into the Cauchy-Schwarz framework, and",00:29:26.690,00:29:30.034
apply Cauchy-Schwarz.,00:29:30.034,00:29:31.195
"But that doesn't really show
what's going on, first of all.",00:29:31.195,00:29:35.296
"And secondly, that assumes you're
familiar with Cauchy-Schwarz.",00:29:35.296,00:29:38.910
So let's just prove it directly.,00:29:38.910,00:29:40.689
"So, first of all, Math classes,",00:29:43.249,00:29:49.606
"you'll often see the acronym WLOG,
Without Loss of Generality.",00:29:49.606,00:29:54.254
"We're going to assume X and
Y are already standardized.",00:29:54.254,00:30:00.154
"If they're not already standardized, so
we're trying to prove this inequality.",00:30:03.985,00:30:08.180
"We may as well just assume from the start
that they've been standardized,",00:30:08.180,00:30:13.270
"standardized meaning that
they have mean 0, variance 1.",00:30:13.270,00:30:16.110
"Because if they weren't standardized,
well,",00:30:16.110,00:30:18.010
"I could just make up some new notation, x
tilde, y tilde for the standardized ones.",00:30:18.010,00:30:23.140
"But this says that the correlation
will be the same anyway, so",00:30:23.140,00:30:26.520
"we may as well assume that
they're already standardized.",00:30:26.520,00:30:28.840
"All right, so
now let's just compute the variance.",00:30:28.840,00:30:32.600
"This is actually good practice
with property seven there.",00:30:32.600,00:30:41.149
Let's compute Var(X + Y).,00:30:41.149,00:30:44.848
"Well, that's Var(X) + Var(Y) + 2 Cov(X,
Y).",00:30:44.848,00:30:51.576
"And for some reasons,
statisticians often like to call",00:30:51.576,00:30:56.778
"the correlation rho, so
I'll follow that trend.",00:30:56.778,00:31:01.544
"Corr(X, Y), that's just notation,
let's just name that rho.",00:31:01.544,00:31:09.119
"All right, so that's the variance, but
I assumed they were standardized, so",00:31:09.119,00:31:13.557
this is 1 + 1.,00:31:13.557,00:31:14.495
"And if they're standardized already,
then the covariance is the correlation,",00:31:14.495,00:31:18.704
because they're standardized.,00:31:18.704,00:31:20.480
"So that's just 1 + 1 + 2 rho, so
that's really just 2 + 2 rho, right?",00:31:20.480,00:31:27.350
"On the other hand, we could look
at the variance of the difference.",00:31:29.040,00:31:34.030
"Again, that's good practice with
variances of sums and differences.",00:31:34.030,00:31:39.042
"A common mistake is to say,
that's Var(X)- Var(Y).",00:31:39.042,00:31:43.221
"Which we talked about that fact before
when we were talking about sums and",00:31:43.221,00:31:46.290
"differences of normals,
variances can't be negative.",00:31:46.290,00:31:49.550
"So think of this not as X- Y,
think of this as X + -Y,",00:31:49.550,00:31:56.192
"so it still adds, Var(X)- Var(Y).",00:31:56.192,00:32:01.380
Now we subtract.,00:32:01.380,00:32:02.540
"Just check this, right,
is the covariance of x-y with itself?",00:32:04.610,00:32:07.430
"So we have a- on the covariance part,
but not on these variance terms.",00:32:07.430,00:32:12.585
So that's just 2- 2 rho.,00:32:14.954,00:32:16.840
"Okay, well we're running
out of space on this board,",00:32:19.980,00:32:22.948
"and that's actually the end of the proof
because variance is non-negative.",00:32:22.948,00:32:27.218
"So these two inequalities say
that rho is between- 1 and 1.",00:32:28.854,00:32:33.606
"All right, so that shows a correlation
is always between- 1 and 1.",00:32:36.705,00:32:41.914
"And so in general it is easier to work
with covariances than correlations but",00:32:44.360,00:32:49.070
correlations are more intuitive and,00:32:49.070,00:32:51.940
"standardized with everything
between- one and one.",00:32:51.940,00:32:55.575
"Okay, so I wanted to for
the rest of the time do some examples with",00:32:55.575,00:33:01.840
"with this thing, and
also with computing covariances for",00:33:03.220,00:33:07.750
"certain problems we
might be interested in.",00:33:07.750,00:33:10.840
"So let's talk about the multinomial cuz
we were talking about that last time.",00:33:10.840,00:33:14.719
"And now we actually have the tools
to deal with the covariances,",00:33:14.719,00:33:19.184
"within a multinomial, okay?",00:33:19.184,00:33:22.030
"So, this is just an example.",00:33:22.030,00:33:24.010
"But it's an important example,
cuz multinomials come up a lot.",00:33:24.010,00:33:27.190
"So we wanna compute covariances,
if we have a multinomial, okay?",00:33:27.190,00:33:32.240
So covariances in a multinomial.,00:33:32.240,00:33:35.630
"That is,
this multinomial is this vector, right?",00:33:37.560,00:33:39.780
"It's about how many people
are in category one,",00:33:39.780,00:33:41.720
"how many people are in category two,
and so on.",00:33:41.720,00:33:43.621
"So you can take any two of those counts
of how many people are in category one,",00:33:43.621,00:33:47.589
"how many people are in category five, and",00:33:47.589,00:33:49.759
"compute the covariance
of those things,right?",00:33:49.759,00:33:52.378
That's a very natural thing to look at.,00:33:52.378,00:33:54.020
"And I actually know four or
five ways to derive this,",00:33:56.690,00:34:02.370
"and I really like this example, so I will
probably come to this later with some of",00:34:02.370,00:34:06.670
"the other methods, but for
now let's just do one method, okay?",00:34:06.670,00:34:10.990
"So, we have this multinomial,
so we have this vector,",00:34:10.990,00:34:15.590
"using the notation from last time,
we have k different categories.",00:34:15.590,00:34:19.650
"And Xj is the number of people or
objects in the jth category.",00:34:19.650,00:34:23.950
"And this is multinomial, n,
that there are n objects or people.",00:34:25.470,00:34:31.190
"And the probabilities are given
back by some vector P.",00:34:31.190,00:34:35.670
"That's just gives the probability for
each category, okay?",00:34:35.670,00:34:40.745
"And we wanna find
the covariance of Xi with Xj,",00:34:40.745,00:34:44.999
"For all i and j, Right?",00:34:50.027,00:34:53.827
"So first of all let's
consider the case I = j.",00:34:56.210,00:35:00.229
"Then we just have a covariance
of Xi with itself.",00:35:02.437,00:35:06.260
And we know that's just a variance of Xi.,00:35:06.260,00:35:09.460
"And last time we talked about
the fact that if we define",00:35:09.460,00:35:14.488
"success to be being in category i,
we just have a binomial, so",00:35:14.488,00:35:20.297
"for this, we just use the variance
of the binomial, npi 1- pi.",00:35:20.297,00:35:26.473
"So that's easy, the more interesting part
is what happens if i is not equal to j.",00:35:28.437,00:35:35.396
"Okay, now if we,",00:35:40.595,00:35:41.984
"I think it's easier to just think
concretely in terms of the first two.",00:35:41.984,00:35:47.557
"So let's just find covariance of X1 and
X2.",00:35:47.557,00:35:50.746
"If we know how to do this,
we could always just relabel things and",00:35:54.010,00:35:58.330
get X5 and X12 or whatever we want.,00:35:58.330,00:36:00.900
"But it's just easier to think
concretely in terms of X1 and",00:36:00.900,00:36:04.090
"X2, rather than having so
much notation going around.",00:36:04.090,00:36:06.290
"Okay, so I'll find this one first.",00:36:07.880,00:36:11.730
And there's a lot of ways to do this.,00:36:11.730,00:36:13.910
"Let's just think about
it intuitively first.",00:36:13.910,00:36:16.280
"Intuitively, you think this is positive,
negative or zero?",00:36:18.720,00:36:21.385
"&gt;&gt; [INAUDIBLE]
&gt;&gt; Negative, why?",00:36:21.385,00:36:26.264
"&gt;&gt; [INAUDIBLE]
&gt;&gt; Exactly, so",00:36:26.264,00:36:29.152
"if you somehow computed this
an you got a positive number,",00:36:29.152,00:36:33.170
"you shouldn't just be happy you're
done with the problem and move on.",00:36:33.170,00:36:38.091
"You should stop and think,
does a positive number make sense here?",00:36:38.091,00:36:40.960
"As you just said, if you knew that there
were more people in the first category,",00:36:40.960,00:36:45.860
"like there's tons of people
in the first category,",00:36:45.860,00:36:48.470
"there's fewer people left over who
could be in the second category.",00:36:48.470,00:36:51.500
"So It's like these categories are kind
of competing for membership, right?",00:36:51.500,00:36:56.150
"You have a fixed number of people,
not like the chicken and",00:36:56.150,00:36:59.171
"egg problem we had a Poisson
number of eggs, okay.",00:36:59.171,00:37:01.835
"So fixed number of eggs competing for
different categories,",00:37:01.835,00:37:05.371
"more in one then you'd expect
less in the other, right?",00:37:05.371,00:37:08.750
"So they should be negatively correlated,
all right?",00:37:08.750,00:37:11.017
"So now how do we do this,
well there's a bunch of ways as I said,",00:37:11.017,00:37:15.918
"but one way that I especially like is,",00:37:15.918,00:37:18.877
"to relate this back to stuff we
did last time, we talked about",00:37:18.877,00:37:23.595
"the lumping property of the multi
nominal try to relate it to this.",00:37:23.595,00:37:29.290
"Normally, you'd think of this as
a way to find the variance of a sum.",00:37:29.290,00:37:32.820
"But if we know this, this and this,
then obviously we know this also.",00:37:32.820,00:37:37.310
So let's actually do it this way and,00:37:37.310,00:37:39.994
"I'll probably do some of the other
methods later, not today.",00:37:39.994,00:37:44.744
"So we have,
Let's take the variance of the sum,",00:37:44.744,00:37:50.101
Let's call this thing C.,00:37:55.685,00:37:56.780
"Just to have some notation, so
we're trying to solve for C, okay.",00:37:59.730,00:38:03.160
"So at the variance of the sum
equals the sum of the variances.",00:38:03.160,00:38:07.365
Now the variance of X1 is nP1 (1- P1) and,00:38:07.365,00:38:11.323
"the variance of X2 is nP2 (1- P2) and
then plus,",00:38:11.323,00:38:15.992
"twice the covariance but
I just named the covariance C,",00:38:15.992,00:38:20.763
"just to have a simple name for
it, so it's + 2C.",00:38:20.763,00:38:25.050
"So the only thing,
we wanna solve for this.",00:38:26.190,00:38:28.680
"The only thing left that
we haven't gotten is this.",00:38:28.680,00:38:31.770
"But then variance of X1 + X2 follows
immediately from what I was talking about",00:38:31.770,00:38:36.327
last time with the lumping property.,00:38:36.327,00:38:38.513
"This just says merge the first two
categories together into one bigger",00:38:38.513,00:38:43.292
"category, okay?",00:38:43.292,00:38:44.590
"If we do that, It's still binomial, right?",00:38:44.590,00:38:47.230
"Now we're defining success to mean being
a member of category one or category two.",00:38:47.230,00:38:54.320
"Still binomial, so we can
immediately right down, thus well n.",00:38:54.320,00:38:59.430
"Now the probability of
success is (P1 + P2)(1- P1 +",00:38:59.430,00:39:04.722
"P2) So now we know everything
in this equation except C.",00:39:04.722,00:39:12.400
"To solve for C,
multiply things out, factor it,",00:39:12.400,00:39:15.955
"however you want just do the algebra,
easy algebra at this point.",00:39:15.955,00:39:20.713
"So I'm not gonna show
you like write that or",00:39:20.713,00:39:24.673
"multiply this times this and
just multiply it out, simplify it and",00:39:24.673,00:39:30.833
"what you'll get is the covariance
of X1 and X2 = -nP1 P2.",00:39:30.833,00:39:36.665
"And so in general, That was just for
X1 and X2, just for concreteness.",00:39:36.665,00:39:44.600
"The general result would be
the covariance of Xi Xj =- nPi Pj for",00:39:44.600,00:39:53.372
i not equal j Notice it is a- number.,00:39:53.372,00:40:00.360
"Okay, so
That's the covariance in a multinomial.",00:40:02.730,00:40:07.341
"And, Let's do",00:40:07.341,00:40:12.315
a few variants examples now.,00:40:12.315,00:40:16.628
"For example, variants of the binomial,
we did derive the variants of",00:40:16.628,00:40:20.345
"the binomial before using indicator
random variables, just directly.",00:40:20.345,00:40:24.265
"Because we didn't have these
tools available yet, okay?",00:40:24.265,00:40:27.126
"So let's redo the variance
of the binomial and",00:40:27.126,00:40:31.436
then do one more example after that.,00:40:31.436,00:40:34.871
"So okay, so
the variance of the binomial NP is NPQ.",00:40:34.871,00:40:39.056
"And let's just derive that really quickly,",00:40:39.056,00:40:43.165
"So let X be binomial np, and",00:40:45.614,00:40:49.468
we write it as we've done many times.,00:40:49.468,00:40:54.836
"X = x1 + blah, blah, blah plus Xn,",00:40:54.836,00:40:59.764
where the Xis are IID Bernoulli P.,00:40:59.764,00:41:04.232
"Now each Xi, let's do a quick little",00:41:08.004,00:41:12.873
indicator random variable review.,00:41:12.873,00:41:17.749
"We can think of these Xj's,
they're Bernoulli's, but",00:41:17.749,00:41:20.263
they're also indicator random variables.,00:41:20.263,00:41:22.222
"It's the indicator of
success on the jth trial,",00:41:22.222,00:41:28.097
solet's just state this in general.,00:41:28.097,00:41:32.691
"Let's let capital I and capital J,",00:41:32.691,00:41:36.569
"let IA be indicator random variable for
event A, just in general.",00:41:36.569,00:41:43.814
"A is any event, IA is it's indicator",00:41:43.814,00:41:47.881
"random variable, indicator rv of event A.",00:41:47.881,00:41:52.749
"Okay, so just a couple quick simple
facts about indicator random variables.",00:41:52.749,00:42:00.226
What's IA squared?,00:42:00.226,00:42:05.145
"It's just IA, cuz you're squaring 0 or 1.",00:42:05.145,00:42:09.246
"Similarly IA cubed = IA and
you can generalise this to",00:42:09.246,00:42:14.140
"other powers if you want,
just it's 0 or 1.",00:42:14.140,00:42:18.396
"There's a very, very,
very simple fact, but",00:42:18.396,00:42:20.956
"I've seen it get overlooked many times,
so I'm emphasizing it now.",00:42:20.956,00:42:24.740
"Pick any positive power, nothing happens
because it's 0 or 1, very easy, okay?",00:42:24.740,00:42:30.991
"Now let's look at something else
IA times I times B where A and",00:42:30.991,00:42:35.516
B are both of events.,00:42:35.516,00:42:37.131
"How would you write that as
one indicator random variable?",00:42:38.789,00:42:43.443
"Intersection, extremely useful simple
fact, but often gets overlooked.",00:42:43.443,00:42:50.580
"Product of these indicators is 0 or
1 times 0 or 1, that's gonna be 0 or 1.",00:42:50.580,00:42:55.640
"It's gonna be 1,",00:42:55.640,00:42:56.636
"if and only if both of those are 1,
that's the definition of intersection.",00:42:56.636,00:43:00.695
"So that's immediately true,
very useful fact.",00:43:00.695,00:43:04.057
"Okay, now coming back to this binomial,
if we want the variance of Xj,",00:43:04.057,00:43:10.655
"That's just E of xj
squared-E of xj squared, But",00:43:14.241,00:43:21.015
"xj squared is xj, so that's just E of xj,
and we know E of xj is p, for Bernoulli P.",00:43:21.015,00:43:27.102
"This one is p squared, so
that's just p1-p, okay?",00:43:27.102,00:43:31.172
"So it's extremely easy to get
the variance of a Bernoulli.",00:43:31.172,00:43:35.760
"If we define this, let's define this as q
then we're just saying p times q, okay?",00:43:35.760,00:43:41.576
"So Bernoulli P,
you get p times q, very easy.",00:43:41.576,00:43:46.385
"So now we want the variance of
the binomial, Well it's just npq, done.",00:43:46.385,00:43:55.115
"Because you're adding up, and
they're independent for the binomial.",00:43:55.115,00:43:58.315
"We have independent Bernoulli trials,
so just to write out a little bit more.",00:43:58.315,00:44:03.811
"Covariance of Xi, Xj = 0 for i not
equal j because they are independent.",00:44:03.811,00:44:10.498
"They are not only uncorrelated
they are independent, so",00:44:10.498,00:44:12.927
we don't have any covariance term.,00:44:12.927,00:44:14.504
"So we just add up the variances and
n times this, npq, all right?",00:44:14.504,00:44:20.520
"So now you can do the variance
of a binomial in your head.",00:44:20.520,00:44:23.944
"You don't need to memorize this,",00:44:23.944,00:44:25.889
"it's just n times the variance of
one of these Bernoulli's, okay?",00:44:25.889,00:44:29.793
"So that's easy, Well let's talk
about a more complicated one though.",00:44:29.793,00:44:36.758
"Hypergeometric, So",00:44:36.758,00:44:42.134
"let X be hypergeometric,
With parameters w,",00:44:42.134,00:44:49.858
"b, n, which we interpret as saying,",00:44:49.858,00:44:54.536
"we have a jar that has w white balls,
b black balls.",00:44:54.536,00:45:00.840
"We take a sample of size n, and",00:45:00.840,00:45:02.506
"we want the distribution of the number
of white balls in the sample.",00:45:02.506,00:45:06.326
"Well again, we can decompose it in
terms of indicator random variables.",00:45:06.326,00:45:13.589
"So Xj = 1,
we can interpret this as drawing balls",00:45:18.594,00:45:23.998
"from that jar one at a time
without replacement.",00:45:23.998,00:45:29.288
"We'd get a binomial if we
did with replacement, but",00:45:29.288,00:45:31.420
"the hypergeometric would
be without replacement.",00:45:31.420,00:45:34.320
"Take the balls one at a time, and",00:45:34.320,00:45:37.524
"we just say one if the jth ball is white,
0 otherwise.",00:45:37.524,00:45:43.001
"The problem, the reason it's more
difficult than this is that these",00:45:43.001,00:45:47.802
"are dependant indicator random variables,
because it's without replacement.",00:45:47.802,00:45:53.623
"So if we write this thing out,
variance of x = so",00:45:57.032,00:46:01.610
"we're gonna write out all
these variance terms and",00:46:01.610,00:46:06.530
all of these covariance terms.,00:46:06.530,00:46:09.633
"Sounds like it's gonna be a nightmare,
okay?",00:46:09.633,00:46:11.743
"But there are some symmetries
that we can take advantage of.",00:46:11.743,00:46:15.710
"First of all,
we have the sum of all the variances,",00:46:15.710,00:46:19.821
"we're gonna use some symmetries
here to make life easier.",00:46:19.821,00:46:24.612
"This goes back to our homework problem,
I'll talk a little bit more about.",00:46:24.612,00:46:29.000
"Variance of x = n times
the variance of x1,",00:46:29.000,00:46:33.699
because let's say we're looking at x7.,00:46:33.699,00:46:38.271
"Let's say the seventh ball,",00:46:38.271,00:46:40.357
"like the homework problem
where you picked two balls.",00:46:40.357,00:46:44.202
"And a lot of students were struggling
somewhat with the fact that to consider",00:46:44.202,00:46:48.110
"the second ball, don't you have
to consider the first ball, okay?",00:46:48.110,00:46:51.661
"But when we're just looking at like x7,
that depends on the seventh ball,",00:46:51.661,00:46:55.601
"we're imagining before
we've done anything, okay?",00:46:55.601,00:46:58.507
"Now the seventh ball is equally
likely to be any of the balls, right?",00:46:58.507,00:47:04.490
"There's isn't like some balls
like to be chosen seventh and",00:47:04.490,00:47:07.372
"other ones don't, right?",00:47:07.372,00:47:08.736
"It's completely symmetrical, so
this is just n times the variance of x1.",00:47:08.736,00:47:13.210
"Similarly, for
all the covariance terms, 2 times and",00:47:13.210,00:47:17.776
"then there are n choose two
of these covariance terms.",00:47:17.776,00:47:22.260
"But we may as well just consider
the covariance of x1 and",00:47:22.260,00:47:27.150
"x2, Symmetry, so",00:47:27.150,00:47:31.782
"you should think through to make sure
you see why this symmetry hold here.",00:47:31.782,00:47:39.059
"So for the first ball,
I mean variance of x1,",00:47:39.059,00:47:43.212
"that would just get using a Bernoulli,
right?",00:47:43.212,00:47:47.485
"So that's easy to get,",00:47:47.485,00:47:48.770
"but let's think a little bit about
the covariance of x1 and x2.",00:47:48.770,00:47:52.320
"So this part we already know, this we
now know if we see the symmetry or not.",00:47:54.840,00:48:00.160
"But you should make sure you see
the symmetry in this problem,",00:48:00.160,00:48:02.968
"cuz if there's symmetry you
only take advantage of it.",00:48:02.968,00:48:05.514
"And if there isn't symmetry you
don't wanna falsely assume, and so",00:48:05.514,00:48:08.399
you have to be very careful about that.,00:48:08.399,00:48:10.400
"Symmetry is powerful but
the danger is, all right?",00:48:10.400,00:48:13.880
Let's quickly get covariance of x1 and x2.,00:48:13.880,00:48:16.537
Well that's E(x1 x2)-E(x1) E(x2).,00:48:16.537,00:48:22.342
"E(x1 x2), let's do the second term first.",00:48:26.081,00:48:30.030
"That's easy,
that's just the fundamental bridge,",00:48:30.030,00:48:32.977
"the probability of the first ball is white
times probably the second one is white.",00:48:32.977,00:48:37.408
"But both of those are w over w + b, Okay?",00:48:37.408,00:48:42.319
"Now for this term, E(x1 x2), let's use
the fact here that the product of two",00:48:42.319,00:48:47.079
"indicator random variables is
the indicator of the intersection.",00:48:47.079,00:48:51.810
"So this event here,
it's expected I have an indicator,",00:48:51.810,00:48:54.772
"fundamental bridge that's the probability
that the first two balls are both white.",00:48:54.772,00:48:59.228
"While the first ball has probably w/w + b,
and then the second ball",00:48:59.228,00:49:04.886
"being white given that the first
ball is white is w-1/w + b-1",00:49:04.886,00:49:10.360
"So then we have the covariance, so
we know this thing, we know this thing.",00:49:12.376,00:49:15.271
"So at this point, we can just do some
algebra and simplify everything together.",00:49:15.271,00:49:18.543
"I'll clean this up next time,
and give the final answer, but",00:49:18.543,00:49:21.456
"at this point we know the answer,
it's just algebra.",00:49:21.456,00:49:24.050
"Okay, so see you on Friday.",00:49:24.050,00:49:25.680

text,start,stop
"The following
content is provided",00:00:00.499,00:00:01.950
"by MIT OpenCourseWare under
a Creative Commons license.",00:00:01.950,00:00:06.060
"Additional information
about our license",00:00:06.060,00:00:08.230
"and MIT OpenCourseWare
in general,",00:00:08.230,00:00:10.490
is available at ocw.mit.edu.,00:00:10.490,00:00:12.160
"PROFESSOR: I thought I
would, in this last lecture",00:00:17.247,00:00:19.330
"before the break, speak
about one specific topic.",00:00:19.330,00:00:23.780
"It's often referred to
as a fast Poisson solver,",00:00:26.660,00:00:30.050
so what does Poisson mean?,00:00:30.050,00:00:33.920
"Poisson means
Laplace's equation.",00:00:33.920,00:00:36.350
"So, this is the
five-point Laplacian,",00:00:36.350,00:00:39.870
"which could be some other
discrete Laplace matrix,",00:00:39.870,00:00:43.700
but let's take the one we know.,00:00:43.700,00:00:46.760
"So we're in two dimensions,
and you use Poisson's name,",00:00:46.760,00:00:52.660
"instead of Laplace's
name, when there's",00:00:52.660,00:00:55.030
a non-zero right-hand side.,00:00:55.030,00:00:57.560
"So otherwise, it's a Laplace
solver, but here Poisson.",00:00:57.560,00:01:02.210
OK.,00:01:02.210,00:01:02.840
"So, remember that
right-hand side comes",00:01:02.840,00:01:05.170
"from maybe a right-hand side
f of x, y in the differential",00:01:05.170,00:01:10.810
"equation, but it also comes from
non-zero boundary conditions,",00:01:10.810,00:01:17.280
"because non-zero boundary
conditions, when the five",00:01:17.280,00:01:21.850
"points hit a boundary, that
known value was moved over",00:01:21.850,00:01:26.580
"to the right-hand side
and becomes part of f.",00:01:26.580,00:01:29.480
"So f comes from the non-zero
boundary conditions, as well as",00:01:29.480,00:01:35.370
the non-zero right-hand side.,00:01:35.370,00:01:37.730
OK.,00:01:37.730,00:01:39.840
"Important problems, but special
on a square or a rectangle",00:01:39.840,00:01:45.580
"or a cube or a box,
so we're speaking",00:01:45.580,00:01:50.040
about special geometry.,00:01:50.040,00:01:53.760
"Today, as we've been doing,
I'll take the case on a square,",00:01:53.760,00:02:00.740
"and you will see that the
whole idea would not fly,",00:02:00.740,00:02:05.410
"if we were on an ellipse
or something like that.",00:02:05.410,00:02:12.060
"But a lot of problems on
rectangular domains do appear",00:02:12.060,00:02:18.420
"in applications,
or we could use --",00:02:18.420,00:02:22.510
"I hope you always think now
about possible preconditioners.",00:02:22.510,00:02:27.370
"Any time you have
something fast,",00:02:27.370,00:02:29.850
"it's a candidate to
be a preconditioner",00:02:29.850,00:02:33.700
for a real problem.,00:02:33.700,00:02:35.890
"The real problem might
not be on a square,",00:02:35.890,00:02:39.280
"or the real problem might not
have the constant coefficient",00:02:39.280,00:02:44.140
that we have in the Laplacian.,00:02:44.140,00:02:48.710
"In that case, you're not
solving the exact problem,",00:02:48.710,00:02:52.970
"but one that could be
reasonably close to it.",00:02:52.970,00:02:56.720
OK.,00:02:56.720,00:02:59.730
"So we've discussed the
solution to this problem,",00:02:59.730,00:03:03.085
"and actually I
have a little more",00:03:03.085,00:03:04.460
"to say about the movie
that is now on the website,",00:03:04.460,00:03:08.640
"because I'm quite
excited about that movie.",00:03:08.640,00:03:11.320
"I'll come to the
movie in a second.",00:03:14.320,00:03:16.050
"Let me just say what
today's lecture would be.",00:03:16.050,00:03:20.830
"The key idea here is that the
eigenvalues and eigenvectors",00:03:20.830,00:03:26.750
"of this giant matrix of order
N squared by N squared --",00:03:26.750,00:03:31.630
size is N squared by N squared.,00:03:31.630,00:03:34.690
"The eigenvalues and the
eigenvectors are known.",00:03:34.690,00:03:37.380
"First of all, they're known.",00:03:37.380,00:03:40.270
"The eigenvectors are nice
discrete sine functions;",00:03:40.270,00:03:44.340
"they're sines,
because I'm assuming",00:03:44.340,00:03:50.080
"they come to zero
at the boundary;",00:03:50.080,00:03:52.350
"so that's why I have a
sine, rather than a cosine.",00:03:52.350,00:03:56.410
"First, they're
known, and second, we",00:03:56.410,00:04:00.260
"can work with them very
quickly, using the FFT.",00:04:00.260,00:04:05.680
"So the point is that
it's quite exceptional;",00:04:05.680,00:04:10.710
"in fact, I don't think I
know any comparable example,",00:04:10.710,00:04:15.290
"in which a linear system is
solved by using the eigenvalues",00:04:15.290,00:04:18.990
and eigenvectors.,00:04:18.990,00:04:21.100
"Usually eigenvalues
and eigenvectors,",00:04:21.100,00:04:23.870
"they pay off for
differential equations",00:04:23.870,00:04:26.810
that are growing in time.,00:04:26.810,00:04:29.160
"Then it is worth computing
them, because then you can just",00:04:29.160,00:04:32.170
"multiply by e to the lambda*t,
and you know what's happening",00:04:32.170,00:04:35.320
later.,00:04:35.320,00:04:35.820
"Eigenvectors, eigenvalues have
other purposes, but very, very",00:04:38.600,00:04:42.860
"rarely are they used to
solve a linear system.",00:04:42.860,00:04:46.230
"I mean, it's usually
far more work.",00:04:46.230,00:04:49.360
"And of course, it would
be incredibly more work",00:04:49.360,00:04:51.800
"if we had to find the
eigenvalues and eigenvectors,",00:04:51.800,00:04:54.380
"but for this problem
we know them.",00:04:54.380,00:04:57.160
"And it also would be incredibly
more work if the matrix",00:04:57.160,00:05:01.010
"of eigenvectors,
the basis matrix,",00:05:01.010,00:05:05.130
"the key matrix that
I'll denote by S --",00:05:05.130,00:05:09.600
"so the eigenvectors
go in a matrix S;",00:05:09.600,00:05:12.850
"the eigenvalues go in a
matrix capital lambda;",00:05:12.850,00:05:17.430
so we know lambda.,00:05:17.430,00:05:19.000
"It's going to be a simple
matrix, just diagonal,",00:05:19.000,00:05:22.350
"got those N numbers -- N
squared numbers, I guess,",00:05:22.350,00:05:25.520
because we're of size N squared.,00:05:25.520,00:05:28.160
But these eigenvectors we know.,00:05:28.160,00:05:31.570
"So we know them, and we can
compute quickly with them,",00:05:31.570,00:05:35.870
"using the FFT, or using,
you might want me to say,",00:05:35.870,00:05:40.270
"fast sine transform,
discrete sine transform,",00:05:40.270,00:05:45.210
"rather than Fourier
transform, which",00:05:45.210,00:05:48.180
"we think of as doing
the complex exponential.",00:05:48.180,00:05:52.400
"So this is a small special
fast Fourier world.",00:05:52.400,00:05:58.890
"It's a special
fast Fourier world,",00:05:58.890,00:06:01.080
"in which the FFT and the
related sine and cosine",00:06:01.080,00:06:07.690
"give us a quick answer,
faster than elimination,",00:06:07.690,00:06:13.290
"because you all
know, it's n log n,",00:06:13.290,00:06:16.240
"if n is the number of
Fourier components,",00:06:16.240,00:06:19.970
and that's hard to beat.,00:06:19.970,00:06:23.490
"Then I'll mention also,
for this same problem,",00:06:23.490,00:06:28.170
"there is another way in
which it can be simplified.",00:06:28.170,00:06:33.640
"It's not a Fourier way,
just a direct combining",00:06:33.640,00:06:41.230
"neighboring equations,
so that'll be number two.",00:06:41.230,00:06:44.260
"OK, but mostly the lecture
is about number one.",00:06:44.260,00:06:46.870
"The best example I know
in which you would use",00:06:49.670,00:06:53.250
"eigenvectors/eigenvalues
to solve",00:06:53.250,00:06:56.220
"an ordinary linear system,
and I'll say in a word",00:06:56.220,00:07:00.420
"just how you do it, and then
what these eigenvectors are.",00:07:00.420,00:07:04.750
OK.,00:07:04.750,00:07:08.030
Pause.,00:07:08.030,00:07:09.180
"Time out to say something
about the movie.",00:07:09.180,00:07:12.330
"So that movie that's
now on the website",00:07:12.330,00:07:18.200
"does sparse elimination, and
the example it takes is K2D,",00:07:18.200,00:07:27.150
"and you can make it 8
squared by 8 squared,",00:07:27.150,00:07:30.700
"10 squared by 10 squared,
20 squared by 20 squared,",00:07:30.700,00:07:34.780
"because it shows the order
that the nodes are eliminated",00:07:34.780,00:07:40.490
"and the graphs of
non-zeros in the matrix.",00:07:40.490,00:07:47.040
It's a bit slow.,00:07:47.040,00:07:49.560
"If you do 20 by 20, you
have to go away for lunch,",00:07:49.560,00:07:55.900
"well maybe not lunch, but at
least coffee before it's done,",00:07:55.900,00:08:01.390
"but when it's done, it
counts the number of --",00:08:01.390,00:08:05.740
"it shows the non-zeros that
are in the elimination,",00:08:05.740,00:08:14.920
"in the factor L
from elimination,",00:08:14.920,00:08:17.930
"and it counts the
number of non-zeros,",00:08:17.930,00:08:20.750
"and I was just talking to
Persson about also getting it",00:08:20.750,00:08:24.730
"to count the number
of actual flops.",00:08:24.730,00:08:27.260
"Well, why am I interested?",00:08:29.810,00:08:33.610
"I'm interested, because I
don't know what power of N",00:08:33.610,00:08:41.170
those numbers are growing with.,00:08:41.170,00:08:43.960
"I don't know whether the number
of non-zeros -- and I did 10,",00:08:43.960,00:08:47.350
"20, 30, and it looked not too
far from the power capital N",00:08:47.350,00:08:56.730
"cubed, but the notes say
for nested dissection,",00:08:56.730,00:09:05.240
"which would be another
ordering, N squared log N. So,",00:09:05.240,00:09:15.540
"and of course, what power we
get depends on what algorithm we",00:09:15.540,00:09:19.670
"use for ordering, so nested
dissection is one ordering,",00:09:19.670,00:09:23.250
"which hopefully we could
put into another movie.",00:09:23.250,00:09:28.700
"The movie now has exact
minimum degree, real MMD,",00:09:28.700,00:09:35.390
"which takes as the
next node the one",00:09:35.390,00:09:41.520
"with absolutely the minimum
degree, not just close to it.",00:09:41.520,00:09:46.670
"Anyway, have a
look at that movie,",00:09:46.670,00:09:51.580
"and if you have
any interest, see",00:09:51.580,00:10:00.130
"how the count increases
as n increases,",00:10:00.130,00:10:03.510
"and also you could
change, slightly adapt,",00:10:03.510,00:10:07.720
"the algorithm that
creates the ordering,",00:10:07.720,00:10:11.630
"creates the permutation
and see what happens there.",00:10:11.630,00:10:15.370
"There's a lot to do with that,
and it's a pretty fundamental",00:10:15.370,00:10:20.540
"problem, actually.",00:10:20.540,00:10:21.430
"We're talking there about what
MATLAB's backslash operation",00:10:21.430,00:10:26.930
will do for this equation.,00:10:26.930,00:10:29.970
"So MATLAB'S backslash
operation will use elimination;",00:10:29.970,00:10:33.100
"it won't use fast
Poisson solvers,",00:10:33.100,00:10:35.470
"but now let me come to
the fast Poisson solver.",00:10:35.470,00:10:38.760
OK.,00:10:38.760,00:10:39.870
"So I guess the main
point is I have",00:10:39.870,00:10:42.800
"to say what are the
eigenvalues and eigenvectors,",00:10:42.800,00:10:45.910
and how do they get used.,00:10:45.910,00:10:47.270
"So let me say, how
do they get used.",00:10:47.270,00:10:49.250
"So how can I use
eigenvalues and eigenvectors",00:10:49.250,00:10:52.390
to solve a problem like that.,00:10:52.390,00:10:55.870
"Let me just call it
K rather than K2D.",00:10:55.870,00:10:58.080
So K*U equals F. OK.,00:10:58.080,00:11:02.380
By eigenvectors.,00:11:04.970,00:11:06.740
"OK, there are three steps.",00:11:09.780,00:11:12.100
Step one.,00:11:12.100,00:11:12.890
"Expand the right-hand
side as a combination",00:11:15.400,00:11:20.530
of the eigenvectors.,00:11:20.530,00:11:22.070
OK.,00:11:22.070,00:11:22.850
"So expand F, this
right-hand side vector,",00:11:22.850,00:11:28.840
"as some combination of
the first eigenvector,",00:11:28.840,00:11:34.250
"maybe I'm going to call it
y_1, second eigenvector,",00:11:34.250,00:11:41.570
"n-th eigenvector, OK, good.",00:11:41.570,00:11:48.412
That means -- what do I mean?,00:11:48.412,00:11:49.620
"I mean you have to find those
c's, that's the job there.",00:11:49.620,00:11:54.000
Find the coefficients.,00:11:54.000,00:11:58.170
"So that's a job, a numerical --
it's a linear system to solve",00:11:58.170,00:12:03.780
"and we'll see what
it amounts to.",00:12:03.780,00:12:05.860
"OK, but suppose
the right-hand side",00:12:05.860,00:12:08.230
"is a combination of
the eigenvectors,",00:12:08.230,00:12:10.300
how can you use that?,00:12:10.300,00:12:11.600
"Well, step two is
the real quick step.",00:12:11.600,00:12:14.320
"Divide each c_i by the
eigenvalue lambda_i.",00:12:14.320,00:12:30.910
"OK, so it's by eigenvector,
so I'm assuming that K*y_i is",00:12:30.910,00:12:36.580
"lambda_i*y_i and that
we know these guys.",00:12:36.580,00:12:41.890
So this is known.,00:12:41.890,00:12:44.670
"And now the question, I'm just
saying, how do we assume known?",00:12:44.670,00:12:48.820
"So my question now
is how do we use it?",00:12:53.340,00:12:55.250
"OK, step one -- the idea is
going to be write everything",00:12:55.250,00:12:59.540
in terms of eigenvectors.,00:12:59.540,00:13:01.850
"Work with the eigenvectors,
because if I've",00:13:01.850,00:13:04.490
"got eigenvectors,
the step is scalar;",00:13:04.490,00:13:09.990
"I just divide these
numbers by those numbers,",00:13:09.990,00:13:12.610
and then I've got the answer.,00:13:12.610,00:13:15.370
"And then construct -- the
correct answer will be U will",00:13:15.370,00:13:21.230
"be c_1 over lambda_1 y_1 and
c_2 over lambda_2 y_2 up to c_n",00:13:21.230,00:13:32.680
"over lambda_n y_n, a combination
of those same eigenvectors with",00:13:32.680,00:13:41.120
"the same coefficients,
just divided by lambda.",00:13:41.120,00:13:43.740
"But this is, of course,
another numerical job;",00:13:47.310,00:13:50.790
"this is like adding
up a Fourier series;",00:13:50.790,00:13:54.540
"this is like finding the
Fourier coefficients,",00:13:54.540,00:13:56.650
"this is like
reconstructing the input.",00:13:56.650,00:14:00.980
"Only because I've
divided by lambda_i,",00:14:00.980,00:14:04.150
"I'm getting the
output here is U,",00:14:04.150,00:14:07.080
"when the input was F.
And do you see that that",00:14:07.080,00:14:10.700
is the correct answer?,00:14:10.700,00:14:13.690
"All I have to do is
check that K*U equals F.",00:14:13.690,00:14:16.840
"So check that this answer from
step three is the right answer.",00:14:16.840,00:14:26.040
"OK, so I multiply by K.
When I multiply y_1 by K,",00:14:26.040,00:14:34.960
"a factor lambda_1 appears, the
eigenvalue; it cancels that;",00:14:34.960,00:14:39.850
"well that's y divided,
so it would cancel,",00:14:39.850,00:14:42.480
and I have c_1*y_1.,00:14:42.480,00:14:44.640
"When I multiply this by
K, K*y_2 is lambda_2*y_2;",00:14:44.640,00:14:49.940
"cancel the lambda_2's, and
you're left with c_2*y_2,",00:14:49.940,00:14:54.250
and so on.,00:14:54.250,00:14:55.000
"So, when I multiplied by
K, I got F. That's it.",00:14:55.000,00:15:02.250
"So that's the whole idea
written out, but now,",00:15:02.250,00:15:09.760
"what actual computations go
into steps one and three?",00:15:09.760,00:15:14.430
Step two is pretty simple.,00:15:14.430,00:15:15.840
"Well, actually this is
a good way to look it.",00:15:15.840,00:15:19.740
"I want to write that same
algorithm in matrix language.",00:15:19.740,00:15:23.790
"OK, so in matrix form.",00:15:23.790,00:15:29.790
"We have the matrix
of eigenvectors,",00:15:29.790,00:15:31.850
"and that's what I'm
calling S. And it's",00:15:31.850,00:15:37.320
"got the eigenvectors y_1,
y_2, y_n in its columns.",00:15:37.320,00:15:43.320
"And the eigenvalue matrix,
we need a name for that too,",00:15:43.320,00:15:52.090
"and that we decided
to call lambda,",00:15:52.090,00:15:54.860
"and that's got the
eigenvalues on its diagonal.",00:15:54.860,00:16:01.480
"So this is 18.06,
linear algebra.",00:16:01.480,00:16:08.030
"The matrix of eigenvectors,
if I multiply K by S,",00:16:08.030,00:16:16.020
"then I'm multiplying K by
all its little eigenvectors,",00:16:16.020,00:16:21.770
"and K times this
gives me lambda_1*y_1,",00:16:21.770,00:16:26.280
"K times y_2 gives
me lambda_2*y_2,",00:16:26.280,00:16:29.250
"K*y_n is lambda_n*y_n, and if
I look to see what this is,",00:16:29.250,00:16:38.250
"this is the same as y_1 to
y_n multiplied by lambda.",00:16:38.250,00:16:44.060
"If I just multiply on
the right by lambda,",00:16:44.060,00:16:47.070
"it will take lambda_1
times the first column,",00:16:47.070,00:16:49.420
"lambda_2 times the second,
lambda_n times the last,",00:16:49.420,00:16:53.650
"which is what we want,
so it's S*lambda.",00:16:53.650,00:16:56.050
"This is all n eigenvalues and
eigenvectors in one matrix",00:16:58.660,00:17:04.810
"equation, that's all that is.",00:17:04.810,00:17:07.330
"It's just K*S equal S*lambda_i
just says this for all i",00:17:07.330,00:17:17.790
"at once, all i at the same time.",00:17:17.790,00:17:20.470
OK.,00:17:20.470,00:17:21.320
"So if I use these
matrices in describing",00:17:21.320,00:17:26.000
"steps one, two,
three, I'll see what's",00:17:26.000,00:17:29.040
happening matrix language.,00:17:29.040,00:17:32.740
"OK, let me just do that.",00:17:32.740,00:17:34.770
"Step one: step one
is looking for F",00:17:34.770,00:17:41.550
"as a combination of
the columns of S.",00:17:41.550,00:17:46.230
"So step one is just
F equals S times c.",00:17:46.230,00:17:52.220
"The vector of coefficients
multiplies the columns of S",00:17:52.220,00:18:00.110
"and adds to give
F. Then step two,",00:18:00.110,00:18:07.700
"which just divides everything
by -- divides by the lambdas.",00:18:07.700,00:18:15.310
"Step two just creates
lambda inverse S*c.",00:18:15.310,00:18:20.310
"So I took what I had
-- let's see, no,",00:18:22.850,00:18:30.880
"the lambda inverse better
be multiplying the c.",00:18:30.880,00:18:34.500
"Well, actually I can do it
all in -- well step two,",00:18:34.500,00:18:41.150
"that's the easiest step,
I should be able to do it.",00:18:41.150,00:18:43.930
"c is changed to
lambda inverse c,",00:18:43.930,00:18:52.150
"so that c becomes
lambda inverse c, OK.",00:18:52.150,00:18:56.030
"And then step three
uses lambda inverse c",00:18:56.030,00:19:01.600
"to construct U. So step
three is: the answer",00:19:01.600,00:19:11.190
"U, what do I have here?",00:19:11.190,00:19:14.540
"I've got a combination
of these vectors,",00:19:14.540,00:19:16.740
"so they're the columns of S,
and what are they multiplied by?",00:19:16.740,00:19:20.580
"They're multiplied by
the c's over lambdas,",00:19:20.580,00:19:22.980
which is what I have here.,00:19:22.980,00:19:24.610
That's S lambda inverse c.,00:19:24.610,00:19:33.650
Those are the three steps.,00:19:33.650,00:19:36.840
And what's the work involved?,00:19:36.840,00:19:42.900
"Here, the work is solving a
linear system with the matrix",00:19:42.900,00:19:47.410
"S. Here, the work is
taking a combination",00:19:47.410,00:19:53.670
"of the columns of s, doing
a matrix multiplication.",00:19:53.670,00:19:59.650
"Those two steps are usually
full-scale matrix operations,",00:19:59.650,00:20:06.820
"and of course, the S --
if I just complete this,",00:20:06.820,00:20:10.610
"I'll see that I get
the right thing,",00:20:10.610,00:20:12.220
"that's S lambda inverse
and c is S inverse F.",00:20:12.220,00:20:22.910
There's the answer.,00:20:22.910,00:20:24.360
"U is S lambda inverse -- that's
a lambda inverse there --",00:20:24.360,00:20:31.200
"S inverse F. That's the correct
answer in matrix language.",00:20:31.200,00:20:37.091
Right.,00:20:37.091,00:20:37.590
"This is K inverse,
that's K inverse.",00:20:40.390,00:20:43.850
"K is S*lambda S inverse, and
if I take the inverse of that,",00:20:43.850,00:20:54.570
"I get S lambda inverse S
inverse to multiply F. Well,",00:20:54.570,00:21:03.830
"I doubt if you're much
impressed by this lower board,",00:21:03.830,00:21:07.230
"because the upper board was
the same thing written out.",00:21:07.230,00:21:13.800
"It took some indication of
what the separate pieces were,",00:21:13.800,00:21:21.390
but it's pretty clear.,00:21:21.390,00:21:23.420
"OK, now the million dollar
question is, is it fast?",00:21:23.420,00:21:33.910
"And the answer is,
almost certainly no.",00:21:33.910,00:21:37.930
"But for the particular matrix
S, which by good fortune,",00:21:37.930,00:21:45.690
"S could also stand
for sine, this matrix",00:21:45.690,00:21:50.720
"of eigenvectors for this
particular problem are sines.",00:21:50.720,00:21:57.130
"These are the discrete sines,
so this is the discrete sine",00:21:57.130,00:22:00.230
transform.,00:22:00.230,00:22:01.600
"That's we're doing, we're doing
the discrete sine transform,",00:22:01.600,00:22:05.630
"because those discrete
sine vectors are",00:22:05.630,00:22:08.430
"the eigenvectors of K. OK, now
let me say what that means.",00:22:08.430,00:22:17.590
First I'm thinking of K in 1D.,00:22:17.590,00:22:21.180
"So this is my 2's and
minus 1's and minus 1's.",00:22:21.180,00:22:28.960
"Its eigenvectors
are discrete sines,",00:22:28.960,00:22:31.540
"if I multiply that
by sine k*h -- well,",00:22:31.540,00:22:38.530
"let me just take the first
one, sine h, sine 2h,",00:22:38.530,00:22:44.070
"sine n minus 1 h, that will
turn out to be an eigenvector.",00:22:44.070,00:22:52.640
"So this is K*y, K*y_1,
the first eigenvector.",00:22:56.620,00:23:02.630
"The eigenvectors are
-- let me draw them.",00:23:02.630,00:23:05.270
"The eigenvectors for that second
difference matrix K are --",00:23:05.270,00:23:13.960
"here's the interval, 0 to 1,
I chop it up in steps of h,",00:23:13.960,00:23:18.620
"and I plot the sine, which
starts at zero and ends",00:23:18.620,00:23:26.700
"at zero, because those are
the boundary conditions,",00:23:26.700,00:23:28.960
"and here is sine h, sine 2h,
sine 3h, sine 4h, sine 5h,",00:23:28.960,00:23:34.500
"so for the five by five case --
maybe I should just be using n",00:23:34.500,00:23:39.880
"here, or maybe I should
even be using capital N,",00:23:39.880,00:23:45.890
"so capital N is 5
in this example.",00:23:45.890,00:23:49.220
Good.,00:23:53.770,00:23:56.250
"What's on the other side
of that equals sign?",00:23:56.250,00:23:59.670
"Some eigenvalue times the
same vector, sine h, sine 2h,",00:23:59.670,00:24:06.940
"down to sine N*h, and
that eigenvalue -- oh,",00:24:06.940,00:24:11.890
"let me just write
lambda 1 for it.",00:24:11.890,00:24:16.230
We know what it is.,00:24:16.230,00:24:18.270
"The fact that this is an
eigenvector is just trig;",00:24:18.270,00:24:24.900
"you know, I multiply minus 1
of that plus 2 of that minus 1",00:24:24.900,00:24:29.670
"of sine 3h, and I use
a little trig identity.",00:24:29.670,00:24:35.050
"So minus that, 2 of
that, minus that,",00:24:35.050,00:24:38.280
"turns out that to be
a multiple of sine 2h.",00:24:38.280,00:24:41.250
"Well, the 2 sine 2h give us a
2, and then the minus sine h",00:24:41.250,00:24:47.770
"and the minus sine 3h
combine into sine 2h times,",00:24:47.770,00:24:54.650
"I think, it's a cosine
of h or something,",00:24:54.650,00:25:00.210
"it's that eigenvector
that's near zero,",00:25:00.210,00:25:07.290
"because the cosine
of h is near 1.",00:25:07.290,00:25:10.150
Does that look familiar?,00:25:10.150,00:25:11.320
"That a combination
of sine h and sine 3h",00:25:11.320,00:25:16.610
"should give us twice sine
2h times some cosine,",00:25:16.610,00:25:21.760
"yep, Elementary trig identity.",00:25:21.760,00:25:28.320
"OK, so those are eigenvectors.",00:25:28.320,00:25:31.000
"That's the first one, the
next one would have h times --",00:25:31.000,00:25:36.230
"the k-th one would have h times
k instead of just h itself,",00:25:36.230,00:25:42.260
"it would take jumps
of every k -- sine.",00:25:42.260,00:25:47.200
"and then a cosine h*k
would show up there,",00:25:47.200,00:25:52.560
"and this would still
be an eigenvector.",00:25:52.560,00:25:56.290
OK.,00:25:56.290,00:25:59.240
"I'm making a little bit
explicit these vectors,",00:25:59.240,00:26:06.820
"but the main point
is they're sines,",00:26:06.820,00:26:10.570
"they're discrete sines
at equally spaced points.",00:26:10.570,00:26:14.570
"That's what the real version
of the FFT just lives on.",00:26:14.570,00:26:22.600
"And it would also live
on discrete cosines;",00:26:22.600,00:26:25.020
"if we had different
boundary conditions,",00:26:25.020,00:26:26.710
"we could do those, too.",00:26:26.710,00:26:28.280
"So this isn't the only -- these
zero boundary conditions are",00:26:28.280,00:26:34.170
"associated with the
name of Dirichlet,",00:26:34.170,00:26:40.240
"where zero slopes are associated
with the name of Neumann,",00:26:40.240,00:26:44.350
"and both -- this one gives
sines, Neumann gives cosines,",00:26:44.350,00:26:49.830
the FFT deals with both.,00:26:49.830,00:26:52.160
OK.,00:26:52.160,00:26:53.600
"So, that's the fast solution,
and it would take N squared --",00:26:53.600,00:27:00.840
well I have to go to 2D.,00:27:00.840,00:27:02.890
"sorry, I guess I
have a little more",00:27:02.890,00:27:05.330
"to say because I have to get
from this one-dimensional",00:27:05.330,00:27:12.020
"second difference to the
five-point two-dimensional",00:27:12.020,00:27:15.510
"second difference,
and that's what's",00:27:15.510,00:27:17.530
going to happen over here.,00:27:17.530,00:27:21.080
"I wrote up some stuff about
the Kronecker operation, which",00:27:21.080,00:27:28.400
"is the nifty way for
these special problems",00:27:28.400,00:27:33.000
to go from 1D to 2D.,00:27:33.000,00:27:36.740
"You remember the deal,
that K2D, our 2D matrix,",00:27:36.740,00:27:42.880
"was this Kronecker product
of K and I, that gave us",00:27:42.880,00:27:51.380
"second differences
in one direction,",00:27:51.380,00:27:54.410
"and then we have to add in the
Kronecker product of I and K",00:27:54.410,00:27:59.040
"to get second differences
in the other direction.",00:27:59.040,00:28:02.150
"And then we better print --
because that matrix is going",00:28:02.150,00:28:06.845
"to be large, I don't
want to print it.",00:28:06.845,00:28:12.680
Yeah.,00:28:12.680,00:28:13.660
What's the point?,00:28:13.660,00:28:15.080
"The point is that if I
know the eigenvectors of k,",00:28:15.080,00:28:19.970
"then I can find the -- if
I know the 1D eigenvectors,",00:28:19.970,00:28:24.130
"I can find the 2D eigenvectors,
and you don't have to know",00:28:24.130,00:28:27.390
Kronecker products to do that.,00:28:27.390,00:28:29.380
"All you have to do is just
make a sensible guess,",00:28:29.380,00:28:32.780
"so the eigenvectors in 2D, are
-- so they have a double index,",00:28:32.780,00:28:42.170
"k and l, and their components
are sines in one direction",00:28:42.170,00:28:57.430
"times sines in the
other direction.",00:28:57.430,00:29:00.160
So what are those sines?,00:29:00.160,00:29:02.410
"There's a k*h, I guess, l*h.",00:29:02.410,00:29:06.810
"Those are the first components,
I guess I have to tell you what",00:29:14.080,00:29:18.860
"all the components are: k --
the seventh component in the x",00:29:18.860,00:29:26.640
"direction, there'd be a factor
7 -- so k*m*h sine l*n*h.",00:29:26.640,00:29:34.840
"This is the (m, n)
component of y_(k, l).",00:29:34.840,00:29:41.810
It's just what we had in 1D.,00:29:48.720,00:29:50.680
"In 1D there was no l, the
components were just sine",00:29:50.680,00:29:56.510
k*m*h.,00:29:56.510,00:29:58.130
"Now we've got two, one
in the x direction --",00:29:58.130,00:30:05.320
"These are the analogs of the
-- the continuous case would be",00:30:05.320,00:30:13.400
sine k*pi*x times sine l*pi*y.,00:30:13.400,00:30:21.120
"Those are the eigenvectors
as eigenfunctions,",00:30:24.740,00:30:29.380
functions of x and y.,00:30:29.380,00:30:31.000
"And the point is
that, once again,",00:30:31.000,00:30:35.300
"with these beautiful
matrices, I can",00:30:35.300,00:30:37.930
"sample these at the
equally spaced points,",00:30:37.930,00:30:43.120
"and I get discrete
sines that the FFT is",00:30:43.120,00:30:47.940
"ready to go with, OK.",00:30:47.940,00:30:49.620
"I'm giving this much detail
partly because the continuous",00:30:54.670,00:31:04.770
"case, of course, our operators
d second by the dx squared and d",00:31:04.770,00:31:09.730
"second by dy squared, and
the whole idea of separating",00:31:09.730,00:31:17.820
"variables, of looking
for solutions u --",00:31:17.820,00:31:24.700
"here is the eigenvalue problem,
the continuous eigenvalue",00:31:24.700,00:31:28.380
problem.,00:31:28.380,00:31:28.930
"The Laplacian of u,
maybe I do a minus,",00:31:28.930,00:31:32.300
"the Laplacian of
u equal lambda*u,",00:31:32.300,00:31:35.670
"and that's a partial
differential equation,",00:31:35.670,00:31:38.760
"usually it's not easy to
solve, but if I'm on a square,",00:31:38.760,00:31:45.540
"and I have zero
boundary conditions,",00:31:45.540,00:31:48.700
"then I've solved it, by
separation of variables,",00:31:48.700,00:31:52.430
"a function of x times
a function of y.",00:31:52.430,00:31:55.600
"And that function of
x times function of y",00:31:55.600,00:31:57.760
"is exactly what
Kronecker product",00:31:57.760,00:32:00.110
"is doing for matrices, yep.",00:32:00.110,00:32:03.480
"I thought maybe this is good to
know when the problem is easy,",00:32:08.730,00:32:15.960
"and as I say, the possibility of
using the easy case on a square",00:32:15.960,00:32:23.750
"for preconditioning a not
so easy case is attractive.",00:32:23.750,00:32:29.560
"All right, so
that's what I wanted",00:32:29.560,00:32:31.920
"to say about number one,
that's the main suggestion,",00:32:31.920,00:32:38.820
"and again, the point was just to
take these three simple steps,",00:32:38.820,00:32:46.400
"provided we know and
like the eigenvectors.",00:32:46.400,00:32:51.730
"Here we know them and
we like them very much,",00:32:51.730,00:32:55.300
"because they're
those discrete sines.",00:32:55.300,00:32:57.580
"OK, now just to
finish comes, what's",00:32:57.580,00:33:02.810
up with odd-even reduction.,00:33:02.810,00:33:06.480
"I'll use the same
1D problem first.",00:33:06.480,00:33:11.040
"It works great in --
that's not good English,",00:33:11.040,00:33:14.440
"but it works very well in
1D, odd-even reduction,",00:33:14.440,00:33:20.680
"you'll see it, you'll
see, oh boy, simple idea.",00:33:20.680,00:33:23.980
"But of course, don't forget
that ordinary elimination",00:33:23.980,00:33:27.430
"is a breeze with a tri-diagonal
matrix, so nothing I could do",00:33:27.430,00:33:33.210
could be faster than that.,00:33:33.210,00:33:35.240
But let's see what you can do.,00:33:35.240,00:33:37.300
"I just want to write
down the -- OK,",00:33:37.300,00:33:40.250
"keep your eye on this matrix,
so I'm going to write out",00:33:40.250,00:33:48.150
the equations.,00:33:48.150,00:33:49.350
"So it'll be minus U_(i-2)
plus 2*U_(i-1) minus U_i,",00:33:49.350,00:33:59.080
"that would be F_(i-1); that
would be equation number i",00:33:59.080,00:34:03.840
"minus 1, right?",00:34:03.840,00:34:04.960
"With a minus 1, 2,
minus 1, centered there.",00:34:04.960,00:34:09.900
"And then the next one will be
a minus U_(i-1) plus 2*U_i --",00:34:09.900,00:34:17.200
"I better move this guy over
further -- minus U_(i+1),",00:34:17.200,00:34:27.270
"that will be F_i, right?",00:34:27.270,00:34:31.340
"That's equation number i,
and now I just want to look",00:34:31.340,00:34:34.180
"at equation number
-- the next equation,",00:34:34.180,00:34:36.450
"minus U_i plus 2*U_(i+1)
minus U_(i+2) is F_(i+1).",00:34:36.450,00:34:44.850
"So I've written down three
consecutive equations,",00:34:49.350,00:34:55.390
"three consecutive rows
from my simple matrix:",00:34:55.390,00:35:04.240
"a row, the next row,
and the next row.",00:35:04.240,00:35:09.420
"So the right-hand sides
are coming in order,",00:35:09.420,00:35:12.350
"and the diagonals are there,
and if I look at that,",00:35:12.350,00:35:17.520
do I get any idea?,00:35:17.520,00:35:18.510
"Well, there is an idea here.",00:35:21.400,00:35:29.030
"I'd like to remove these
guys, the U_(i-1)'s and the",00:35:31.900,00:35:39.330
"U_(i+1)'s, so that's where
this word odd-even reduction is",00:35:39.330,00:35:43.710
coming in.,00:35:43.710,00:35:44.210
"I'm going to reduce
this system by keeping",00:35:44.210,00:35:46.820
"only every second unknown
and eliminating those.",00:35:46.820,00:35:53.750
How to do that?,00:35:53.750,00:35:55.480
"Well, you can see
how to eliminate.",00:35:55.480,00:35:57.260
"If I multiply this
equation by 2.",00:35:57.260,00:36:01.050
"If I multiply that
middle equation",00:36:01.050,00:36:03.090
"by 2, that becomes a 4,
this becomes a minus 2,",00:36:03.090,00:36:07.380
"this becomes a 2, and
now what shall I do?",00:36:07.380,00:36:10.120
Add.,00:36:12.950,00:36:14.720
"If I add the equations
together, I get minus U_(i-2) --",00:36:14.720,00:36:20.510
"these cancel, that was the
point -- plus 4 minus a couple,",00:36:20.510,00:36:25.270
"so that's two u_i's,
minus this guy,",00:36:25.270,00:36:30.550
"u_(i-2) is that sum F_(i-1),
two F_i's, and F_(i+1).",00:36:30.550,00:36:44.200
"Well, sorry it's squeezed down
here, but this is the point.",00:36:47.270,00:36:52.680
The main point is look at this.,00:36:52.680,00:36:56.800
What do we have?,00:36:56.800,00:37:00.050
"We've got a typical
equation, but now we've",00:37:00.050,00:37:06.880
removed half the unknowns.,00:37:06.880,00:37:09.720
The problem is now half-sized.,00:37:09.720,00:37:12.030
"We've only got the
even-numbered unknowns",00:37:12.030,00:37:16.450
"at a small cost in updating
the right-hand side,",00:37:16.450,00:37:24.590
and the problem's cut in half.,00:37:24.590,00:37:27.280
"So that's this
odd-even reduction,",00:37:27.280,00:37:30.250
"and it cuts a problem in
half, and everybody knows",00:37:30.250,00:37:35.400
right away what to do next.,00:37:35.400,00:37:38.820
"The one mantra of computer
science, ""Do it again.""",00:37:38.820,00:37:43.670
"So that is the same
problem on the even,",00:37:43.670,00:37:48.760
"we do it again,
so I should really",00:37:48.760,00:37:51.910
"call it cyclic reduction, we
just cycle with this reduction,",00:37:51.910,00:37:57.440
"and in the end, we
have a 2 by 2 problem.",00:37:57.440,00:38:01.140
"That seems pretty smart,
pretty successful move,",00:38:04.700,00:38:10.200
"and I guess if we do
an operation count,",00:38:10.200,00:38:12.730
"well, I haven't
thought that through.",00:38:12.730,00:38:14.970
"What does the operation
count look like?",00:38:14.970,00:38:18.530
"It must be pretty quick, right?",00:38:18.530,00:38:21.860
"Well, undoubtedly we're
solving this linear system",00:38:21.860,00:38:25.200
in O of N steps.,00:38:25.200,00:38:27.620
"Well, no big surprise to be able
to deal with that matrix in O",00:38:27.620,00:38:32.330
"of N steps, because elimination
would take O of N steps,",00:38:32.330,00:38:37.740
"it's size N, but it's bandwidth
is 1, so 2N or something steps",00:38:37.740,00:38:44.390
"would do it, and
maybe, I don't know",00:38:44.390,00:38:46.670
how many steps we have here.,00:38:46.670,00:38:50.090
"I guess, when we cut it
in half, that required",00:38:50.090,00:38:54.020
us to do that much.,00:38:54.020,00:38:56.370
"It's order N, it's order N.",00:38:56.370,00:39:03.920
"So the key question
is, can we do it 2D?",00:39:03.920,00:39:10.580
Can we do the same thing in 2D?,00:39:10.580,00:39:13.790
"So I want to follow that
plan in two dimensions",00:39:13.790,00:39:20.330
"where now U will be a
whole row at a time,",00:39:20.330,00:39:25.960
"so I'm doing block 2D, block 2D.",00:39:25.960,00:39:30.700
"So, can I write down the
equations in block 2D,",00:39:30.700,00:39:34.430
for whole rows?,00:39:34.430,00:39:36.030
"So U_i is the vector -- So
now this is the 2D problem,",00:39:36.030,00:39:42.590
"so it'll be minus
the identity --",00:39:42.590,00:39:45.450
"where instead of instead of 1,
I have to write the identity --",00:39:45.450,00:39:48.860
"U_(i-2) and 2K, right?",00:39:48.860,00:39:56.800
"Oh no, what is the middle
-- what's on the --",00:39:56.800,00:40:03.650
"so multiplying U_i, U_(i-1).",00:40:03.650,00:40:08.040
"I wanted to just
say the same thing,",00:40:08.040,00:40:09.540
"but I have to write down the --
this is going to be N squared",00:40:09.540,00:40:15.300
"equations N at a time,
a whole row at a time,",00:40:15.300,00:40:19.720
"and what's on the
diagonal of K2D?",00:40:19.720,00:40:23.050
Not 2K.,00:40:23.050,00:40:27.310
"It's 2I, is it 2I plus K?",00:40:27.310,00:40:32.190
Yeah.,00:40:32.190,00:40:32.800
"Yeah, K plus 2I.",00:40:32.800,00:40:36.150
"Isn't that what we have on
the diagonal of the K2D one?",00:40:36.150,00:40:42.860
"Times U_(i-1) minus
I*U_i is equal to some --",00:40:42.860,00:40:53.960
that's a whole row at a time.,00:40:53.960,00:40:55.670
"These are all vectors with
N components now, right?",00:40:55.670,00:41:02.320
"The minus i, the 2I, and
the minus I are the second",00:41:02.320,00:41:06.500
"differences of one of
rows, they're difference is",00:41:06.500,00:41:10.420
"in the vertical direction,
and this K*U_i the second",00:41:10.420,00:41:14.560
difference is along the row.,00:41:14.560,00:41:15.840
"OK, so same equation
at the next row.",00:41:19.340,00:41:23.160
"So the next row is minus
I*U_(i-1), K plus 2 U_i --",00:41:23.160,00:41:34.880
"because that's now
the diagonal block --",00:41:34.880,00:41:37.020
minus I*U_(i+1) is F_(i+1).,00:41:37.020,00:41:42.760
"And it'll just take me a
second to write this one.",00:41:42.760,00:41:46.640
"This is K plus 2I U_(i+1),
minus I*U_(i+2) is F_(i+2).",00:41:46.640,00:41:57.690
OK.,00:41:57.690,00:41:58.190
"Exactly the same, but now
a row at a time in 2D.",00:42:01.290,00:42:08.980
"So the same idea is
going to work, right?",00:42:08.980,00:42:11.620
What do I do?,00:42:11.620,00:42:13.280
"I want to cancel this, so I
multiply that row by K plus 2I.",00:42:13.280,00:42:20.990
"Before I multiplied
it by 2, but now",00:42:20.990,00:42:23.060
"I have to multiply
it by K plus 2I.",00:42:23.060,00:42:26.300
"Times the row, the whole row.",00:42:26.300,00:42:29.700
"So when I do that this
cancels, so I have minus this,",00:42:32.350,00:42:36.480
this guy wasn't affected.,00:42:36.480,00:42:40.240
"And this will
cancel, the K plus 2I",00:42:40.240,00:42:44.100
"will cancel this
one, just as before.",00:42:44.100,00:42:46.460
"This will not be
affected, U_(i+2),",00:42:46.460,00:42:53.640
"and I have F_i and K plus
2I F_(i+1) and F_(i+2).",00:42:53.640,00:43:07.220
"That should have been i minus
1, and this should have been i,",00:43:10.830,00:43:14.640
"and this should
have been i plus 1,",00:43:14.640,00:43:16.220
"sorry, mis-labeled the
F's, but no big deal.",00:43:16.220,00:43:24.630
"The point is the left
side, and the point",00:43:24.630,00:43:27.650
is what's in that space.,00:43:27.650,00:43:30.160
What goes in that space?,00:43:32.860,00:43:34.080
"Well, it's minus I, K
plus 2I, squared, minus I,",00:43:34.080,00:43:41.140
"so this is K plus 2I squared,
which before was so easy,",00:43:41.140,00:43:49.670
"and then the minus I, and
the minus I is the minus 2I,",00:43:49.670,00:43:53.850
is multiplying the U_i.,00:43:53.850,00:43:55.390
"Yeah, that was minus I.",00:43:55.390,00:44:00.490
"OK, this is my matrix
from odd-even reduction.",00:44:00.490,00:44:09.640
"It's just natural
to try the idea,",00:44:09.640,00:44:12.620
"and the idea works,
but not so perfectly,",00:44:12.620,00:44:18.540
"because previously in 1D, that
just gave us the answer 2;",00:44:18.540,00:44:26.330
it was 4 minus 2.,00:44:26.330,00:44:28.180
"But now in 2D, we have a
matrix, not surprising,",00:44:28.180,00:44:34.150
"but what we don't
like is the fact",00:44:34.150,00:44:36.460
"that the bandwidth is
growing. k was tri-diagonal,",00:44:36.460,00:44:40.690
"but when we square it, it
will have five diagonals,",00:44:40.690,00:44:44.290
"and when we repeat the odd-even
cycles, when we do it again,",00:44:44.290,00:44:49.430
"those five diagonals will be
nine diagonals, and onwards.",00:44:49.430,00:44:56.070
"So, I'm getting
half-sized problems.",00:44:56.070,00:45:04.380
"All the odd-numbered rows
in my square are eliminated,",00:45:04.380,00:45:11.400
"this just involves the
even-numbered rows,",00:45:11.400,00:45:13.970
"but the matrix is not
tri-diagonal anymore,",00:45:13.970,00:45:21.500
it's growing in bandwidth.,00:45:21.500,00:45:24.640
"So, and then you have
to keep track of it,",00:45:24.640,00:45:28.480
"so the final conclusion is that
this is a pretty good idea,",00:45:28.480,00:45:37.650
"but it's not quite
as good as this one.",00:45:37.650,00:45:41.800
"It's not as good as
the FFT-based idea.",00:45:41.800,00:45:45.330
"Also, if you look to see
what is most efficient --",00:45:45.330,00:45:50.230
"see the eigenvectors
are still here,",00:45:50.230,00:45:52.540
"so I could do three steps of
this and then go to Fourier,",00:45:52.540,00:45:59.690
"and that probably
is about right.",00:45:59.690,00:46:03.360
"So, if you really wanted to
polish off a fast Poisson",00:46:03.360,00:46:09.100
"solver, you could do
maybe two steps or three",00:46:09.100,00:46:13.760
"of odd-even cyclic
reduction, but then",00:46:13.760,00:46:18.390
"your matrix is getting messy and
you switch to the fast Poisson",00:46:18.390,00:46:27.010
solver.,00:46:27.010,00:46:27.510
"So it's not quite
Poisson anymore,",00:46:27.510,00:46:30.060
"because it's has
a messier matrix,",00:46:30.060,00:46:32.970
"but it still has the
same eigenvectors.",00:46:32.970,00:46:38.440
"As long as we stay
with the matrix K,",00:46:38.440,00:46:41.880
"we know its eigenvectors, and
we know that they're sines",00:46:41.880,00:46:48.410
"and that they're
quick to work with.",00:46:48.410,00:46:51.170
OK.,00:46:51.170,00:46:52.300
"Anyway, there you go.",00:46:52.300,00:46:53.510
"That's a fast algorithm for
the lecture before the spring",00:46:53.510,00:47:00.780
break.,00:47:00.780,00:47:02.590
"So after the break is, first
of all, discussion of projects.",00:47:02.590,00:47:08.210
"If your project could
include a page --",00:47:08.210,00:47:13.070
"and you could maybe email
the whole project to Mr. Cho.",00:47:13.070,00:47:19.240
"Maybe also, could you email
to me a sort of summary page",00:47:19.240,00:47:27.100
"that tells me what you did, so
I'll save the summary pages,",00:47:27.100,00:47:31.040
"and I'll have for Mr. Cho the
complete project with printout",00:47:31.040,00:47:37.540
"and graph, as far
as appropriate,",00:47:37.540,00:47:42.030
"and so we'll spend
some time on that,",00:47:42.030,00:47:44.490
"and then move to the big topic
of the rest of the course,",00:47:44.490,00:47:53.850
"which is solving optimization
problems, minimization,",00:47:53.850,00:48:00.450
maximization in many variables.,00:48:00.450,00:48:05.660
"OK, so have a good spring
break and see you a week",00:48:05.660,00:48:14.830
from Wednesday.,00:48:14.830,00:48:16.120
Good.,00:48:16.120,00:48:17.370

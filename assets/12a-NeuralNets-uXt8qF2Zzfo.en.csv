text,start,stop
"The following content is
provided under a Creative",00:00:00.040,00:00:02.410
Commons license.,00:00:02.410,00:00:03.790
"Your support will help
MIT OpenCourseWare",00:00:03.790,00:00:06.030
"continue to offer high-quality
educational resources for free.",00:00:06.030,00:00:10.100
"To make a donation or to
view additional materials",00:00:10.100,00:00:12.680
"from hundreds of MIT courses,
visit MIT OpenCourseWare",00:00:12.680,00:00:16.590
at fsae@mit.edu.,00:00:16.590,00:00:18.270
"PATRICK WINSTON: It was in
2010, yes, that's right.",00:00:37.340,00:00:42.313
It was in 2010.,00:00:42.313,00:00:44.630
"We were having our
annual discussion",00:00:44.630,00:00:46.590
"about what we would dump fro
6034 in order to make room",00:00:46.590,00:00:49.970
for some other stuff.,00:00:49.970,00:00:51.720
"And we almost killed
off neural nets.",00:00:51.720,00:00:54.540
"That might seem strange
because our heads",00:00:54.540,00:00:58.010
are stuffed with neurons.,00:00:58.010,00:00:59.860
"If you open up your skull
and pluck them all out,",00:00:59.860,00:01:02.000
you don't think anymore.,00:01:02.000,00:01:03.870
"So it would seem
that neural nets",00:01:03.870,00:01:07.914
"would be a fundamental
and unassailable topic.",00:01:07.914,00:01:12.610
"But many of us felt that
the neural models of the day",00:01:12.610,00:01:16.710
"weren't much in the way
of faithful models of what",00:01:16.710,00:01:23.380
"actually goes on
inside our heads.",00:01:23.380,00:01:25.476
"And besides that,
nobody had ever",00:01:25.476,00:01:26.850
"made a neural net that was
worth a darn for doing anything.",00:01:26.850,00:01:30.270
So we almost killed it off.,00:01:30.270,00:01:33.070
"But then we said,
well, everybody",00:01:33.070,00:01:34.694
"would feel cheated
if they take a course",00:01:34.694,00:01:36.360
"in artificial intelligence,
don't learn anything",00:01:36.360,00:01:38.060
"about neural nets,
and then they'll",00:01:38.060,00:01:39.334
"go off and invent
them themselves.",00:01:39.334,00:01:40.640
"And they'll waste
all sorts of time.",00:01:40.640,00:01:42.240
So we kept the subject in.,00:01:42.240,00:01:44.740
"Then two years
later, Jeff Hinton",00:01:44.740,00:01:48.420
"from the University of
Toronto stunned the world",00:01:48.420,00:01:52.070
"with some neural network
he had done on recognizing",00:01:52.070,00:01:56.450
and classifying pictures.,00:01:56.450,00:01:58.810
"And he published
a paper from which",00:01:58.810,00:02:00.610
"I am now going to show
you a couple of examples.",00:02:00.610,00:02:04.910
"Jeff's neural net, by the
way, had 60 million parameters",00:02:04.910,00:02:08.289
in it.,00:02:08.289,00:02:09.389
"And its purpose was to determine
which of 1,000 categories",00:02:09.389,00:02:15.110
best characterized a picture.,00:02:15.110,00:02:17.087
So there it is.,00:02:22.900,00:02:24.010
"There's a sample of things
that the Toronto neural net",00:02:24.010,00:02:31.280
"was able to recognize
or make mistakes on.",00:02:31.280,00:02:35.416
"I'm going to blow
that up a little bit.",00:02:35.416,00:02:37.040
"I think I'm going
to look particularly",00:02:37.040,00:02:38.623
"at the example labeled
container ship.",00:02:38.623,00:02:42.930
"So what you see here is that
the program returned its best",00:02:42.930,00:02:47.140
"estimate of what it was
ranked, first five, according",00:02:47.140,00:02:51.630
"to the likelihood,
probability, or the certainty",00:02:51.630,00:02:55.090
"that it felt that a
particular class was",00:02:55.090,00:02:59.240
characteristic of the picture.,00:02:59.240,00:03:01.030
"And so you can see this
one is extremely confident",00:03:01.030,00:03:03.420
that it's a container ship.,00:03:03.420,00:03:06.260
"It also was fairly
moved by the idea",00:03:06.260,00:03:09.920
that it might be a lifeboat.,00:03:09.920,00:03:13.590
"Now, I'm not sure about you,
but I don't think this looks",00:03:13.590,00:03:17.390
much like a lifeboat.,00:03:17.390,00:03:18.666
"But it does look like
a container ship.",00:03:18.666,00:03:20.290
"So if I look at only the best
choice, it looks pretty good.",00:03:20.290,00:03:23.450
"Here are the other things
they did pretty well,",00:03:23.450,00:03:25.760
"got the right answer
is the first choice--",00:03:25.760,00:03:27.980
is this first choice.,00:03:27.980,00:03:30.257
"So over on the left,
you see that it's",00:03:30.257,00:03:31.840
"decided that the picture
is a picture of a mite.",00:03:31.840,00:03:35.280
"The mite is not anywhere near
the center of the picture,",00:03:35.280,00:03:37.700
"but somehow it managed to find
it-- the container ship again.",00:03:37.700,00:03:41.440
"There is a motor scooter, a
couple of people sitting on it.",00:03:41.440,00:03:44.110
"But it correctly characterized
the picture as a motor scooter.",00:03:44.110,00:03:48.490
"And then on the
right, a Leopard.",00:03:48.490,00:03:50.160
"And everything else
is a cat of some sort.",00:03:50.160,00:03:52.450
"So it seems to be
doing pretty well.",00:03:52.450,00:03:54.280
"In fact, it does do pretty well.",00:03:54.280,00:03:56.930
"But anyone who does
this kind of work",00:03:56.930,00:03:59.124
"has an obligation
to show you some",00:03:59.124,00:04:00.540
"of the stuff that
doesn't work so well on",00:04:00.540,00:04:03.110
or doesn't get quite right.,00:04:03.110,00:04:04.670
"And so these pictures also
occurred in Hinton's paper.",00:04:04.670,00:04:10.010
"So the first one is
characterized as a grill.",00:04:10.010,00:04:12.840
"But the right answer was
supposed to be convertible.",00:04:12.840,00:04:15.910
"Oh, no, yes, yeah, right
answer was convertible.",00:04:15.910,00:04:19.480
"In the second case,
the characterization",00:04:19.480,00:04:22.430
is of a mushroom.,00:04:22.430,00:04:24.560
"And the alleged right
answer is agaric.",00:04:24.560,00:04:28.320
Is that pronounced right?,00:04:28.320,00:04:29.740
"It turns out that's a kind of
mushroom-- so no problem there.",00:04:29.740,00:04:33.730
"In the next case, it
said it was a cherry.",00:04:33.730,00:04:36.280
"But it was supposed
to be a dalmatian.",00:04:36.280,00:04:37.930
"Now, I think a dalmatian is
a perfectly legitimate answer",00:04:37.930,00:04:41.200
"for that particular picture--
so hard to fault it for that.",00:04:41.200,00:04:44.620
"And the last case,
the correct answer",00:04:44.620,00:04:47.640
was not in any of the top five.,00:04:47.640,00:04:50.700
"I'm not sure if you've
ever seen a Madagascar cap.",00:04:50.700,00:04:53.100
But that's a picture of one.,00:04:53.100,00:04:54.720
"And it's interesting
to compare that",00:04:54.720,00:04:56.220
"with the first choice of the
program, the squirrel monkey.",00:04:56.220,00:04:58.970
This is the two side by side.,00:04:58.970,00:05:02.160
"So in a way, it's not
surprising that it",00:05:02.160,00:05:04.190
"thought that the
Madagascar cat was",00:05:04.190,00:05:06.330
"a picture of a squirrel
monkey-- so pretty impressive.",00:05:06.330,00:05:10.790
It blew away the competition.,00:05:10.790,00:05:12.050
"It did so much better the
second place wasn't even close.",00:05:12.050,00:05:16.225
"And for the first time, it
demonstrated that a neural net",00:05:16.225,00:05:18.600
could actually do something.,00:05:18.600,00:05:20.350
"And since that time, in the
three years since that time,",00:05:20.350,00:05:23.810
"there's been an enormous
amount of effort",00:05:23.810,00:05:26.150
"put into neural net technology,
which some say is the answer.",00:05:26.150,00:05:31.470
"So what we're going to
do today and tomorrow",00:05:31.470,00:05:34.440
"is have a look at this stuff
and ask ourselves why it works,",00:05:34.440,00:05:39.130
"when it might not work,
what needs to be done,",00:05:39.130,00:05:41.560
"what has been done, and all
those kinds of questions",00:05:41.560,00:05:43.790
will emerge.,00:05:43.790,00:05:44.470
"So I guess the first thing to
do is think about what it is",00:05:51.520,00:05:55.640
that we are being inspired by.,00:05:55.640,00:05:57.970
"We're being inspired
by those things that",00:05:57.970,00:06:00.980
"are inside our head-- all
10 to the 11th of them.",00:06:00.980,00:06:04.650
"And so if we take one of those
10 to the 11th and look at it,",00:06:04.650,00:06:09.755
"you know from 700 something
or other approximately",00:06:09.755,00:06:12.700
what a neuron looks like.,00:06:12.700,00:06:14.570
"And by the way, I'm going
to teach you in this lecture",00:06:14.570,00:06:17.760
"how to answer questions
about neurobiology",00:06:17.760,00:06:20.230
"with an 80% probability that
you will give the same answer",00:06:20.230,00:06:23.170
as a neurobiologist.,00:06:23.170,00:06:25.470
So let's go.,00:06:25.470,00:06:28.310
So here's a neuron.,00:06:28.310,00:06:30.140
It's got a cell body.,00:06:30.140,00:06:31.930
And there is a nucleus.,00:06:31.930,00:06:33.530
"And then out here is
a long thingamajigger",00:06:33.530,00:06:36.350
"which divides maybe a
little bit, but not much.",00:06:36.350,00:06:41.230
And we call that the axon.,00:06:41.230,00:06:42.395
"So then over here, we've got
this much more branching type",00:06:46.080,00:06:50.100
"of structure that looks
maybe a little bit like so.",00:06:50.100,00:06:54.572
"Maybe like that-- and this
stuff branches a whole lot.",00:07:02.144,00:07:04.550
"And that part is called
the dendritic tree.",00:07:04.550,00:07:06.520
"Now, there are a
couple of things",00:07:15.400,00:07:17.310
"we can note about this is that
these guys are connected axon",00:07:17.310,00:07:21.350
to dendrite.,00:07:21.350,00:07:22.640
"So over here, they'll be
a so-called pre-synaptic",00:07:22.640,00:07:28.430
thickening.,00:07:28.430,00:07:29.640
"And over here will be some
other neuron's dendrite.",00:07:29.640,00:07:33.390
"And likewise, over here
some other neuron's axon",00:07:33.390,00:07:38.220
"is coming in here and hitting
the dendrite of our the one",00:07:38.220,00:07:45.855
"that occupies most
of our picture.",00:07:45.855,00:07:47.517
"So if there is enough
stimulation from this side",00:07:53.220,00:07:57.260
"in the axonal tree,
or the dendritic tree,",00:07:57.260,00:08:01.060
"then a spike will
go down that axon.",00:08:01.060,00:08:04.620
"It acts like a
transmission line.",00:08:04.620,00:08:07.820
"And then after that
happens, the neuron",00:08:07.820,00:08:12.370
"will go quiet for a while as
it's recovering its strength.",00:08:12.370,00:08:15.000
"That's called the
refractory period.",00:08:15.000,00:08:16.550
"Now, if we look at that
connection in a little more",00:08:19.560,00:08:23.230
"detail, this little piece right
here sort of looks like this.",00:08:23.230,00:08:30.000
Here's the axon coming in.,00:08:30.000,00:08:32.820
"It's got a whole bunch
of little vesicles in it.",00:08:32.820,00:08:35.870
"And then there's a
dendrite over here.",00:08:35.870,00:08:39.049
"And when the axon is stimulated,
it dumps all these vesicles",00:08:39.049,00:08:42.730
into this inner synaptic space.,00:08:42.730,00:08:45.202
"For a long time, it wasn't
known whether those things",00:08:45.202,00:08:47.410
were actually separated.,00:08:47.410,00:08:49.430
"I think it was
Raamon and Cahal who",00:08:49.430,00:08:51.110
"demonstrated that one
neuron is actually",00:08:51.110,00:08:54.410
not part of the next one.,00:08:54.410,00:08:56.030
"They're actually separated
by these synaptic gaps.",00:08:56.030,00:09:01.610
So there it is.,00:09:01.610,00:09:04.120
"How can we model,
that sort of thing?",00:09:04.120,00:09:06.770
"Well, here's what's
usually done.",00:09:06.770,00:09:08.440
"Here's what is done in
the neural net literature.",00:09:08.440,00:09:11.320
"First of all, we've got
some kind of binary input,",00:09:17.070,00:09:20.960
"because these things either
fire or they don't fire.",00:09:20.960,00:09:23.370
"So it's an all-or-none
kind of situation.",00:09:23.370,00:09:26.060
"So over here, we have
some kind of input value.",00:09:26.060,00:09:29.510
We'll call it x1.,00:09:29.510,00:09:31.000
And is either a 0 or 1.,00:09:31.000,00:09:34.010
So it comes in here.,00:09:34.010,00:09:35.910
"And then it gets multiplied
times some kind of weight.",00:09:35.910,00:09:40.450
We'll call it w1.,00:09:40.450,00:09:42.635
"So this part here is modeling
this synaptic connection.",00:09:45.620,00:09:49.910
It may be more or less strong.,00:09:49.910,00:09:51.890
"And if it's more strong,
this weight goes up.",00:09:51.890,00:09:53.850
"And if it's less strong,
this weight goes down.",00:09:53.850,00:09:56.510
"So that reflects the
influence of the synapse",00:09:56.510,00:10:01.220
"on whether or not the whole
axon decides it's stimulated.",00:10:01.220,00:10:05.900
"Then we got other inputs down
here-- x sub n, also 0 or 1.",00:10:05.900,00:10:12.110
"It's also multiplied
by a weight.",00:10:12.110,00:10:15.180
We'll call that w sub n.,00:10:15.180,00:10:17.570
"And now, we have to
somehow represent",00:10:17.570,00:10:20.560
"the way in which these inputs
are collected together--",00:10:20.560,00:10:26.960
how they have collective force.,00:10:26.960,00:10:29.290
"And we're going to
model that very, very",00:10:29.290,00:10:30.960
"simply just by saying, OK,
we'll run it through a summer",00:10:30.960,00:10:37.610
like so.,00:10:37.610,00:10:39.660
"But then we have to decide if
the collective influence of all",00:10:39.660,00:10:43.390
"those inputs is sufficient
to make the neuron fire.",00:10:43.390,00:10:48.630
"So we're going to
do that by running",00:10:48.630,00:10:51.430
"this guy through a
threshold box like so.",00:10:51.430,00:10:56.110
"Here is what the box looks like
in terms of the relationship",00:10:56.110,00:10:59.930
between input and the output.,00:10:59.930,00:11:02.540
"And what you can see
here is that nothing",00:11:02.540,00:11:04.430
"happens until the input
exceeds some threshold t.",00:11:04.430,00:11:09.040
"If that happens, then
the output z is a 1.",00:11:09.040,00:11:13.960
"Otherwise, it's a 0.",00:11:13.960,00:11:16.270
"So binary, binary out-- we
model the synaptic weights",00:11:16.270,00:11:20.040
by these multipliers.,00:11:20.040,00:11:21.530
"We model the cumulative effect
of all that input to the neuron",00:11:21.530,00:11:26.850
by a summer.,00:11:26.850,00:11:28.500
"We decide if it's going to be
an all-or-none 1 by running it",00:11:28.500,00:11:31.749
"through this threshold
box and seeing",00:11:31.749,00:11:33.290
"if the sum of the products add
up to more than the threshold.",00:11:33.290,00:11:39.370
"If so, we get a 1.",00:11:39.370,00:11:41.610
"So what, in the end,
are we in fact modeling?",00:11:41.610,00:11:46.820
"Well, with this model,
we have number 1, all",00:11:46.820,00:11:54.900
"or none-- number 2, cumulative
influence-- number 3, oh, I,",00:11:54.900,00:12:12.890
suppose synaptic weight.,00:12:12.890,00:12:14.171
"But that's not all
that there might",00:12:20.432,00:12:21.890
be to model in a real neuron.,00:12:21.890,00:12:24.940
"We might want to deal with
the refractory period.",00:12:24.940,00:12:27.142
"In these biological models that
we build neural nets out of,",00:12:39.180,00:12:43.470
"we might want to model
axonal bifurcation.",00:12:43.470,00:12:45.385
"We do get some division
in the axon of the neuron.",00:12:53.280,00:12:56.330
"And it turns out that that
pulse will either go down",00:12:56.330,00:12:59.070
one branch or the other.,00:12:59.070,00:13:01.200
"And which branch it
goes down depends",00:13:01.200,00:13:02.970
"on electrical activity in
the vicinity of the division.",00:13:02.970,00:13:06.870
"So these things might actually
be a fantastic coincidence",00:13:06.870,00:13:09.560
detectors.,00:13:09.560,00:13:10.840
But we're not modeling that.,00:13:10.840,00:13:12.006
We don't know how it works.,00:13:12.006,00:13:15.630
"So axonal bifurcation
might be modeled.",00:13:15.630,00:13:17.290
"We might also have a
look at time patterns.",00:13:17.290,00:13:21.900
"See, what we don't
know is we don't",00:13:26.402,00:13:27.860
"know if the timing of the
arrival of these pulses",00:13:27.860,00:13:32.240
"in the dendritic
tree has anything",00:13:32.240,00:13:34.290
"to do with what that neuron
is going to recognize--",00:13:34.290,00:13:38.050
so a lot of unknowns here.,00:13:38.050,00:13:40.320
"And now, I'm going
to show you how",00:13:40.320,00:13:42.650
"to answer a question
about neurobiology",00:13:42.650,00:13:45.010
"with 80% probability
you'll get it right.",00:13:45.010,00:13:47.590
"Just say, we don't know.",00:13:47.590,00:13:51.910
"And that will be with
80% probability what",00:13:51.910,00:13:53.890
the neurobiologist would say.,00:13:53.890,00:13:55.270
"So this is a model inspired
by what goes on in our heads.",00:13:58.700,00:14:03.240
"But it's far from clear
if what we're modeling",00:14:03.240,00:14:07.240
"is the essence of why those guys
make possible what we can do.",00:14:07.240,00:14:13.280
"Nevertheless, that's where
we're going to start.",00:14:13.280,00:14:15.280
That's where we're going to go.,00:14:15.280,00:14:16.571
"So we've got this model
of what a neuron does.",00:14:16.571,00:14:20.500
"So what about what does a
collection of these neurons do?",00:14:20.500,00:14:25.180
"Well, we can think of your skull
as a big box full of neurons.",00:14:25.180,00:14:31.830
"Maybe a better way
to think of this",00:14:37.680,00:14:39.170
"is that your head
is full of neurons.",00:14:39.170,00:14:42.030
"And they in turn are full of
weights and thresholds like so.",00:14:42.030,00:14:51.490
"So into this box come a variety
of inputs x1 through xm.",00:14:51.490,00:14:56.676
"And these find their
way to the inside",00:15:00.080,00:15:01.780
of this gaggle of neurons.,00:15:01.780,00:15:04.790
"And out here come a bunch
of outputs c1 through zn.",00:15:04.790,00:15:13.320
"And there a whole bunch
of these maybe like so.",00:15:13.320,00:15:17.110
"And there are a lot
of inputs like so.",00:15:17.110,00:15:19.680
"And somehow these inputs
through the influence",00:15:19.680,00:15:23.630
"of the weights of the thresholds
come out as a set of outputs.",00:15:23.630,00:15:29.260
"So we can write
that down a little",00:15:29.260,00:15:31.350
"fancier by just saying
that z is a vector, which",00:15:31.350,00:15:36.770
"is a function of, certainly
the input vector, but also",00:15:36.770,00:15:42.220
"the weight vector and
the threshold vector.",00:15:42.220,00:15:45.510
So that's all a neural net is.,00:15:45.510,00:15:47.780
"And when we train
a neural net, all",00:15:47.780,00:15:49.430
"we're going to be able to
do is adjust those weights",00:15:49.430,00:15:52.210
"and thresholds so that what
we get out is what we want.",00:15:52.210,00:15:57.430
"So a neural net is a
function approximator.",00:15:57.430,00:16:00.570
It's good to think about that.,00:16:00.570,00:16:02.270
It's a function approximator.,00:16:02.270,00:16:03.478
"So maybe we've got some sample
data that gives us an output",00:16:05.420,00:16:11.560
"vector that's desired as
another function of the input,",00:16:11.560,00:16:17.865
"forgetting about what the
weights and the thresholds are.",00:16:17.865,00:16:20.240
That's what we want to get out.,00:16:20.240,00:16:22.550
"And so how well we're
doing can be figured out",00:16:22.550,00:16:24.990
"by comparing the desired
value with the actual value.",00:16:24.990,00:16:31.440
"So we might think
then that we can",00:16:31.440,00:16:33.520
"get a handle on how well
we're doing by constructing",00:16:33.520,00:16:38.450
"some performance function, which
is determined by the desired",00:16:38.450,00:16:45.770
"vector and the input
vector-- sorry,",00:16:45.770,00:16:50.960
"the desired vector and
the actual output vector",00:16:50.960,00:16:54.330
"for some particular input
or for some set of inputs.",00:16:54.330,00:16:58.370
"And the question is what
should that function be?",00:16:58.370,00:17:01.340
"How should we
measure performance",00:17:01.340,00:17:03.770
"given that we have
what we want out here",00:17:03.770,00:17:06.980
"and what we actually
got out here?",00:17:06.980,00:17:09.670
"Well, one simple
thing to do is just",00:17:09.670,00:17:12.349
"to measure the magnitude
of the difference.",00:17:12.349,00:17:16.520
That makes sense.,00:17:16.520,00:17:18.130
"But of course, that would give
us a performance function that",00:17:18.130,00:17:24.180
"is a function of the
distance between those",00:17:24.180,00:17:26.119
vectors would look like this.,00:17:26.119,00:17:28.289
"But this turns out
to be mathematically",00:17:32.000,00:17:35.880
inconvenient in the end.,00:17:35.880,00:17:36.980
"So how do you think we're going
to turn it up a little bit?",00:17:36.980,00:17:38.730
AUDIENCE: Normalize it?,00:17:38.730,00:17:39.910
PATRICK WINSTON: What's that?,00:17:39.910,00:17:41.118
AUDIENCE: Normalize it?,00:17:41.118,00:17:42.120
"PATRICK WINSTON:
Well, I don't know.",00:17:42.120,00:17:43.750
How about just we square it?,00:17:43.750,00:17:46.372
"And that way we're going to go
from this little sharp point",00:17:46.372,00:17:49.890
"down there to something
that looks more like that.",00:17:49.890,00:17:54.800
"So it's best when the
difference is 0, of course.",00:17:54.800,00:17:58.300
"And it gets worse as
you move away from 0.",00:17:58.300,00:18:02.380
"But what we're
trying to do here is",00:18:02.380,00:18:04.080
"we're trying to get
to a minimum value.",00:18:04.080,00:18:07.370
And I hope you'll forgive me.,00:18:07.370,00:18:09.360
"I just don't like
the direction we're",00:18:09.360,00:18:11.170
"going here, because I like to
think in terms of improvement",00:18:11.170,00:18:14.040
"as going uphill
instead of down hill.",00:18:14.040,00:18:16.740
"So I'm going to dress this up
one more step-- put a minus",00:18:16.740,00:18:21.070
sign out there.,00:18:21.070,00:18:22.580
"And then our performance
function looks like this.",00:18:22.580,00:18:25.540
It's always negative.,00:18:25.540,00:18:26.810
"And the best value it
can possibly be is zero.",00:18:26.810,00:18:29.640
"So that's what we're going to
use just because I am who I am.",00:18:29.640,00:18:33.040
"And it doesn't matter, right?",00:18:33.040,00:18:34.590
"Still, you're trying to
either minimize or maximize",00:18:34.590,00:18:37.260
some performance function.,00:18:37.260,00:18:40.490
"OK, so what do we got to do?",00:18:40.490,00:18:41.660
"I guess what we could do is we
could treat this thing-- well,",00:18:41.660,00:18:46.630
we already know what to do.,00:18:46.630,00:18:49.000
"I'm not even sure why we're
devoting our lecture to this,",00:18:49.000,00:18:51.860
"because it's clear that
what we're trying to do",00:18:51.860,00:18:55.580
"is we're trying to take our
weights and our thresholds",00:18:55.580,00:18:59.840
"and adjust them so as
to maximize performance.",00:18:59.840,00:19:03.110
"So we can make a
little contour map here",00:19:03.110,00:19:05.570
"with a simple neural net
with just two weights in it.",00:19:05.570,00:19:09.020
"And maybe it looks like
this-- contour map.",00:19:09.020,00:19:11.562
"And at any given time
we've got a particular w1",00:19:14.990,00:19:18.810
and particular w2.,00:19:18.810,00:19:20.470
"And we're trying to
find a better w1 and w2.",00:19:20.470,00:19:23.570
So here we are right now.,00:19:23.570,00:19:26.550
And there's the contour map.,00:19:26.550,00:19:28.760
And it's a 6034.,00:19:28.760,00:19:29.940
So what do we do?,00:19:29.940,00:19:31.634
AUDIENCE: Climb.,00:19:31.634,00:19:33.030
"PATRICK WINSTON: Simple matter
of hill climbing, right?",00:19:33.030,00:19:35.520
"So we'll take a step
in every direction.",00:19:35.520,00:19:38.360
"If we take a step in that
direction, not so hot.",00:19:38.360,00:19:41.860
That actually goes pretty bad.,00:19:41.860,00:19:44.100
These two are really ugly.,00:19:44.100,00:19:46.550
"Ah, but that one--
that one takes us",00:19:46.550,00:19:48.280
up the hill a little bit.,00:19:48.280,00:19:50.430
"So we're done,
except that I just",00:19:50.430,00:19:54.120
"mentioned that
Hinton's neural net had",00:19:54.120,00:19:55.870
60 million parameters in it.,00:19:55.870,00:19:57.962
"So we're not going to hill
climb with 60 million parameters",00:19:57.962,00:20:00.420
"because it explodes
exponentially",00:20:00.420,00:20:03.812
"in the number of
weights you've got",00:20:03.812,00:20:05.270
"to deal with-- the number
of steps you can take.",00:20:05.270,00:20:08.936
"So this approach is
computationally intractable.",00:20:08.936,00:20:10.936
"Fortunately, you've all taken
1801 or the equivalent thereof.",00:20:13.560,00:20:19.280
So you have a better idea.,00:20:19.280,00:20:21.690
"Instead of just taking a
step in every direction, what",00:20:21.690,00:20:24.770
"we're going to do is
we're going to take",00:20:24.770,00:20:27.590
some partial derivatives.,00:20:27.590,00:20:30.150
"And we're going to
see what they suggest",00:20:30.150,00:20:33.160
"to us in terms of how we're
going to get around in space.",00:20:33.160,00:20:36.720
"So we might have a partial
of that performance function",00:20:36.720,00:20:39.130
up there with respect to w1.,00:20:39.130,00:20:42.969
"And we might also take
a partial derivative",00:20:42.969,00:20:44.760
of that guy with respect to w2.,00:20:44.760,00:20:48.450
"And these will tell us
how much improvement",00:20:48.450,00:20:50.480
"we're getting by making a little
movement in those directions,",00:20:50.480,00:20:53.510
right?,00:20:53.510,00:20:55.220
"How much a change is
given that we're just",00:20:55.220,00:20:57.408
going right along the axis.,00:20:57.408,00:20:58.532
"So maybe what we ought
to do is if this guy is",00:21:01.180,00:21:07.020
"much bigger than
this guy, it would",00:21:07.020,00:21:08.910
"suggest we mostly want to
move in this direction,",00:21:08.910,00:21:12.120
"or to put it in 1801
terms, what we're",00:21:12.120,00:21:13.990
"going to do is we're going
to follow the gradient.",00:21:13.990,00:21:16.550
"And so the change
in the w vector",00:21:16.550,00:21:21.320
"is going to equal to this
partial derivative times",00:21:21.320,00:21:25.776
"i plus this partial
derivative times j.",00:21:25.776,00:21:30.010
"So what we're going to end up
doing in this particular case",00:21:30.010,00:21:32.600
"by following that formula is
moving off in that direction",00:21:32.600,00:21:36.610
"right up to the steepest
part of the hill.",00:21:36.610,00:21:40.310
"And how much we
move is a question.",00:21:40.310,00:21:43.840
"So let's just have a rate
constant R that decides how",00:21:43.840,00:21:47.400
big our step is going to be.,00:21:47.400,00:21:50.110
And now you think we were done.,00:21:50.110,00:21:53.080
"Well, too bad for our side.",00:21:53.080,00:21:55.740
We're not done.,00:21:55.740,00:21:57.380
"There's a reason
why we can't use--",00:21:57.380,00:21:59.410
"create ascent, or in the case
that I've drawn our gradient,",00:21:59.410,00:22:04.214
"descent if we take the
performance function",00:22:04.214,00:22:06.005
the other way.,00:22:06.005,00:22:07.220
Why can't we use it?,00:22:07.220,00:22:09.128
AUDIENCE: Local maxima.,00:22:09.128,00:22:10.086
"PATRICK WINSTON: The
remark is local maxima.",00:22:12.237,00:22:14.070
And that is certainly true.,00:22:14.070,00:22:15.683
But it's not our first obstacle.,00:22:15.683,00:22:17.016
"Why doesn't gradient
ascent work?",00:22:19.860,00:22:21.892
"AUDIENCE: So you're
using a step function.",00:22:26.301,00:22:28.250
"PATRICK WINSTON: Ah,
there's something",00:22:28.250,00:22:28.550
wrong with our function.,00:22:28.550,00:22:30.580
That's right.,00:22:30.580,00:22:31.690
"It's non-linear, but
rather, it's discontinuous.",00:22:31.690,00:22:35.590
"So gradient ascent requires
a continuous space,",00:22:35.590,00:22:39.100
continuous surface.,00:22:39.100,00:22:40.870
So too bad our side.,00:22:40.870,00:22:43.880
It isn't.,00:22:43.880,00:22:46.010
So what to do?,00:22:46.010,00:22:48.910
"Well, nobody knew what
to do for 25 years.",00:22:48.910,00:22:52.840
"People were screwing around
with training neural nets",00:22:52.840,00:22:55.120
"for 25 years before Paul
Werbos sadly at Harvard in 1974",00:22:55.120,00:23:01.340
gave us the answer.,00:23:01.340,00:23:03.010
"And now I want to tell
you what the answer is.",00:23:03.010,00:23:05.120
"The first part of the answer is
those thresholds are annoying.",00:23:05.120,00:23:09.920
"They're just extra
baggage to deal with.",00:23:09.920,00:23:15.440
"What we really like instead of
c being a function of xw and t",00:23:15.440,00:23:19.310
"was we'd like c prime
to be a function f",00:23:19.310,00:23:23.465
prime of x and the weights.,00:23:23.465,00:23:27.875
"But we've got to account
for the threshold somehow.",00:23:27.875,00:23:30.000
So here's how you do that.,00:23:30.000,00:23:31.960
"What you do is
you say let us add",00:23:31.960,00:23:36.220
another input to this neuron.,00:23:36.220,00:23:40.320
"And it's going to
have a weight w0.",00:23:40.320,00:23:44.515
"And it's going to be
connected to an input that's",00:23:49.160,00:23:52.120
always minus 1.,00:23:52.120,00:23:55.340
You with me so far?,00:23:55.340,00:23:56.732
"Now what we're
going to do is we're",00:23:56.732,00:23:58.190
"going to say, let w0 equal t.",00:23:58.190,00:24:04.070
"What does that do to the
movement of the threshold?",00:24:06.578,00:24:08.702
"What it does is it
takes that threshold",00:24:11.660,00:24:13.760
and moves it back to 0.,00:24:13.760,00:24:16.060
"So this little trick here
takes this pink threshold",00:24:16.060,00:24:19.750
"and redoes it so that the new
threshold box looks like this.",00:24:19.750,00:24:24.550
Think about it.,00:24:30.370,00:24:31.480
"If this is t, and this is
minus 1, then this is minus t.",00:24:31.480,00:24:35.930
"And so this thing ought to
fire if everything's over--",00:24:35.930,00:24:38.490
if the sum is over 0.,00:24:38.490,00:24:39.750
So it makes sense.,00:24:39.750,00:24:41.080
"And it gets rid of the
threshold thing for us.",00:24:41.080,00:24:43.420
"So now we can just
think about weights.",00:24:43.420,00:24:46.920
"But still, we've got
that step function there.",00:24:46.920,00:24:53.740
And that's not good.,00:24:53.740,00:24:55.714
"So what we're going
to do is we're",00:24:55.714,00:24:57.130
going to smooth that guy out.,00:24:57.130,00:25:00.700
So this is trick number two.,00:25:00.700,00:25:03.846
"Instead of a step
function, we're",00:25:03.846,00:25:05.220
"going to have this
thing we lovingly",00:25:05.220,00:25:07.610
"call a sigmoid
function, because it's",00:25:07.610,00:25:09.840
kind of from an s-type shape.,00:25:09.840,00:25:12.110
"And the function we're going
to use is this one-- one,",00:25:12.110,00:25:18.280
"well, better make it a little
bit different-- 1 over 1 plus",00:25:18.280,00:25:23.710
"e to the minus
whatever the input is.",00:25:23.710,00:25:27.230
Let's call the input alpha.,00:25:27.230,00:25:30.070
Does that makes sense?,00:25:30.070,00:25:32.610
"Is alpha is 0, then it's 1
over 1 plus 1 plus one half.",00:25:32.610,00:25:37.560
"If alpha is extremely big,
then even the minus alpha",00:25:37.560,00:25:40.960
is extremely small.,00:25:40.960,00:25:42.060
And it becomes one.,00:25:42.060,00:25:44.100
"It goes up to an asymptotic
value of one here.",00:25:44.100,00:25:47.460
"On the other hand, if alpha
is extremely negative,",00:25:47.460,00:25:50.510
"than the minus alpha
is extremely positive.",00:25:50.510,00:25:53.840
And it goes to 0 asymptotically.,00:25:53.840,00:25:56.470
"So we got the right
look to that function.",00:25:56.470,00:25:59.830
It's a very convenient function.,00:25:59.830,00:26:01.680
"Did God say that neurons
ought to be-- that threshold",00:26:01.680,00:26:05.990
ought to work like that?,00:26:05.990,00:26:08.080
"No, God didn't say so.",00:26:08.080,00:26:09.140
Who said so?,00:26:09.140,00:26:11.760
The math says so.,00:26:11.760,00:26:13.540
"It has the right shape
and look and the math.",00:26:13.540,00:26:16.960
"And it turns out to
have the right math,",00:26:16.960,00:26:19.522
as you'll see in a moment.,00:26:19.522,00:26:20.605
So let's see.,00:26:23.530,00:26:24.357
Where are we?,00:26:24.357,00:26:25.450
"We decided that
what we'd like to do",00:26:25.450,00:26:26.950
"is take these
partial derivatives.",00:26:26.950,00:26:29.022
"We know that it was awkward
to have those thresholds.",00:26:29.022,00:26:31.230
So we got rid of them.,00:26:31.230,00:26:32.294
"And we noted that it was
impossible to have the step",00:26:32.294,00:26:34.460
function.,00:26:34.460,00:26:34.960
So we got rid of it.,00:26:34.960,00:26:36.450
"Now, we're a situation
where we can actually",00:26:36.450,00:26:38.520
"take those partial derivatives,
and see if it gives us",00:26:38.520,00:26:41.170
"a way of training
the neural net so as",00:26:41.170,00:26:43.180
"to bring the actual output into
alignment with what we desire.",00:26:43.180,00:26:46.155
"So to deal with
that, we're going",00:26:48.525,00:26:49.900
"to have to work with the
world's simplest neural net.",00:26:49.900,00:26:54.120
"Now, if we've got one
neuron, it's not a net.",00:26:54.120,00:26:57.896
"But if we've got two-word
neurons, we've got a net.",00:26:57.896,00:27:00.020
"And it turns out that's the
world's simplest neuron.",00:27:00.020,00:27:02.560
"So we're going to look at it--
not 60 million parameters,",00:27:02.560,00:27:05.880
"but just a few, actually,
just two parameters.",00:27:05.880,00:27:11.390
So let's draw it out.,00:27:11.390,00:27:13.350
We've got input x.,00:27:13.350,00:27:16.090
That goes into a multiplier.,00:27:16.090,00:27:18.560
And it gets multiplied times w1.,00:27:18.560,00:27:22.790
"And that goes into a
sigmoid box like so.",00:27:22.790,00:27:27.700
"We'll call this p1, by the
way, product number one.",00:27:27.700,00:27:30.750
Out here comes y.,00:27:30.750,00:27:33.270
"Y gets multiplied
times another weight.",00:27:33.270,00:27:37.030
We'll call that w2.,00:27:37.030,00:27:40.630
"The neck produces another
product which we'll call p2.",00:27:40.630,00:27:44.940
"And that goes into
a sigmoid box.",00:27:44.940,00:27:49.200
And then that comes out as z.,00:27:49.200,00:27:51.920
"And z is the number
that we use to determine",00:27:51.920,00:27:54.230
how well we're doing.,00:27:54.230,00:27:55.820
"And our performance
function p is",00:27:55.820,00:28:00.270
"going to be one
half minus one half,",00:28:00.270,00:28:02.944
"because I like
things are going in",00:28:02.944,00:28:04.360
"a direction, times the
difference between the desired",00:28:04.360,00:28:08.330
"output and the actual
output squared.",00:28:08.330,00:28:11.006
"So now let's decide what
those partial derivatives",00:28:14.480,00:28:18.364
are going to be.,00:28:18.364,00:28:19.030
Let me do it over here.,00:28:25.220,00:28:26.180
"So what are we
trying to compute?",00:28:32.976,00:28:34.350
"Partial of the performance
function p with respect to w2.",00:28:34.350,00:28:39.100
OK.,00:28:42.553,00:28:43.052
"Well, let's see.",00:28:47.970,00:28:50.324
"We're trying to figure
out how much this",00:28:50.324,00:28:51.990
wiggles when we wiggle that.,00:28:51.990,00:28:54.536
"But you know it goes
through this variable p2.",00:28:57.390,00:29:01.176
"And so maybe what we
could do is figure",00:29:01.176,00:29:02.800
"out how much this wiggles--
how much z wiggles",00:29:02.800,00:29:05.750
"when we wiggle p2
and then how much p2",00:29:05.750,00:29:08.830
wiggles when we wiggle w2.,00:29:08.830,00:29:13.290
"I just multiplied
those together.",00:29:13.290,00:29:15.580
I forget.,00:29:15.580,00:29:16.080
What's that called?,00:29:16.080,00:29:18.840
N180-- something or other.,00:29:18.840,00:29:20.310
AUDIENCE: The chain rule,00:29:20.310,00:29:21.310
PATRICK WINSTON: The chain rule.,00:29:21.310,00:29:22.754
"So what we're going
to do is we're",00:29:22.754,00:29:24.170
"going to rewrite that partial
derivative using chain rule.",00:29:24.170,00:29:27.230
"And all it's doing is
saying that there's",00:29:27.230,00:29:29.200
an intermediate variable.,00:29:29.200,00:29:31.250
"And we can compute how much
that end wiggles with respect",00:29:31.250,00:29:35.380
"how much that end
wiggles by multiplying",00:29:35.380,00:29:39.755
how much the other guys wiggle.,00:29:39.755,00:29:41.545
Let me write it down.,00:29:41.545,00:29:42.420
"It makes more sense
in mathematics.",00:29:42.420,00:29:45.420
"So that's going to be
able to the partial of p",00:29:45.420,00:29:48.260
"with respect to z times the
partial of z with respect",00:29:48.260,00:29:58.650
to p2.,00:29:58.650,00:29:59.950
Keep me on track here.,00:30:04.140,00:30:06.200
Partial of z with respect to w2.,00:30:06.200,00:30:09.490
"Now, I'm going to do something
for which I will hate myself.",00:30:12.310,00:30:15.920
"I'm going to erase
something on the board.",00:30:15.920,00:30:17.780
I don't like to do that.,00:30:17.780,00:30:18.780
"But you know what I'm
going to do, don't you?",00:30:18.780,00:30:21.900
"I'm going to say this is
true by the chain rule.",00:30:21.900,00:30:27.910
"But look, I can
take this guy here",00:30:27.910,00:30:30.550
"and screw around with it
with the chain rule too.",00:30:30.550,00:30:34.060
"And in fact, what
I'm going to do",00:30:34.060,00:30:35.880
"is I'm going to replace
that with partial of z",00:30:35.880,00:30:39.996
"with respect to p2 and partial
of p2 with respect to w2.",00:30:39.996,00:30:48.139
So I didn't erase it after all.,00:30:48.139,00:30:49.430
"But you can see what
I'm going to do next.",00:30:49.430,00:30:52.110
"Now, I'm going to
do same thing with",00:30:52.110,00:30:53.610
the other partial derivative.,00:30:53.610,00:30:55.780
"But this time, instead of
writing down and writing over,",00:30:55.780,00:30:58.890
"I'm just going to expand it
all out in one go, I think.",00:30:58.890,00:31:02.580
"So partial of p
with respect to w1",00:31:05.200,00:31:10.620
"is equal to the partial
of p with respect to z,",00:31:10.620,00:31:15.140
"the partial of z with respect
to p2, the partial of p2",00:31:15.140,00:31:21.810
with respect to what?,00:31:21.810,00:31:23.700
Y?,00:31:23.700,00:31:26.260
"Partial of y with respect
to p1-- partial of p1",00:31:26.260,00:31:35.170
with respect to w1.,00:31:35.170,00:31:38.950
"So that's going like a zipper
down that string of variables",00:31:38.950,00:31:43.680
"expanding each by
using the chain",00:31:43.680,00:31:45.940
rule until we got to the end.,00:31:45.940,00:31:48.490
"So there are some
expressions that provide",00:31:48.490,00:31:50.330
those partial derivatives.,00:31:50.330,00:31:51.910
"But now, if you'll
forgive me, it",00:31:56.660,00:32:03.030
"was convenient to write
them out that way.",00:32:03.030,00:32:05.370
"That matched the
intuition in my head.",00:32:05.370,00:32:07.050
"But I'm just going
to turn them around.",00:32:07.050,00:32:08.674
It's just a product.,00:32:11.080,00:32:12.960
"I'm just going to
turn them around.",00:32:12.960,00:32:14.790
"So partial p2, partial
w2, times partial of z,",00:32:14.790,00:32:22.610
"partial p2, times the
partial of p with respect",00:32:22.610,00:32:28.360
to z-- same thing.,00:32:28.360,00:32:30.040
"And now, this one.",00:32:30.040,00:32:31.860
"Keep me on track, because
if there's a mutation here,",00:32:31.860,00:32:34.190
it will be fatal.,00:32:34.190,00:32:35.740
"Partial of p1-- partial
of w1, partial of y,",00:32:35.740,00:32:41.860
"partial p1, partial of p2,
partial of y, partial of z.",00:32:41.860,00:32:52.180
"There's a partial of p2,
partial of a performance",00:32:52.180,00:32:56.920
function with respect to z.,00:32:56.920,00:32:58.142
"Now, all we have to do is figure
out what those partials are.",00:33:01.380,00:33:04.740
"And we have solved
this simple neural net.",00:33:04.740,00:33:08.709
So it's going to be easy.,00:33:08.709,00:33:09.750
Where is my board space?,00:33:14.530,00:33:15.880
"Let's see, partial of p2
with respect to-- what?",00:33:15.880,00:33:22.360
That's the product.,00:33:22.360,00:33:23.220
"The partial of z-- the
performance function",00:33:23.220,00:33:25.740
with respect to z.,00:33:25.740,00:33:27.130
"Oh, now I can see why I
wrote it down this way.",00:33:27.130,00:33:30.201
Let's see.,00:33:30.201,00:33:30.700
It's going to be d minus e.,00:33:30.700,00:33:33.699
We can do that one in our head.,00:33:33.699,00:33:34.990
"What about the partial
of p2 with respect to w2.",00:33:41.110,00:33:43.634
"Well, p2 is equal to y
times w2, so that's easy.",00:33:46.520,00:33:50.250
That's just y.,00:33:50.250,00:33:51.050
"Now, all we have to do
is figure out the partial",00:33:57.830,00:34:00.110
of z with respect to p2.,00:34:00.110,00:34:02.110
"Oh, crap, it's going
through this threshold box.",00:34:02.110,00:34:07.020
"So I don't know exactly what
that partial derivative is.",00:34:07.020,00:34:11.070
"So we'll have to
figure that out, right?",00:34:11.070,00:34:13.780
"Because the function relating
them is this guy here.",00:34:13.780,00:34:18.414
"And so we have to figure out
the partial of that with respect",00:34:18.414,00:34:20.955
to alpha.,00:34:20.955,00:34:24.030
"All right, so we got to do it.",00:34:24.030,00:34:26.120
There's no way around it.,00:34:26.120,00:34:28.330
So we have to destroy something.,00:34:28.330,00:34:32.620
"OK, we're going to
destroy our neuron.",00:34:32.620,00:34:36.440
"So the function
we're dealing with",00:34:49.989,00:34:52.060
"is, we'll call it
beta, equal to 1 over 1",00:34:52.060,00:34:55.620
plus e to the minus alpha.,00:34:55.620,00:35:00.100
"And what we want
is the derivative",00:35:00.100,00:35:02.711
with respect to alpha of beta.,00:35:02.711,00:35:07.050
"And that's equal to d by
d alpha of-- you know,",00:35:07.050,00:35:13.080
"I can never remember
those quotient formulas.",00:35:13.080,00:35:16.530
"So I am going to rewrite
it a little different way.",00:35:16.530,00:35:19.260
"I am going to write it as 1
minus e to the minus alpha",00:35:19.260,00:35:23.518
"to the minus 1, because I
can't remember the formula",00:35:23.518,00:35:28.340
for differentiating a quotient.,00:35:28.340,00:35:31.490
"OK, so let's differentiate it.",00:35:31.490,00:35:33.030
"So that's equal to 1 minus e to
the minus alpha to the minus 2.",00:35:33.030,00:35:45.572
"And we got that minus comes
out of that part of it.",00:35:48.380,00:35:51.140
"Then we got to differentiate
the inside of that expression.",00:35:51.140,00:35:56.660
"And when we differentiate the
inside of that expression,",00:35:56.660,00:35:59.410
we get e to the minus alpha.,00:35:59.410,00:36:01.156
AUDIENCE: Dr. Winston--,00:36:01.156,00:36:02.142
PATRICK WINSTON: Yeah?,00:36:02.142,00:36:03.130
AUDIENCE: That should be 1 plus.,00:36:03.130,00:36:05.267
"PATRICK WINSTON: Oh,
sorry, thank you.",00:36:05.267,00:36:06.850
"That was one of those fatal
mistakes you just prevented.",00:36:06.850,00:36:09.183
So that's 1 plus.,00:36:09.183,00:36:10.680
That's 1 plus here too.,00:36:10.680,00:36:12.400
"OK, so we've
differentiated that.",00:36:12.400,00:36:15.590
"We've turned that
into a minus 2.",00:36:15.590,00:36:17.170
"We brought the
minus sign outside.",00:36:17.170,00:36:18.890
"Then we're differentiating
the inside.",00:36:18.890,00:36:21.320
"The derivative and the
exponential is an exponential.",00:36:21.320,00:36:23.640
"Then we got to
differentiate that guy.",00:36:23.640,00:36:25.979
"And that just helps us
get rid of the minus",00:36:25.979,00:36:27.770
sign we introduced.,00:36:27.770,00:36:29.690
So that's the derivative.,00:36:29.690,00:36:32.380
"I'm not sure how much
that helps except that I'm",00:36:32.380,00:36:36.640
"going to perform a parlor
trick here and rewrite",00:36:36.640,00:36:40.040
that expression thusly.,00:36:40.040,00:36:43.510
"We want to say
that's going to be",00:36:43.510,00:36:47.170
"e to the minus alpha over
1 plus e to the minus",00:36:47.170,00:36:53.988
"alpha times 1 over 1 plus
e to the minus alpha.",00:36:53.988,00:37:01.415
That OK?,00:37:01.415,00:37:03.919
"I've got a lot of
nodding heads here.",00:37:03.919,00:37:05.460
So I think I'm on safe ground.,00:37:05.460,00:37:08.465
"But now, I'm going to
perform another parlor trick.",00:37:08.465,00:37:10.590
"I am going to add 1, which
means I also have to subtract 1.",00:37:13.700,00:37:19.770
All right?,00:37:24.270,00:37:24.840
That's legitimate isn't it?,00:37:24.840,00:37:27.520
"So now, I can rewrite
this as 1 plus e",00:37:27.520,00:37:32.540
"to the minus alpha over 1
plus e to the minus alpha",00:37:32.540,00:37:38.820
"minus 1 over 1 plus e to the
minus alpha times 1 over 1 plus",00:37:38.820,00:37:48.085
e to the minus alpha.,00:37:48.085,00:37:51.660
"Any high school
kid could do that.",00:37:51.660,00:37:53.200
I think I'm on safe ground.,00:37:53.200,00:37:55.580
"Oh, wait, this is beta.",00:37:55.580,00:38:02.150
This is beta.,00:38:02.150,00:38:04.464
AUDIENCE: That's the wrong side.,00:38:04.464,00:38:05.940
"PATRICK WINSTON: Oh,
sorry, wrong side.",00:38:05.940,00:38:08.440
"Better make this
beta and this 1.",00:38:08.440,00:38:11.320
Any high school kid could do it.,00:38:11.320,00:38:13.964
"OK, so what we've
got then is that this",00:38:13.964,00:38:16.490
"is equal to 1 minus
beta times beta.",00:38:16.490,00:38:22.310
That's the derivative.,00:38:22.310,00:38:23.590
"And that's weird
because the derivative",00:38:23.590,00:38:25.950
"of the output with
respect to the input",00:38:25.950,00:38:27.960
"is given exclusively
in terms of the output.",00:38:27.960,00:38:31.520
It's strange.,00:38:31.520,00:38:33.020
It doesn't really matter.,00:38:33.020,00:38:34.350
But it's a curiosity.,00:38:34.350,00:38:36.240
"And what we get out of this is
that partial derivative there--",00:38:36.240,00:38:39.560
"that's equal to well,
the output is p2.",00:38:39.560,00:38:47.680
"No, the output is z.",00:38:47.680,00:38:48.680
So it's z time 1 minus e.,00:38:48.680,00:38:52.340
"So whenever we see
the derivative of one",00:38:52.340,00:38:54.380
"of these sigmoids with
respect to its input,",00:38:54.380,00:38:57.300
"we can just write the output
times one minus alpha,",00:38:57.300,00:38:59.500
and we've got it.,00:38:59.500,00:39:00.230
"So that's why it's
mathematically convenient.",00:39:00.230,00:39:02.290
"It's mathematically
convenient because when",00:39:02.290,00:39:04.081
"we do this differentiation, we
get a very simple expression",00:39:04.081,00:39:08.640
in terms of the output.,00:39:08.640,00:39:10.597
We get a very simple expression.,00:39:10.597,00:39:11.930
That's all we really need.,00:39:11.930,00:39:13.015
"So would you like to
see a demonstration?",00:39:16.050,00:39:20.360
"It's a demonstration of
the world's smallest neural",00:39:20.360,00:39:22.800
net in action.,00:39:22.800,00:39:23.494
Where is neural nets?,00:39:31.080,00:39:32.430
Here we go.,00:39:32.430,00:39:32.930
So there's our neural net.,00:39:37.707,00:39:38.790
"And what we're
going to do is we're",00:39:38.790,00:39:40.248
"going to train it to
do absolutely nothing.",00:39:40.248,00:39:42.100
"What we're going to do is
train it to make the output",00:39:42.100,00:39:44.308
the same as the input.,00:39:44.308,00:39:47.460
"Not what I'd call a fantastic
leap of intelligence.",00:39:47.460,00:39:49.660
But let's see what happens.,00:39:49.660,00:39:50.785
Wow!,00:39:58.930,00:39:59.430
Nothing's happening.,00:39:59.430,00:40:00.263
"Well, it finally
got to the point",00:40:07.050,00:40:09.120
"where the maximum error,
not the performance,",00:40:09.120,00:40:12.530
"but the maximum error
went below a threshold",00:40:12.530,00:40:14.590
"that I had previously
determined.",00:40:14.590,00:40:16.600
"So if you look at the
input here and compare that",00:40:16.600,00:40:18.810
"with the desired output
on the far right,",00:40:18.810,00:40:21.200
"you see it produces an output,
which compared with the desired",00:40:21.200,00:40:24.060
"output, is pretty close.",00:40:24.060,00:40:26.010
"So we can test the
other way like so.",00:40:26.010,00:40:29.070
"And we can see that
the desired output",00:40:29.070,00:40:30.950
"is pretty close to the actual
output in that case too.",00:40:30.950,00:40:34.300
"And it took 694 iterations
to get that done.",00:40:34.300,00:40:37.130
Let's try it again.,00:40:37.130,00:40:37.952
"To 823-- of course, this is all
a consequence of just starting",00:40:56.090,00:40:59.190
off with random weights.,00:40:59.190,00:41:01.265
"By the way, if you started with
all the weights being the same,",00:41:01.265,00:41:03.890
what would happen?,00:41:03.890,00:41:04.780
"Nothing because it would
always stay the same.",00:41:04.780,00:41:07.495
"So you've got to put
some randomization",00:41:07.495,00:41:09.120
in in the beginning.,00:41:09.120,00:41:11.580
So it took a long time.,00:41:11.580,00:41:12.670
"Maybe the problem is our
rate constant is too small.",00:41:12.670,00:41:15.450
"So let's crank up the
rate counts a little bit",00:41:15.450,00:41:18.106
and see what happens.,00:41:18.106,00:41:18.980
That was pretty fast.,00:41:22.430,00:41:23.730
"Let's see if it was a
consequence of random chance.",00:41:23.730,00:41:26.510
Run.,00:41:29.510,00:41:30.920
"No, it's pretty fast there--
57 iterations-- third try-- 67.",00:41:30.920,00:41:38.110
"So it looks like at my initial
rate constant was too small.",00:41:38.110,00:41:42.020
"So if 0.5 was not
as good as 5.0,",00:41:42.020,00:41:45.240
"why don't we crank it up
to 50 and see what happens.",00:41:45.240,00:41:47.698
"Oh, in this case, 124--
let's try it again.",00:41:51.830,00:41:54.702
"Ah, in this case 117-- so
it's actually gotten worse.",00:41:58.470,00:42:02.546
"And not only has
it gotten worse.",00:42:02.546,00:42:03.920
"You'll see there's a little a
bit of instability showing up",00:42:03.920,00:42:09.270
"as it courses along its
way toward a solution.",00:42:09.270,00:42:12.510
"So what it looks like is that
if you've got a rate constant",00:42:12.510,00:42:15.200
"that's too small,
it takes forever.",00:42:15.200,00:42:17.040
"If you've get a rate
constant that's too big,",00:42:17.040,00:42:19.020
"it can of jump too far, as in
my diagram which is somewhere",00:42:19.020,00:42:25.310
"underneath the board, you can
go all the way across the hill",00:42:25.310,00:42:29.207
and get to the other side.,00:42:29.207,00:42:30.290
"So you have to be careful
about the rate constant.",00:42:30.290,00:42:31.900
"So what you really
want to do is you",00:42:31.900,00:42:33.399
"want your rate constant
to vary with what",00:42:33.399,00:42:36.010
"is happening as you progress
toward an optimal performance.",00:42:36.010,00:42:43.920
"So if your performance is going
down when you make the jump,",00:42:43.920,00:42:46.420
"you know you've got a rate
constant that's too big.",00:42:46.420,00:42:48.572
"If your performance is going
up when you make a jump,",00:42:48.572,00:42:50.780
"maybe you want to
increase-- bump it up",00:42:50.780,00:42:52.404
"a little bit until it
doesn't look so good.",00:42:52.404,00:42:57.460
So is that all there is to it?,00:42:57.460,00:42:58.960
"Well, not quite, because
this is the world's simplest",00:42:58.960,00:43:03.010
neural net.,00:43:03.010,00:43:04.002
"And maybe we ought to
look at the world's",00:43:04.002,00:43:05.710
second simplest neural net.,00:43:05.710,00:43:08.450
"Now, let's call this--
well, let's call this x.",00:43:08.450,00:43:13.982
"What we're going to do is we're
going to have a second input.",00:43:13.982,00:43:18.410
And I don't know.,00:43:18.410,00:43:19.850
Maybe this is screwy.,00:43:19.850,00:43:21.096
"I'm just going to
use color coding here",00:43:21.096,00:43:22.720
"to differentiate between
the two inputs and the stuff",00:43:22.720,00:43:26.734
they go through.,00:43:26.734,00:43:27.400
"Maybe I'll call this z2 and
this z1 and this x1 and x2.",00:43:34.010,00:43:39.666
"Now, if I do that-- if I've
got two inputs and two outputs,",00:43:42.300,00:43:45.140
"then my performance
function is going",00:43:45.140,00:43:47.760
"to have two numbers in it-- the
two desired values and the two",00:43:47.760,00:43:51.800
actual values.,00:43:51.800,00:43:53.560
"And I'm going to
have two inputs.",00:43:53.560,00:43:55.350
But it's the same stuff.,00:43:55.350,00:43:57.680
"I just repeat what I did in
white, only I make it orange.",00:43:57.680,00:44:01.031
"Oh, but what happens if--
what happens if I do this?",00:44:07.440,00:44:12.654
"Say put little cross
connections in there.",00:44:28.850,00:44:31.750
"So these two streams
are going to interact.",00:44:31.750,00:44:35.030
"And then there might
be some-- this y can",00:44:35.030,00:44:37.340
"go into another multiplier
here and go into a summer here.",00:44:37.340,00:44:43.290
"And likewise, this
y can go up here",00:44:43.290,00:44:46.220
and into a multiplier like so.,00:44:46.220,00:44:50.920
"And there are weights all
over the place like so.",00:44:50.920,00:45:01.330
This guy goes up in here.,00:45:01.330,00:45:05.070
And now what happens?,00:45:05.070,00:45:06.430
"Now, we've got a
disaster on our hands,",00:45:06.430,00:45:08.800
"because there are all kinds
of paths through this network.",00:45:08.800,00:45:11.900
"And you can imagine that if this
was not just two neurons deep,",00:45:11.900,00:45:16.260
"but three neurons
deep, what I would find",00:45:16.260,00:45:19.170
"is expressions that
look like that.",00:45:19.170,00:45:22.300
"But you could go this way,
and then down through, and out",00:45:22.300,00:45:25.890
here.,00:45:25.890,00:45:27.470
"Or you could go this way and
then back up through here.",00:45:27.470,00:45:33.150
"So it looks like there is an
exponentially growing number",00:45:33.150,00:45:37.470
of paths through that network.,00:45:37.470,00:45:39.910
"And so we're back to
an exponential blowup.",00:45:39.910,00:45:41.820
And it won't work.,00:45:41.820,00:45:42.570
"Yeah, it won't
work except that we",00:45:50.890,00:45:53.396
"need to let the math
sing to us a little bit.",00:45:53.396,00:45:55.270
"And we need to look
at the picture.",00:45:55.270,00:45:57.670
"And the reason I turned
this guy around was actually",00:45:57.670,00:46:01.190
"because from a point of view
of letting the math sing to us,",00:46:01.190,00:46:06.580
"this piece here is the
same as this piece here.",00:46:06.580,00:46:11.500
"So part of what we
needed to do to calculate",00:46:11.500,00:46:13.570
"the partial derivative
with respect to w1",00:46:13.570,00:46:16.064
"has already been done
when we calculated",00:46:16.064,00:46:17.730
"the partial derivative
with respect to w2.",00:46:17.730,00:46:22.550
"And not only that,
if we calculated",00:46:22.550,00:46:26.970
"the partial wit respect
to these green w's",00:46:26.970,00:46:29.200
"at both levels, what
we would discover",00:46:29.200,00:46:32.460
"is that sort of repetition
occurs over and over again.",00:46:32.460,00:46:37.840
"And now, I'm going to try
to give you an intuitive",00:46:37.840,00:46:41.330
"idea of what's going on here
rather than just write down",00:46:41.330,00:46:44.070
the math and salute it.,00:46:44.070,00:46:46.720
"And here's a way to think
about it from an intuitive",00:46:46.720,00:46:49.980
point of view.,00:46:49.980,00:46:50.748
"Whatever happens to this
performance function",00:46:53.740,00:46:56.960
"that's back of these p's
here, the stuff over there can",00:46:56.960,00:47:04.440
"influence p only
by going through,",00:47:04.440,00:47:07.150
"and influence performance
only going through this column",00:47:07.150,00:47:09.830
of p's.,00:47:09.830,00:47:12.460
"And there's a fixed
number of those.",00:47:12.460,00:47:13.960
"So it depends on the width,
not the depth of the network.",00:47:13.960,00:47:16.335
"So the influence of that
stuff back there on p",00:47:19.350,00:47:26.030
"is going to end up going
through these guys.",00:47:26.030,00:47:28.620
"And it's going to end
up being so that we're",00:47:28.620,00:47:34.840
"going to discover that a lot of
what we need to compute in one",00:47:34.840,00:47:38.050
"column has already been computed
in the column on the right.",00:47:38.050,00:47:43.150
"So it isn't going to
explode exponentially,",00:47:43.150,00:47:47.430
"because the influence-- let
me say it one more time.",00:47:47.430,00:47:50.643
"The influences of changes of
changes in p on the performance",00:47:54.120,00:47:58.440
"is all we care about when
we come back to this part",00:47:58.440,00:48:01.370
"of the network, because
this stuff cannot influence",00:48:01.370,00:48:05.450
"the performance except by going
through this column of p's.",00:48:05.450,00:48:09.859
"So it's not going to
blow up exponentially.",00:48:09.859,00:48:11.650
"We're going to be able to
reuse a lot of the computation.",00:48:11.650,00:48:14.560
So it's the reuse principle.,00:48:14.560,00:48:17.040
"Have we ever seen the reuse
principle at work before.",00:48:17.040,00:48:21.350
Not exactly.,00:48:21.350,00:48:22.109
"But you remember
that little business",00:48:22.109,00:48:23.650
about the extended list?,00:48:23.650,00:48:25.770
"We know that we've
seen-- we know",00:48:25.770,00:48:31.184
we've seen something before.,00:48:31.184,00:48:32.350
So we can stop computing.,00:48:32.350,00:48:34.450
It's like that.,00:48:34.450,00:48:35.810
"We're going to be able
to reuse the computation.",00:48:35.810,00:48:37.870
"We've already done it to
prevent an exponential blowup.",00:48:37.870,00:48:40.721
"By the way, for those of
you who know about fast",00:48:40.721,00:48:42.720
"Fourier transform-- same
kind of idea-- reuse",00:48:42.720,00:48:45.880
of partial results.,00:48:45.880,00:48:48.570
"So in the end, what can
we say about this stuff?",00:48:48.570,00:48:52.590
"In the end, what we can say
is that it's linear in depth.",00:48:52.590,00:49:02.725
"That is to say if we
increase the number of layers",00:49:05.710,00:49:08.680
"to so-called depth,
then we're going",00:49:08.680,00:49:10.720
"to increase the
amount of computation",00:49:10.720,00:49:12.430
"necessary in a linear way,
because the computation we",00:49:12.430,00:49:15.990
"need in any column
is going to be fixed.",00:49:15.990,00:49:20.420
"What about how it goes
with respect to the width?",00:49:20.420,00:49:26.900
"Well, with respect to
the width, any neuron",00:49:31.070,00:49:33.500
"here can be connected to
any neuron in the next row.",00:49:33.500,00:49:36.902
"So the amount of work
we're going to have to do",00:49:36.902,00:49:38.860
"will be proportional to
the number of connections.",00:49:38.860,00:49:41.550
"So with respect to width,
it's going to be w-squared.",00:49:41.550,00:49:47.210
"But the fact is that in the end,
this stuff is readily computed.",00:49:47.210,00:49:52.260
"And this, phenomenally enough,
was overlooked for 25 years.",00:49:52.260,00:49:58.120
So what is it in the end?,00:49:58.120,00:50:00.740
"In the end, it's an
extremely simple idea.",00:50:00.740,00:50:02.490
All great ideas are simple.,00:50:02.490,00:50:03.670
"How come there
aren't more of them?",00:50:03.670,00:50:05.150
"Well, because frequently,
that simplicity",00:50:05.150,00:50:08.272
"involves finding
a couple of tricks",00:50:08.272,00:50:09.730
"and making a couple
of observations.",00:50:09.730,00:50:12.150
"So usually, we humans
are hardly ever",00:50:12.150,00:50:14.950
"go beyond one trick
or one observation.",00:50:14.950,00:50:16.944
"But if you cascade
a few together,",00:50:16.944,00:50:18.360
"sometimes something
miraculous falls out",00:50:18.360,00:50:20.270
"that looks in retrospect
extremely simple.",00:50:20.270,00:50:23.250
"So that's why we got the
reuse principle at work--",00:50:23.250,00:50:25.856
and our reuse computation.,00:50:25.856,00:50:27.510
"In this case, the
miracle was a consequence",00:50:27.510,00:50:29.580
"of two tricks plus
an observation.",00:50:29.580,00:50:31.590
"And the overall idea
is all great ideas",00:50:31.590,00:50:33.960
"are simple and easy to
overlook for a quarter century.",00:50:33.960,00:50:37.500

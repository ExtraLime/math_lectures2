text,start,stop
"NARRATOR: The
following content is",00:00:00.000,00:00:01.470
"provided by MIT OpenCourseWare
under a Creative Commons",00:00:01.470,00:00:04.690
license.,00:00:04.690,00:00:06.090
"Additional information
about our license",00:00:06.090,00:00:08.230
"and MIT OpenCourseWare
in general",00:00:08.230,00:00:10.460
is available at ocw.mit.edu.,00:00:10.460,00:00:14.370
"PROFESSOR: So I'm thinking
of large sparse matrices.",00:00:14.370,00:00:21.540
"So the point about A -- the
kind of matrices that we've been",00:00:24.600,00:00:29.280
"writing down, maybe
two-dimensional,",00:00:29.280,00:00:32.010
"maybe three-dimensional
difference matrices.",00:00:32.010,00:00:35.620
"So it might be five or seven
non-zeros on a typical row",00:00:35.620,00:00:42.570
"and that means that you don't
want to invert such a matrix.",00:00:42.570,00:00:47.500
These are big matrices.,00:00:47.500,00:00:49.470
"The order could
easily be a million.",00:00:49.470,00:00:51.640
"What you can do fast is
multiply A by a vector,",00:00:51.640,00:00:56.990
"because multiplying a
sparse matrix by a vector,",00:00:56.990,00:00:59.880
"you only have maybe five
non-zeros times the n --",00:00:59.880,00:01:05.720
"the vector of length n -- so say
5n operations, so that's fast.",00:01:05.720,00:01:11.240
"I'm thinking that we are at
a size where elimination,",00:01:16.230,00:01:21.840
"even with a good ordering,
is too expensive in time",00:01:21.840,00:01:27.960
or in storage.,00:01:27.960,00:01:29.760
"So I'm going from direct
elimination methods",00:01:29.760,00:01:33.640
to iterative methods.,00:01:33.640,00:01:36.800
"Iterative means that
I never actually get",00:01:36.800,00:01:39.640
"to the perfect answer x, but I
get close and what I want to do",00:01:39.640,00:01:46.390
is get close quickly.,00:01:46.390,00:01:48.650
"So a lot of thought
has gone into creating",00:01:48.650,00:01:53.170
iterative methods.,00:01:53.170,00:01:55.630
"I'll write down
one key word here.",00:01:55.630,00:01:59.710
"Part of the decision is to
choose a good preconditioner.",00:01:59.710,00:02:06.130
"I'll call that matrix P. So it's
a matrix that's supposed to be",00:02:06.130,00:02:12.920
"-- that we have some freedom to
choose and it very much speeds",00:02:12.920,00:02:17.060
"up -- so the decisions are,
what preconditioner to choose?",00:02:17.060,00:02:21.640
"A preconditioner that's
somehow like the matrix A,",00:02:21.640,00:02:25.990
but hopefully a lot simpler.,00:02:25.990,00:02:28.640
"There are a bunch
of iterative methods",00:02:36.540,00:02:39.890
"and that's what today's
lecture is about.",00:02:39.890,00:02:42.770
"Multigrid is an idea that
just keeps developing.",00:02:42.770,00:02:49.040
"It really is producing answers
to very large problems very",00:02:49.040,00:02:54.630
"fast, so that will be
the following lectures.",00:02:54.630,00:02:59.590
"And then come these methods --
you may not be familiar with",00:02:59.590,00:03:04.560
that word Krylov.,00:03:04.560,00:03:05.710
"Let me mention that in this
family, the best known method",00:03:05.710,00:03:10.360
"it is called
conjugate gradients.",00:03:10.360,00:03:12.840
The conjugate gradient method.,00:03:12.840,00:03:14.750
"So I'll just write that
down, conjugate gradient.",00:03:14.750,00:03:19.120
So that will come next week.,00:03:19.120,00:03:20.870
"That's a method
that's a giant success",00:03:24.620,00:03:27.570
"for symmetric, positive
definite problem.",00:03:27.570,00:03:32.040
"So I don't assume
in general that A is",00:03:32.040,00:03:35.040
symmetric or positive definite.,00:03:35.040,00:03:37.420
"Often it is if it comes
from a finite difference",00:03:37.420,00:03:42.090
or finite element approximation.,00:03:42.090,00:03:47.250
"If it is, then
conjugate gradients",00:03:47.250,00:03:49.980
is highly recommended.,00:03:49.980,00:03:53.690
"I'll start with the
general picture.",00:03:53.690,00:03:56.910
"Let me put the general
picture of what",00:03:56.910,00:04:00.160
an iteration looks like.,00:04:00.160,00:04:02.250
"So an iteration
computes the new x.",00:04:02.250,00:04:06.170
"Let me call it x-new or
x_(k+1) from the known x.",00:04:06.170,00:04:17.380
"So we start with x_0,
any x_0, In principle.",00:04:17.380,00:04:23.570
"It's not too important
for x_0 to be close.",00:04:23.570,00:04:27.570
"In linear methods, you
could start even at 0.",00:04:27.570,00:04:33.110
"In nonlinear methods,
Newton methods,",00:04:33.110,00:04:35.940
"you do want to be close,
and a lot of attention",00:04:35.940,00:04:38.910
goes into a good start.,00:04:38.910,00:04:40.590
"Here the start
isn't too important;",00:04:40.590,00:04:42.670
it's the steps that you taking.,00:04:42.670,00:04:45.300
"Let me -- so I guess
I'm going to --",00:04:45.300,00:04:53.300
I'm trying to solve A*x equal b.,00:04:53.300,00:04:55.150
"I'm going to split
that equation into --",00:04:57.770,00:05:05.910
"I'm just going to write
the same equation this way.",00:05:05.910,00:05:08.140
"I could write it as x
equals I minus A x plus b.",00:05:08.140,00:05:18.195
"That would be the
same equation, right?",00:05:18.195,00:05:19.820
Because x and x cancel.,00:05:19.820,00:05:22.500
"A*x comes over here
and I have A*x equal b.",00:05:22.500,00:05:25.200
"I've split up the equation and
now actually, let me bring --",00:05:29.640,00:05:34.640
"so that's without
preconditioner.",00:05:34.640,00:05:36.860
"You don't see a matrix P. That's
just an ordinary iteration,",00:05:36.860,00:05:44.190
no preconditioner.,00:05:44.190,00:05:46.440
"The preconditioner will come --
I'll have P and it'll go here",00:05:46.440,00:05:52.230
too.,00:05:52.230,00:05:54.310
"Then, of course, I have
to change that to a P.",00:05:54.310,00:06:00.080
"So that's still the
same equation, right?",00:06:00.080,00:06:02.380
"P*x is on both sides, cancels
-- and I have A*x equal b.",00:06:02.380,00:06:07.080
"So that's the same equation,
but it suggests the iteration",00:06:07.080,00:06:12.590
that I want to analyze.,00:06:12.590,00:06:14.570
"On the right-hand side is
the known approximation.",00:06:17.570,00:06:28.900
"On the left side is the
next approximation, x_(k+1).",00:06:28.900,00:06:33.230
"I multiply by P so I -- the
idea is, P should be close to A.",00:06:36.100,00:06:43.800
"Of course, if it was
exactly A -- if P equaled A,",00:06:43.800,00:06:48.710
"then this wouldn't be here,
and I would just be solving,",00:06:48.710,00:06:52.570
"in one step, A*x equal b.",00:06:52.570,00:06:56.730
So P equal A is one extreme.,00:06:56.730,00:07:01.490
"I mean, it's totally,
perfectly preconditioned,",00:07:01.490,00:07:05.210
"but the point is
that if P equals A,",00:07:05.210,00:07:09.520
"we don't want to
solve that system,",00:07:09.520,00:07:13.320
"we're trying to escape
from solving that system.",00:07:13.320,00:07:15.790
"But we're willing to solve a
related system that is simpler.",00:07:15.790,00:07:23.450
Why simpler?,00:07:23.450,00:07:25.190
"Here are some possible
preconditioners.",00:07:25.190,00:07:27.960
P equals the diagonal.,00:07:27.960,00:07:32.060
"Just take the diagonal
of A and that choice",00:07:32.060,00:07:37.550
is named after Jacobi.,00:07:37.550,00:07:41.000
That's Jacobi's method.,00:07:41.000,00:07:43.670
"Actually, it's
already a good idea,",00:07:43.670,00:07:47.770
"because the effect of that
is somehow to properly scale",00:07:47.770,00:07:54.550
the equations.,00:07:54.550,00:07:56.400
"When I had the identity
here, I had no idea",00:07:56.400,00:07:59.410
"whether the identity and A
are in the right scaling.",00:07:59.410,00:08:04.180
"A may be divided by delta
x squared or something.",00:08:04.180,00:08:07.810
"It could be totally
different units, but P --",00:08:07.810,00:08:13.060
"if I take the diagonal of A,
at least they're comparable.",00:08:13.060,00:08:16.510
"Then you see what -- we'll
study Jacobi's method.",00:08:16.510,00:08:22.190
So that's a very simple choice.,00:08:22.190,00:08:24.600
"Takes no more work than
the identity, really.",00:08:24.600,00:08:28.650
"Second choice would be -- so
that would be P for Jacobi.",00:08:28.650,00:08:32.230
"A second choice, that you
would think of pretty fast,",00:08:32.230,00:08:36.280
"is the diagonal and the
lower triangular part.",00:08:36.280,00:08:42.460
"So it's the -- I'll
say the triangular --",00:08:42.460,00:08:46.760
"I'm going to say
lower triangular,",00:08:46.760,00:08:48.980
"triangular part of A,
including the diagonal.",00:08:48.980,00:08:54.210
"This is associated with
Gauss's great name and Seidel.",00:08:58.100,00:09:05.390
"So that's the
Gauss-Seidel choice.",00:09:05.390,00:09:07.520
So why triangular?,00:09:11.470,00:09:15.300
"We'll analyze the
effect of this choice.",00:09:15.300,00:09:18.980
"If P is triangular,
then we can solve --",00:09:18.980,00:09:27.600
"I'm never going to write
out an inverse matrix.",00:09:27.600,00:09:29.700
"That's understood, but
I can compute the new x",00:09:29.700,00:09:35.340
"from the old x just
by back substitution.",00:09:35.340,00:09:41.380
"I find the first component
of x_(k+1), then the second,",00:09:41.380,00:09:47.100
then the third.,00:09:47.100,00:09:48.840
"It's just because the first
equation -- if P is triangular,",00:09:48.840,00:09:56.750
"the first equation will
only have one term,",00:09:56.750,00:10:01.320
"one new term on the left side
and everything else will be",00:10:01.320,00:10:03.860
over there.,00:10:03.860,00:10:05.250
"The second equation will
have a diagonal and something",00:10:05.250,00:10:09.850
"that uses the number I just
found for the first component",00:10:09.850,00:10:17.580
and onward.,00:10:17.580,00:10:18.650
"So in other words,
triangular matrices are good.",00:10:18.650,00:10:23.320
"Then people thought about
combinations of these.",00:10:23.320,00:10:27.270
"A big lot of activity
went in something called",00:10:30.400,00:10:34.440
"overrelaxation,
where you did a --",00:10:34.440,00:10:39.520
"you had a weighting of these
and it turned out to be if you",00:10:39.520,00:10:43.450
"adjusted the weight, you
could speed up the method,",00:10:43.450,00:10:49.900
"sometimes called SOR--
successive overrelaxation.",00:10:49.900,00:10:55.230
"So when I was in graduate
school, that was a very --",00:10:55.230,00:11:00.890
"that was sort of the beginning
of progress beyond Jacobi",00:11:00.890,00:11:05.240
and Gauss-Seidel.,00:11:05.240,00:11:07.740
"Then after that came
a very natural --",00:11:07.740,00:11:12.960
"ILU is the next one that I want
-- the I stands for incomplete,",00:11:12.960,00:11:20.260
incomplete LU.,00:11:20.260,00:11:21.290
So what are L and U?,00:11:24.400,00:11:27.520
"That's the letters I always
use as a signal for the lower",00:11:27.520,00:11:32.080
"triangular and upper triangular
factors of A, the ones",00:11:32.080,00:11:38.600
"that exact elimination
would produce.",00:11:38.600,00:11:41.310
"But we don't want to
do exact elimination.",00:11:41.310,00:11:43.410
Why?,00:11:43.410,00:11:44.450
Because non-zeros fill in.,00:11:44.450,00:11:48.040
"Non-zeros appear in those
spaces where A itself is 0.",00:11:48.040,00:11:55.230
"So the factors are much --
have many more non-zeros than",00:11:55.230,00:12:01.630
"the original A.
They're not as sparse.",00:12:01.630,00:12:04.700
"So the idea of incomplete LU is
-- and I'll come back to it --",00:12:04.700,00:12:10.470
"the idea of incomplete
LU, you might say,",00:12:10.470,00:12:12.630
"should have occurred
to people earlier --",00:12:12.630,00:12:16.400
"only keep the non-zeros that
are non-zeros in the original A.",00:12:16.400,00:12:24.010
"Don't allow fill-in or allow
a certain amount of fill-in.",00:12:24.010,00:12:27.970
"You could have a tolerance and
you would include a non-zero",00:12:27.970,00:12:32.210
"if it was big enough, but the
many, many small non-zeros",00:12:32.210,00:12:36.350
"that just fill in and
make the elimination long,",00:12:36.350,00:12:43.060
you throw them out.,00:12:43.060,00:12:44.840
"So P is -- in this
case, P is L --",00:12:44.840,00:12:48.870
"is an approximate L
times an approximate U.",00:12:48.870,00:12:56.290
"So it's close to A, but
because we've refused some",00:12:56.290,00:13:05.150
"of the fill-in,
it's not exactly A.",00:13:05.150,00:13:08.930
"And you have a tolerance there
where if you set the tolerance",00:13:08.930,00:13:12.520
"high, then these are exact, but
you've got all that fill-in;",00:13:12.520,00:13:17.930
"if you set the tolerance to 0,
you've got the other extreme.",00:13:17.930,00:13:25.540
"So that would give you an
idea of preconditioners,",00:13:25.540,00:13:29.330
of reasonable choices.,00:13:29.330,00:13:32.660
"Now I'm going to think
about -- first, how do --",00:13:32.660,00:13:39.600
"how to decide whether the x's
approach the correct answer?",00:13:39.600,00:13:47.300
"I need an equation for the error
and because my equations are",00:13:47.300,00:13:50.980
"linear, I'm just
going to subtract --",00:13:50.980,00:13:55.290
"I'll subtract the upper
equation from the lower one",00:13:55.290,00:13:59.470
"and I'll call the -- so the
error e_k will be x minus",00:13:59.470,00:14:04.850
the true answer.,00:14:04.850,00:14:06.370
This is x exact.,00:14:06.370,00:14:08.140
"Minus x, my k-th approximation.",00:14:08.140,00:14:13.110
"So when I subtract
that from that,",00:14:13.110,00:14:15.820
"I have e_(k+1) and here
I have P minus A --",00:14:15.820,00:14:22.510
"when I subtract that
from that, I have e_k,",00:14:22.510,00:14:25.380
"and when I subtract
that from that, 0.",00:14:25.380,00:14:28.280
So very simple error equation.,00:14:28.280,00:14:32.410
"So this is an equation, this
would be the error equation.",00:14:32.410,00:14:43.340
"You see the b has
disappeared, because I'm only",00:14:43.340,00:14:48.620
looking at differences now.,00:14:48.620,00:14:51.910
"Let me write that
over on this board,",00:14:54.440,00:14:59.090
even slightly differently.,00:14:59.090,00:15:01.380
"Now I will multiply
through by P inverse.",00:15:01.380,00:15:05.360
"That really shows
it just so clearly.",00:15:05.360,00:15:08.670
"So the new error is, if
I multiply by P inverse,",00:15:08.670,00:15:15.600
"P inverse times P is I, and
I have P inverse A, e_k.",00:15:15.600,00:15:21.170
So what does that say?,00:15:29.500,00:15:31.300
"That says that at
every iteration,",00:15:31.300,00:15:35.130
"I multiply the error
by that matrix.",00:15:35.130,00:15:40.380
"Of course, if P
equals A, if I pick",00:15:40.380,00:15:44.410
"the perfect preconditioner, then
P inverse A is the identity,",00:15:44.410,00:15:50.920
"this is the zero matrix, and the
error in the first step is 0.",00:15:50.920,00:15:55.390
I'm solving exactly.,00:15:55.390,00:15:57.140
I don't plan to do that.,00:15:57.140,00:15:59.360
"I plan to have a P that's
close to A, so in some way,",00:15:59.360,00:16:05.820
"this should be close to I.
This whole matrix should",00:16:05.820,00:16:08.930
"be close to 0,
close in some sense.",00:16:08.930,00:16:14.130
"Let me give a name
for that matrix.",00:16:14.130,00:16:16.620
"Let's call that
matrix M. So that's",00:16:16.620,00:16:21.730
"the iteration matrix, which
we're never going to display.",00:16:21.730,00:16:28.970
"I mean, it would be
madness to display.",00:16:28.970,00:16:30.750
"We don't want to compute P
inverse explicitly or display",00:16:30.750,00:16:36.140
"it, but to understand it -- to
understand convergence or not",00:16:36.140,00:16:40.740
"convergence, it's M that
controls everything.",00:16:40.740,00:16:45.120
"So this is what I would
call pure iteration or maybe",00:16:45.120,00:16:51.730
"another word that's
used instead of pure",00:16:51.730,00:16:54.330
is stationary iteration.,00:16:54.330,00:16:56.630
What does stationary mean?,00:16:56.630,00:16:58.430
"It means that you do the
same thing all the time.",00:16:58.430,00:17:01.380
"At every step, you just use
the same equation, where --",00:17:01.380,00:17:10.810
these methods will be smarter.,00:17:10.810,00:17:15.690
They'll adapt.,00:17:15.690,00:17:16.530
They'll use more information.,00:17:16.530,00:17:17.960
"They'll converge faster, but
this is the simple one --",00:17:17.960,00:17:22.450
"and this one plays
a part in those.",00:17:22.450,00:17:26.830
"So what's the story
on convergence now?",00:17:26.830,00:17:32.310
What about that matrix M?,00:17:32.310,00:17:35.690
"If I take a starting
vector, e_0 --",00:17:35.690,00:17:38.940
"my initial error
could be anything.",00:17:38.940,00:17:41.430
I multiply by M to get e_1.,00:17:41.430,00:17:45.620
"Then I multiply by
M again to get e_2.",00:17:45.620,00:17:48.900
"Every step, I multiply
by M. So after k steps,",00:17:48.900,00:17:54.580
"I've multiplied k times
by M. I want to know --",00:17:54.580,00:18:01.240
"so the key question
is, does that go to 0?",00:18:01.240,00:18:04.690
"So the question, does it go to
0, and if it does, how fast?",00:18:04.690,00:18:08.900
"The two parts, does it
go to 0 and how fast,",00:18:14.460,00:18:18.600
"are pretty much controlled
by the same property of M. So",00:18:18.600,00:18:24.350
what's the answer?,00:18:24.350,00:18:25.220
"When does -- if I take -- I
don't know anything about e_0,",00:18:25.220,00:18:31.360
"so what property of M is going
to decide whether its powers",00:18:31.360,00:18:37.260
"get small, go to 0, so
that the error goes to 0?",00:18:37.260,00:18:41.750
That's convergence.,00:18:41.750,00:18:44.550
"Or, maybe blow up, or
maybe stay away from 0.",00:18:44.550,00:18:52.790
What controls it?,00:18:52.790,00:18:56.590
One word is the answer.,00:18:56.590,00:18:59.090
The eigenvalues.,00:18:59.090,00:19:00.240
"It's the eigenvalues of
M, and in particular, it's",00:19:00.240,00:19:03.760
the biggest one.,00:19:03.760,00:19:06.650
"Because if I have an eigenvalue
of M, as far as I know,",00:19:06.650,00:19:12.240
"e_naught could be
the eigenvector,",00:19:12.240,00:19:15.060
"and then every
step would multiply",00:19:15.060,00:19:17.610
by that eigenvalue lambda.,00:19:17.610,00:19:19.700
"So the condition is that
all eigenvalues of M",00:19:19.700,00:19:27.860
"have to be smaller than
1, in magnitude of course,",00:19:27.860,00:19:32.931
"because they could be negative,
they could be complex.",00:19:32.931,00:19:35.180
"So that's the total
condition, that's",00:19:38.890,00:19:41.540
"exactly the requirement
on M. How do",00:19:41.540,00:19:45.990
we say how fast they go to 0?,00:19:45.990,00:19:48.950
"How fast -- if all the
eigenvalues are below 1,",00:19:48.950,00:19:53.000
"then the slowest
convergence is going to come",00:19:53.000,00:19:59.470
"for the eigenvalue
that's the closest to 1.",00:19:59.470,00:20:02.140
"So really it will be -- and this
is called the spectral radius",00:20:02.140,00:20:11.720
"and it's usually denoted
by a Greek rho of M,",00:20:11.720,00:20:16.890
"which is the maximum
of these guys.",00:20:16.890,00:20:19.540
It's the max of the eigenvalues.,00:20:19.540,00:20:23.710
And that has to be below 1.,00:20:23.710,00:20:26.320
"So it's this
number, that number,",00:20:26.320,00:20:31.090
"because it's that number
which really says what I'm",00:20:31.090,00:20:35.450
multiplying by at every step.,00:20:35.450,00:20:37.710
"Because most likely, e_naught,
my initial unknown error,",00:20:37.710,00:20:42.640
"has some component in the
direction of this biggest",00:20:42.640,00:20:47.890
"eigenvalue, in the direction
of its eigenvector.",00:20:47.890,00:20:52.300
"Then that component will
multiply at every step",00:20:52.300,00:20:55.960
"by lambda, but that one will
be the biggest guy, rho.",00:20:55.960,00:21:00.330
So it's the maximum eigenvalue.,00:21:02.910,00:21:05.170
That's really what it comes to.,00:21:05.170,00:21:06.690
What is the maximum eigenvalue?,00:21:06.690,00:21:09.230
Let me take Jacobi.,00:21:09.230,00:21:11.900
"Can we figure out the maximum
eigenvalue for Jacobi?",00:21:11.900,00:21:17.440
"Of course, if I don't know
anything about the matrix,",00:21:17.440,00:21:22.364
"I certainly don't want
to compute eigenvalues.",00:21:22.364,00:21:24.280
I could run Jacobi's method.,00:21:26.840,00:21:29.450
"So again, Jacobi's
method, I'm taking",00:21:29.450,00:21:32.110
P to be the diagonal part.,00:21:32.110,00:21:33.840
"Let me make that explicit
and let me take the matrix",00:21:33.840,00:21:38.310
that we know the best.,00:21:38.310,00:21:40.180
"I'll call it K. So k --
or A, this is the A --",00:21:40.180,00:21:45.850
"is my friend with 2's on the
diagonal, minus 1's above.",00:21:45.850,00:21:51.820
"I apologize -- I don't
really apologize,",00:21:51.820,00:21:55.760
"but I'll pretend to apologize
-- for bringing this matrix",00:21:55.760,00:22:02.390
in so often.,00:22:02.390,00:22:03.350
Do we remember its eigenvalues?,00:22:06.889,00:22:08.180
"We computed them last semester,
but that's another world.",00:22:14.780,00:22:22.180
I'll say what they are.,00:22:22.180,00:22:24.900
"The eigenvalues of A
-- this isn't M now --",00:22:24.900,00:22:29.830
"the eigenvalues of this matrix
A -- I see 2's on the diagonal.",00:22:29.830,00:22:36.140
So that's a 2.,00:22:36.140,00:22:37.460
"That accounts for the diagonal
part, 2 times the identity.",00:22:37.460,00:22:41.240
"I have a minus and then the
part that comes from these 1's,",00:22:41.240,00:22:47.580
which are forward and back.,00:22:47.580,00:22:49.460
What would von Neumann say?,00:22:49.460,00:22:51.360
"What would von Neumann
do with that matrix?",00:22:51.360,00:22:53.580
"Of course, he would test it on
exponentials and he would get 2",00:22:53.580,00:23:03.590
"minus e to the i*k minus
e to the minus i*k.",00:23:03.590,00:23:11.050
"He would put the two
exponentials together",00:23:11.050,00:23:13.860
"into cosines and
that's the answer.",00:23:13.860,00:23:16.640
"It's 2 minus 2 cosine
of whatever angle theta.",00:23:16.640,00:23:21.500
Those are the eigenvalues.,00:23:21.500,00:23:23.590
"The j-th eigenvalue is --
the cosine is at some angle",00:23:23.590,00:23:31.250
theta_j.,00:23:31.250,00:23:33.020
"It's worth since -- maybe just
to take again a minute on this",00:23:33.020,00:23:37.190
matrix.,00:23:37.190,00:23:39.180
"So the eigenvalues
are between 0 and 4.",00:23:39.180,00:23:42.670
"The eigenvalues never
go negative, right?",00:23:42.670,00:23:44.420
"Because the cosine
can't be bigger than 1.",00:23:44.420,00:23:48.890
"Actually, for this matrix
the cosine doesn't reach 1.",00:23:48.890,00:23:52.910
"So the smallest eigenvalue
is 2 minus 2 times",00:23:52.910,00:23:57.340
that cosine close to 1.,00:23:57.340,00:23:59.860
"It's too close for
comfort, but it is below 1,",00:24:03.210,00:24:09.710
so the eigenvalues are positive.,00:24:09.710,00:24:12.790
And they're between 0 and 4.,00:24:12.790,00:24:17.560
"At the other extreme,
theta is near pi,",00:24:17.560,00:24:22.620
"the cosine is near minus 1,
and we have something near 4.",00:24:22.620,00:24:26.430
"So the eigenvalues of that --
if you look at that matrix,",00:24:26.430,00:24:28.890
you should see.,00:24:28.890,00:24:32.050
"Special matrix eigenvalues
in this interval, 0 to 4.",00:24:32.050,00:24:38.650
"The key point is,
how close to 0?",00:24:38.650,00:24:42.300
"Now what about M --
what about P first?",00:24:42.300,00:24:45.210
What's P?,00:24:45.210,00:24:48.195
"For Jacobi, so P for Jacobi
is the diagonal part,",00:24:48.195,00:24:56.290
which is exactly 2I.,00:24:56.290,00:24:59.680
"That's a very simple
diagonal part, very simple P.",00:24:59.680,00:25:07.620
"It produces the matrix
M. You remember,",00:25:07.620,00:25:11.080
"M is the identity matrix minus
P inverse A. That's beautiful.",00:25:11.080,00:25:22.790
So P is just 2I.,00:25:22.790,00:25:25.740
"So I'm just dividing A by
2, which puts a 1 there,",00:25:25.740,00:25:33.450
"and then subtracting
from the identity.",00:25:33.450,00:25:35.430
So it has 0's on the diagonal.,00:25:35.430,00:25:37.220
That's what we expect.,00:25:37.220,00:25:38.660
"It will be 0's on the diagonal
and off the diagonal, what",00:25:38.660,00:25:47.990
do I have?,00:25:47.990,00:25:48.490
"Minus -- with that minus,
P is 2, so it's 1/2.",00:25:48.490,00:25:53.150
"It's 1/2 off the diagonal
and otherwise, all 0's.",00:25:53.150,00:26:02.370
"That's the nice matrix that
we get for M in one dimension.",00:26:02.370,00:26:11.750
"I have to say, of
course, as you know,",00:26:11.750,00:26:14.420
"we wouldn't be doing any of this
for this one-dimensional matrix",00:26:14.420,00:26:19.180
A. It's tridiagonal.,00:26:19.180,00:26:21.880
We can't ask for more than that.,00:26:21.880,00:26:23.960
"Direct elimination
would be top speed,",00:26:23.960,00:26:28.090
"but the 2D case is what we
understand that it has a big",00:26:28.090,00:26:36.060
"bandwidth because
it's two-dimensional,",00:26:36.060,00:26:38.960
"three-dimensional even bigger
-- and the eigenvalues will come",00:26:38.960,00:26:43.870
"directly from the
eigenvalues of this 1D case.",00:26:43.870,00:26:46.700
"So if we understand
the 1D case, we'll",00:26:46.700,00:26:49.590
"knock out the two- and
three-dimensional easily.",00:26:49.590,00:26:53.090
"So I'll stay with this
one-dimensional case,",00:26:53.090,00:26:56.560
"where the matrix M
-- and I'll even,",00:26:56.560,00:27:00.470
"maybe should write down exactly
what the iteration does --",00:27:00.470,00:27:06.360
"but if I'm looking
now at convergence,",00:27:06.360,00:27:08.720
"do I have or don't I have
convergence, and how fast?",00:27:08.720,00:27:13.980
"What are the eigenvalues
and what's the biggest one?",00:27:13.980,00:27:18.030
"The eigenvalues of A are
this and P is just --",00:27:18.030,00:27:24.230
"P inverse is just 1/2
times the identity.",00:27:24.230,00:27:29.950
So what are the eigenvalues?,00:27:32.910,00:27:35.360
"Again, I mean, I know
the eigenvalues of A,",00:27:35.360,00:27:40.220
so I divide by 2.,00:27:40.220,00:27:42.920
"Let me write down what
I'm going to get now",00:27:42.920,00:27:44.880
"for the eigenvalues of M,
remembering this connection.",00:27:44.880,00:27:53.230
"So the eigenvalues of M are 1
minus 1/2 times the eigenvalues",00:27:53.230,00:27:58.230
of A. Better write that down.,00:27:58.230,00:28:00.080
"1 minus 1/2 times
the eigenvalues of A.",00:28:00.080,00:28:03.730
"Of course, it's
very special that we",00:28:03.730,00:28:10.000
"have this terrific
example matrix where",00:28:10.000,00:28:15.500
"we know the eigenvalues and
really can see what's happening",00:28:15.500,00:28:19.160
and we know the matrix.,00:28:19.160,00:28:21.670
So what happens?,00:28:21.670,00:28:22.520
"I take half of that --
that makes that a 1.",00:28:22.520,00:28:27.140
"When I subtract it
from that 1, it's gone.",00:28:27.140,00:28:32.090
"1/2 of that, that 2 is cancelled
-- I just get cos theta.",00:28:32.090,00:28:36.980
"Theta sub -- whatever
that angle was.",00:28:36.980,00:28:40.990
"It's a cosine and
it's less than 1.",00:28:43.910,00:28:48.390
"So convergence is going to
happen and convergence is going",00:28:51.190,00:28:55.060
"to be slow or fast according as
this is very near 1 or not --",00:28:55.060,00:29:01.950
"and unfortunately, it is pretty
near 1, because the first,",00:29:01.950,00:29:06.120
"the largest one -- I could
tell you what the angles are.",00:29:06.120,00:29:11.040
"That's -- to complete
this information,",00:29:11.040,00:29:13.340
"I should have put in what --
and von Neumann got them right.",00:29:13.340,00:29:18.610
"That's j -- is there
maybe a pi over N plus 1?",00:29:18.610,00:29:27.610
I think so.,00:29:27.610,00:29:28.440
"Those are the
angles that come in,",00:29:31.200,00:29:33.160
"just the equally spaced
angles in the finite case.",00:29:33.160,00:29:37.580
So we have N eigenvalues.,00:29:37.580,00:29:40.940
"And the biggest one is
going to be when j is 1.",00:29:40.940,00:29:44.900
"So the biggest one, rho,
the maximum of these guys,",00:29:44.900,00:29:50.780
"of these lambdas, will be
the cosine of, when j is 1,",00:29:50.780,00:29:56.040
pi over N plus 1.,00:29:56.040,00:30:00.180
"That's the one that's
slowing us down.",00:30:00.180,00:30:01.860
"Notice the eigenvector
that's slowing us down,",00:30:06.800,00:30:10.390
"because it's the
eigenvector that",00:30:10.390,00:30:13.590
"goes with that eigenvalue that's
the biggest eigenvalue that's",00:30:13.590,00:30:17.270
"going to hang around the
longest, decay the slowest.",00:30:17.270,00:30:21.920
That eigenvector is very smooth.,00:30:21.920,00:30:25.980
It's the low frequency.,00:30:25.980,00:30:27.850
It's frequency 1.,00:30:27.850,00:30:29.660
So this is j equal to 1.,00:30:29.660,00:30:32.180
"This is the low frequency,
j equal 1, low frequency.",00:30:32.180,00:30:35.780
"The eigenvector is just
samples of the sine,",00:30:39.030,00:30:49.070
of a discrete sine.,00:30:49.070,00:30:52.500
"Lowest frequency that
the grid can sustain.",00:30:52.500,00:30:57.140
"Why do I emphasize
which eigenvector it is?",00:31:00.600,00:31:04.250
"Because when you know which
eigenvector is slowing you down",00:31:04.250,00:31:08.420
"-- and here it's the eigenvector
you would think would be",00:31:08.420,00:31:12.330
"the simplest one to handle
-- this is the easiest",00:31:12.330,00:31:15.770
"eigenvector, higher-frequency
eigenvectors have eigenvalues",00:31:15.770,00:31:21.830
that start dropping.,00:31:21.830,00:31:24.110
"Cosine of 2*pi, cosine of 3*pi
over N plus 1 is going to be",00:31:24.110,00:31:29.810
further away from 1.,00:31:29.810,00:31:31.420
"Now I have to admit that
the very highest frequency,",00:31:31.420,00:31:38.630
"when j is capital N -- so when j
is capital N I have to admit it",00:31:38.630,00:31:48.600
"happens to come back
with a minus sign,",00:31:48.600,00:31:51.450
"but the magnitude is
still just as bad.",00:31:51.450,00:31:54.820
"The cosine of N*pi over N plus
1 is just the negative of that",00:31:54.820,00:32:03.040
one.,00:32:03.040,00:32:04.820
"So actually, we
have two eigenvalues",00:32:04.820,00:32:07.650
"that are both hitting
the spectral radius",00:32:07.650,00:32:13.070
"and both of them are the
worst, the slowest converging",00:32:13.070,00:32:20.350
eigenvectors.,00:32:20.350,00:32:22.610
"This eigenvector
looks very smooth.",00:32:22.610,00:32:27.360
"The eigenvector for
that high-frequency one",00:32:27.360,00:32:29.880
is the fastest oscillation.,00:32:29.880,00:32:32.760
"If I graph the eigenvectors,
eigenvalues, I will.",00:32:32.760,00:32:37.520
I'll graph the eigenvalues.,00:32:37.520,00:32:39.950
"You'll see that it's the
first and the last that",00:32:39.950,00:32:42.500
are nearest in magnitude to 1.,00:32:42.500,00:32:46.160
"I was just going to
say, can I anticipate",00:32:49.010,00:32:52.020
multigrid for a minute?,00:32:52.020,00:32:53.110
"So multigrid does -- is going
to try to get these guys to go",00:32:55.850,00:33:01.680
faster.,00:33:01.680,00:33:03.460
"Now this one can be
handled, you'll see,",00:33:03.460,00:33:06.510
"by just a simple
adjustment of Jacobi.",00:33:06.510,00:33:09.650
"I just weight
Jacobi a little bit.",00:33:09.650,00:33:13.500
"Instead of 2I, I take 3I or 4I.",00:33:13.500,00:33:19.350
"3I would be very good
for the preconditioner.",00:33:19.350,00:33:24.330
"That will have the effect
on this high frequency one",00:33:24.330,00:33:28.440
"that it'll be well
down now, below 1.",00:33:28.440,00:33:32.290
"This will still be the
slowpoke in converging.",00:33:32.290,00:33:37.980
"That's where multigrid
says, that's low frequency.",00:33:37.980,00:33:44.460
That's too low.,00:33:44.460,00:33:45.740
That's not converging quickly.,00:33:45.740,00:33:48.530
"Take a different grid
on which that same thing",00:33:48.530,00:33:52.380
looks higher frequency.,00:33:52.380,00:33:54.300
We'll see that tomorrow.,00:33:54.300,00:33:58.360
"So if I stay with Jacobi,
what have I learned?",00:33:58.360,00:34:02.060
I've learned that this is rho.,00:34:02.060,00:34:05.410
"For this matrix,
the spectral radius",00:34:05.410,00:34:07.870
"is that number and let's
just see how big it is.",00:34:07.870,00:34:11.550
"How big is -- that's a
cosine of a small number.",00:34:11.550,00:34:15.380
"So the cosine of a small
number, theta near 0,",00:34:15.380,00:34:20.320
"the cosine of theta is 1
minus -- so it's below 1,",00:34:20.320,00:34:25.600
of course -- 1/2 theta squared.,00:34:25.600,00:34:27.960
"Theta is pi over
N plus 1 squared.",00:34:27.960,00:34:33.370
"All these simple
estimates really",00:34:33.370,00:34:36.730
"tell you what happens when
you do Jacobi iteration.",00:34:36.730,00:34:44.230
"You'll see it on the output
from -- I mean, you can --",00:34:44.230,00:34:52.450
"I hope will -- code
up Jacobi's method,",00:34:52.450,00:34:59.690
"taking P to be a multiple of
the identity, so very fast,",00:34:59.690,00:35:06.710
"and you'll see the convergence
rate, you can graph.",00:35:06.710,00:35:11.090
"Here's an interesting point
and I'll make it again.",00:35:11.090,00:35:13.900
"What should you
graph in seeing --",00:35:13.900,00:35:19.970
"to display convergence
and rate of convergence?",00:35:19.970,00:35:24.640
"Let me draw a
possible graph here.",00:35:24.640,00:35:28.400
Let me draw two possible graphs.,00:35:28.400,00:35:30.950
"I could graph -- this is
K. As I take more steps,",00:35:30.950,00:35:38.660
"I could graph the error,
well, the size of the error.",00:35:38.660,00:35:42.890
"It's a vector, so
I take its length.",00:35:42.890,00:35:47.980
"I don't know where I
start -- at e_naught.",00:35:47.980,00:35:50.340
This is k equals 0.,00:35:50.340,00:35:51.257
This is the initial guess.,00:35:51.257,00:35:52.340
"If I use -- I'm going
to draw a graph,",00:35:55.380,00:36:00.620
"which you'll draw much better
by actually getting MATLAB to do",00:36:00.620,00:36:04.990
it.,00:36:04.990,00:36:05.490
"Before I draw it, let me mention
the other graph we could draw.",00:36:08.690,00:36:13.680
"This tells us the error
in -- this is the error",00:36:13.680,00:36:22.350
in the solution.,00:36:22.350,00:36:25.257
"If I think that's the
natural thing to draw,",00:36:25.257,00:36:27.090
"it is, but we could
also draw the error",00:36:27.090,00:36:32.360
"in the equation,
the residual error.",00:36:32.360,00:36:36.410
"So I'll just put up here
what I mean by that.",00:36:36.410,00:36:39.730
"If I put in -- so
A*x equals b exactly.",00:36:39.730,00:36:48.220
A*x_k is close.,00:36:48.220,00:36:52.790
"The residual is -- let me call
it r, as everybody does --",00:36:56.990,00:37:01.070
"is the difference A*x minus
A*x_k It's how close we are",00:37:01.070,00:37:13.820
"to b, so it's b minus A*x_k.",00:37:13.820,00:37:18.850
"The true A*x gets
b exactly right.",00:37:18.850,00:37:21.610
"The wrong A*x gets
b nearly right.",00:37:21.610,00:37:23.770
"So you see, because we
have this linear problem,",00:37:28.030,00:37:32.090
"it's A times the error.
x minus x_k is e_k.",00:37:32.090,00:37:38.860
"So this is the residual
and I could graph that.",00:37:38.860,00:37:49.770
"That's how close my answer
comes to getting b right,",00:37:49.770,00:37:55.470
how close the equation is.,00:37:55.470,00:37:57.360
"Where this is how
close the x_k is.",00:37:57.360,00:38:01.440
"So that would be the
other thing I could graph.",00:38:01.440,00:38:03.520
"r_k, the size of the
residual, which is A*e_k.",00:38:06.570,00:38:12.680
"So now I have to remember what
I think these graphs look like,",00:38:19.670,00:38:23.610
"but of course, it's a little
absurd for me to draw them when",00:38:23.610,00:38:26.305
"-- I think what we see is that
the residual drops pretty fast.",00:38:26.305,00:38:35.040
"The residual drops pretty
fast and keeps going,",00:38:35.040,00:38:40.660
"for, let's say, Jacobi's method,
or, in a minute or maybe next",00:38:40.660,00:38:49.010
"time, another
iterative method --",00:38:49.010,00:38:52.940
"but the error in the
solution starts down fast",00:38:52.940,00:38:57.000
"and what's happening here is the
high-frequency components are",00:38:57.000,00:39:01.340
"getting killed, but those
low-frequency components are",00:39:01.340,00:39:08.950
"not disappearing, because we
figured out that they give",00:39:08.950,00:39:13.500
"the spectral radius,
the largest eigenvalue.",00:39:13.500,00:39:16.410
"So somehow it slopes
off and it's --",00:39:16.410,00:39:27.840
"the price of such a simple
method is that you don't get",00:39:27.840,00:39:31.680
great convergence.,00:39:31.680,00:39:33.020
"Of course, if I put
this on a log-log plot,",00:39:33.020,00:39:38.520
"the slope would be
exactly connected",00:39:38.520,00:39:42.470
to the spectral radius.,00:39:42.470,00:39:44.310
That's the rate of decay.,00:39:44.310,00:39:45.930
"It's the -- e_k is
approximately --",00:39:48.970,00:39:53.320
"the size of e_k is approximately
that worst eigenvalue",00:39:53.320,00:39:57.750
to the k-th power.,00:39:57.750,00:39:58.710
Do you see how this can happen?,00:40:04.830,00:40:06.410
"You see, this can
be quite small,",00:40:09.080,00:40:11.830
the residual quite small.,00:40:11.830,00:40:13.750
"I think it's very
important to see the two",00:40:13.750,00:40:16.330
different quantities.,00:40:16.330,00:40:17.760
"The residual can be quite small,
but then, from the residual,",00:40:17.760,00:40:22.780
"if I wanted to get
back to the error,",00:40:22.780,00:40:24.640
"I'd have to multiply
by A inverse",00:40:24.640,00:40:27.480
and A inverse is not small.,00:40:27.480,00:40:30.730
"Our matrix A is pretty --
has eigenvalues close to 0.",00:40:30.730,00:40:36.090
So A inverse is pretty big.,00:40:36.090,00:40:38.180
"A inverse times A*e_k -- this
is small but A inverse picks up",00:40:38.180,00:40:48.170
"those smallest
eigenvalues of A --",00:40:48.170,00:40:51.220
"picks up those low
frequencies and it's bigger,",00:40:51.220,00:40:59.110
"slower to go to 0 as
we see in this picture.",00:40:59.110,00:41:02.580
"So it's this picture
that is our problem.",00:41:02.580,00:41:08.080
"That shows where Jacobi
is not good enough.",00:41:08.080,00:41:14.580
"So what more to
say about Jacobi?",00:41:14.580,00:41:18.760
"For this matrix K, we
know its eigenvalues.",00:41:18.760,00:41:21.840
"The eigenvalues of
M we know exactly.",00:41:24.650,00:41:27.560
The matrix M we know exactly.,00:41:27.560,00:41:30.000
"Do you see in that matrix
-- now if I had said,",00:41:30.000,00:41:33.740
"look at that matrix and
tell me something about its",00:41:33.740,00:41:37.040
"eigenvalues, what would you say?",00:41:37.040,00:41:38.740
"You would say, it's symmetric,
so the eigenvalues are real",00:41:41.370,00:41:45.630
"and you would say that every
eigenvalue is smaller than 1.",00:41:45.630,00:41:49.870
Why?,00:41:49.870,00:41:50.880
"Because every eigenvalue is --
if you like to remember this",00:41:50.880,00:41:55.120
"business of silly circles,
every eigenvalue is in a circle",00:41:55.120,00:42:01.540
"centered at this number -- in
other words, centered at 0 --",00:42:01.540,00:42:05.260
"and its radius is the
sum of those, which is 1.",00:42:05.260,00:42:10.090
"So we know that every
eigenvalue is in the unit circle",00:42:10.090,00:42:16.180
"and actually, it comes
inside the unit circle,",00:42:16.180,00:42:21.510
"but not very much --
only by 1 over N squared.",00:42:21.510,00:42:25.570
"What do I learn from
this 1 over N squared?",00:42:25.570,00:42:28.610
"How many iterations
do I have to take",00:42:32.090,00:42:33.950
"to reduce the error by, let's
say, 10 or e or whatever?",00:42:33.950,00:42:39.630
"If the eigenvalues are 1 minus
a constant over N squared,",00:42:39.630,00:42:44.900
"I'll have to take
the N squared power.",00:42:44.900,00:42:47.930
"So if I have a
matrix of order 100,",00:42:47.930,00:42:50.900
"the number of iterations
I'm going to need",00:42:50.900,00:42:52.820
"is 100 squared, 10,000 steps.",00:42:52.820,00:42:59.600
"Every step is fast, especially
with the Jacobi choice.",00:42:59.600,00:43:04.560
"Here's my iteration -- and P is
just a multiple of the identity",00:43:09.420,00:43:14.920
here.,00:43:14.920,00:43:15.630
So every step is certainly fast.,00:43:18.350,00:43:20.830
"Every step involves a
matrix multiplication",00:43:20.830,00:43:23.050
"and that's why I began
the lecture by saying,",00:43:23.050,00:43:25.850
matrix multiplications are fast.,00:43:25.850,00:43:28.000
"A times x_k -- that's the
work and it's not much.",00:43:28.000,00:43:32.860
"It's maybe five
non-zeros in A -- so 5N.",00:43:32.860,00:43:37.440
"That's fast, but if I
have to do it 10,000 times",00:43:37.440,00:43:40.890
"just to reduce the error
by some constant factor,",00:43:40.890,00:43:44.720
that's not good.,00:43:44.720,00:43:47.210
"So that's why people think,
okay, Jacobi gives us the idea.",00:43:47.210,00:43:52.530
"It does knock off the high
frequencies quite quickly,",00:43:52.530,00:43:56.130
"but it leaves those
low frequencies.",00:43:56.130,00:44:00.350
"Well, I was going to
mention weighed Jacobi.",00:44:00.350,00:44:03.860
"Weighted Jacobi is to
take, instead of p --",00:44:03.860,00:44:12.040
put in a weight.,00:44:12.040,00:44:15.230
"So weighted Jacobi -- let me put
this here and we'll come back",00:44:15.230,00:44:18.970
to it.,00:44:18.970,00:44:19.510
"So weighted Jacobi
simply takes P",00:44:19.510,00:44:27.330
"to be the diagonal
over a factor omega.",00:44:27.330,00:44:35.390
"For example, omega equals
2/3 is a good choice.",00:44:39.240,00:44:49.300
"Let me draw the -- you
can imagine that if I --",00:44:49.300,00:44:53.200
"this is 2 times the identity,
so it's just 2 over omega I --",00:44:53.200,00:44:58.020
"I'm just choosing a
different constant.",00:44:58.020,00:45:01.280
"I'm just choosing a
different constant in P",00:45:01.280,00:45:03.540
"and it has a simple affect
on M. I'll draw the picture.",00:45:03.540,00:45:09.360
"The eigenvalues of
M are these cosines.",00:45:12.100,00:45:16.180
"So I'm going to draw those
cosines, the eigenvalues of M.",00:45:16.180,00:45:19.830
"So they start -- I'll just draw
the picture of cos theta if I",00:45:19.830,00:45:23.290
can.,00:45:23.290,00:45:23.940
"What does a graph of
cos theta look like?",00:45:23.940,00:45:26.250
From 0 to pi?,00:45:32.060,00:45:34.840
"So I'm going to see --
this is theta equals 0.",00:45:34.840,00:45:38.910
This is theta equal pi.,00:45:38.910,00:45:41.890
"If I just draw cos theta
-- please tell me if this",00:45:41.890,00:45:45.670
isn't it.,00:45:45.670,00:45:47.070
Is it something like that?,00:45:47.070,00:45:48.430
"Now where are these theta_1
-- the bad one -- theta_2,",00:45:52.620,00:45:58.940
"theta_3, theta_4, theta_5,
equally bad, down here.",00:45:58.940,00:46:03.580
"So there's cos --
there's the biggest one.",00:46:03.580,00:46:07.180
"That's rho and you see
it's pretty near 1.",00:46:07.180,00:46:09.600
"Here I'm going to hit
-- at this frequency,",00:46:12.130,00:46:14.890
I'm going to hit minus rho.,00:46:14.890,00:46:17.620
"It'll be, again, near 1.",00:46:17.620,00:46:20.730
"So the spectral radius
is that, that's rho.",00:46:20.730,00:46:27.180
"The cosine of that smallest
angle, theta over N plus 1.",00:46:27.180,00:46:34.160
"What happens when I
bring in these weights?",00:46:34.160,00:46:36.740
"It turns out that -- so,
now the eigenvalues of M,",00:46:36.740,00:46:43.090
"with the weights, will
be 1 -- it turns out --",00:46:43.090,00:46:46.780
"1 minus omega plus
omega cos theta.",00:46:46.780,00:46:50.740
"You'll see it in the
notes, no problem.",00:46:53.820,00:46:56.720
"So it's simply
influenced by omega.",00:46:56.720,00:47:00.990
"If omega is 2/3 --
if omega is 2/3,",00:47:00.990,00:47:04.660
"then I have 1 minus omega
is 1/3 plus 2/3 cos theta.",00:47:04.660,00:47:10.600
"Instead of graphing cos
theta, which I've done --",00:47:15.620,00:47:18.240
"let me draw -- let me complete
this graph for cos theta.",00:47:18.240,00:47:23.220
"The eigenvalues unweighted,
with weight omega equal 1.",00:47:23.220,00:47:27.880
Now it's this thing I draw.,00:47:27.880,00:47:31.450
So what happens?,00:47:31.450,00:47:33.420
What's going on here?,00:47:33.420,00:47:36.170
"For a small theta,
cosine is near 1.",00:47:36.170,00:47:40.890
I'm again near 1.,00:47:40.890,00:47:42.200
"In fact, I'm probably even
little worse, probably up here.",00:47:42.200,00:47:46.100
I think it goes down like that.,00:47:51.230,00:47:57.410
"So it starts at near 1, but
it ends when theta is pi.",00:47:57.410,00:48:03.180
This is 1/3 minus 2/3.,00:48:03.180,00:48:05.140
It ends at minus 1/3.,00:48:05.140,00:48:06.170
"You see, that's a lot smarter.",00:48:06.170,00:48:10.150
"This one ended at minus 1,
that one ended at near minus 1,",00:48:10.150,00:48:14.470
"but this one will end with omega
-- this is the omega equal 2/3,",00:48:14.470,00:48:21.130
the weighted one.,00:48:21.130,00:48:22.400
"All you've done is fix
the high frequency.",00:48:22.400,00:48:26.830
"When I say, oh, that
was a good thing to do.",00:48:26.830,00:48:30.080
"The high frequencies
are no longer a problem.",00:48:30.080,00:48:34.060
The low frequencies still are.,00:48:34.060,00:48:37.340
"So the only way we'll
get out of that problem",00:48:37.340,00:48:39.800
is moving to multigrid.,00:48:39.800,00:48:43.470
"Can I, in the remaining
minute, report on Gauss-Seidel",00:48:43.470,00:48:49.970
and then I'll come back to it?,00:48:49.970,00:48:51.220
"But quick report
on Gauss-Seidel.",00:48:51.220,00:48:54.320
"So what's the difference
in Gauss-Seidel?",00:48:54.320,00:48:58.150
What does Gauss-Seidel do?,00:48:58.150,00:48:59.560
It uses the latest information.,00:48:59.560,00:49:01.740
"As soon as you compute
an x, you use it.",00:49:01.740,00:49:05.770
"Jacobi computed all -- had to
keep all these x's until it",00:49:05.770,00:49:14.350
"found all the new x's, but
the Gauss-Seidel idea is,",00:49:14.350,00:49:18.780
"as soon as you have a better
x, put it in the system.",00:49:18.780,00:49:22.010
"So the storage is cut in
half, because you're not",00:49:22.010,00:49:25.620
"saving a big old vector while
you compute the new one.",00:49:25.620,00:49:32.380
"I'll write down
Gauss-Seidel again.",00:49:32.380,00:49:34.550
"So it's more up to
date and the effect",00:49:34.550,00:49:38.650
"is that the Jacobi
eigenvalues get squared.",00:49:38.650,00:49:44.080
"So you're squaring numbers
that are less than 1.",00:49:44.080,00:49:50.380
"So Gauss-Seidel, this gives
the Jacobi eigenvalues squared.",00:49:50.380,00:50:01.460
"The result is that if I draw
the same curve for Gauss-Seidel,",00:50:01.460,00:50:06.310
"I'll be below, right?",00:50:06.310,00:50:08.590
"Essentially, a Gauss-Seidel
step is worth two Jacobi steps,",00:50:11.920,00:50:15.590
"because eigenvalues
get squared as they",00:50:15.590,00:50:18.300
would do in two Jacobi steps.,00:50:18.300,00:50:20.496
"If I do Jacobi twice, I
square its eigenvalues.",00:50:20.496,00:50:23.460
"So it seems like
absolutely a smart move.",00:50:23.460,00:50:27.110
"It's not brilliant, because
this becomes cosine squared.",00:50:27.110,00:50:35.320
"I lose the 1/2, but I'm
still very, very close to 1.",00:50:35.320,00:50:41.960
"It still takes order N squared
iterations to get anywhere.",00:50:41.960,00:50:47.150
"So Jacobi is, you
could say, replaced",00:50:47.150,00:50:51.060
"by Gauss-Seidel as
being one step better,",00:50:51.060,00:50:53.760
but it's not good enough.,00:50:53.760,00:50:55.750
"And that's why the
subject kept going.",00:50:55.750,00:51:02.950
"This gives rho as
1 minus order of 1",00:51:02.950,00:51:07.490
"over N, first power, where these
were 1 minus 1 over N squared.",00:51:07.490,00:51:14.920
"So this is definitely
further from 1",00:51:14.920,00:51:17.460
"and then this can be
way further from 1.",00:51:17.460,00:51:20.460
"So you can see some of
the numerical experiments",00:51:20.460,00:51:23.980
and analysis that's coming.,00:51:23.980,00:51:26.340
"I'll see you Wednesday to finish
this and move into multigrid,",00:51:26.340,00:51:31.280
which uses these guys.,00:51:31.280,00:51:33.300

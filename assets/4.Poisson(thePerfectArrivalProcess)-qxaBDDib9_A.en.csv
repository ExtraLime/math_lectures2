text,start,stop
"The following content is
provided under a Creative",00:00:00.530,00:00:02.960
Commons license.,00:00:02.960,00:00:04.370
"Your support will help MIT
OpenCourseWare continue to",00:00:04.370,00:00:07.410
"offer high quality educational
resources for free.",00:00:07.410,00:00:11.060
"To make a donation or view
additional materials from",00:00:11.060,00:00:13.960
"hundreds of MIT courses, visit
MIT OpenCourseWare at",00:00:13.960,00:00:17.890
ocw.mit.edu.,00:00:17.890,00:00:19.140
"PROFESSOR: As the question said,
where are we as far as",00:00:22.424,00:00:25.930
the text goes.,00:00:25.930,00:00:26.800
"We're just going to
start Chapter 2",00:00:26.800,00:00:28.960
"today, Poisson processes.",00:00:28.960,00:00:31.290
"I want to spend about five
minutes reviewing a little bit",00:00:31.290,00:00:35.270
"about convergence, the things
we said last time,",00:00:35.270,00:00:39.130
and then move on.,00:00:39.130,00:00:41.880
"There's a big break in the
course, at this point between",00:00:41.880,00:00:46.240
"Chapter 1 and Chapter 2 in the
sense that Chapter 1 is very",00:00:46.240,00:00:51.570
"abstract, a little
theoretical.",00:00:51.570,00:00:54.940
"It's dealing with probability
theory in general and the most",00:00:54.940,00:01:00.750
"general theorems in probability
stated in very",00:01:00.750,00:01:04.239
simple and elementary form.,00:01:04.239,00:01:06.270
"But still, we're essentially
not dealing with",00:01:06.270,00:01:11.490
applications at all.,00:01:11.490,00:01:12.660
"We're dealing with very, very
abstract things in Chapter 1.",00:01:12.660,00:01:16.850
Chapter 2 is just the reverse.,00:01:16.850,00:01:19.140
"A Poisson process is the
most concrete thing",00:01:19.140,00:01:22.480
you can think of.,00:01:22.480,00:01:24.420
"People use that as a model
for almost everything.",00:01:24.420,00:01:28.150
"Whether it's a reasonable
model or",00:01:28.150,00:01:29.710
not is another question.,00:01:29.710,00:01:31.400
"But people use it as
a model constantly.",00:01:31.400,00:01:34.300
"And everything about
it is simple.",00:01:34.300,00:01:37.570
"For a Poisson process, you can
almost characterize it as",00:01:37.570,00:01:41.570
"saying everything you could
think of about a Poisson",00:01:41.570,00:01:44.310
"process is either true or
it's obviously false.",00:01:44.310,00:01:49.270
"And when you get to
that point, you",00:01:49.270,00:01:50.830
understand Poisson processes.,00:01:50.830,00:01:52.670
"And you can go on to other
things and never have to",00:01:52.670,00:01:55.630
"really think about them
very hard again.",00:01:55.630,00:01:57.530
"Because at that point, you
really understand them.",00:01:57.530,00:02:00.820
OK.,00:02:00.820,00:02:01.130
"So let's go on and review
things a little bit.",00:02:01.130,00:02:06.130
"What's convergence and how does
it affect sequences of",00:02:06.130,00:02:11.060
IID random variables?,00:02:11.060,00:02:12.550
"Convergence is more
general than",00:02:12.550,00:02:14.550
just IID random variables.,00:02:14.550,00:02:16.840
"But it applies to any sequence
of random variables.",00:02:16.840,00:02:21.700
"And the definition is that a
sequence of random variables",00:02:21.700,00:02:26.270
"converges in distribution to
another random variable z, if",00:02:26.270,00:02:30.970
"the limit, as n goes to
infinity, of the distribution",00:02:30.970,00:02:36.120
function of zn converges to the,00:02:36.120,00:02:39.200
distribution function of z.,00:02:39.200,00:02:41.700
"For this definition to make
sense, it doesn't matter what",00:02:41.700,00:02:46.190
"z is. z can be outside of
the sample space even.",00:02:46.190,00:02:52.880
"The only thing we're interested
in is this",00:02:52.880,00:02:54.720
"particular distribution
function.",00:02:54.720,00:02:57.110
"So what we're really saying is
a sequence of distributions",00:02:57.110,00:03:00.760
"converges to another
distribution function if in",00:03:00.760,00:03:05.670
"fact this limit occurs at every
point where f of z is",00:03:05.670,00:03:12.980
continuous.,00:03:12.980,00:03:13.450
"In other words, if f of z is
discontinuous someplace, we",00:03:13.450,00:03:17.590
"had an example of that where
we're looking at the law of",00:03:17.590,00:03:20.440
"large numbers and the
distribution function.",00:03:20.440,00:03:24.490
"Looked at in the right way
was a step function.",00:03:24.490,00:03:27.330
"It wasn't continuous
at the step.",00:03:27.330,00:03:30.010
"And therefore, you can't expect
anything to be said",00:03:30.010,00:03:34.000
about that.,00:03:34.000,00:03:36.360
"So the typical example of
convergence in distribution is",00:03:36.360,00:03:40.710
"the central limit theorem which
says, if x1, x2 are IID,",00:03:40.710,00:03:45.460
"they have a variance
sigma squared.",00:03:45.460,00:03:47.900
"And if s sub n the sum of these
random variables is a",00:03:47.900,00:03:51.370
"sum of x1 to xn, then zn
is the normalized sum.",00:03:51.370,00:03:56.410
"In other words, you
take this sum.",00:03:56.410,00:03:58.660
"You subtract off
the mean of it.",00:03:58.660,00:04:01.530
"And I think in the reproduction
of these slides",00:04:01.530,00:04:06.430
"that you have, I think that
n, right there was--",00:04:06.430,00:04:16.232
here we go.,00:04:16.232,00:04:17.492
That's the other y.,00:04:17.492,00:04:19.680
I think that n was left off.,00:04:19.680,00:04:24.280
"If that n wasn't left off,
another n was left off.",00:04:24.280,00:04:27.880
It's obviously needed there.,00:04:27.880,00:04:29.470
"This is a normalized random
variable because the variance",00:04:29.470,00:04:32.640
"of sn and of sn minus nx bar is
just sigma squared times n.",00:04:32.640,00:04:42.120
"Because they're any of these
random variables.",00:04:42.120,00:04:44.340
"So we're dividing by the
standard deviation.",00:04:44.340,00:04:49.020
"So this is a random variable
for each n which has a",00:04:49.020,00:04:53.700
"standard deviation
1 and mean 0.",00:04:53.700,00:04:57.250
So it's normalized.,00:04:57.250,00:04:58.680
"And it converges in distribution
to a Gaussian",00:04:58.680,00:05:01.610
"random variable of mean 0 and
standard deviation 1.",00:05:01.610,00:05:06.470
"This notation here is
sort of standard.",00:05:06.470,00:05:08.870
"And we'll use it at
various times.",00:05:08.870,00:05:11.180
"It means a Gaussian distribution
with mean 0 and",00:05:11.180,00:05:15.780
variance 1.,00:05:15.780,00:05:17.690
"So for an example of that, if
x1, x2, so forth, are IID with",00:05:17.690,00:05:22.270
"mean expected value of x and the
sum here, then sn over n",00:05:22.270,00:05:30.410
"converges in distribution to
the deterministic random",00:05:30.410,00:05:33.670
variable x bar.,00:05:33.670,00:05:36.230
That's a nice example of this.,00:05:36.230,00:05:37.730
"So we have two examples of
convergence and distribution.",00:05:37.730,00:05:42.850
And that's what that says.,00:05:42.850,00:05:49.500
"So next, a sequence of random
variables converges in",00:05:49.500,00:05:53.290
probability.,00:05:53.290,00:05:55.700
"When we start talking about
convergence in probability,",00:05:55.700,00:05:59.400
"there's another idea which we
are going to bring in, mostly",00:05:59.400,00:06:04.360
"in Chapter 4 when we get
to it, which is called",00:06:04.360,00:06:07.510
"convergence with
probability 1.",00:06:07.510,00:06:09.840
"Don't confuse those two
things because they're",00:06:09.840,00:06:11.950
very different ideas.,00:06:11.950,00:06:14.880
"Because people confuse them,
many people call convergence",00:06:14.880,00:06:20.050
"with probability 1 almost sure
convergence or almost",00:06:20.050,00:06:24.490
everywhere convergence.,00:06:24.490,00:06:27.030
I don't like that notation.,00:06:27.030,00:06:30.200
"So I'll stay with the notation
with probability 1.",00:06:30.200,00:06:34.090
"But it means something very
different than converging in",00:06:34.090,00:06:36.950
probability.,00:06:36.950,00:06:38.230
"So the definition is that a
set of random variables",00:06:38.230,00:06:42.120
"converges in probability to some
other random variable if",00:06:42.120,00:06:48.280
this limit holds true.,00:06:48.280,00:06:50.000
"And you remember that diagram
we showed you last time.",00:06:50.000,00:06:54.840
Let me just quickly redraw it.,00:06:54.840,00:06:57.890
"Have this set of distribution
functions.",00:07:01.124,00:07:04.420
"Here's the mean here,
x bar, limits,",00:07:04.420,00:07:09.370
plus and minus epsilon.,00:07:09.370,00:07:12.840
"And this sequence of random
variables has to come in down",00:07:12.840,00:07:18.000
here and go out up there.,00:07:18.000,00:07:22.280
"This distance here and the
distance there gets very small",00:07:22.280,00:07:27.930
"and goes to 0 as n
gets very large.",00:07:27.930,00:07:30.570
"And that's the meaning
of what this says.",00:07:30.570,00:07:32.580
"So if you don't remember that
diagram, go look at it in the",00:07:32.580,00:07:35.860
"lecture notes last time or in
the text where it's explained",00:07:35.860,00:07:39.370
in a lot more detail.,00:07:39.370,00:07:42.830
"So the typical example of that
is the weak law of large",00:07:42.830,00:07:47.590
"numbers of x1, blah, blah,
blah, are IID with mean,",00:07:47.590,00:07:52.370
expected value of x.,00:07:52.370,00:07:53.420
"Remember now that we say that
a random variable has a mean",00:07:53.420,00:07:58.170
"if the expected value of the
absolute value of x is finite.",00:07:58.170,00:08:04.370
"It's not enough to have things
which have a distribution",00:08:04.370,00:08:09.510
"function which is badly behaved
for very big x, and",00:08:09.510,00:08:15.060
"badly behaved for very small
x, and the two of them",00:08:15.060,00:08:17.770
cancelled out.,00:08:17.770,00:08:18.510
That doesn't work.,00:08:18.510,00:08:19.800
"That doesn't mean
you have a mean.",00:08:19.800,00:08:21.620
"You need the expected value
as the absolute",00:08:21.620,00:08:24.140
value of x to be finite.,00:08:24.140,00:08:25.420
"Now, the weak law of large
numbers says that the random",00:08:28.240,00:08:33.530
"variables sn over n, in other
words, the sample average, in",00:08:33.530,00:08:38.929
"fact converges to the
deterministic",00:08:38.929,00:08:42.870
random variable x bar.,00:08:42.870,00:08:44.840
"And that convergence is
convergence in probability.",00:08:44.840,00:08:48.220
"Which means it's this kind
of convergence here.",00:08:48.220,00:08:50.530
"Which means that it's going
to a distribution",00:08:50.530,00:08:53.300
which is a step function.,00:08:53.300,00:08:55.260
"There's a very big difference
between a distribution which",00:08:55.260,00:08:59.010
"is a step function and a
distribution which is",00:08:59.010,00:09:01.870
"something like a Gaussian
random variable.",00:09:01.870,00:09:04.510
"And what the big difference is
is that the random variables",00:09:04.510,00:09:08.450
"that are converging to each
other, if a bunch of random",00:09:08.450,00:09:11.570
"variables are all converging to
a constant, then they all",00:09:11.570,00:09:14.670
"have to be very close
to each other.",00:09:14.670,00:09:16.730
"And that's the property you're
really interested in in",00:09:16.730,00:09:21.930
convergence in probability.,00:09:21.930,00:09:25.000
"So convergence in mean square,
finally, last definition which",00:09:25.000,00:09:29.430
is easy to deal with.,00:09:29.430,00:09:31.725
"If a sequence of random
variables converges in the",00:09:31.725,00:09:34.990
"mean square to another random
variable, if this limit of the",00:09:34.990,00:09:39.740
"expected value, of the
difference between the two",00:09:39.740,00:09:43.150
"random variables squared, goes
to 0, this n gets big.",00:09:43.150,00:09:47.690
"That's what we had with the weak
law of large numbers if",00:09:47.690,00:09:51.500
"you assume that the random
variables each had a variance.",00:09:51.500,00:09:55.035
So on to something new.,00:09:58.090,00:10:00.350
On to Poisson processes.,00:10:00.350,00:10:02.890
"We first have to explain what
an arrival process is.",00:10:02.890,00:10:05.830
"And then we can get into
Poisson processes.",00:10:05.830,00:10:08.760
"Because arrival processes are
a very broad class of",00:10:08.760,00:10:13.540
"stochastic processes, in fact
discrete stochastic processes.",00:10:13.540,00:10:17.930
"But they have this property of
being characterized by things",00:10:17.930,00:10:21.750
"happening at various random
instance of time as opposed to",00:10:21.750,00:10:27.090
"a noise waveform or something
of that sort.",00:10:27.090,00:10:30.710
"So an arrival process is a
sequence of increasing random",00:10:30.710,00:10:34.390
"variables, 0 less than
s1, less than s2.",00:10:34.390,00:10:38.540
"What's it mean for a random
variable s1 to be less than a",00:10:38.540,00:10:42.850
random variable s2?,00:10:42.850,00:10:45.760
"It means exactly the
same thing as it",00:10:45.760,00:10:47.870
means for real numbers.,00:10:47.870,00:10:50.870
"s1 is less than s2 if the random
variable s2 minus s1 is",00:10:50.870,00:10:56.850
"a positive random variable,
namely if it only takes on",00:10:56.850,00:11:00.710
"non-negative values for all
omega in the sample space or",00:11:00.710,00:11:06.520
"for all omega except for some
peculiar set of probability 0.",00:11:06.520,00:11:13.060
"The differences in these arrival
epochs, why do I call",00:11:13.060,00:11:19.050
them arrival epochs?,00:11:19.050,00:11:20.580
"Why do other people call
them arrival epochs?",00:11:20.580,00:11:23.200
"Because time is something which
gets used so often here",00:11:23.200,00:11:27.780
that it gets confusing.,00:11:27.780,00:11:29.000
"So it's nice to call one thing
epochs instead of time.",00:11:29.000,00:11:32.680
"And then you know what you're
talking about a little better.",00:11:32.680,00:11:35.930
"The difference is s sub i minus
s sub i minus 1 for all",00:11:35.930,00:11:42.070
"i greater than or equal to 2
here, with x1 equal to s1.",00:11:42.070,00:11:49.040
"These are called interarrival
times and the si are called",00:11:49.040,00:11:52.640
arrival epochs.,00:11:52.640,00:11:53.910
"The picture here really
shows it all.",00:11:53.910,00:11:58.170
"Here we have a sequence of
arrival instance, which is",00:11:58.170,00:12:02.380
where these arrivals occur.,00:12:02.380,00:12:04.260
"By definition, x1 is the time at
which is the first arrival",00:12:04.260,00:12:09.970
"occurs, x2 is the difference
between the time when the",00:12:09.970,00:12:13.450
"second arrival occurs and
the first arrival",00:12:13.450,00:12:16.030
"occurs, and so forth.",00:12:16.030,00:12:18.080
"n of t is the number of arrivals
that have occurred up",00:12:18.080,00:12:21.560
until time t.,00:12:21.560,00:12:23.390
"Which is, if we draw a staircase
function for each of",00:12:23.390,00:12:27.280
"these arrivals, n of t
is just the value of",00:12:27.280,00:12:30.920
that staircase function.,00:12:30.920,00:12:32.310
"In other words, the counting
process, the",00:12:32.310,00:12:34.490
arrival counting process--,00:12:34.490,00:12:36.360
"here's another typo in the
notes that you've got.",00:12:36.360,00:12:39.400
"It calls it Poisson
counting process.",00:12:39.400,00:12:41.050
"It should be arrival
counting process.",00:12:41.050,00:12:43.625
"What this staircase function
is is in fact",00:12:48.540,00:12:52.290
the counting process.,00:12:52.290,00:12:54.120
"It says how many arrivals
there have been",00:12:54.120,00:12:56.550
up until time t.,00:12:56.550,00:12:57.950
"And every once in a while,
that jumps up by 1.",00:12:57.950,00:13:00.320
"So it keeps increasing by
1 at various times.",00:13:00.320,00:13:03.920
"So that's the arrival
counting process.",00:13:03.920,00:13:06.600
"The important thing to get out
of this is if you understand",00:13:06.600,00:13:11.970
"everything about these random
variables, then you understand",00:13:11.970,00:13:15.760
"everything about these
random variables.",00:13:15.760,00:13:18.170
"And then you understand
everything about",00:13:18.170,00:13:20.580
these random variables.,00:13:20.580,00:13:22.800
"There's a countable number of
these random variables.",00:13:22.800,00:13:25.950
"There's a countable number of
these random variables.",00:13:25.950,00:13:29.050
"There's an unaccountably
infinite number of these",00:13:29.050,00:13:31.230
random variables.,00:13:31.230,00:13:32.060
"In other words, for every
t, n of t is a",00:13:32.060,00:13:35.070
different random variable.,00:13:35.070,00:13:37.240
"I mean, it tends to be the
same for relatively large",00:13:37.240,00:13:42.020
intervals of t sometimes.,00:13:42.020,00:13:44.020
"But this is a different random
variable for each value of t.",00:13:44.020,00:13:49.110
So let's proceed with that.,00:13:49.110,00:13:53.600
"A sample path or sample function
of the process is a",00:13:53.600,00:13:56.970
sequence of sample values.,00:13:56.970,00:13:58.870
"That's the same as we
have everywhere.",00:13:58.870,00:14:01.180
"You look at a sample point
of the process.",00:14:01.180,00:14:05.450
"Sample point of the whole
probability space maps into",00:14:05.450,00:14:13.160
"this sequence of random
variables, s1,",00:14:13.160,00:14:15.520
"s2, s3, and so forth.",00:14:15.520,00:14:18.990
"If you know what the sample
value is of each one of these",00:14:18.990,00:14:22.200
"random variables, then in fact,
you can draw this step",00:14:22.200,00:14:27.570
function here.,00:14:27.570,00:14:28.890
"If you know the value, the
sample value, of each one of",00:14:28.890,00:14:32.460
"these, in the same way,
you can again",00:14:32.460,00:14:37.640
draw this step function.,00:14:37.640,00:14:39.890
"And if you know what the
subfunction is, the step",00:14:39.890,00:14:42.330
"function in fact is the sample
value of n of t.",00:14:42.330,00:14:46.390
"Now, there's one thing a
little peculiar here.",00:14:46.390,00:14:51.380
"Each sample path corresponds
to a",00:14:51.380,00:14:53.620
particular staircase function.,00:14:53.620,00:14:56.030
"And the process can be viewed
as the ensemble with joint",00:14:56.030,00:14:58.880
"probability distributions of
such staircase functions.",00:14:58.880,00:15:03.110
"Now, what does all that
gobbledygook mean?",00:15:03.110,00:15:07.280
"Very, very often in probability",00:15:07.280,00:15:09.230
"theory, we draw pictures.",00:15:09.230,00:15:11.590
"And these pictures are pictures
of what happens to",00:15:11.590,00:15:15.790
random variables.,00:15:15.790,00:15:17.490
"And there's a cheat
in all of that.",00:15:17.490,00:15:19.840
"And the cheat here is that in
fact, this step function here",00:15:19.840,00:15:26.160
"is just a generic
step function.",00:15:26.160,00:15:28.910
"These points at which changes
occur are generic values at",00:15:28.910,00:15:34.860
which changes occur.,00:15:34.860,00:15:36.880
"And we're representing those
values as random variables.",00:15:36.880,00:15:41.400
"When you represent these as
random variables, this whole",00:15:41.400,00:15:44.380
"function here, namely n
of t itself, becomes--",00:15:44.380,00:15:48.010
"if you have a particular set
of values for each one of",00:15:56.510,00:15:59.680
"these, then you have a
particular staircase function.",00:15:59.680,00:16:03.140
"With that particular staircase
function, you have a",00:16:03.140,00:16:06.350
"particular sample
path for n of t.",00:16:06.350,00:16:09.860
"In other words, a sample path
for any set of these random",00:16:09.860,00:16:13.350
"variables-- the arrival epochs,
or the interarrival",00:16:13.350,00:16:16.620
"intervals, or n of
t for each t--",00:16:16.620,00:16:19.740
"all of these are equivalent
to each other.",00:16:19.740,00:16:22.400
"For this reason, when we talk
about arrival processes, it's",00:16:22.400,00:16:29.130
"a little different than
what we usually do.",00:16:29.130,00:16:31.240
"Because usually, we say a random
process is a sequence",00:16:31.240,00:16:36.470
"or an uncountable number
of random variables.",00:16:36.470,00:16:39.740
"Here, just because we can
describe it in three different",00:16:39.740,00:16:43.470
"ways, this same stochastic
process gets described either",00:16:43.470,00:16:51.330
"as a sequence of interarrival
intervals, or as a sequence of",00:16:51.330,00:16:56.700
"arrival epochs, or as a
countable number these n of t",00:16:56.700,00:17:01.660
random variables.,00:17:01.660,00:17:03.010
"And from now on, we make no
distinction between any of",00:17:03.010,00:17:06.839
these things.,00:17:06.839,00:17:07.589
"We will, every once in while,
have to remind ourselves what",00:17:07.589,00:17:11.560
"these pictures mean because
they look very simple.",00:17:11.560,00:17:15.300
"They look like the pictures
of functions that",00:17:15.300,00:17:17.180
you're used to drawing.,00:17:17.180,00:17:18.550
"But they don't really
mean the same thing.",00:17:18.550,00:17:20.980
"Because this picture is drawing
a generic sample path.",00:17:20.980,00:17:25.579
"For that generic sample path,
you have a set of sample",00:17:25.579,00:17:29.380
"values for the Xs, a sample path
for the arrival epochs, a",00:17:29.380,00:17:37.580
"sample set of values
for n of t.",00:17:37.580,00:17:41.750
"And when we draw the picture
calling these random",00:17:41.750,00:17:44.050
"variables, we really
mean the set of",00:17:44.050,00:17:46.120
all such step functions.,00:17:46.120,00:17:47.850
"And we just automatically use
all those properties and those",00:17:47.850,00:17:51.340
relationships.,00:17:51.340,00:17:53.390
"So it's not quite as simple as
what it appears to be, but",00:17:53.390,00:17:57.510
it's almost as simple.,00:17:57.510,00:17:58.760
"You can also see that any sample
path can be specified",00:18:01.090,00:18:06.280
"by the sample values n of t for
all t, by si for all i, or",00:18:06.280,00:18:10.370
by xi for all i.,00:18:10.370,00:18:13.050
"So that essentially, an arrival
process is specified",00:18:13.050,00:18:17.190
by any one of these things.,00:18:17.190,00:18:18.310
"That's exactly what
I just said.",00:18:18.310,00:18:20.830
"The major relation we need to
relate the counting process to",00:18:20.830,00:18:25.720
"the arrival process, well,
there's one relationship here,",00:18:25.720,00:18:31.040
"which is perhaps the simplest
relationship.",00:18:31.040,00:18:34.260
"But this relationship is a nice
relationship to say what",00:18:34.260,00:18:38.790
"n of t is if you know
what s sub n is.",00:18:38.790,00:18:42.980
"It's not quite so nice if you
know what n of t is to figure",00:18:42.980,00:18:48.430
what s sub n is.,00:18:48.430,00:18:50.820
"I mean, the information is
tucked into the statement, but",00:18:50.820,00:18:53.960
"it's tucked in a more convenient
way into this",00:18:53.960,00:18:56.800
statement down here.,00:18:56.800,00:18:59.280
"This statement, I can see it
now after many years of",00:18:59.280,00:19:04.520
dealing with it.,00:19:04.520,00:19:06.910
"I'm sure that you can
see it if you stare",00:19:06.910,00:19:10.400
at it for five minutes.,00:19:10.400,00:19:11.650
"You will keep forgetting
the intuitive picture",00:19:14.700,00:19:16.900
that goes with it.,00:19:16.900,00:19:18.310
"So I suggest that this is one
of the rare things in this",00:19:18.310,00:19:21.340
"course that you just
ought to remember.",00:19:21.340,00:19:24.780
"And then once you remember
it, you can always figure",00:19:24.780,00:19:27.530
out why it's true.,00:19:27.530,00:19:28.740
"Here's the reason
why it's true.",00:19:28.740,00:19:31.670
"If s sub n is equal to tau for
some tau less than or equal to",00:19:31.670,00:19:35.960
"t, then n of tau has
to be equal to n.",00:19:35.960,00:19:40.466
"If s sub n is equal to tau,
here's the picture here,",00:19:40.466,00:19:43.880
except there's not a tau here.,00:19:43.880,00:19:45.300
"If s sub 2 is equal to
tau, then n of 2--",00:19:45.300,00:19:52.180
"these are right continuous,
so n of 2 is equal to 2.",00:19:52.180,00:19:56.460
"And therefore, n of tau is less
than or equal to n of t.",00:19:56.460,00:20:01.100
"So that's the whole
reason down there.",00:20:01.100,00:20:04.360
"You can turn this
argument around.",00:20:04.360,00:20:06.350
"You can start out with n of t is
greater than or equal to n.",00:20:06.350,00:20:12.160
"It means n of t is equal
to some particular n.",00:20:12.160,00:20:15.220
"And turn the argument
upside down.",00:20:15.220,00:20:18.030
And you get the same argument.,00:20:18.030,00:20:19.490
"So this tells you
what this is.",00:20:19.490,00:20:25.800
This tells you what this is.,00:20:25.800,00:20:28.160
"If you do this for every n and
every t, then you do this for",00:20:28.160,00:20:32.290
every n and every t.,00:20:32.290,00:20:36.100
"It's a very bizarre statement
because usually when you have",00:20:36.100,00:20:40.590
"relationships between functions,
you don't have the",00:20:40.590,00:20:44.820
"Ns and the Ts switching
around.",00:20:44.820,00:20:47.380
"And in this case, the
n is the subscript.",00:20:47.380,00:20:50.330
"That's the thing which says
which random variable you're",00:20:50.330,00:20:52.620
talking about.,00:20:52.620,00:20:53.970
"And over here, t is the thing
which says what random",00:20:53.970,00:20:57.670
variable you're talking about.,00:20:57.670,00:20:59.690
"So it's peculiar
in that sense.",00:20:59.690,00:21:01.860
"It's a statement which
requires a",00:21:01.860,00:21:03.440
little bit of thought.,00:21:03.440,00:21:04.860
"I apologize for dwelling
on it because once you",00:21:04.860,00:21:07.910
"see it, it's obvious.",00:21:07.910,00:21:09.580
"But many of these obvious
things are not obvious.",00:21:09.580,00:21:12.940
"What we're going to do as we
move on is we're going to talk",00:21:19.030,00:21:21.830
"about these arrival processes in
any of these three ways we",00:21:21.830,00:21:26.040
choose to talk about them.,00:21:26.040,00:21:28.010
"And we're going to go back
and forth between them.",00:21:28.010,00:21:30.760
"And with Poisson processes,
that's particularly easy.",00:21:30.760,00:21:34.670
"We can't do a whole lot more
with arrival processes.",00:21:34.670,00:21:37.770
They're just too complicated.,00:21:37.770,00:21:39.910
"I mean, arrival processes
involve almost any kind of",00:21:39.910,00:21:43.650
"thing where things happen at
various points in time.",00:21:43.650,00:21:47.790
"So we simplify it to something
called a renewal process.",00:21:47.790,00:21:51.960
"Renewal processes are the
topic of Chapter 4.",00:21:51.960,00:21:55.750
"When you get to Chapter 4, you
will perhaps say that renewal",00:21:55.750,00:21:59.010
"processes are too complicated
to talk about also.",00:21:59.010,00:22:03.320
"I hope after we finish Chapter
4, you won't believe that it's",00:22:03.320,00:22:06.730
too complicated to talk about.,00:22:06.730,00:22:08.990
"But these are fairly complicated
processes.",00:22:08.990,00:22:12.280
"But even here, it's an arrival
process where the interarrival",00:22:12.280,00:22:16.370
"intervals are independent and
identically distributed.",00:22:16.370,00:22:21.060
"Finally, a Poisson process is
a renewal process for which",00:22:21.060,00:22:25.730
"each x sub i has an exponential
distribution.",00:22:25.730,00:22:30.240
"Each interarrival has to have
the same distribution because",00:22:30.240,00:22:35.070
"since it's a renewal process,
these are all IID.",00:22:35.070,00:22:39.110
"And we let this distribution
function X be the generic",00:22:39.110,00:22:44.040
random variable.,00:22:44.040,00:22:45.330
"And this is talking about
the distribution",00:22:45.330,00:22:47.350
function of all of them.,00:22:47.350,00:22:49.610
"I don't know whether that 1
minus is in the slides I",00:22:49.610,00:22:53.760
passed out.,00:22:53.760,00:22:54.470
"There's one kind of
error like that.",00:22:54.470,00:22:57.910
And I'm not sure where it is.,00:22:57.910,00:22:59.960
"So anyway, lambda is some fixed
parameter called the",00:22:59.960,00:23:03.090
rate of the Poisson process.,00:23:03.090,00:23:05.180
"So for each lambda greater than
0, you have a Poisson",00:23:05.180,00:23:09.770
"process where each of these
interarrival intervals are",00:23:09.770,00:23:14.190
"exponential random variables
of rate lambda.",00:23:14.190,00:23:18.300
"So that defines a
Poisson process.",00:23:18.300,00:23:20.460
"So we can all go home now
because we now know everything",00:23:20.460,00:23:22.940
"about Poisson processes
in principle.",00:23:22.940,00:23:26.890
"Everything we're going to say
from now on comes from this",00:23:26.890,00:23:31.490
"one simple statement here that
these interarrival intervals",00:23:31.490,00:23:35.620
are exponential.,00:23:35.620,00:23:37.070
"There's something very, very
special about this exponential",00:23:37.070,00:23:39.910
distribution.,00:23:39.910,00:23:41.740
"And that's what makes Poisson
processes so very special.",00:23:41.740,00:23:44.975
"And that special thing is this
memoryless property.",00:23:53.570,00:23:58.070
"A random variable is memoryless
if it's positive.",00:23:58.070,00:24:01.760
"And for all real t greater than
0 and x greater than 0,",00:24:01.760,00:24:06.530
"the probability that x is
greater than t plus x is equal",00:24:06.530,00:24:10.580
"to the probability that x is
greater than t times the",00:24:10.580,00:24:13.580
"probability that x is
greater than x.",00:24:13.580,00:24:16.260
"If you plug that in, then the
statement is the same whether",00:24:16.260,00:24:21.470
"you're dealing with densities,
or PMFs, or",00:24:21.470,00:24:24.620
distribution function.,00:24:24.620,00:24:27.640
"You get the same product
relationship in each case.",00:24:27.640,00:24:30.980
"Since the interarrival interval
is exponential, the",00:24:30.980,00:24:34.470
"probability that a random
variable x is greater than",00:24:34.470,00:24:38.540
"some particular value x is equal
to e to the minus lambda",00:24:38.540,00:24:43.570
x for x greater than zero.,00:24:43.570,00:24:46.330
"This you'll recognize, not as a
distribution function but as",00:24:46.330,00:24:50.870
"the complementary distribution
function.",00:24:50.870,00:24:53.190
"It's the probability that
X is greater than x.",00:24:53.190,00:24:56.560
"So it's the complementary
distribution function",00:24:56.560,00:25:00.120
evaluated at the value of x.,00:25:00.120,00:25:02.430
"This is an exponential
which is going down.",00:25:02.430,00:25:05.530
"So these random variables
have a probability",00:25:05.530,00:25:11.650
density which is this.,00:25:11.650,00:25:16.694
"This is f sub x of X. And they
have a distribution function",00:25:16.694,00:25:25.110
which is this.,00:25:25.110,00:25:28.150
"And they have a complementary
distribution",00:25:28.150,00:25:30.420
function which is this.,00:25:30.420,00:25:33.300
"Now, this is f of c.",00:25:33.300,00:25:35.940
This is f.,00:25:35.940,00:25:38.290
"So there's nothing
much to them.",00:25:38.290,00:25:40.080
"And now there's a theorem
which says that a random",00:25:42.780,00:25:47.770
"variable is memoryless if and
only if it is exponential.",00:25:47.770,00:25:52.200
"We just showed here that an
exponential random variable is",00:25:52.200,00:25:55.520
memoryless.,00:25:55.520,00:25:57.180
"To show it the other way
is almost obvious.",00:25:57.180,00:26:02.700
You take this definition here.,00:26:02.700,00:26:04.810
"You take the logarithm of
each of these sides.",00:26:04.810,00:26:08.600
"When you get the logarithm of
this, it says the logarithm of",00:26:08.600,00:26:12.850
"the probability x is greater
than p plus x is the logarithm",00:26:12.850,00:26:18.070
"of this plus the logarithm
of this.",00:26:18.070,00:26:21.920
"What we have to show to get an
exponential is that this",00:26:21.920,00:26:25.600
"logarithm is linear
in its argument t.",00:26:25.600,00:26:31.430
"Now, if you have this is equal
to the sum of this and this",00:26:31.430,00:26:36.990
"for all t and x, it's sort
of says it's linear.",00:26:36.990,00:26:41.220
"There's an exercise, I think
it's Exercise 2.4, which shows",00:26:41.220,00:26:47.235
"that you have to be a
little bit careful.",00:26:47.235,00:26:49.660
"Or at least as it points
out, very, very picky",00:26:49.660,00:26:53.310
"mathematicians have to be a
little bit careful with that.",00:26:53.310,00:26:57.050
"And you can worry about that
or not as you choose.",00:26:57.050,00:27:01.980
So that's the theorem.,00:27:01.980,00:27:03.620
"That's why Poisson processes
are special.",00:27:03.620,00:27:07.140
"And that's why we can
do all the things",00:27:07.140,00:27:08.900
we can do with them.,00:27:08.900,00:27:11.480
"The reason why we call it
memoryless is more apparent if",00:27:11.480,00:27:15.400
"we use conditional
probabilities.",00:27:15.400,00:27:17.670
"With conditional probabilities,
the probability",00:27:17.670,00:27:20.210
"that the random variable X is
greater than t plus x, given",00:27:20.210,00:27:25.390
"that it's greater than t, is
equal to the probability that",00:27:25.390,00:27:29.340
X is greater than x.,00:27:29.340,00:27:31.410
"If people in a checkout line
have exponential service times",00:27:31.410,00:27:35.530
"and you've waited 15 minutes for
the person in front, what",00:27:35.530,00:27:40.420
"is his or her remaining service
time, assuming the",00:27:40.420,00:27:44.130
service time is exponential?,00:27:44.130,00:27:46.760
What's the answer?,00:27:46.760,00:27:47.310
You've waited 15 minutes.,00:27:47.310,00:27:48.890
"Your original service time is
exponential with rate lambda.",00:27:48.890,00:27:53.460
"What's the remaining
service time?",00:27:53.460,00:27:55.990
"Well, the answer is
it's exponential.",00:27:55.990,00:27:58.560
"That's this memoryless
property.",00:27:58.560,00:28:00.700
"It's called memoryless because
the random variable doesn't",00:28:00.700,00:28:05.880
"remember how long it
hasn't happened.",00:28:05.880,00:28:09.330
"So you can think of an
exponential random variable as",00:28:11.880,00:28:15.190
"something which takes
place in time.",00:28:15.190,00:28:17.520
"And in each instant of time, it
might or might not happen.",00:28:17.520,00:28:21.000
"And if it hasn't happened yet,
there's still the same",00:28:21.000,00:28:24.110
"probability in every remaining
increment that it's going to",00:28:24.110,00:28:27.210
happen then.,00:28:27.210,00:28:28.310
"So you haven't gained anything
and you haven't lost anything",00:28:28.310,00:28:32.510
by having to wait this long.,00:28:32.510,00:28:34.280
"Here's an interesting question
which you can tie yourself in",00:28:37.320,00:28:41.120
knots for a little bit.,00:28:41.120,00:28:42.800
"Has your time waiting
been wasted?",00:28:42.800,00:28:44.835
"Namely the time you still have
to wait is exponential with",00:28:48.360,00:28:52.860
"the same rate as
it was before.",00:28:52.860,00:28:56.700
"So the expected amount of time
you have to wait is still the",00:28:56.700,00:29:00.070
"same as when you got into line
15 minutes ago with this one",00:29:00.070,00:29:07.790
"very slow person in
front of you.",00:29:07.790,00:29:11.320
So have you wasting your time?,00:29:11.320,00:29:14.400
"Well, you haven't
gained anything.",00:29:14.400,00:29:15.700
"But you haven't really wasted
your time either.",00:29:19.010,00:29:21.510
"Because if you have to get
served in that line, then at",00:29:21.510,00:29:27.210
"some point, you're going to
have to go in that line.",00:29:27.210,00:29:31.500
"And you might look for a time
when the line is very short.",00:29:31.500,00:29:34.370
"You might be lucky and find
a time when the line is",00:29:34.370,00:29:36.910
completely empty.,00:29:36.910,00:29:37.860
"And then you start getting
served right away.",00:29:37.860,00:29:40.360
"But if you ignore those issues,
then in fact, in a",00:29:40.360,00:29:47.760
"sense, you have wasted
your time.",00:29:47.760,00:29:51.590
"Another more interesting
question then is why do you",00:29:51.590,00:29:54.390
"move to another line if somebody
takes a long time?",00:29:54.390,00:29:58.410
"All of you have had
this experience.",00:29:58.410,00:30:00.160
You're in a supermarket.,00:30:00.160,00:30:03.170
"Or you're at an airplane counter
or any of the places",00:30:03.170,00:30:08.130
"where you have to wait
for service.",00:30:08.130,00:30:10.790
"There's somebody, one person in
front of you, who has been",00:30:10.790,00:30:14.200
there forever.,00:30:14.200,00:30:16.150
"And it seems as if they're going
to stay there forever.",00:30:16.150,00:30:18.990
"You notice another line
that only has one",00:30:18.990,00:30:21.200
person being served.,00:30:21.200,00:30:22.950
"And most of us, especially very
impatient people like me,",00:30:22.950,00:30:27.595
"I'm going to walk over and
get into that other line.",00:30:27.595,00:30:31.610
"And the question is, is that
rational or isn't it rational?",00:30:31.610,00:30:36.180
If the service times are,00:30:36.180,00:30:37.740
"exponential, it is not rational.",00:30:37.740,00:30:40.750
"It doesn't make any difference
whether I stay where I am or",00:30:40.750,00:30:44.030
go to the other line.,00:30:44.030,00:30:46.390
"If the service times are fixed
duration, namely suppose every",00:30:46.390,00:30:51.110
"service time takes 10 minutes
and I've waited for a long",00:30:51.110,00:30:55.190
"time, is it rational for me
to move to the other line?",00:30:55.190,00:30:59.410
"Absolutely not because I'm
almost at the end of that 10",00:30:59.410,00:31:03.840
minutes now.,00:31:03.840,00:31:05.190
And I'm about to be served.,00:31:05.190,00:31:08.160
So why do we move?,00:31:08.160,00:31:09.470
"Is it just psychology, that
we're very impatient?",00:31:09.470,00:31:14.020
I don't think so.,00:31:14.020,00:31:14.890
"I think it's because we have all
seen that an awful lot of",00:31:14.890,00:31:19.090
"lines, particularly airline
reservation lines, and if your",00:31:19.090,00:31:23.730
"plane doesn't fly or something,
and you're trying",00:31:23.730,00:31:25.950
"to get rescheduled, or any of
these things, the service time",00:31:25.950,00:31:31.960
"is worse than Poisson in the
sense that if you've waited",00:31:31.960,00:31:36.860
"for 10 minutes, your expected
remaining waiting time is",00:31:36.860,00:31:41.390
"greater than it was before
you started waiting.",00:31:41.390,00:31:44.400
"The longer you wait, the longer
your expected remaining",00:31:44.400,00:31:48.190
waiting time is.,00:31:48.190,00:31:49.685
"And that's called a heavy-tailed
distribution.",00:31:49.685,00:31:52.693
"What most of us have noticed, I
think, in our lives is that",00:31:56.000,00:31:59.500
"an awful lot of waiting lines
that human beings wait in are",00:31:59.500,00:32:03.390
in fact heavy-tailed.,00:32:03.390,00:32:05.140
"So that in fact is part of the
reason why we move if somebody",00:32:05.140,00:32:09.920
takes a long time.,00:32:09.920,00:32:11.780
"It's interesting to see
how the brain works.",00:32:11.780,00:32:15.520
"Because I'm sure that none
of you have ever really",00:32:15.520,00:32:19.080
"rationally analyzed this
question of why you move.",00:32:19.080,00:32:22.240
Have you?,00:32:22.240,00:32:23.390
"I mean, I have because
I teach probability",00:32:23.390,00:32:25.390
courses all the time.,00:32:25.390,00:32:27.760
"But I don't think anyone who
doesn't teach probability",00:32:27.760,00:32:30.420
"courses would be crazy enough
to waste their time on a",00:32:30.420,00:32:35.290
question like this.,00:32:35.290,00:32:37.240
"But your brain automatically
figures that out.",00:32:37.240,00:32:40.730
"I mean, your brain is smart
enough to know that if you've",00:32:40.730,00:32:43.500
"waited for a long time, you're
probably going to have to wait",00:32:43.500,00:32:46.300
for an even longer time.,00:32:46.300,00:32:48.510
"And it makes sense to move to
another line where your",00:32:48.510,00:32:51.570
"waiting time is probably
going to be shorter.",00:32:51.570,00:32:55.090
"So you're pretty smart if you
don't think about it too much.",00:32:55.090,00:32:59.460
"Here's an interesting theorem
now that makes use of this",00:33:01.980,00:33:05.980
memoryless property.,00:33:05.980,00:33:08.890
"This is Theorem 2.2.1
in the text.",00:33:08.890,00:33:12.720
"It's not stated terribly
well there.",00:33:12.720,00:33:14.990
"And I'll tell you why
in a little bit.",00:33:14.990,00:33:17.660
It's not stated too badly.,00:33:17.660,00:33:19.450
"I mean, it's stated correctly.",00:33:19.450,00:33:20.680
"But it's just a little hard to
understand what it says.",00:33:20.680,00:33:23.620
"If you have a Poisson process
of rate lambda and you're",00:33:23.620,00:33:27.460
"looking at any given time
t, here's t down here.",00:33:27.460,00:33:32.020
"You're looking at the
process of time t.",00:33:32.020,00:33:35.380
The interval z--,00:33:35.380,00:33:37.690
here's the interval z here--,00:33:37.690,00:33:39.580
"from t until the next arrival
has distribution e to the",00:33:39.580,00:33:50.760
minus lambda z.,00:33:50.760,00:33:52.080
"And it has this distribution
for all real numbers",00:33:52.080,00:33:57.930
greater than 0.,00:33:57.930,00:33:59.480
"The random variable Z is
independent of n of t.",00:33:59.480,00:34:04.880
"In other words, this random
variable here is independent",00:34:04.880,00:34:09.260
"of how many arrivals there
have been at time t.",00:34:09.260,00:34:14.080
"And given this, it's independent
of s sub n, which",00:34:14.080,00:34:20.540
"is the time at which the
last arrival occurred.",00:34:20.540,00:34:24.330
"Namely, here's n
of t equals 2.",00:34:24.330,00:34:27.429
Here's s of 2 at time tau.,00:34:27.429,00:34:30.870
"So given both n of t and s sub
2 in this case, or s sub n of",00:34:30.870,00:34:37.630
"t as we might call it, and
that's what gets confusing.",00:34:37.630,00:34:40.549
"And I'll talk about
that later.",00:34:40.549,00:34:42.980
"Given those two things, the
number n of arrivals in 0t--",00:34:42.980,00:34:48.380
"well, I got off.",00:34:48.380,00:34:51.900
"The random variable Z is
independent of n of t.",00:34:51.900,00:34:54.960
"And given n of t, Z is
independent of all of these",00:34:54.960,00:34:59.110
"arrival epochs up
until time t.",00:34:59.110,00:35:02.075
"And it's also independent of
n of t for all values of",00:35:02.075,00:35:07.800
tau up until t.,00:35:07.800,00:35:10.660
"That's what the theorem
states.",00:35:10.660,00:35:12.260
"What the theorem states is that
this memoryless property",00:35:12.260,00:35:16.120
"that we've just stated for
random variables is really a",00:35:16.120,00:35:19.890
"property of the Poisson
process.",00:35:19.890,00:35:23.470
"When we say that if a random
variable, it's a little hard",00:35:23.470,00:35:26.340
"to see why would anyone was
calling it memoryless.",00:35:26.340,00:35:30.080
"When you state it for a Poisson
process, it's very",00:35:30.080,00:35:33.390
"obvious why we want to
call it memoryless.",00:35:33.390,00:35:37.190
"It says that this time here from
t, from any arbitrary t,",00:35:37.190,00:35:41.850
"until the next arrival occurs,
that this is independent of",00:35:41.850,00:35:45.790
"all this junk that happens
before or up to time t.",00:35:45.790,00:35:52.380
That's what the theorem says.,00:35:52.380,00:35:54.570
"Here's a sort of a
half proof of it.",00:35:54.570,00:35:57.600
"There's a careful proof
in the notes.",00:35:57.600,00:35:59.780
"The statement in the notes
is not that careful,",00:35:59.780,00:36:01.800
but the proof is.,00:36:01.800,00:36:02.790
"And the proof is drawn
out perhaps too much.",00:36:02.790,00:36:09.010
"You can find your comfort level
between this and the",00:36:09.010,00:36:12.730
"much longer version
in the notes.",00:36:12.730,00:36:14.960
"You might understand
it well from this.",00:36:14.960,00:36:17.910
"Given n of t is equal to 2 in
this case, and in general,",00:36:17.910,00:36:21.430
"given that n of t is equal to
any constant n, and given that",00:36:21.430,00:36:26.410
"s sub 2 where this 2 is equal to
that 2, given that s sub 2",00:36:26.410,00:36:31.140
"is equal to tau, then x3, this
value here, the interarrival",00:36:31.140,00:36:38.010
"arrival time from this previous
arrival before t to",00:36:38.010,00:36:43.120
"the next arrival after t, namely
x3, is the thing which",00:36:43.120,00:36:48.470
"bridges across this time that
we selected, t. t is not a",00:36:48.470,00:36:54.160
random thing.,00:36:54.160,00:36:55.770
"t is just something you're
interested in.",00:36:55.770,00:36:58.700
"I want to catch a plane at 7
o'clock tomorrow evening.",00:36:58.700,00:37:02.550
"t then is 7 o'clock
tomorrow evening.",00:37:02.550,00:37:05.520
"What's the time from the last
plane that went out to New",00:37:05.520,00:37:09.080
"York until the next plane that's
going out to New York?",00:37:09.080,00:37:12.460
"If the planes are so screwed
up that the schedule means",00:37:12.460,00:37:16.510
"nothing, then they're just
flying out whenever",00:37:16.510,00:37:18.990
they can fly out.,00:37:18.990,00:37:21.920
"That's the meaning
of this x3 here.",00:37:21.920,00:37:25.410
"That says that x3, in fact,
has to be bigger",00:37:25.410,00:37:31.020
than t minus tau.,00:37:31.020,00:37:32.680
"If we're given that n of t is
equal to 2 and that the time",00:37:32.680,00:37:38.380
"of the previous arrival is at
tau, we're given that there",00:37:38.380,00:37:42.210
"haven't been any arrivals
between the last arrival",00:37:42.210,00:37:45.390
before t and t.,00:37:45.390,00:37:47.100
That's what we're given.,00:37:47.100,00:37:48.470
"This was the last arrival before
t by the assumption",00:37:48.470,00:37:52.060
we've made.,00:37:52.060,00:37:52.940
"So we're assuming there's
nothing in this interval.",00:37:52.940,00:37:56.450
"And then we're asking what is
the remaining time until x3 is",00:37:56.450,00:38:01.720
all finished.,00:38:01.720,00:38:02.840
"And that's the random variable
that we call Z. So Z is x3",00:38:02.840,00:38:06.850
minus t minus tau.,00:38:06.850,00:38:09.270
"The complementary distribution
function of Z conditional on",00:38:09.270,00:38:14.260
"both n and on s, this n here
and this s here is then",00:38:14.260,00:38:20.640
"exponential with e to
the minus lambda Z.",00:38:20.640,00:38:24.580
"Now, if I know that this is
exponential, what can I say",00:38:24.580,00:38:29.740
"about the random variable
Z itself?",00:38:29.740,00:38:31.800
"Well, there's an easy way to
find the distribution of Z",00:38:34.510,00:38:39.100
"when you know Z conditional
onto other things.",00:38:39.100,00:38:42.315
"You take what the distribution
is conditional on, each value",00:38:48.200,00:38:51.910
of n and s.,00:38:51.910,00:38:53.600
"You then multiply that by the
probability that n and s have",00:38:53.600,00:38:58.510
those particular values.,00:38:58.510,00:39:00.120
And then you integrate.,00:39:00.120,00:39:03.120
"Now, we can look at this and
say we don't have to go",00:39:03.120,00:39:06.830
through all of that.,00:39:06.830,00:39:08.260
"And in fact, we won't know what
the distribution of n is.",00:39:08.260,00:39:11.720
"And we certainly won't know what
the distribution of this",00:39:11.720,00:39:14.670
"previous arrival is for
quite a long time.",00:39:14.670,00:39:18.760
"Why don't we need
to know that?",00:39:18.760,00:39:21.310
"Well, because we know that
whatever n of t is and",00:39:21.310,00:39:26.590
"whatever s sub n of t is doesn't
make any difference.",00:39:26.590,00:39:31.185
"The distribution of Z is
still the same thing.",00:39:31.185,00:39:35.170
"So we know this has to be the
unconditional distribution",00:39:35.170,00:39:39.620
"function of Z also even without
knowing anything about",00:39:39.620,00:39:43.880
n or knowing about s.,00:39:43.880,00:39:47.020
"And that means that the
complementary distribution",00:39:47.020,00:39:51.930
"function of Z is equal to e to
the minus lambda Z also.",00:39:51.930,00:39:57.790
"So that's sort of a proof if you
want to be really picky.",00:39:57.790,00:40:03.210
"And I would suggest you
try to be picky.",00:40:03.210,00:40:05.870
"When you read the notes, try to
understand why one has to",00:40:05.870,00:40:09.640
"say a little more than
one says here.",00:40:09.640,00:40:12.260
"Because that's the
way you really",00:40:12.260,00:40:13.620
understand these things.,00:40:13.620,00:40:15.720
"But this really gives you
the idea of the proof.",00:40:15.720,00:40:18.630
"And it's pretty close
to a complete proof.",00:40:18.630,00:40:20.630
"This is saying what
we just said.",00:40:24.830,00:40:26.430
"The conditional distribution
of Z doesn't vary with the",00:40:26.430,00:40:30.380
conditioning values.,00:40:30.380,00:40:33.050
n of t equals n.,00:40:33.050,00:40:35.240
And s sub n equals tau.,00:40:35.240,00:40:37.150
"So Z is statistically
independent of n of t and s",00:40:37.150,00:40:42.040
sub n of t.,00:40:42.040,00:40:43.550
"You should look at the text
again, as I said, for more",00:40:43.550,00:40:47.890
careful proof of that.,00:40:47.890,00:40:49.880
"What is this random variable
s sub n of t?",00:40:49.880,00:40:53.940
"It's clear from the picture
what it is.",00:40:53.940,00:40:57.640
"s sub n of t is the last
arrival before",00:40:57.640,00:41:04.400
we're at time t.,00:41:04.400,00:41:07.580
"That's what it is in
the picture here.",00:41:07.580,00:41:10.970
"How do you define a random
variable like that?",00:41:10.970,00:41:13.540
"There's a temptation to
do it the following",00:41:17.080,00:41:21.700
way which is incorrect.,00:41:21.700,00:41:24.170
"There's a temptation to say,
well, conditional on n of t,",00:41:24.170,00:41:28.710
suppose n of t is equal to n.,00:41:28.710,00:41:31.500
"Let me then find the
distribution of s sub n.",00:41:31.500,00:41:36.730
"And that's not the right
way to do it.",00:41:36.730,00:41:39.510
"Because s sub n of t and n
of t are certainly not",00:41:39.510,00:41:44.500
independent.,00:41:44.500,00:41:45.510
"n of t tells you what random
variable you want to look at.",00:41:45.510,00:41:48.590
"How do you define a random
variable in terms of a mapping",00:41:52.110,00:41:55.750
"from the sample space omega onto
the set of real numbers?",00:41:55.750,00:42:02.520
"So what you do here is you look
at a sample point omega.",00:42:02.520,00:42:07.040
"It maps into this random
variable n of t, the sample",00:42:07.040,00:42:12.200
"value of that at omega,
that's sum value n.",00:42:12.200,00:42:16.520
"And then you map that same
sample point into--",00:42:16.520,00:42:22.210
"now, you know which random
variable it is",00:42:22.210,00:42:24.520
you're looking at.,00:42:24.520,00:42:25.660
"You take that same omega and
map it into sub time tau.",00:42:25.660,00:42:30.160
"So that's what we mean
by s sub n of t.",00:42:30.160,00:42:34.740
"If your mind glazes over at
that, don't worry about it.",00:42:34.740,00:42:38.800
"Think about it a
little bit now.",00:42:38.800,00:42:40.650
"Come back and think
about it later.",00:42:40.650,00:42:42.900
"Every time I don't think about
this for two weeks, my mind",00:42:42.900,00:42:46.220
glazes over when I look at it.,00:42:46.220,00:42:47.910
"And I have to think very hard
about what this very peculiar",00:42:47.910,00:42:51.310
looking random variable is.,00:42:51.310,00:42:53.610
"When I have a random variable
where I have a sequence of",00:42:53.610,00:42:58.660
"random variables, and I have a
random variable which is a",00:42:58.660,00:43:02.460
"random selection among those
random variables, it's a very",00:43:02.460,00:43:06.270
complicated animal.,00:43:06.270,00:43:08.180
And that's what this is.,00:43:08.180,00:43:09.977
"But we've just said
what it is.",00:43:09.977,00:43:12.990
"So you can think about
it as you go.",00:43:12.990,00:43:18.760
"The theorem essentially
extends the idea of",00:43:18.760,00:43:20.600
"memorylessness to the entire
Poisson process.",00:43:20.600,00:43:23.340
"In other words, this says that
a Poisson process is",00:43:23.340,00:43:26.390
memoryless.,00:43:26.390,00:43:27.910
"You look at a particular
time t.",00:43:27.910,00:43:30.440
"And the time until the next
arrival is independent of",00:43:30.440,00:43:33.980
"everything that's going
before that.",00:43:33.980,00:43:35.780
"Starting at any time tau,
yeah, well, subsequent",00:43:40.490,00:43:43.530
"interrarrival times are
independent of Z",00:43:43.530,00:43:47.360
and also of the past.,00:43:47.360,00:43:50.150
"I'm waving my hands
a little bit here.",00:43:50.150,00:43:53.420
"But in fact, what I'm
saying is right.",00:43:53.420,00:43:55.370
"We have these interarrival
intervals that we know are",00:43:55.370,00:43:58.930
independent.,00:43:58.930,00:44:00.130
"The interarrival intervals which
have occurred completely",00:44:00.130,00:44:04.090
"before time t are independent of
this random variable Z. The",00:44:04.090,00:44:10.400
"next interarrival interval after
Z is independent of all",00:44:10.400,00:44:14.760
"the interarrival intervals
before that.",00:44:14.760,00:44:17.370
"And those interarrival intervals
before that are",00:44:17.370,00:44:25.550
"determined by the counting
process up until time t.",00:44:25.550,00:44:30.580
"So the counting process
corresponds to this",00:44:30.580,00:44:33.230
"corresponding interarrival
process.",00:44:33.230,00:44:37.020
"It's n of t prime minus n of t
for t prime greater than t.",00:44:37.020,00:44:41.830
"In other words, we now want to
look at a counting process",00:44:41.830,00:44:44.520
"which starts at time t and
follows whatever it has to",00:44:44.520,00:44:49.460
"follow from this original
counting process.",00:44:49.460,00:44:52.960
"And what we're saying is this
first arrival and this process",00:44:52.960,00:44:56.800
"starting at time t is
independent of everything that",00:44:56.800,00:45:00.740
went before.,00:45:00.740,00:45:02.100
"And every subsequent
interarrival time after that",00:45:02.100,00:45:05.960
"is independent of everything
before time t.",00:45:05.960,00:45:10.070
"So this says that the process n
of t prime minus n of t as a",00:45:10.070,00:45:15.420
process nt prime.,00:45:15.420,00:45:17.270
"This is a counting process nt
prime defined for t prime",00:45:17.270,00:45:21.560
greater than t.,00:45:21.560,00:45:22.590
"So for fixed t, we now have
something which we can view",00:45:22.590,00:45:27.870
"over variable t prime as
a counting process.",00:45:27.870,00:45:31.670
"It's a Poisson process shifted
to start at time t, ie, for",00:45:31.670,00:45:36.050
"each t prime, n of t prime minus
the n of t has the same",00:45:36.050,00:45:40.520
"distribution as n of
t prime minus t.",00:45:40.520,00:45:45.230
Same for joint distributions.,00:45:45.230,00:45:47.040
"In other words, this
random variable Z",00:45:47.040,00:45:49.740
is exponential again.,00:45:49.740,00:45:50.710
"And all the future interarrival
times are",00:45:50.710,00:45:53.960
exponential.,00:45:53.960,00:45:55.000
"So it's defined in exactly the
same way as the original",00:45:55.000,00:45:58.940
random process is.,00:45:58.940,00:46:01.100
"So it's statistically
the same process.",00:46:01.100,00:46:03.155
"Which says two things
about it.",00:46:05.770,00:46:08.390
Everything is the same.,00:46:08.390,00:46:09.810
And everything is independent.,00:46:09.810,00:46:12.890
We will call that stationary.,00:46:12.890,00:46:15.560
Everything is the same.,00:46:15.560,00:46:17.140
"And independent, everything
is independent.",00:46:17.140,00:46:20.400
"And then we'll try to sort out
how things can be the same but",00:46:20.400,00:46:23.310
also be independent.,00:46:23.310,00:46:25.750
"Oh, we already know that.",00:46:25.750,00:46:27.850
"We have two IID random
variables, x1 and x2.",00:46:27.850,00:46:33.010
They're IID.,00:46:33.010,00:46:34.250
"They're independent and
identically distributed.",00:46:34.250,00:46:36.890
"Identity distributed means
that in one sense,",00:46:36.890,00:46:39.330
they are the same.,00:46:39.330,00:46:40.870
"But they're also independent
of each other.",00:46:40.870,00:46:43.630
"So the random variables are
defined in the same way.",00:46:43.630,00:46:48.080
"And in that sense, they're
stationary.",00:46:48.080,00:46:50.180
"But they're independent of each
other by the definition",00:46:50.180,00:46:53.160
of independence.,00:46:53.160,00:46:55.390
"So our new process is
independent of the old process",00:46:55.390,00:47:00.570
in the interval 0 up to t.,00:47:00.570,00:47:08.950
"When we're talking about Poisson
processes and also",00:47:08.950,00:47:11.750
"arrival processes, we always
talk about intervals which are",00:47:11.750,00:47:17.350
"open on the left and closed
on the right.",00:47:17.350,00:47:21.460
That's completely arbitrary.,00:47:21.460,00:47:23.920
"But if you don't make one
convention or the other, you",00:47:23.920,00:47:27.250
"could make them closed on the
left and open on the right,",00:47:27.250,00:47:31.940
"and that would be
consistent also.",00:47:31.940,00:47:34.830
But nobody does.,00:47:34.830,00:47:35.675
"And it would be much
more confusing.",00:47:35.675,00:47:38.210
"So it's much easier to make
things closed on the right.",00:47:38.210,00:47:42.870
"So we're up to stationary and
independent increments.",00:47:47.280,00:47:50.820
"Well, we're not up to there.",00:47:50.820,00:47:52.990
"We're almost finished
with that.",00:47:52.990,00:47:54.960
"We've virtually already said
that increments are stationary",00:47:54.960,00:47:59.340
and independent.,00:47:59.340,00:48:00.000
"And an increment is just a piece
of a Poisson process.",00:48:00.000,00:48:04.560
"That's an increment,
a piece of it.",00:48:04.560,00:48:06.220
"So a counting process has the
stationary increment property",00:48:09.620,00:48:16.070
"if n of t prime minus n of t has
the same distribution as n",00:48:16.070,00:48:21.493
"of t prime minus t for all
t prime greater than t",00:48:21.493,00:48:26.130
greater than 0.,00:48:26.130,00:48:28.060
"In other words, you look at
this counting process.",00:48:28.060,00:48:30.950
Goes up.,00:48:30.950,00:48:32.740
"Then you start at some
particular value of t.",00:48:32.740,00:48:35.970
Let me draw a picture of that.,00:48:35.970,00:48:38.300
Make it a little clearer.,00:48:38.300,00:48:39.550
"And the new Poisson process
starts at this value and goes",00:49:00.330,00:49:05.960
up from there.,00:49:05.960,00:49:08.360
"So this thing here is
what we call n of t",00:49:08.360,00:49:13.970
prime minus n of t.,00:49:13.970,00:49:18.000
Because here's n of t.,00:49:18.000,00:49:19.520
"Here's t prime out here for
any value out here.",00:49:22.190,00:49:26.560
"And we're looking at the number
of arrivals up until",00:49:26.560,00:49:29.880
time t prime.,00:49:29.880,00:49:31.410
"And what we're talking about,
when we're talking about n of",00:49:31.410,00:49:34.442
"t prime minus n of t, we're
talking about what happens in",00:49:34.442,00:49:40.110
this region here.,00:49:40.110,00:49:43.210
"And we're saying that this is
a Poisson process again.",00:49:43.210,00:49:48.010
"And now in a minute, we're going
to say that this Poisson",00:49:48.010,00:49:50.530
"process is independent of what
happened up until time t.",00:49:50.530,00:49:57.720
But Poisson processes have this,00:49:57.720,00:49:59.460
stationary increment property.,00:49:59.460,00:50:02.540
"And a counting process has the
independent increment property",00:50:02.540,00:50:08.930
"if for every sequence of times,
t1, t2, up to t sub n.",00:50:08.930,00:50:14.970
"The random variables n of t1 and
tilde of t1, t2, we didn't",00:50:14.970,00:50:22.830
talk about that.,00:50:22.830,00:50:23.830
"But I think it's defined
on one of those slides.",00:50:23.830,00:50:27.750
"n of t and t prime is defined as
n of t prime minus n of t.",00:50:27.750,00:50:41.480
"So n of t and t prime is really
the number of arrivals",00:50:44.830,00:50:49.740
"that have occurred from
t up until t prime--",00:50:49.740,00:50:55.360
"open on t, closed on t prime.",00:50:55.360,00:50:59.370
"So a counting process has the
independent increment property",00:50:59.370,00:51:07.410
"if for every finite set of
times, these random variables",00:51:07.410,00:51:11.310
here are independent.,00:51:11.310,00:51:13.920
"The number of arrivals in the
first increment, number of",00:51:13.920,00:51:16.860
"arrivals in the second
increment, number of arrivals",00:51:16.860,00:51:19.860
"in the third increment, no
matter how you choose t1, t2,",00:51:19.860,00:51:23.320
"up to t sub n, what happens here
is independent of what",00:51:23.320,00:51:27.490
"happens here, is independent
of what happens",00:51:27.490,00:51:29.600
"here, and so forth.",00:51:29.600,00:51:31.790
"It's not only that
what happens in",00:51:31.790,00:51:33.270
Las Vegas stays there.,00:51:33.270,00:51:35.540
"It's that what happens in
Boston stays there, what",00:51:35.540,00:51:38.200
"happens in Philadelphia stays
there, and so forth.",00:51:38.200,00:51:41.050
"What happens anywhere
stays anywhere.",00:51:41.050,00:51:43.740
It never gets out of there.,00:51:43.740,00:51:45.850
"That's what we mean by
independence in this case.",00:51:45.850,00:51:48.170
So it's a strong statement.,00:51:48.170,00:51:51.710
"But we've essentially said that
Poisson processes have",00:51:51.710,00:51:55.000
that property.,00:51:55.000,00:51:57.410
"So this property implies is the
number of arrivals in each",00:51:57.410,00:52:00.540
"of the set of non-overlapping
intervals are independent",00:52:00.540,00:52:04.640
random variables.,00:52:04.640,00:52:06.270
"For a Poisson process, we've
seen that the number of",00:52:06.270,00:52:18.010
"arrivals in t sub i minus 1 to t
sub i is independent of this",00:52:18.010,00:52:23.450
"whole set of random
variables here.",00:52:23.450,00:52:26.960
"Now, remember that when we're
talking about multiple random",00:52:26.960,00:52:30.520
"variables, we say that multiple
random variables are",00:52:30.520,00:52:34.010
independent.,00:52:34.010,00:52:35.400
"It's not enough to be pairwise
independent.",00:52:35.400,00:52:37.740
"They all have to
be independent.",00:52:37.740,00:52:40.080
"But this thing we've just said
says that this is independent",00:52:40.080,00:52:44.260
of all of these things.,00:52:44.260,00:52:46.560
"If this is independent of all of
these things, and then the",00:52:46.560,00:52:50.260
"next interval n of ti, ti plus
1, is independent of",00:52:50.260,00:52:55.610
"everything in the past, and so
forth all the way up, then all",00:52:55.610,00:52:59.290
"of those random variables are
statistically independent of",00:52:59.290,00:53:02.870
each other.,00:53:02.870,00:53:03.710
"So in fact, we're saying more
than pairwise statistical",00:53:03.710,00:53:08.130
independence.,00:53:08.130,00:53:10.790
"If you're panicking about
these minor differences",00:53:10.790,00:53:14.970
"between pairwise independence
and real independence, don't",00:53:14.970,00:53:19.860
worry about it too much.,00:53:19.860,00:53:21.240
"Because the situations
where that happens",00:53:21.240,00:53:24.430
are relatively rare.,00:53:24.430,00:53:26.240
"They don't happen
all the time.",00:53:26.240,00:53:28.030
"But they do happen
occasionally.",00:53:28.030,00:53:29.720
So you should be aware of it.,00:53:29.720,00:53:33.157
"You shouldn't get in
a panic about it.",00:53:33.157,00:53:35.100
"Because normally, you don't
have to worry about it.",00:53:35.100,00:53:38.200
"In other words, when you're
taking a quiz, don't worry",00:53:38.200,00:53:42.240
about any of the fine points.,00:53:42.240,00:53:45.520
"Figure out roughly how
to do the problems.",00:53:45.520,00:53:49.060
Do them more or less.,00:53:49.060,00:53:51.620
"And then come back and deal with
the fine points later.",00:53:51.620,00:53:56.010
"Don't spend the whole quiz time
wrapped up on one little",00:53:56.010,00:53:59.430
"fine point and not get
to anything else.",00:53:59.430,00:54:03.370
"One of the important things to
learn in understanding a",00:54:03.370,00:54:06.340
"subject like this is to figure
out what are the fine points,",00:54:06.340,00:54:10.380
what are the important points.,00:54:10.380,00:54:12.370
"How do you tell whether
something is important in a",00:54:12.370,00:54:14.740
particular context.,00:54:14.740,00:54:16.000
And that just takes intuition.,00:54:16.000,00:54:19.870
"That takes some intuition from
working with these processes.",00:54:19.870,00:54:23.730
"And you pick that
up as you go.",00:54:23.730,00:54:26.970
"But anyway, we wind up now
with the statement that",00:54:26.970,00:54:30.840
"Poisson processes have
stationary and independent",00:54:30.840,00:54:34.440
increments.,00:54:34.440,00:54:35.070
"Which means that what happens
in each interval is",00:54:35.070,00:54:37.730
"independent of what happens
in each other interval.",00:54:37.730,00:54:42.800
"So we're done with that until
we get to alternate",00:54:42.800,00:54:47.390
"definitions of a Poisson
process.",00:54:47.390,00:54:50.045
"And we now want to deal with
the Erlang and the Poisson",00:54:50.045,00:54:54.590
"distributions, which are just
very plug and chug kinds of",00:54:54.590,00:55:00.400
things to a certain extent.,00:55:00.400,00:55:03.700
"For a Poisson process of rate
lambda, the density function",00:55:03.700,00:55:09.695
"of arrival epoch s2, s2 is
the sum of x1 plus x2.",00:55:09.695,00:55:17.130
"x1 is an exponential random
variable of rate lambda.",00:55:17.130,00:55:20.730
"x2 is an independent random
variable of rate lambda.",00:55:20.730,00:55:26.420
"How do you find the probability
density function",00:55:26.420,00:55:31.470
"as a sum of two independent
random variables, which both",00:55:31.470,00:55:34.470
have a density?,00:55:34.470,00:55:35.810
You convolve them.,00:55:35.810,00:55:37.930
"That's something that you've
known ever since you studied",00:55:37.930,00:55:43.610
"any kind of linear systems, or
from any probability, or",00:55:43.610,00:55:47.170
anything else.,00:55:47.170,00:55:47.970
"Convolution is the way to
solve this problem.",00:55:47.970,00:55:51.050
"When you convolve these
two random variables,",00:55:51.050,00:55:54.670
here I've done it.,00:55:54.670,00:55:56.400
"You get lambda squared t times
e to the minus lambda t.",00:55:56.400,00:56:00.322
"This kind of form here with an
e to the minus lambda t, and",00:56:03.040,00:56:07.400
"with a t, or t squared, or so
forth, is a particularly easy",00:56:07.400,00:56:11.430
form to integrate.,00:56:11.430,00:56:13.500
"So we just do this
again and again.",00:56:13.500,00:56:16.790
"And when we do it again and
again, we find out that the",00:56:16.790,00:56:19.400
"density function as a sum of n
of these random variables, you",00:56:19.400,00:56:25.010
"keep picking up an extra lambda
every time you convolve",00:56:25.010,00:56:27.840
"in another exponential
random variable.",00:56:27.840,00:56:31.860
"You pick up an extra factor of
t whenever you do this again.",00:56:31.860,00:56:36.450
"This stays the same
as it does here.",00:56:36.450,00:56:39.770
"And strangely enough, this n
minus 1 factorial appears down",00:56:39.770,00:56:45.480
"here when you start integrating
something with",00:56:45.480,00:56:51.770
some power of t in it.,00:56:51.770,00:56:54.315
"So when you integrate this,
this is what you get.",00:56:54.315,00:56:57.030
"And it's called the
Erlang density.",00:56:57.030,00:57:01.030
Any questions about this?,00:57:01.030,00:57:02.230
"Or any questions
about anything?",00:57:02.230,00:57:04.620
I'm getting hoarse.,00:57:08.650,00:57:09.420
I need questions.,00:57:09.420,00:57:10.310
[LAUGHS],00:57:10.310,00:57:14.860
"There's nothing much to
worry about there.",00:57:14.860,00:57:19.380
"But now, we want to stop and
smell the roses while doing",00:57:19.380,00:57:21.900
all this computation.,00:57:21.900,00:57:24.710
"Let's do this a slightly
different way.",00:57:24.710,00:57:27.640
"The joint density of x1 up to
x sub n is lambda x1 times e",00:57:27.640,00:57:40.100
"to the minus lambda x1, times
lambda x2, times e to the",00:57:40.100,00:57:44.850
"minus lambda x2, and so forth.",00:57:44.850,00:57:48.270
So excuse me.,00:57:48.270,00:57:51.180
"The probability density of an
exponential random variable is",00:57:51.180,00:57:54.880
"lambda times e to the
minus lambda x.",00:57:54.880,00:57:57.950
"So the joint density is lambda
e to the minus lambda x1.",00:57:57.950,00:58:06.900
"I told you I was
getting hoarse.",00:58:06.900,00:58:08.270
And my mind is getting hoarse.,00:58:08.270,00:58:09.580
"So you better start asking
some questions or I will",00:58:09.580,00:58:14.280
"evolve into meaningless
chatter.",00:58:14.280,00:58:16.480
"And this is just lambda to the
n times e to the minus lambda",00:58:20.490,00:58:26.770
"times the summation of x sub
i from i equals 1 to n.",00:58:26.770,00:58:34.840
"Now, that's sort of interesting
because this joint",00:58:34.840,00:58:37.160
"density is just this
simple-minded thing.",00:58:37.160,00:58:43.710
"You can write it as lambda to
the n times e to the minus",00:58:43.710,00:58:47.010
"lambda s sub n, where
s sub n is the",00:58:47.010,00:58:50.730
time of the n-th arrival.,00:58:50.730,00:58:53.610
"This says that the joint
distribution of all of these",00:58:53.610,00:58:57.910
"interarrival times only
depends on when the",00:58:57.910,00:59:01.610
last one comes in.,00:59:01.610,00:59:04.310
"And you can transform that to a
joint density on each of the",00:59:04.310,00:59:09.750
"arrival epochs as lambda to
the n times e to the minus",00:59:09.750,00:59:14.900
lambda s sub n.,00:59:14.900,00:59:17.290
Is this obvious to everyone?,00:59:17.290,00:59:18.540
You're lying.,00:59:21.620,00:59:22.990
"If you're not shaking your
head, you're lying.",00:59:22.990,00:59:26.700
"Because it's not
obvious at all.",00:59:26.700,00:59:29.350
"What we're doing here, it's sort
of obvious if you look at",00:59:29.350,00:59:35.110
the picture.,00:59:35.110,00:59:37.040
"It's not obvious when you
do the mathematics.",00:59:37.040,00:59:40.420
"What the picture says
is-- let me see if I",00:59:40.420,00:59:44.280
find the picture again.,00:59:44.280,00:59:45.530
OK.,00:59:52.930,00:59:53.720
Here's the picture up here.,00:59:53.720,00:59:55.800
"We're looking at these
interarrival intervals.",00:59:55.800,00:59:58.953
"I think it'll be clearer if
I draw it a different way.",01:00:01.890,01:00:04.450
There we go.,01:00:07.820,01:00:09.070
"Let's just draw this
in a line.",01:00:14.360,01:00:17.410
Here's 0.,01:00:17.410,01:00:19.590
Here's s1.,01:00:19.590,01:00:21.970
Here's s2.,01:00:21.970,01:00:24.360
Here's s3.,01:00:24.360,01:00:26.826
And here's s4.,01:00:26.826,01:00:28.076
And here's x1.,01:00:30.790,01:00:33.970
Here's x2.,01:00:33.970,01:00:35.220
Here's x3.,01:00:41.130,01:00:42.380
And here's x4.,01:00:44.726,01:00:45.976
"Now, what we're talking about,
we can go from the density of",01:00:49.930,01:00:53.370
"each of these intervals to the
density of each of these sums",01:00:53.370,01:01:01.150
"in a fairly straightforward
way.",01:01:01.150,01:01:02.980
"If you write this all out as a
density, what you find is that",01:01:02.980,01:01:12.530
"in making a transformation
from the density of these",01:01:12.530,01:01:15.780
"interarrival intervals to the
density of these, what you're",01:01:15.780,01:01:21.230
"essentially doing is taking this
density and multiplying",01:01:21.230,01:01:25.170
it by a matrix.,01:01:25.170,01:01:27.900
"And the matrix is a diagonal
matrix, is an",01:01:27.900,01:01:33.770
upper triangular matrix.,01:01:33.770,01:01:35.840
"Because this depends
only on this.",01:01:35.840,01:01:38.490
"This depends only on
this and this.",01:01:38.490,01:01:40.680
"This depends only on
this and this.",01:01:40.680,01:01:42.930
"This depends only on
each of these.",01:01:42.930,01:01:45.620
"So it's a triangular matrix with
terms on the diagonal.",01:01:45.620,01:01:50.670
"And when you look at a matrix
like that, the terms on the",01:01:50.670,01:01:53.300
"diagonal are 1s because what's
getting added each time is 1",01:01:53.300,01:01:57.720
times a new variable.,01:01:57.720,01:01:58.900
"So we have a matrix with 1s on
the main diagonal and other",01:01:58.900,01:02:03.430
stuff above that.,01:02:03.430,01:02:06.250
"And what that means is that
when you make this",01:02:06.250,01:02:09.000
"transformation in densities,
the determinant of",01:02:09.000,01:02:11.770
that matrix is 1.,01:02:11.770,01:02:13.550
"And the value that you then
get when you go from the",01:02:13.550,01:02:17.400
"density of these to the density
of these, it's a",01:02:17.400,01:02:24.420
uniform density again.,01:02:24.420,01:02:26.160
"So in fact, it has to
look like what we",01:02:26.160,01:02:29.760
said it looks like.,01:02:29.760,01:02:32.530
So I was kidding you there.,01:02:32.530,01:02:34.300
"It's not so obvious how to
do that although it looks",01:02:34.300,01:02:39.310
reasonable.,01:02:39.310,01:02:40.560
AUDIENCE: [INAUDIBLE].,01:02:50.195,01:02:51.180
PROFESSOR: Yeah.,01:02:51.180,01:02:51.870
AUDIENCE: I'm sorry.,01:02:51.870,01:02:53.380
"Is it also valid to make an
argument based on symmetry?",01:02:53.380,01:02:58.140
PROFESSOR: It will be later.,01:02:58.140,01:03:01.700
"The symmetry is not
clear here yet.",01:03:01.700,01:03:04.670
"I mean, the symmetry isn't
clear because you're",01:03:04.670,01:03:06.570
starting at time 0.,01:03:06.570,01:03:08.996
"And because you're starting
at time 0, you don't have",01:03:08.996,01:03:14.050
symmetry here yet.,01:03:14.050,01:03:16.530
"If we started at time 0 and we
ended at some time t, we could",01:03:16.530,01:03:20.750
"try to claim there is some
kind of symmetry between",01:03:20.750,01:03:23.120
"everything that happened
in the middle.",01:03:23.120,01:03:25.010
"And we'll try to
do that later.",01:03:25.010,01:03:27.880
"But at the moment, we would get
into even more trouble if",01:03:27.880,01:03:33.370
we try to do it by symmetry.,01:03:33.370,01:03:34.620
"But anyway, what this is saying
is that this joint",01:03:41.220,01:03:44.440
density is really--,01:03:44.440,01:03:48.310
"if you know where this point is,
the joint density of all",01:03:48.310,01:03:51.740
"of these things remains the same
no matter how you move",01:03:51.740,01:03:55.570
these things around.,01:03:55.570,01:03:57.450
"If I move s1 around a little
bit, it means that x1 gets a",01:03:57.450,01:04:01.210
"little smaller, x2 gets
a little bit bigger.",01:04:01.210,01:04:04.500
"And if you look at the joint
density there, the joint",01:04:04.500,01:04:07.200
"density stays absolutely the
same because you have e to the",01:04:07.200,01:04:11.120
"minus lambda x1 times e to
the minus lambda x2.",01:04:11.120,01:04:15.240
"And the sum of the two for a
fixed value here is the same",01:04:15.240,01:04:17.980
as it was before.,01:04:17.980,01:04:19.370
"So you can move all of these
things around in any",01:04:19.370,01:04:22.120
way you want to.,01:04:22.120,01:04:23.610
"And the joint density depends
only on the last one.",01:04:23.610,01:04:28.760
"And that's a very strange
property and it's a very",01:04:28.760,01:04:30.600
interesting property.,01:04:30.600,01:04:32.910
"And it sort of is the same as
this independent increment",01:04:32.910,01:04:36.200
"property that we've been
talking about.",01:04:36.200,01:04:37.960
"But we'll see why that
is in just a minute.",01:04:37.960,01:04:41.790
"But anyway, once we have that
property, we can then",01:04:41.790,01:04:46.280
"integrate this over the volume
of s1, s2, s3, and s4, over",01:04:46.280,01:04:55.480
"that volume which has the
property that it stops at that",01:04:55.480,01:05:01.310
one particular point there.,01:05:01.310,01:05:03.480
"And we do that integration
subject to the fact that s3",01:05:03.480,01:05:07.490
"has to be less than or equal to
s4, s2 has to be less than",01:05:07.490,01:05:11.490
"or equal to s3, and
so forth down.",01:05:11.490,01:05:14.790
"When you do that integration,
you get exactly the same thing",01:05:14.790,01:05:18.360
"as you got before when you
did the integration.",01:05:18.360,01:05:20.540
"The integration that you did
before was essentially doing",01:05:20.540,01:05:23.420
"this, if you look at what
you did before.",01:05:23.420,01:05:26.180
"You were taking lambda times e
to the minus lambda x times",01:05:29.700,01:05:35.700
lambda times t minus x.,01:05:35.700,01:05:38.650
"And the x doesn't make
any difference here.",01:05:38.650,01:05:41.390
The x cancels out.,01:05:41.390,01:05:43.340
"That's exactly what's
going on.",01:05:43.340,01:05:45.410
"And if you do it in terms of s1
and s2, the s1 cancels out.",01:05:45.410,01:05:50.185
The s1 is the same as x here.,01:05:50.185,01:05:52.330
"So there is that cancellation
here.",01:05:52.330,01:05:54.810
"And therefore, this Erlang
density is just a marginal",01:05:54.810,01:05:59.090
"distribution of a very
interesting joint",01:05:59.090,01:06:02.100
"distribution, which depends
only on the last term.",01:06:02.100,01:06:04.670
"So next, we have a theorem
which says for a Poisson",01:06:09.040,01:06:14.760
"process, the PMF for n of t, the
Probability Mass Function,",01:06:14.760,01:06:21.250
is the Poisson PMF.,01:06:21.250,01:06:23.360
"It sounds like I'm not really
saying anything because what",01:06:23.360,01:06:26.710
else would it be?,01:06:26.710,01:06:28.640
"Because you've always heard that
the Poisson PMF is this",01:06:28.640,01:06:35.320
particular function here.,01:06:35.320,01:06:37.750
"Well, in fact, there's
some reason for that.",01:06:37.750,01:06:40.960
"And in fact, if we want to say
that a Poisson process is",01:06:40.960,01:06:44.180
"defined in terms of these
exponential interarrival",01:06:44.180,01:06:49.000
"times, then we have to
show that this is",01:06:49.000,01:06:50.960
consistent with that.,01:06:50.960,01:06:54.320
"The way I'll prove that
here, this is a",01:06:54.320,01:06:58.370
little more than a PF.,01:06:58.370,01:07:01.330
"Maybe we should say it's a
P-R-O-F. Leave out the double",01:07:01.330,01:07:05.445
"O because it's not
quite complete.",01:07:05.445,01:07:08.980
"But what we want to do is to
calculate the probability that",01:07:08.980,01:07:13.350
"the n plus first arrival occurs
sometime between t and",01:07:13.350,01:07:18.770
t plus delta.,01:07:18.770,01:07:21.770
"And we'll do it in two
different ways.",01:07:21.770,01:07:24.160
"And one way involves the
probability mass function for",01:07:24.160,01:07:28.220
the Poisson.,01:07:28.220,01:07:29.440
"The other way involves
the Erlang density.",01:07:29.440,01:07:33.730
"And since we already know the
Erlang density, we can use",01:07:33.730,01:07:36.630
"that to get the PMF
for n of t.",01:07:36.630,01:07:39.850
"So using the Erlang density,
the probability that the n",01:07:42.480,01:07:47.930
"plus first arrival falls in
this little tiny interval",01:07:47.930,01:07:51.730
"here, we're thinking of
delta as being small.",01:07:51.730,01:07:53.950
"And we're going to let
delta approach 0.",01:07:53.950,01:07:56.740
"It's going to be the density
of the n plus first arrival",01:07:56.740,01:08:02.520
times delta plus o of delta.,01:08:02.520,01:08:05.690
"o of delta is something that
goes to 0 as delta increases",01:08:05.690,01:08:10.490
faster than delta does.,01:08:10.490,01:08:12.110
"It's something which has the
property that o of delta",01:08:12.110,01:08:17.170
"divided by delta goes to
0 as delta gets large.",01:08:17.170,01:08:20.670
"So this is just saying that this
is approximately equal to",01:08:20.670,01:08:24.310
"the density of the n plus
first arrival times this",01:08:24.310,01:08:30.540
[INAUDIBLE],01:08:30.540,01:08:31.100
here.,01:08:31.100,01:08:31.479
"The density stays essentially
constant over",01:08:31.479,01:08:34.609
a very small delta.,01:08:34.609,01:08:36.029
It's a continuous density.,01:08:36.029,01:08:38.609
"Next, we use the independent
increment property, which says",01:08:38.609,01:08:42.479
"that the probability that t is
less than sn plus 1, is less",01:08:42.479,01:08:46.729
"than or equal to t plus delta,
is the PMF that n of t is",01:08:46.729,01:08:52.930
"equal to n at the beginning is
the interval, and then that in",01:08:52.930,01:08:59.649
"the middle of the interval,
there's exactly one arrival.",01:08:59.649,01:09:05.279
"And the probabilities of exactly
one arrival, is just",01:09:05.279,01:09:09.970
lambda delta plus o of delta.,01:09:09.970,01:09:12.830
"Namely, that's because of the",01:09:15.500,01:09:16.580
independent increment property.,01:09:16.580,01:09:19.830
"What's this o of delta
doing out here?",01:09:19.830,01:09:22.990
"Why isn't this exactly
equal to this?",01:09:22.990,01:09:26.609
"And why do I need
something else?",01:09:26.609,01:09:28.179
"What am I leaving out
of this equation?",01:09:31.460,01:09:35.279
"The probability that
our arrival comes--",01:09:35.279,01:09:37.710
here's t.,01:09:45.720,01:09:47.789
Here's t plus delta.,01:09:47.789,01:09:51.755
"I'm talking about something
happening here.",01:09:51.755,01:09:54.880
"At this point, n of t is here.",01:09:54.880,01:09:56.990
"And I'm finding the probability
that n of t plus",01:09:59.710,01:10:04.290
"delta is equal to n of
t plus 1 essentially.",01:10:04.290,01:10:10.850
"I'm looking for the probability
of there being one",01:10:10.850,01:10:12.500
arrival in this interval here.,01:10:12.500,01:10:15.070
"So what's the matter
with that equation?",01:10:18.980,01:10:20.515
"This is the probability that
the n plus first arrival",01:10:28.870,01:10:33.520
"occurs somewhere in this
interval here.",01:10:33.520,01:10:37.220
Yeah.,01:10:37.220,01:10:37.940
"AUDIENCE: Is that last term
then the probability that",01:10:37.940,01:10:41.144
"there's not anymore other
parameter standards as well?",01:10:41.144,01:10:44.360
"PROFESSOR: It doesn't
include-- yes.",01:10:44.360,01:10:46.380
"This last term which I had to
add is in fact the negligible",01:10:46.380,01:10:52.560
"term that at time n of t, there
is less than n arrivals.",01:10:52.560,01:11:01.440
"And then I get 2 arrivals in
this little interval delta.",01:11:01.440,01:11:04.530
"So that's why I need
that extra term.",01:11:07.150,01:11:09.300
"But anyway, when I relate these
two terms, I get the",01:11:09.300,01:11:13.390
"probability mass function of n
of t is equal to the Erlang",01:11:13.390,01:11:18.050
"density at t, where
the n plus first",01:11:18.050,01:11:22.820
arrival divided by lambda.,01:11:22.820,01:11:25.140
"And that's what that
term is there.",01:11:25.140,01:11:28.130
"So that gives us the
Poisson PMF.",01:11:34.440,01:11:37.940
"Interesting observation about
this, it's a function",01:11:37.940,01:11:42.500
only of lambda t.,01:11:42.500,01:11:43.840
"It's not a function of lambda
or t separately.",01:11:43.840,01:11:47.340
"It's a function only of the
two of them together.",01:11:47.340,01:11:50.210
It has to be that.,01:11:50.210,01:11:52.860
"Because you can use scaling
arguments on this.",01:11:52.860,01:11:56.180
"If you have a Poisson process
of rate lambda and I measure",01:11:56.180,01:12:00.530
"things in millimeters instead
of centimeters,",01:12:00.530,01:12:03.520
what's going to happen?,01:12:03.520,01:12:05.400
"My rate is going to change
by a factor of 10.",01:12:05.400,01:12:09.250
"My values of t are going to
change by a factor of 10.",01:12:09.250,01:12:13.060
"This is a probability
mass function.",01:12:15.615,01:12:17.630
That has to stay the same.,01:12:17.630,01:12:20.240
"So this has to be a function
only of the product lambda t",01:12:20.240,01:12:24.860
"because of scaling
argument here.",01:12:24.860,01:12:26.910
"Now, the other thing here, and
this is interesting because if",01:12:30.020,01:12:35.290
"you look at n of t, the number
of arrivals up until time t is",01:12:35.290,01:12:40.830
"the sum of the number of
arrivals up until some shorter",01:12:40.830,01:12:45.500
"time t1 plus the number of
arrivals between t1 and t.",01:12:45.500,01:12:52.800
"We know that the number
of arrivals up",01:12:52.800,01:12:54.650
until time t1 is Poisson.,01:12:54.650,01:12:57.100
"The number of arrivals between
t1 and t is Poisson.",01:12:57.100,01:13:01.280
"Those two values are independent
of each other.",01:13:01.280,01:13:04.830
"I can choose t1 in the middle
to be anything I",01:13:04.830,01:13:07.580
want to make it.,01:13:07.580,01:13:09.190
"And this says that the sum of
two Poisson random variables",01:13:09.190,01:13:13.080
has to be Poisson.,01:13:13.080,01:13:16.180
"Now, I'm very lazy.",01:13:16.180,01:13:17.330
"And I've gone through life
without ever convolving this",01:13:17.330,01:13:23.800
"PMF to find out that in fact
the sum of 2 Poisson random",01:13:23.800,01:13:28.530
"variables is in fact
Poisson itself.",01:13:28.530,01:13:32.920
"Because I actually believe the
argument I just went through.",01:13:32.920,01:13:35.700
"If you're skeptical, you will
probably want to actually do",01:13:40.510,01:13:44.600
"the digital convolution to
show that the sum of two",01:13:44.600,01:13:48.420
"independent Poisson random
variables is in fact Poisson.",01:13:48.420,01:13:54.540
"And it extends to any [? tay ?]
disjoint interval.",01:13:54.540,01:13:56.860
"So the same argument says that
any sum of Poisson random",01:13:56.860,01:14:01.050
variables is Poisson.,01:14:01.050,01:14:04.210
"I do want to get through any
alternate definitions of a",01:14:04.210,01:14:07.650
"Poisson process because
that makes a natural",01:14:07.650,01:14:12.240
stopping point here.,01:14:12.240,01:14:14.630
"Question-- is it true that any
arrival process for which n of",01:14:14.630,01:14:18.810
"t has a Poisson probability
mass function for a given",01:14:18.810,01:14:22.680
"lambda and for all
t is a Poisson",01:14:22.680,01:14:26.070
process of rate lambda?,01:14:26.070,01:14:27.330
"In other words, that's a
pretty strong property.",01:14:30.150,01:14:33.695
"It says I found the probability
mass functions for",01:14:33.695,01:14:36.620
n of t at every value of t.,01:14:36.620,01:14:38.915
Does that describe a process?,01:14:38.915,01:14:42.690
"Well, you see the
answer there.",01:14:42.690,01:14:44.525
"As usual, marginal PMFs,
distribution functions don't",01:14:44.525,01:14:50.950
"specify a process because they
don't specify the joint",01:14:50.950,01:14:55.480
probabilities.,01:14:55.480,01:14:57.180
"But here, we've just pointed
out that these joint",01:14:57.180,01:15:00.080
"probabilities are
all independent.",01:15:00.080,01:15:01.690
"You can take a set of
probability mass functions for",01:15:01.690,01:15:05.590
"this interval, this interval,
this interval, this interval,",01:15:05.590,01:15:08.840
and so forth.,01:15:08.840,01:15:10.020
"And for any set of t1, t2, and
so forth up, we know that the",01:15:10.020,01:15:18.090
"number of arrivals in zero to
t1, the number of arrivals in",01:15:18.090,01:15:22.820
"t1 to t2, and so forth all the
way up are all independent",01:15:22.820,01:15:26.850
random variables.,01:15:26.850,01:15:28.110
"And therefore, when we know the
Poisson probability mass",01:15:28.110,01:15:32.830
"function, we really also know,
and we've also shown, that",01:15:32.830,01:15:40.770
"these random variables are
independent of each other.",01:15:40.770,01:15:43.990
"We have the joint PMF for any
sum of these random variables.",01:15:43.990,01:15:49.010
"So in fact, in this particular
case, it's enough to know what",01:15:49.010,01:15:59.690
"the probability mass function
is at each time t plus the",01:15:59.690,01:16:03.790
fact that we have this,01:16:03.790,01:16:05.820
independent increment property.,01:16:05.820,01:16:08.020
"And we need the stationary
increment property, too, to",01:16:08.020,01:16:10.610
"know that these values are
the same at each t.",01:16:10.610,01:16:14.030
"So the theorem is that if an
arrival process has the",01:16:14.030,01:16:17.300
"stationary and independent
increment properties, and if n",01:16:17.300,01:16:21.420
"of t has the Poisson PMF for
given lambda and all t greater",01:16:21.420,01:16:26.840
"than 0, then the process itself
has to be Poisson.",01:16:26.840,01:16:32.090
"VHW stands for Violently
Hand Waving.",01:16:32.090,01:16:36.890
"So that's even a little
worse than a PF.",01:16:39.690,01:16:43.150
"Says the stationary and
independent increment",01:16:43.150,01:16:45.220
"properties show that the joint
distribution of arrivals over",01:16:45.220,01:16:48.580
"any given set of disjoint
intervals is that",01:16:48.580,01:16:51.580
of a Poisson process.,01:16:51.580,01:16:53.410
And clearly that's enough.,01:16:53.410,01:16:55.390
And it almost is.,01:16:55.390,01:16:56.660
"And you should read the proof in
the notes which does just a",01:16:56.660,01:16:59.470
"little more than that to make
this an actual proof.",01:16:59.470,01:17:03.240
OK.,01:17:03.240,01:17:03.550
I think I'll stop there.,01:17:03.550,01:17:05.330
"And we will talk a little
bit about the Bernoulli",01:17:05.330,01:17:08.020
process next time.,01:17:08.020,01:17:10.060
Thank you.,01:17:10.060,01:17:11.310

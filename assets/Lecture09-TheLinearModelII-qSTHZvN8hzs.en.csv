text,start,stop
"ANNOUNCER: The following program
is brought to you by Caltech.",00:00:01.580,00:00:04.275
YASER ABU-MOSTAFA: Welcome back.,00:00:16.780,00:00:19.540
"Last time, we talked about the bias-variance
decomposition of the",00:00:19.540,00:00:24.880
out-of-sample error.,00:00:24.880,00:00:27.550
"And we managed, by taking the expected
value of the out-of-sample with",00:00:27.550,00:00:32.509
"respect to the identity of the data
points-- the size is fixed at N--",00:00:32.509,00:00:37.350
"to get rid of the variation
due to the data set.",00:00:37.350,00:00:41.710
"And we ended up with a very clean
decomposition of the expected value of",00:00:41.710,00:00:45.580
"the E_out, into a bias term and
a variance term, that have very",00:00:45.580,00:00:49.280
interesting interpretation.,00:00:49.280,00:00:51.330
"And they're illustrated here
in terms of a tradeoff.",00:00:51.330,00:00:54.690
"If you have a small hypothesis set,
the chances are the target",00:00:54.690,00:00:58.300
function is far away.,00:00:58.300,00:01:00.030
"And therefore, there is a significant
bias term, which is the blue one.",00:01:00.030,00:01:03.890
"And if you have a bigger hypothesis
set, perhaps big enough to include",00:01:03.890,00:01:07.670
"your target, you don't have very much
of a bias, perhaps none at all.",00:01:07.670,00:01:12.120
"But on the other hand, you do have
variance depending on which hypothesis",00:01:12.120,00:01:17.660
"you zoom in based on the
data set you have.",00:01:17.660,00:01:20.610
"So in the bias-variance decomposition,
we basically had two hops from the",00:01:20.610,00:01:26.160
"final hypothesis you produced based on
a particular data set, into the target",00:01:26.160,00:01:31.170
"function which is what we're
trying to approximate.",00:01:31.170,00:01:33.800
"The intermediate step was the new
notion, which is the expected value",00:01:33.800,00:01:37.190
"of the hypothesis set with respect to
D. And the jump, from your actual",00:01:37.190,00:01:42.000
"hypothesis to that fictitious hypothesis,
describes the variance,",00:01:42.000,00:01:47.040
"because here is the centroid of this,
which is the g bar, and you are",00:01:47.040,00:01:51.670
somewhere here.,00:01:51.670,00:01:52.880
"So in order to get there, that is
described by the size of the red",00:01:52.880,00:01:55.720
"region, and that is the variance.",00:01:55.720,00:01:58.210
"And then there is another one, which is
inevitable for a given hypothesis",00:01:58.210,00:02:01.470
"set, which is the hop from that
one, which designates the best",00:02:01.470,00:02:05.000
"approximation, in a certain sense, within
the hypothesis set, of your target",00:02:05.000,00:02:09.120
"function, into your target function.",00:02:09.120,00:02:10.949
"And that difference is the bias,
which is captured here.",00:02:10.949,00:02:15.680
"After doing the bias-variance
decomposition, we went into",00:02:15.680,00:02:18.480
"an illustrative tool called the learning
curves, where we plot the expected",00:02:18.480,00:02:22.750
"value of the in-sample error and the
out-of-sample error, as you increase",00:02:22.750,00:02:27.280
the sample size.,00:02:27.280,00:02:29.140
"If you look at the curve-- let's
look at the bottom one here.",00:02:29.140,00:02:31.680
"Not surprisingly, as you increase the
number of examples, the out-of-sample",00:02:31.680,00:02:36.240
error goes down.,00:02:36.240,00:02:37.110
"If you have more examples to learn from,
you are likely to perform better",00:02:37.110,00:02:40.250
out-of-sample.,00:02:40.250,00:02:41.710
"And another interesting observation is
that when you have fewer examples, the",00:02:41.710,00:02:46.935
in-sample error goes down.,00:02:46.935,00:02:48.690
"And that is because you are fitting fewer
examples, and you have the same",00:02:48.690,00:02:52.480
resources to fit.,00:02:52.480,00:02:53.640
So you tend to fit them better.,00:02:53.640,00:02:55.430
"And the discrepancy between them
describes the generalization error.",00:02:55.430,00:02:59.460
"Then we contrasted the analysis of the
bias-variance decomposition, which is",00:02:59.460,00:03:03.230
"the top curve, to the VC analysis,
which we have done before.",00:03:03.230,00:03:06.810
"And we realized that both of them
describe a tradeoff between",00:03:06.810,00:03:09.790
approximation and generalization.,00:03:09.790,00:03:11.790
"In the bias-variance case, the
approximation is an absolute",00:03:11.790,00:03:15.260
approximation--,00:03:15.260,00:03:16.280
"how your best hypothesis approximates
the target.",00:03:16.280,00:03:19.700
"And that is described by g bar, again
with certain liberty.",00:03:19.700,00:03:25.020
"And in the case of the VC analysis, the
approximation was approximation",00:03:25.020,00:03:29.410
in sample only.,00:03:29.410,00:03:30.800
So it was E_in.,00:03:30.800,00:03:32.160
"And then the jump from the approximation
to the final performance",00:03:32.160,00:03:35.600
"describes the generalization, whether it's
this red region or this red region,",00:03:35.600,00:03:38.800
"which have basically the same
monotonicity, except that they have",00:03:38.800,00:03:42.830
different terms.,00:03:42.830,00:03:44.660
"The final lesson from the theory, which
is really what we're going to",00:03:44.660,00:03:48.630
"carry out through the techniques, which
start today until the end of the",00:03:48.630,00:03:51.600
"course, is that the number of examples
needed to achieve a certain",00:03:51.600,00:03:56.300
"performance is proportional
to the VC dimension.",00:03:56.300,00:03:58.860
"And I'm putting it between quotes,
because we defined it formally only for",00:03:58.860,00:04:02.070
classification.,00:04:02.070,00:04:03.220
"But then we took the linear regression
case, which is not classification, and",00:04:03.220,00:04:06.710
"we found out that the corresponding
quantity, which is the degrees of",00:04:06.710,00:04:09.730
"freedom-- same thing, d plus 1--",00:04:09.730,00:04:12.010
"happens to also describe the
generalization property.",00:04:12.010,00:04:16.750
"And therefore, we basically have
a certain rule that you need examples",00:04:16.750,00:04:20.180
"in proportion to the VC dimension,
or to the effective degrees of",00:04:20.180,00:04:23.760
"freedom, in order to
do generalization.",00:04:23.760,00:04:27.400
"And the more you have, the better
performance you get.",00:04:27.400,00:04:30.000
This is the key observation.,00:04:30.000,00:04:33.240
"Today, I'm going to start
a series of techniques.",00:04:33.240,00:04:36.690
"And today is special, because the
techniques of the linear models have",00:04:36.690,00:04:41.900
already been covered in part.,00:04:41.900,00:04:43.190
"Remember, this is the part that was
split into two portions.",00:04:43.190,00:04:46.060
"And we got a portion very early on,
out of sequence, just to give you",00:04:46.060,00:04:49.770
something to work with.,00:04:49.770,00:04:51.480
"And then, I'm going to complete the
exposition of the linear models today.",00:04:51.480,00:04:57.590
So let's see where we are.,00:04:57.590,00:05:01.010
"This is the big picture
of linear models.",00:05:01.010,00:05:04.240
"And they start with linear
classification-- perceptrons.",00:05:04.240,00:05:08.070
We have seen that.,00:05:08.070,00:05:09.910
And then go on to linear regression.,00:05:09.910,00:05:11.960
We have also seen that.,00:05:11.960,00:05:13.050
That was the part that was covered.,00:05:13.050,00:05:15.680
"There is a third one, which is another
linear model that is neither linear",00:05:15.680,00:05:20.480
"classification nor linear regression,
which will be the bulk of",00:05:20.480,00:05:23.180
the lecture today.,00:05:23.180,00:05:24.370
It will be called logistic regression.,00:05:24.370,00:05:27.060
"And then, for all of these linear models,
we have this nice trick called",00:05:27.060,00:05:31.220
"nonlinear transforms that allows us to
use the learning algorithms",00:05:31.220,00:05:37.320
"of linear models, which are
very simple ones, and apply",00:05:37.320,00:05:41.320
them to nonlinear transformation.,00:05:41.320,00:05:43.200
"And if you remember, that the observation
here was that linearity in the",00:05:43.200,00:05:46.110
"parameters was the key issue
for deriving the algorithm.",00:05:46.110,00:05:50.330
"So let's see what we finished, and what
we didn't finish in these topics.",00:05:50.330,00:05:55.670
"Linear classification
is pretty much done.",00:05:55.670,00:05:58.020
"We know the algorithm,
perceptron or pocket.",00:05:58.020,00:06:00.410
"There are obviously more sophisticated
algorithms.",00:06:00.410,00:06:02.850
"And we did the generalization
analysis.",00:06:02.850,00:06:05.390
"We got the VC dimension of
perceptrons explicitly.",00:06:05.390,00:06:07.930
"And therefore, we are able to predict
the generalization ability of linear",00:06:07.930,00:06:11.510
classification.,00:06:11.510,00:06:12.290
So this is a done deal.,00:06:12.290,00:06:15.240
Linear regression is also a done deal.,00:06:15.240,00:06:16.970
"We have the algorithm, remember, that
was the pseudo-inverse, the",00:06:16.970,00:06:19.820
one-step learning.,00:06:19.820,00:06:21.310
"And last time, we did a very brief
analysis of generalization ability.",00:06:21.310,00:06:25.515
"And we found that it parallels that
of the perceptron, d plus 1 is the",00:06:25.515,00:06:28.630
"operative point, the operative
quantity in this case.",00:06:28.630,00:06:31.860
"And therefore, we have linear regression
as a technique, and as",00:06:31.860,00:06:34.920
a generalization ability.,00:06:34.920,00:06:37.640
"In the case of nonlinear transforms,
we are almost done.",00:06:37.640,00:06:40.540
"We did the techniques, but remember, we
left at a point where we say",00:06:40.540,00:06:44.960
"nonlinear transforms are
a very useful tool.",00:06:44.960,00:06:47.910
"And they actually can make us separate
any data points, by going to",00:06:47.910,00:06:51.920
a sufficiently high-dimensional space.,00:06:51.920,00:06:54.290
"And we had a suspicion that this is not
really a safe process to follow.",00:06:54.290,00:07:00.060
"We have to worry about generalization
issues.",00:07:00.060,00:07:02.590
"So the generalization issues
were left out.",00:07:02.590,00:07:05.070
"And I'm going to start this lecture by
tying up the loose ends in",00:07:05.070,00:07:09.110
"generalization for nonlinear transforms,
before I go into the main",00:07:09.110,00:07:12.690
"topic, which is the third one.",00:07:12.690,00:07:14.290
That is logistic regression.,00:07:14.290,00:07:16.220
So.,00:07:16.220,00:07:16.650
"Very brief analysis of nonlinear
transforms in terms of generalization,",00:07:16.650,00:07:22.200
"then logistic regression
beginning to end.",00:07:22.200,00:07:24.420
That's the plan.,00:07:24.420,00:07:26.400
OK.,00:07:26.400,00:07:27.500
Nonlinear transforms.,00:07:27.500,00:07:28.480
Let's remind ourselves.,00:07:28.480,00:07:30.020
We were working in an X space.,00:07:30.020,00:07:32.150
"And we had a d-dimensional vector
that represented the input.",00:07:32.150,00:07:35.180
"And we added the constant
+1 coordinate, to",00:07:35.180,00:07:37.720
take care of the threshold.,00:07:37.720,00:07:39.220
"And now, we are going to transform
this into another space using",00:07:39.220,00:07:43.040
a transformation we called phi.,00:07:43.040,00:07:45.440
"This takes us into the Z space,
or the feature space.",00:07:45.440,00:07:49.160
"So each of these guys is derived
from the raw input x.",00:07:49.160,00:07:54.240
"And the transformation we have can be
quite general. If you",00:07:54.240,00:07:59.460
"look at it,",00:07:59.460,00:08:00.650
"any one of these coordinates can be
an arbitrary nonlinear transformation of",00:08:00.650,00:08:06.080
the entire vector x.,00:08:06.080,00:08:07.530
It doesn't take one coordinate.,00:08:07.530,00:08:08.760
It takes the entire vector--,00:08:08.760,00:08:11.030
so all of these guys--,00:08:11.030,00:08:12.310
"computes any formula you want, and then
puts it as the feature here.",00:08:12.310,00:08:17.890
"So you can imagine how
general this can be.",00:08:17.890,00:08:20.140
"And also, the length of
this can be arbitrary.",00:08:20.140,00:08:22.150
You can have this as long as you want.,00:08:22.150,00:08:24.230
"As a matter of fact, when we move to
support vector machines, we will be",00:08:24.230,00:08:27.320
"able to go to infinite-dimensional
feature space, which is an interesting",00:08:27.320,00:08:31.350
generalization.,00:08:31.350,00:08:32.289
"So, each of them is
a general transformation.",00:08:32.289,00:08:35.490
"And therefore, the small phi_i is
a member of the big transformation,",00:08:35.490,00:08:40.870
"capital phi, that takes the vector x,
and produces the vector z working in",00:08:40.870,00:08:44.830
the Z space.,00:08:44.830,00:08:45.800
That's that transform.,00:08:45.800,00:08:47.900
"An example for that, which we
used, was 2nd order.",00:08:47.900,00:08:51.210
"Instead of using linear surfaces here,
we wanted to use quadratic surfaces.",00:08:51.210,00:08:56.600
"And quadratic surfaces in the X space
correspond to linear surfaces in",00:08:56.600,00:09:01.880
"a quadratic-transformed space, Z.",00:09:01.880,00:09:04.850
So this would be the transformation.,00:09:04.850,00:09:06.680
"We got all possible factors that
contribute to a 2nd-order",00:09:06.680,00:09:11.460
term.,00:09:11.460,00:09:12.000
"And now if you put coefficients to each
of these and sum them up, you",00:09:12.000,00:09:14.790
"will get a general 2nd-order
surface in the X space.",00:09:14.790,00:09:19.030
"Now the final hypothesis for us
will always live in the X space.",00:09:19.030,00:09:24.760
"The Z space is transparent
to the user.",00:09:24.760,00:09:27.850
"This is our tool in order to get more
sophisticated surfaces in the X space,",00:09:27.850,00:09:33.530
"while we are able to use the
linear techniques.",00:09:33.530,00:09:37.060
"So the final hypothesis will be
mentioned as g of x equals--",00:09:37.060,00:09:43.230
"So you have the linear thing, but you
have that transformed version of x.",00:09:43.230,00:09:46.400
"That's what you get the
dot product with.",00:09:46.400,00:09:47.740
So this is z.,00:09:47.740,00:09:49.700
"And in the case of classification, you
take the sign, +1 or -1,",00:09:49.700,00:09:52.760
"according to the signal whether
it is positive or negative.",00:09:52.760,00:09:55.110
"And in the case of linear regression,
you get the raw signal itself.",00:09:55.110,00:10:00.230
"And as we will see, we will have a third
one, which is in-between taking",00:10:00.230,00:10:04.190
"the sign and leaving the quantity alone,
when we talk about logistic",00:10:04.190,00:10:07.360
regression.,00:10:07.360,00:10:08.190
"That's the summary of nonlinear
transforms that we have seen so far.",00:10:08.190,00:10:12.870
"And now we talk about generalization,",00:10:12.870,00:10:14.500
"and ask ourselves: what is the price we pay
when we do a nonlinear transform?",00:10:14.500,00:10:19.860
"The price we will pay, obviously,
in terms of generalization.",00:10:19.860,00:10:23.460
So this is the transform.,00:10:23.460,00:10:25.710
"Now if you look at the X space, and
ask: what is the generalization",00:10:25.710,00:10:29.790
behavior in the X space?,00:10:29.790,00:10:31.460
"Let's say that you don't do
the nonlinear transform.",00:10:31.460,00:10:34.130
"You do the linear model
in the X space.",00:10:34.130,00:10:36.940
"Well, in that case, you are going to
get a weight vector in the X space.",00:10:36.940,00:10:41.420
"And the dimensionality of the weight
vector is the same as here, so it will",00:10:41.420,00:10:44.190
be d plus 1.,00:10:44.190,00:10:45.070
"There is a weight corresponding
to each of these coordinates.",00:10:45.070,00:10:47.740
"And then you take the dot product, and
whether you use threshold, or report it,",00:10:47.740,00:10:51.780
"depending on which type of linear
model you are talking about.",00:10:51.780,00:10:54.850
"But basically, you have d plus 1
free parameters.",00:10:54.850,00:10:58.030
"And we realize that d plus 1
free parameters correspond",00:10:58.030,00:11:00.560
directly to a VC dimension.,00:11:00.560,00:11:04.150
"In the case of the Z space, the
feature space, we have",00:11:04.150,00:11:10.040
potentially a longer vector--,00:11:10.040,00:11:11.840
"much longer, possibly.",00:11:11.840,00:11:13.550
"And the dimensionality
here is d tilde.",00:11:13.550,00:11:16.870
That's the notation we give for it.,00:11:16.870,00:11:18.960
"And the vector that will apply
here will be w tilde.",00:11:18.960,00:11:21.440
"That will be a much longer vector
in general than w.",00:11:21.440,00:11:25.110
"So for example, in our case, if we
used the linear with x_0, x_1, x_2,",00:11:25.110,00:11:29.450
we would have 3.,00:11:29.450,00:11:31.020
"If we did the full 2nd order,
we would get 6.",00:11:31.020,00:11:34.720
So we get more there.,00:11:34.720,00:11:36.320
"And we have seen that the VC dimension
is, in this case, for the perceptron,",00:11:36.320,00:11:41.105
d plus 1.,00:11:41.105,00:11:42.560
"So this is the price you pay
for the generalization.",00:11:42.560,00:11:45.040
"And here, the price you pay for
the generalization can be",00:11:45.040,00:11:47.330
pretty serious.,00:11:47.330,00:11:49.850
"Now you see that if I want to separate
the points, and then I go to",00:11:49.850,00:11:54.870
"a 17th-order polynomial,",00:11:54.870,00:11:56.660
"general 17th-order polynomial, and
then you count the number of",00:11:56.660,00:11:59.160
"coordinates you have gone to, the VC
dimension would be so large that, in",00:11:59.160,00:12:02.690
"spite of the fact that you were able
to fit-- because you went to a 17th",00:12:02.690,00:12:05.180
order polynomial--,00:12:05.180,00:12:06.160
"you don't really have a real
chance of generalization.",00:12:06.160,00:12:09.310
"Just to be accurate, this is really not
equality here, because we always",00:12:09.310,00:12:13.710
"measure the VC dimension
in the X space.",00:12:13.710,00:12:16.650
"Again, the Z space is transparent
to the user.",00:12:16.650,00:12:19.690
"So we go here, and we come back, and we
ask ourselves what can we shatter",00:12:19.690,00:12:22.700
here and whatnot.,00:12:22.700,00:12:24.230
"So in spite of the fact that in this
case, if you had the full space, and",00:12:24.230,00:12:27.780
"you were able to choose the points any
which way you want, you will be able to get",00:12:27.780,00:12:31.310
"exactly that VC dimension, it is
possible that there are certain",00:12:31.310,00:12:35.510
"combinations of points here that
are impossible to come by as",00:12:35.510,00:12:39.000
transformations of legal points here.,00:12:39.000,00:12:41.540
"If you want a simple case, let me just
take two coordinates to be identical--",00:12:41.540,00:12:45.330
same transformation.,00:12:45.330,00:12:47.300
"So obviously, now I'm stuck.",00:12:47.300,00:12:48.580
"I don't have the full benefit of the
coordinates, because if I choose one,",00:12:48.580,00:12:51.310
the other one is dictated.,00:12:51.310,00:12:53.080
"So just because of that fact, in order
to be accurate, we will say that it is",00:12:53.080,00:12:57.100
"actually, at most d plus 1.",00:12:57.100,00:12:58.790
"Usually, it's very close to d plus 1--",00:12:58.790,00:13:00.620
d tilde plus 1.,00:13:00.620,00:13:03.020
"So let's apply this to two cases where
we use nonlinear transformations, in",00:13:03.050,00:13:07.490
"order to appreciate, in practical
terms, what is the price we pay.",00:13:07.490,00:13:12.940
"The first non-separable
case is a pretty easy one.",00:13:12.940,00:13:17.080
"It's almost separable, except for some points
that you can consider, maybe, outliers.",00:13:17.080,00:13:23.340
"This red point is in the blue region,
this blue in the red region.",00:13:23.340,00:13:26.940
"But otherwise, everything can
be classified linearly.",00:13:26.940,00:13:29.670
"So one may think of this case--
this case is really linearly",00:13:29.670,00:13:32.110
"separable, and we just have a bunch of
outliers, maybe we shouldn't use",00:13:32.110,00:13:37.160
"nonlinear transform, just settle
for the linear--",00:13:37.160,00:13:39.670
We will talk about that.,00:13:39.670,00:13:40.950
"So this is one class of things,
when we look at nonlinear",00:13:40.950,00:13:44.850
transforms.,00:13:44.850,00:13:46.320
The other one is genuinely nonlinear.,00:13:46.320,00:13:49.890
"This thing-- I really don't stand
a chance if I use a line, and therefore,",00:13:49.890,00:13:54.630
"I'm really talking about something
that needs to be transformed.",00:13:54.630,00:13:57.710
"So let's see how the generalization
behavior goes, for both of them when we",00:13:57.710,00:14:01.760
apply the nonlinear transforms.,00:14:01.760,00:14:04.935
The first case is pretty easy.,00:14:04.935,00:14:08.580
It's almost linearly separable.,00:14:08.580,00:14:10.370
So here are the choices.,00:14:10.370,00:14:12.550
"You can use a linear model in X space,
in the input space that you have, and",00:14:12.550,00:14:17.860
"then accept that the in-sample
error will be positive.",00:14:17.860,00:14:22.190
It's not going to be 0.,00:14:22.190,00:14:24.160
"In this case, here's the picture.",00:14:24.160,00:14:25.950
"There is an in-sample error, because this
guy is erroneously classified, and",00:14:25.950,00:14:29.220
"this guy is erroneously classified
by your hypothesis.",00:14:29.220,00:14:33.220
So this is option number one.,00:14:33.220,00:14:35.470
"Now, option number two is to say I
would like to get E_in to be 0, so you",00:14:35.470,00:14:39.530
insist on E_in being zero.,00:14:39.530,00:14:44.260
"And in order to do that, you have
to go to another space.",00:14:44.260,00:14:47.510
"So you decide to go to
a high-dimensional space.",00:14:47.510,00:14:51.790
"Now you can see what the problem is
here, because we are just taking care",00:14:51.790,00:14:55.750
"of two points, for crying
out loud!",00:14:55.750,00:14:58.250
"And in order to actually be able to classify
them using a surface, believe it",00:14:58.250,00:15:01.490
"or not, you're not going to be able to
do it with a 2nd-order surface, or",00:15:01.490,00:15:05.400
a 3rd-order surface.,00:15:05.400,00:15:07.340
"You will have to go to a 4th-order
surface in order to get it all right.",00:15:07.340,00:15:11.050
"And when you do that, this
is what you get.",00:15:11.050,00:15:13.440
"Now, you don't need the VC analysis to
realize that this is an overkill, and",00:15:16.600,00:15:21.680
"this doesn't have a very good
chance of generalizing.",00:15:21.680,00:15:24.680
"Of course, you can do
the formal thing.",00:15:24.680,00:15:25.900
You see 4th order.,00:15:25.900,00:15:27.380
"Instead of having 3, I
have however many there are.",00:15:27.380,00:15:30.880
"And therefore, for the limited number
of examples I have, when I see the",00:15:30.880,00:15:34.150
"generalization behavior, I am
completely in the dark.",00:15:34.150,00:15:38.360
"So in this case, that is
a straightforward application of the",00:15:38.360,00:15:41.730
"approximation-generalization
tradeoff.",00:15:41.730,00:15:44.270
We went to a more complex model.,00:15:44.270,00:15:46.400
"We were able to approximate the data
better, but we are generalizing worse.",00:15:46.400,00:15:51.420
"This has been completely covered
already, so there is no surprise in",00:15:51.420,00:15:54.580
"this, other than to understand the fact
that at times, you might as",00:15:54.580,00:15:58.870
"well settle for a small training error,
in order not to use too high",00:15:58.870,00:16:04.670
a complexity for the hypothesis set.,00:16:04.670,00:16:09.490
"The other one is the case where
you really don't stand",00:16:09.490,00:16:14.640
a chance with linear.,00:16:14.640,00:16:15.770
"It will be very, very poor
approximation and generalization.",00:16:15.770,00:16:19.160
"The data seems to be coming from
inherently a nonlinear surface.",00:16:19.160,00:16:23.610
"And in this case, we used
this transformation.",00:16:23.610,00:16:28.460
"And this transformation is my
way of putting a general",00:16:28.460,00:16:31.220
2nd-order surface.,00:16:31.220,00:16:33.180
"And if you look at it, if we use only the
x, which would be conveniently the",00:16:33.180,00:16:38.670
"first three guys-- that would be
the vector x, get weights--",00:16:38.670,00:16:42.100
"I have 3 weights, so I pay
for the price of 3.",00:16:42.100,00:16:45.460
"Whereas if I use this, I have 6,
so I pay for a price of 6.",00:16:45.460,00:16:49.500
"So basically, in our mind, you need
twice as many examples to do the same",00:16:49.500,00:16:53.050
level of performance.,00:16:53.050,00:16:54.270
"Not that we have a choice in this case,
because the linear doesn't work,",00:16:54.270,00:16:56.840
"but this is basically the
formula we have in mind.",00:16:56.840,00:16:59.830
"Now comes an interesting
discussion.",00:16:59.830,00:17:03.100
I don't want to pay the 6.,00:17:03.100,00:17:05.099
"I want to go to the nonlinear space,
but I don't want to pay the 6.",00:17:05.099,00:17:08.960
I want to get a discount!,00:17:08.960,00:17:11.220
Here is a way to get a discount--,00:17:11.220,00:17:13.980
"not necessarily legitimate, but let's
pursue it and see why it would be",00:17:13.980,00:17:17.130
legitimate or not.,00:17:17.130,00:17:19.400
"Why not transform x into
this guy only?",00:17:19.400,00:17:26.069
"The idea here, this
is the origin.",00:17:26.069,00:17:29.840
"x_1 goes like this, x_2 goes like this,
so it seems that I only need x_1",00:17:29.840,00:17:34.870
squared and x_2 squared.,00:17:34.870,00:17:36.530
"These guys are just making me pay,
without really contributing.",00:17:36.530,00:17:41.010
"So I'm just going to
use this model.",00:17:41.010,00:17:43.690
"Well, if I use this model, it looks
like I have now 3.",00:17:43.690,00:17:46.500
"So I have exactly the same number of
examples as if this was linear, and as",00:17:46.500,00:17:49.960
if I was doing it in the X space.,00:17:49.960,00:17:53.540
"If you are smelling a rat,
you are correct!",00:17:53.540,00:17:56.590
"And in order to make it clear,
let's just pursue this line.",00:17:56.590,00:18:00.010
I can do even better than this.,00:18:00.010,00:18:01.650
"Why not take 2 guys
instead of 3?",00:18:04.310,00:18:08.070
"I have the second guy being x_1 squared
plus x_2 squared, because I really don't",00:18:08.070,00:18:11.100
"care about x_1 squared and x_2
squared being independent.",00:18:11.100,00:18:13.660
They are just the radius in my mind.,00:18:13.660,00:18:15.300
So I need to do this.,00:18:15.300,00:18:17.890
"Now we have achieved a lot, because now
we have even fewer parameters than",00:18:17.890,00:18:22.160
"if we use the linear guy, so the
generalization must be getting better",00:18:22.160,00:18:25.330
and better.,00:18:25.330,00:18:27.330
"Now let's get carried away,
and go for the ultimate.",00:18:27.330,00:18:34.980
I have one guy.,00:18:34.980,00:18:35.870
"I even let go of the mandatory
constant.",00:18:35.870,00:18:40.230
I just have this guy.,00:18:40.230,00:18:42.060
"And all I'm learning in this case is
just what is outside and what is",00:18:42.060,00:18:44.830
"inside the circle, really.",00:18:44.830,00:18:46.190
"It doesn't even need a parameter,
just needs a binary one.",00:18:46.190,00:18:49.450
"Now I have one guy, and
the VC dimension is 1.",00:18:49.450,00:18:52.180
And I can generalize greatly--,00:18:52.180,00:18:53.590
"dot, dot, dot.",00:18:53.590,00:18:55.840
"Well, something is wrong.",00:18:55.840,00:18:58.710
"Now it's clear that something is wrong,
but it's very important to",00:18:58.710,00:19:01.200
articulate what is wrong.,00:19:01.200,00:19:03.720
"What is wrong is that you're charging
the VC dimension of this hypothesis",00:19:03.720,00:19:11.400
"set, right?",00:19:11.400,00:19:13.410
"Think of the VC inequality as providing
you with a warranty.",00:19:13.410,00:19:20.430
"Now in order for the warranty to be
valid, you cannot look at the data",00:19:20.430,00:19:26.700
before you choose the model.,00:19:26.700,00:19:29.020
That will forfeit the warranty.,00:19:29.020,00:19:31.960
why does it forfeit it?,00:19:31.960,00:19:34.700
Because of the following.,00:19:34.700,00:19:37.690
"I am going to charge you-- if I
do the analysis correctly--",00:19:37.690,00:19:41.450
"not the VC dimension of
the final guy you got.",00:19:41.450,00:19:45.980
"I'm not going to get the VC dimension
of this fellow.",00:19:45.980,00:19:50.310
"I'm going to charge you the VC dimension
of the entire hypothesis",00:19:50.310,00:19:55.450
"space, that you explored in your mind
in getting there, because you have",00:19:55.450,00:20:02.200
"acted as a learning algorithm,
unknowingly.",00:20:02.200,00:20:06.130
You looked at the data.,00:20:06.130,00:20:08.430
"Before you looked at the
data, you had no idea.",00:20:08.430,00:20:11.030
"Let's say that you decided ahead
of time, I'm going to use",00:20:11.030,00:20:13.000
a 2nd-order set.,00:20:13.000,00:20:14.180
"2nd order, OK?",00:20:14.180,00:20:15.470
"Now you look at the data, and you
realize that some coefficients are 0.",00:20:15.470,00:20:19.880
"That's called learning, right?",00:20:19.880,00:20:21.850
I don't need this.,00:20:21.850,00:20:22.530
I don't need this.,00:20:22.530,00:20:23.160
I don't need this.,00:20:23.160,00:20:23.820
"You did it very quickly
in your mind.",00:20:23.820,00:20:26.390
"So the hypothesis that was learned
was a hierarchical learning.",00:20:26.390,00:20:30.270
"First, you learned, and then you passed
it on to the algorithm to",00:20:30.270,00:20:34.340
complete the learning.,00:20:34.340,00:20:36.090
"So the effective hypothesis set
is what you started with.",00:20:36.090,00:20:40.560
You see where the point is.,00:20:40.560,00:20:42.380
"And the lesson learned from this is that,
if you look at the data before",00:20:42.380,00:20:51.310
"you choose the model, this can be
hazardous to your health.",00:20:51.310,00:20:56.480
"Not your health, but the generalization
health.",00:20:56.480,00:21:02.450
Why is that?,00:21:02.450,00:21:03.690
"Because now, the quantity that
describes generalization",00:21:03.690,00:21:08.620
becomes very vague.,00:21:08.620,00:21:09.740
"When we propose a particular model, I can
go and mathematically estimate the",00:21:09.740,00:21:13.150
VC dimension.,00:21:13.150,00:21:14.630
"If you look at the data, we said
that you did learning.",00:21:14.630,00:21:17.470
"Now I'm asking, what is exactly the
full hypothesis space that you",00:21:17.470,00:21:21.070
explored in the beginning?,00:21:21.070,00:21:22.410
That's a little bit vague--,00:21:22.410,00:21:23.650
very difficult to pin down.,00:21:23.650,00:21:25.210
"Definitely bigger than what you are
going to use if you use the VC",00:21:25.210,00:21:28.250
"dimension of the hypothesis set
that you ended up with.",00:21:28.250,00:21:32.610
"And this is a manifestation
of the biggest trap that",00:21:32.610,00:21:36.080
practitioners fall into.,00:21:36.080,00:21:37.970
"When you go into machine learning, I want
you to learn from the data, and",00:21:37.970,00:21:40.960
choosing the model is very tricky.,00:21:40.960,00:21:43.310
"Some model may work, some
model may not work.",00:21:43.310,00:21:45.640
So it's tempting.,00:21:45.640,00:21:47.580
"Let me just look at the data, and
pick something suitable.",00:21:47.580,00:21:50.820
"Well, you are allowed to do that.",00:21:50.820,00:21:52.360
"I'm not saying that this
is against the law.",00:21:52.360,00:21:54.250
You can do it.,00:21:54.250,00:21:55.200
Just charge accordingly.,00:21:55.200,00:21:58.250
"Remember that if you do this, and
you end up with a small hypothesis",00:21:58.250,00:22:01.880
"set, and you have a VC dimension, you have
already forfeited the warranty that is",00:22:01.880,00:22:06.650
"given by the VC inequality
according to that.",00:22:06.650,00:22:09.300
"It applies only to the VC
dimension to begin with.",00:22:09.300,00:22:12.310
"And this is a manifestation
of basically snooping.",00:22:12.310,00:22:15.650
You snooped into the data.,00:22:15.650,00:22:16.950
"You looked at it in a way
that is not allowed.",00:22:16.950,00:22:20.460
"And when you do this,
bad things happen.",00:22:20.460,00:22:24.480
"And the formal term for it, actually,
in machine learning, is called data",00:22:24.480,00:22:28.850
snooping.,00:22:28.850,00:22:30.110
"And we will dedicate one third of
a lecture just describing data snooping.",00:22:30.110,00:22:34.710
"This is the most obvious manifestation
of data snooping.",00:22:34.710,00:22:37.740
"You look at the data before
you choose the model.",00:22:37.740,00:22:39.900
"But there are other ways that are so
subtle, that it's very likely that even",00:22:39.900,00:22:44.310
"a smart person may fall
into those traps.",00:22:44.310,00:22:46.880
"And it's very important to understand
what these traps are, in order to avoid",00:22:46.880,00:22:50.090
"them, and make sure that when you apply
the theory, after all of the",00:22:50.090,00:22:53.200
"sweat we had in order to get
these things, they might as",00:22:53.200,00:22:55.920
well be valid.,00:22:55.920,00:22:57.360
"And you can immediately make them not
valid by doing these things.",00:22:57.360,00:23:02.560
"So this is the subject
of data snooping.",00:23:02.560,00:23:07.200
"And I'm not minimizing the idea of
choosing a model. There will be ways to",00:23:07.200,00:23:11.790
choose a model.,00:23:11.790,00:23:12.400
"When we talk about validation,
model selection will be the",00:23:12.400,00:23:14.650
order of the day.,00:23:14.650,00:23:15.500
"But it will be a legitimate
means of model selection.",00:23:15.500,00:23:18.360
"It's a model selection that does
not contaminate the data.",00:23:18.360,00:23:21.630
"The data here was used
to choose the model.",00:23:21.630,00:23:23.420
"Therefore, it's contaminated.",00:23:23.420,00:23:24.540
"It's no longer trusted to reflect the
real performance, because you have",00:23:24.540,00:23:28.090
already used it in learning.,00:23:28.090,00:23:30.770
This is the lesson we get.,00:23:30.770,00:23:32.530
"And if you remember when I said linear
models are an economy car, nonlinear",00:23:32.530,00:23:36.750
transforms give you a truck.,00:23:36.750,00:23:38.610
"And we saw that the truck
is very strong.",00:23:38.610,00:23:40.660
I can go to very high-dimensional space.,00:23:40.660,00:23:42.320
I can have very sophisticated surface.,00:23:42.320,00:23:44.320
"And then I warned you that, be careful
when you drive a truck.",00:23:44.320,00:23:48.030
And this is what I meant by that.,00:23:48.030,00:23:50.670
That there are dangers.,00:23:50.670,00:23:51.950
"You could be well meaning, and you could
simply crash, instead of having",00:23:51.950,00:23:56.630
"the smaller car that may not be as
impressive, but it will definitely get",00:23:56.630,00:24:00.040
you where you want.,00:24:00.040,00:24:01.390
"Now we move into the main topic
of the lecture, which is logistic",00:24:01.390,00:24:06.940
"regression, which is a very
important linear model.",00:24:06.940,00:24:10.350
"And it complements the two models
we have seen so far--",00:24:10.350,00:24:13.090
"linear classification-- the perceptron,
and linear regression.",00:24:13.090,00:24:16.630
And there are three pieces.,00:24:16.630,00:24:18.190
"First, I'm going to describe
the model.",00:24:18.190,00:24:19.620
"What is the hypothesis set that
I'm trying to implement.",00:24:19.620,00:24:22.360
"And then we're going to devise
an error measure for it, which is",00:24:22.360,00:24:24.970
a pretty interesting error measure.,00:24:24.970,00:24:27.020
"And finally, we are going to go
for the learning algorithm",00:24:27.020,00:24:29.040
that goes with it.,00:24:29.040,00:24:29.800
"It turns out that the model is
different, the error measure is",00:24:29.800,00:24:32.190
"different, and the learning algorithm
is different from what",00:24:32.190,00:24:34.540
we have seen before.,00:24:34.540,00:24:35.690
"So by the time we have done this, we
will have covered enough territory in",00:24:35.690,00:24:38.350
these variations.,00:24:38.350,00:24:39.400
"And this will really be
very representative of",00:24:39.400,00:24:41.800
machine learning at large.,00:24:41.800,00:24:43.140
"So linear models will not only be
a very useful model to use.",00:24:43.140,00:24:46.230
"They also cover the concepts
in many techniques that",00:24:46.230,00:24:48.390
you will see otherwise.,00:24:48.390,00:24:49.490
"For example, the learning algorithm here
is the same learning algorithm we",00:24:49.490,00:24:52.470
"are going to use in neural
networks next time.",00:24:52.470,00:24:55.840
So let's start with the model.,00:24:55.840,00:24:58.570
Here's the third linear model.,00:24:58.570,00:25:00.930
"Being linear means that we take your
inputs, compute a signal that is",00:25:00.930,00:25:05.880
"a linear combination of the
input with weights, s.",00:25:05.880,00:25:09.050
"And then I take s, and
do stuff with it.",00:25:09.050,00:25:12.230
"And the stuff could be linear
classification, perceptrons.",00:25:12.230,00:25:17.160
And what was that?,00:25:17.160,00:25:18.920
"In this case, you take your
hypothesis to be a decision,",00:25:18.920,00:25:23.090
+1 or -1.,00:25:23.090,00:25:24.360
"And that decision is a direct
thresholding, with",00:25:24.360,00:25:28.360
"respect to 0, of the signal.",00:25:28.360,00:25:30.090
"So you take this linear signal, and this
is what you do to it in order to",00:25:30.090,00:25:32.620
"get the output, and that will
give you the perceptron.",00:25:32.620,00:25:35.300
Let's put it in a picture.,00:25:35.920,00:25:37.170
Here are your inputs.,00:25:41.940,00:25:43.750
"x_1 up to x_d, this is
the genuine input.",00:25:43.750,00:25:46.110
"This is the +1 that takes
care of the threshold.",00:25:46.110,00:25:49.000
"They go with these into the sum, so
this would be weights going, attached",00:25:49.000,00:25:55.240
to these guys.,00:25:55.240,00:25:56.360
"And then they are summed,
in order to give me s.",00:25:56.360,00:25:59.500
"And then one linear model or another
will be doing different things to s.",00:25:59.500,00:26:03.040
"The first model will take s and
pass it through this threshold, in",00:26:03.040,00:26:06.310
order to get +1 or -1.,00:26:06.310,00:26:08.600
"Now the second guy
was linear regression.",00:26:11.380,00:26:15.530
"What did we do to the signal in
the case of linear regression?",00:26:15.530,00:26:18.470
Nothing.,00:26:21.530,00:26:22.300
We left it alone.,00:26:22.300,00:26:23.340
"That was our output, right?",00:26:23.340,00:26:25.800
"So if you want to put it in
the same diagram, you can.",00:26:25.800,00:26:28.890
"And in this case, you do the linear
sum, et cetera, and you",00:26:28.890,00:26:32.990
get the signal.,00:26:32.990,00:26:33.520
"And then you have the identity
function, if you want.",00:26:33.520,00:26:36.720
You output what you input.,00:26:36.720,00:26:38.690
And that's what you get.,00:26:38.690,00:26:40.720
"Now when you go to the third guy, the
new guy, which is called logistic",00:26:40.720,00:26:43.440
"regression, we are going to take s
and apply a nonlinearity to it.",00:26:43.440,00:26:49.920
"The nonlinearity,",00:26:49.920,00:26:50.880
"which we're going to call theta,
the logistic function--",00:26:50.880,00:26:53.630
"it is not as harsh as
this nonlinearity.",00:26:53.630,00:26:56.750
"It is somewhere between this
and leaving it alone.",00:26:56.750,00:27:00.930
And it looks like this.,00:27:00.930,00:27:03.810
"You can see that's
an interesting thing.",00:27:03.810,00:27:06.540
I am bounded.,00:27:06.540,00:27:08.320
This is the least I can report.,00:27:08.320,00:27:09.760
This is the most I can report.,00:27:09.760,00:27:10.960
It looks bounded like this.,00:27:10.960,00:27:12.450
"It actually looks pretty much like this,
except for the softening of it.",00:27:12.450,00:27:15.820
"But it's a real value, I can return any
real value between this value and",00:27:15.820,00:27:19.360
"this value, so it has something
of the linear regression.",00:27:19.360,00:27:23.010
"And the main utility of logistic
regression is that the output is going",00:27:23.010,00:27:26.330
to be interpreted as a probability.,00:27:26.330,00:27:29.100
"And that will cover a lot of problems,
where we want to estimate the",00:27:29.100,00:27:31.710
probability of something.,00:27:31.710,00:27:34.330
Let's be specific.,00:27:34.330,00:27:36.940
"Let's look at the logistic
function theta, the",00:27:36.940,00:27:38.870
nonlinearity I talked about.,00:27:38.870,00:27:41.320
It looks like this.,00:27:41.320,00:27:43.780
"It can serve as a probability,
because it goes here from 0 to 1.",00:27:43.780,00:27:49.520
"And if you look at the signal, if the
signal is very, very negative, you're",00:27:49.520,00:27:52.890
close to probability 0.,00:27:52.890,00:27:54.300
"If the signal is very, very
positive, it's close to 1.",00:27:54.330,00:27:57.270
"And at signal 0, it's probability half.",00:27:57.270,00:28:00.120
"So that actually corresponds
to something meaningful.",00:28:00.120,00:28:01.930
"The signal corresponds to the level
of certainty I have about something.",00:28:01.930,00:28:06.270
"If I have a huge signal, I'm pretty sure
that a binary event will happen.",00:28:06.270,00:28:10.510
"And if the signal is very negative,
I'm pretty sure that it will not",00:28:10.510,00:28:13.660
"happen, so the probability is 0.",00:28:13.660,00:28:16.230
"Now there are many formulas that I can
have that we give you this shape.",00:28:16.230,00:28:20.150
"This shape is what I'm
interested in.",00:28:20.150,00:28:22.810
"And I'm going to choose
a particular formula.",00:28:22.810,00:28:25.500
And the formula is this.,00:28:25.500,00:28:29.570
It's not that difficult.,00:28:30.780,00:28:32.310
"I have an exponential
here and here.",00:28:32.310,00:28:34.540
"Let's say that you take s to go to
plus infinity-- very large signal.",00:28:34.540,00:28:38.250
This will be huge.,00:28:38.250,00:28:39.500
"And this will be the same amount, huge.",00:28:39.500,00:28:40.940
"The 1 will be negligible, so
the ratio will get closer",00:28:40.940,00:28:43.450
and closer to 1.,00:28:43.450,00:28:45.210
"If this s is negative, this is very
small, and this is very small.",00:28:45.210,00:28:47.930
"So the 1 dominates, and you will get
something small divided by 1, which is",00:28:47.930,00:28:51.110
very close to 0.,00:28:51.110,00:28:52.090
That's what I get here.,00:28:52.090,00:28:53.240
"And indeed, if you get s equals 0, you
will get 1 over 1 plus 1, which is",00:28:53.240,00:28:57.100
"a half, so it will give you this.",00:28:57.100,00:28:58.370
This is a nice function.,00:28:58.370,00:29:00.060
"It's indeed odd around
half, if you will.",00:29:00.060,00:29:02.890
"This part is the same
as this part.",00:29:02.890,00:29:05.260
"And the reason for taking this
particular form is that, when we plug",00:29:05.260,00:29:09.420
"it in, and we get the error measure, and
then we go for the optimization,",00:29:09.420,00:29:13.160
it will be a very friendly formula.,00:29:13.160,00:29:15.440
"You can have another formula that has
the same shape, and then you run into",00:29:15.440,00:29:18.540
"trouble when you go into
the next steps.",00:29:18.540,00:29:21.150
"So this is with a view to
what is going to happen.",00:29:21.150,00:29:24.300
"This thing is called soft threshold,
for obvious reasons.",00:29:26.650,00:29:30.110
"That is, the hard version would
be, just decide this or this.",00:29:30.110,00:29:34.150
"So this softens it, and gives you
a reliability of a decision.",00:29:34.150,00:29:37.020
"If you think that we are talking about
the credit card application, it",00:29:37.020,00:29:40.520
"used to be that I want to know
if a customer is good or bad.",00:29:40.520,00:29:43.380
"Instead of deciding if the customer
is good or bad, which is",00:29:43.380,00:29:45.500
"a binary classification, I ask
myself: what is the probability",00:29:45.500,00:29:48.500
that this customer will be good?,00:29:48.500,00:29:50.410
"Or what is the probability
that they will be bad?",00:29:50.410,00:29:52.000
"That is, what is the probability
of default?",00:29:52.000,00:29:54.670
"And then I can say 0.8, 0.7, 0.3, and
let the bank decide what to do",00:29:54.670,00:29:59.870
"according to this probability, to extend
credit, how much credit to do,",00:29:59.870,00:30:03.140
and so on.,00:30:03.140,00:30:04.150
So there is a utility for that.,00:30:04.150,00:30:06.270
"And the soft threshold
reflects the uncertainty.",00:30:06.270,00:30:08.880
"Seldom do we know the binary decision
with certainty, and it might be more",00:30:08.880,00:30:12.810
"information to give you the uncertainty
as part of the deal.",00:30:12.810,00:30:15.720
"And that is reflected in
this soft threshold.",00:30:15.720,00:30:19.070
"It's also called sigmoid, for a simple
reason-- because it looks like",00:30:19.070,00:30:24.270
a flattened out s.,00:30:24.270,00:30:26.460
"This is an s, so you call it sigmoid.",00:30:26.460,00:30:27.810
"You'll hear sigmoidal function, or soft
threshold, and whatnot.",00:30:27.810,00:30:30.590
"And there's more than one sigmoidal
function or soft threshold.",00:30:30.590,00:30:33.390
I told you this was one formula.,00:30:33.390,00:30:34.870
There are other formulas.,00:30:34.870,00:30:36.350
"In fact, when we go to neural networks,
there will be another formula that is",00:30:36.350,00:30:38.930
very closely related.,00:30:38.930,00:30:40.150
"And you can invent other
formulas, as well.",00:30:40.150,00:30:42.320
So this is the logistic function.,00:30:42.320,00:30:44.100
This is the model.,00:30:44.100,00:30:44.560
We know what the model does.,00:30:44.560,00:30:46.540
"The main idea is the probability
interpretation.",00:30:46.540,00:30:51.130
"We have the model-- the model is:
you take the linear signal, pass it",00:30:51.130,00:30:56.530
"through this logistic function, and
that will be your value of the",00:30:56.530,00:31:00.090
"hypothesis at the x that gave
rise to that signal.",00:31:00.090,00:31:05.200
"And we are going to interpret
it as a probability.",00:31:05.200,00:31:08.140
"So we think that there is a probability
sitting out there,",00:31:08.140,00:31:10.350
generating examples.,00:31:10.350,00:31:11.630
"Let's say a probability of default,
based on credit information.",00:31:11.630,00:31:18.550
"I'm going to give you an example
where it's patent that you",00:31:18.550,00:31:21.180
actually need a probability.,00:31:21.180,00:31:22.280
"It would be absurd to try to predict the
thing outright as a decision.",00:31:22.280,00:31:26.550
"And that is the unfortunate
prediction of heart attacks.",00:31:26.550,00:31:31.330
"Now heart attacks, or the risk of heart
attacks, depends on a number of",00:31:31.330,00:31:34.320
factors.,00:31:34.320,00:31:35.510
"And you would like to predict whether
there is a big risk or",00:31:35.510,00:31:38.280
a small risk.,00:31:38.280,00:31:39.990
"The kind of input you will have is
data that are relevant to having",00:31:39.990,00:31:45.410
a heart attack.,00:31:45.410,00:31:46.120
"You can look at cholesterol level,
the age, the weight-- the weight of",00:31:46.120,00:31:49.830
"the person, not the weight
of the input--",00:31:49.830,00:31:51.650
and so on.,00:31:51.650,00:31:52.370
"And then the output here would be
a probability, because if I told you",00:31:52.370,00:32:01.240
"to predict whether the person will have
a heart attack or not, and you",00:32:01.240,00:32:03.720
"say +1 or -1, I think this will
be laughable, because there are",00:32:03.720,00:32:07.160
"so many factors that affect it. You
will be correct very, very small",00:32:07.160,00:32:11.860
"amount of the time, and it's very difficult
to tell that your predictions are",00:32:11.860,00:32:15.070
better than another one.,00:32:15.070,00:32:15.900
"Both of you are wrong most
of the time.",00:32:15.900,00:32:18.490
"What we are doing here, we are actually
predicting the probability of",00:32:18.490,00:32:21.820
"a heart attack within
a time horizon.",00:32:21.820,00:32:23.770
"Let's say that you take this data today,
and I'm asking: what is the",00:32:23.770,00:32:27.130
"probability that you will get a heart
attack within the next 12 months?",00:32:27.130,00:32:30.110
That's the game.,00:32:30.110,00:32:31.000
"You return a number, and that will be
reflected by the output of logistic",00:32:31.000,00:32:33.940
regression.,00:32:33.940,00:32:37.660
"Now if you look at the signal that
goes into this thing, the signal",00:32:37.660,00:32:41.550
"goes from minus infinity
to plus infinity.",00:32:41.550,00:32:43.480
"And that's a linear sum of those guys,
that we take and process in order to",00:32:43.480,00:32:47.640
make it a probability.,00:32:47.640,00:32:49.150
Two things to observe.,00:32:49.150,00:32:50.530
"First, this remains linear.",00:32:50.530,00:32:53.600
"That is, you are actually
giving an importance--",00:32:53.600,00:32:57.390
"I'm going to call it ""importance""
because there's a weight here.",00:32:57.390,00:32:59.260
"So you're going to give an importance
for the age, an importance for the",00:32:59.260,00:33:02.210
"cholesterol level, an importance
for the other factor.",00:33:02.210,00:33:04.560
"But from then on, all you do is just
give the importance weight, and",00:33:04.560,00:33:11.250
sum them up.,00:33:11.250,00:33:12.600
"Now it's conceivable, obviously,
that this is bad, because",00:33:12.600,00:33:14.620
you look at the age.,00:33:14.620,00:33:15.260
"In terms of risk, basically 40
is critical for these things.",00:33:15.260,00:33:19.790
So it's not really linear.,00:33:19.790,00:33:21.310
"Above 40 or below 40 makes
a big difference.",00:33:22.060,00:33:24.590
So there is a non-linearity there.,00:33:24.590,00:33:26.120
Does this bother us?,00:33:26.120,00:33:27.810
No.,00:33:27.810,00:33:28.520
"We know that we can study
the clean linear system.",00:33:28.520,00:33:31.610
"And when the time comes to apply it,
we can always transform those to",00:33:31.610,00:33:35.510
"relevant features, and we have the
same machinery in place.",00:33:35.510,00:33:39.900
"The other aspect of the signal is
that this can be interpreted.",00:33:39.900,00:33:42.280
"You can think of it as a risk
score, if you will.",00:33:42.280,00:33:45.880
Remember the credit score?,00:33:45.880,00:33:47.660
"We have the credit score, and then
compare it to a threshold to decide",00:33:47.660,00:33:49.800
"to extend credit, or
not to extend credit.",00:33:49.800,00:33:51.810
This is a risk score.,00:33:51.810,00:33:53.360
"Although it's translated to probability
to make it meaningful, I",00:33:53.360,00:33:55.880
"can tell you, you add this up, and you
are 700, you're in trouble.",00:33:55.880,00:34:00.755
"You're minus 200, you're in
good shape, in general.",00:34:00.755,00:34:04.040
"But obviously, in order to interpret
them in an operational way, you need",00:34:04.040,00:34:06.840
"to put them through the logistic, in
order to get a probability which can",00:34:06.840,00:34:09.260
be interpreted.,00:34:09.260,00:34:10.080
"This is the probability that someone
will get a heart attack within",00:34:10.080,00:34:12.430
a certain time horizon.,00:34:12.430,00:34:14.070
"Now I'd like to make the point
that this is genuine probability.",00:34:14.070,00:34:19.190
What do I mean by that?,00:34:19.190,00:34:21.310
"You have a hypothesis that
goes from 0 to 1.",00:34:21.310,00:34:25.610
I'm interpreting it as a probability.,00:34:25.610,00:34:28.110
"But you could just think of it as
a function between 0 and 1.",00:34:28.110,00:34:32.900
"If I give you examples-- here is x, and
here is the probability, which is",00:34:32.900,00:34:36.030
a number between 0 and 1--,00:34:36.030,00:34:37.040
I'm going to learn it.,00:34:37.040,00:34:38.420
"And the fact that you are using it as
a probability is your business.",00:34:38.420,00:34:41.590
"I'm just going to take two functions,
try to get the difference between",00:34:41.590,00:34:44.230
"them, let's say mean squared
error, and learn.",00:34:44.230,00:34:46.880
"The main point here is that the output
of the logistic regression is",00:34:46.880,00:34:50.260
"treated genuinely as a probability,
even during learning.",00:34:50.260,00:34:54.550
Why is that?,00:34:54.550,00:34:56.630
"This is because the data that is
given to you does not tell you the",00:34:56.630,00:34:59.940
probability.,00:34:59.940,00:35:01.570
"I don't give you the first patient,
and here are the data.",00:35:01.570,00:35:07.080
"And-- this is supervised learning,
right, so I have to give",00:35:07.080,00:35:10.000
you the label--,00:35:10.000,00:35:11.600
"the probability of getting a heart
attack in 12 months is 25%.",00:35:11.600,00:35:17.650
How the heck would I know that?,00:35:17.650,00:35:20.050
"I can only get whether someone
got a heart attack or",00:35:20.050,00:35:22.590
didn't get a heart attack.,00:35:22.590,00:35:24.190
"Well, that is affected by the
probability, but I don't have access",00:35:24.190,00:35:26.860
to the probability.,00:35:26.860,00:35:28.140
"So this is a noisy case, where the nature
of the example is that I give",00:35:28.140,00:35:31.680
"you a binary output that is affected
by the probability.",00:35:31.680,00:35:35.450
"This is generated by a noisy target,
so let's put the noisy target",00:35:35.450,00:35:39.610
"in order to understand where the
examples are coming from.",00:35:39.610,00:35:43.460
It's the probability of y given x.,00:35:43.460,00:35:45.690
That is what noisy targets are.,00:35:45.690,00:35:48.450
"And they have the form of a certain
probability that the person gets heart",00:35:48.450,00:35:54.340
"attack, and a certain probability that
they don't get a heart attack, given",00:35:54.340,00:35:58.960
their data.,00:35:58.960,00:36:01.910
"And this is generated by the target
that I want to learn.",00:36:01.910,00:36:05.420
"So I'm going to call the probability
the target function itself.",00:36:05.420,00:36:09.470
"The probability that someone
gets heart attack is f of x.",00:36:09.470,00:36:14.050
"And the probability that they don't--
it's a binary thing-- has to be 1",00:36:14.050,00:36:16.890
minus f of x.,00:36:16.890,00:36:19.050
"And I'm trying to learn f,
notwithstanding the fact that the",00:36:19.050,00:36:23.610
"examples that I am getting are giving me
just sample values of y, that happen",00:36:23.610,00:36:29.420
to be generated by f.,00:36:29.420,00:36:31.260
"I want to take the examples, and then
generate h that approximates the",00:36:31.260,00:36:35.950
hidden target function.,00:36:35.950,00:36:38.360
Understood the game?,00:36:38.360,00:36:39.580
That's why it's genuine probability.,00:36:39.580,00:36:41.210
"It's not only 0,1, it's also that the
examples that I am given have already,",00:36:41.210,00:36:45.310
"inherently, a probabilistic
interpretation.",00:36:45.310,00:36:48.260
So the target is from,00:36:48.260,00:36:52.130
"the d-dimensional Euclidean space
to 0,1, the interval.",00:36:52.130,00:36:55.720
"And it is interpreted
as a probability.",00:36:55.720,00:36:58.890
"And now, you want to learn the final
hypothesis, which will be called g of",00:36:58.890,00:37:03.950
"x, which happens to have the
form of logistic regression.",00:37:03.950,00:37:08.380
That's the model we are talking about.,00:37:08.380,00:37:09.850
"So you are going to find the weights,
and then you're going to",00:37:09.850,00:37:14.310
"dot-product it with x, and pass
it through the nonlinearity.",00:37:14.310,00:37:16.880
"And the claim you are going to end
up saying is that this is",00:37:16.880,00:37:20.590
approximately f of x--,00:37:20.590,00:37:22.420
"the real guy, the probability
here.",00:37:22.420,00:37:26.610
"And you are going to try to make that as
true as possible, according to some",00:37:26.610,00:37:29.840
"error measure that we
are going to define.",00:37:29.840,00:37:32.110
And what is under your control?,00:37:32.110,00:37:34.140
"As always with linear models, what
is under your control are",00:37:34.140,00:37:37.850
the parameters.,00:37:37.850,00:37:39.630
"You change the parameters in order
to get one hypothesis or another.",00:37:39.630,00:37:42.370
"So the question now becomes, how do
I choose the weights such that the",00:37:42.370,00:37:46.730
"logistic regression hypothesis reflects
the target function, knowing",00:37:46.730,00:37:50.990
"that the target function is the way
the examples were generated?",00:37:50.990,00:37:54.860
That's the game.,00:37:54.860,00:37:56.720
"So let's talk about
the error measure.",00:37:56.720,00:38:00.740
"Now again, remember in error measures,
we had the proper way of generating",00:38:00.740,00:38:05.630
"an error measure, and then we had plan B.
In the absence of very specific way of",00:38:05.630,00:38:11.000
paying a price--,00:38:11.000,00:38:11.960
"if you predict a heart attack and
it doesn't happen, what is the cost?",00:38:11.960,00:38:17.030
"The guy is alarmed, et cetera.",00:38:17.030,00:38:18.610
"If you don't predict it, and it happens,
then maybe you should have",00:38:18.610,00:38:21.430
taken precautions and whatnot.,00:38:21.430,00:38:22.640
"And you can put prices, and do
all of the analysis.",00:38:22.640,00:38:24.782
That is not done.,00:38:24.782,00:38:26.050
"Now we are resorting to the case where
we use analytic properties,",00:38:26.050,00:38:29.440
"something plausible that makes this
look like an error measure--",00:38:29.440,00:38:32.650
"Yeah, if I actually minimize
this error measure, I'm",00:38:32.650,00:38:34.590
going to be doing well.,00:38:34.590,00:38:35.870
"Or take something that will be friendly
to the optimizer, that after",00:38:35.870,00:38:40.120
"I do it and pass it to the optimizer,
the optimizer will easy time",00:38:40.120,00:38:42.740
minimizing it.,00:38:42.740,00:38:43.760
"Well, it turns out that in this case,
the error measure that I'm going to",00:38:43.760,00:38:47.110
describe has both properties.,00:38:47.110,00:38:48.820
"It's plausible, and friendly.",00:38:48.820,00:38:50.990
So it's a very popular error measure.,00:38:50.990,00:38:53.140
So let's construct it.,00:38:53.140,00:38:55.960
"For each point x and y-- and remember
that y is binary, +1 or -1,",00:38:55.960,00:38:59.930
"that is generated by the
target function f.",00:38:59.930,00:39:02.380
"We have the following plausible
error measure.",00:39:07.450,00:39:09.800
Here is the argument.,00:39:09.800,00:39:12.500
It is based on likelihood.,00:39:12.500,00:39:15.340
"Likelihood is a very established
notion in statistics, not without",00:39:15.340,00:39:19.620
controversy.,00:39:19.620,00:39:20.230
"But nonetheless, it's
very widely applied.",00:39:20.230,00:39:23.960
"And the idea of it is that I'm going
to grade different hypotheses",00:39:23.960,00:39:27.570
"according to the likelihood that they
are actually the target that",00:39:27.570,00:39:31.360
generated the data.,00:39:31.360,00:39:33.120
So let's be specific.,00:39:33.120,00:39:36.010
"We assume that, at your current
hypothesis, let's say that this was",00:39:36.010,00:39:39.510
"actually the target function,
just for the moment.",00:39:39.510,00:39:43.460
"You have the data, right?",00:39:43.460,00:39:44.950
"The data was generated by
the target function.",00:39:44.950,00:39:47.310
"So you can ask: what is the probability
of generating this data if",00:39:47.310,00:39:50.870
your assumption is true?,00:39:50.870,00:39:52.980
"If that probability is very small, then
your assumption must be poor.",00:39:52.980,00:39:57.700
"And if that probability is high, then
your assumption has more plausibility.",00:39:57.700,00:40:01.870
"So I can use this to build a comparative
way to saying that this is more",00:40:01.870,00:40:06.030
"plausible hypothesis than another,
because the data becomes more likely",00:40:06.030,00:40:09.340
"under a scenario of this hypothesis,
rather than this hypothesis, being the",00:40:09.340,00:40:13.340
actual target function.,00:40:13.340,00:40:15.640
This is the idea.,00:40:15.640,00:40:17.650
"You ask yourself, how likely?",00:40:17.650,00:40:18.870
And the difference--,00:40:18.870,00:40:19.530
So I said about controversy.,00:40:19.530,00:40:20.950
"The controversy is a bit subtle, related
to the controversy that I raised",00:40:20.950,00:40:24.120
early on in the course.,00:40:24.120,00:40:26.210
"The thing you are really trying to
find, if you decided to use",00:40:26.210,00:40:30.000
"a probabilistic approach, which is your
thing, for choosing the hypothesis.",00:40:30.000,00:40:34.630
"What you're trying to find is, what
is the most probable hypothesis",00:40:34.630,00:40:38.030
given the data?,00:40:38.030,00:40:39.810
That would be completely clean.,00:40:39.810,00:40:42.350
"Now here, you are asking, what is the
probability of the data given the",00:40:42.350,00:40:47.830
"hypothesis, which is backwards.",00:40:47.830,00:40:50.420
"It has plausibility. That's
why it's called likelihood.",00:40:50.420,00:40:52.940
"It's not exactly the probability
we want.",00:40:52.940,00:40:54.750
"And the people who don't like likelihood
would add a prior and use",00:40:54.750,00:40:59.650
"a Bayesian approach, which looks
principled, but then there's a big",00:40:59.650,00:41:02.700
assumption in it.,00:41:02.700,00:41:03.370
"So this is never a completely
clean thing.",00:41:03.370,00:41:06.520
"It could be clean in terms of
derivation, but conceptually,",00:41:06.520,00:41:09.560
there's always a funny aspect to it.,00:41:09.560,00:41:12.050
"But we will swallow that, because
it looks very reasonable that",00:41:12.050,00:41:15.190
"if I choose a hypothesis under which having
that data is very plausible, it",00:41:15.190,00:41:19.050
"looks like this hypothesis is likely,
hence the likelihood name.",00:41:19.050,00:41:23.270
"So this is the probability
distribution we have, right?",00:41:23.270,00:41:29.330
"This is the genuine probability
distribution for generating the y.",00:41:29.330,00:41:32.910
"So under the assumption that h is f,
that probability would be-- if I used",00:41:32.910,00:41:38.560
"h to generate it, that would be my
measure of the probability of the data",00:41:38.560,00:41:42.170
under this assumption.,00:41:42.170,00:41:43.770
"This will be the way for you
to define the likelihood.",00:41:43.770,00:41:49.350
"Assume it was generated by h, compute
this probability, and that will be the",00:41:49.350,00:41:54.010
"likelihood of the hypothesis given
one data point x,y.",00:41:54.010,00:41:58.620
"Now let's use this in order to derive
a full-fledged error measure.",00:41:58.620,00:42:02.960
What are you going to do?,00:42:02.960,00:42:04.560
"You are going to take the
formula for likelihood--",00:42:08.740,00:42:10.610
this.,00:42:10.610,00:42:11.590
That is for one point.,00:42:11.590,00:42:14.140
"And then you have a formula
for h of x.",00:42:14.140,00:42:18.560
You're using logistic regression.,00:42:18.560,00:42:20.170
"So in this case, this thing happens
to be that formula.",00:42:20.170,00:42:24.810
"It does depend on x, as
you expect it to.",00:42:24.810,00:42:27.220
"And it does the dependency through
the choice of parameters w, and",00:42:27.220,00:42:30.540
passing through a nonlinearity theta.,00:42:30.540,00:42:33.280
"Now I don't like the fact that
these are cases, because I",00:42:33.280,00:42:36.540
want something analytic.,00:42:36.540,00:42:37.560
"This is a number that I'm going to take
and multiply, and take logarithms",00:42:37.560,00:42:40.390
"of, and I don't want to keep
worrying about cases.",00:42:40.390,00:42:43.440
"So now something comes handy here, which
is the following observation.",00:42:43.440,00:42:48.600
"The sigmoid-- the logistic function--
happens to satisfy that theta of minus",00:42:48.600,00:42:53.350
s equals 1 minus theta of s.,00:42:53.350,00:42:55.900
"Let's first look at it
pictorially, and then",00:42:55.900,00:42:58.300
see why this is useful.,00:42:58.300,00:42:59.770
Can you verify that?,00:42:59.770,00:43:00.630
Yeah.,00:43:00.630,00:43:01.250
We said that this is odd around half.,00:43:01.250,00:43:03.080
"If you go theta of minus s, this
would be 1 minus this guy.",00:43:03.080,00:43:05.590
"Very easy to verify, and you can
verify it from the formula,",00:43:05.590,00:43:07.700
straightforward.,00:43:07.700,00:43:08.960
"The good thing about it is that 1 minus
theta looks like this fellow.",00:43:08.960,00:43:13.690
"And I only add a minus sign in this
case, but the minus sign is readily",00:43:13.690,00:43:17.650
"available, because this case
goes for y equals -1.",00:43:17.650,00:43:20.570
"So it's already crying for a simplification,
and the simplification",00:43:20.570,00:43:23.910
"would be that P of y given x, in
general, equals this.",00:43:23.910,00:43:30.520
What did I do?,00:43:30.520,00:43:31.050
"For the case +1, I have it
straightforward, because this is +1.",00:43:31.050,00:43:34.990
Nothing changes.,00:43:34.990,00:43:35.860
"And in the case -1, I have minus this,
which is 1 minus, and that gives",00:43:35.860,00:43:39.660
me this formula.,00:43:39.660,00:43:40.540
"So it's summarized by this
very simply case.",00:43:40.540,00:43:42.970
"So I have one example x and y, and I
want to get the likelihood of this w,",00:43:42.970,00:43:47.050
given a single example.,00:43:47.050,00:43:48.520
"This would be the measure
for that likelihood.",00:43:48.520,00:43:51.430
That's good.,00:43:51.430,00:43:53.560
"Now we have the likelihood
of the entire data set.",00:43:53.560,00:43:56.960
"Someone gives you a bunch of
patients, and whether they had a heart",00:43:56.960,00:44:00.210
"attack within 12 months
of the measurements--",00:44:00.210,00:44:02.570
and quite a number of them.,00:44:02.570,00:44:04.380
"And now I would like to say,
what is the likelihood of",00:44:04.380,00:44:06.260
this entire data set?,00:44:06.260,00:44:08.030
"The assumption, as always,
is the independence from",00:44:08.030,00:44:10.050
one example to another.,00:44:10.050,00:44:11.560
"And therefore, if I want to get the
likelihood of the data set, I'm going",00:44:11.560,00:44:15.330
to simply--,00:44:15.330,00:44:16.515
let me magnify this--,00:44:16.515,00:44:20.005
"this would be: I'm multiplying the
likelihood of individual ones from n",00:44:20.005,00:44:23.690
"equals 1 to N, covering
the data set.",00:44:23.690,00:44:25.980
"Now I need a formula for that. Well,
it's ready because I already have",00:44:28.760,00:44:32.410
a formula for P of y given x.,00:44:32.410,00:44:34.230
All I need to do is plug it.,00:44:34.230,00:44:35.370
"And when I plug it, I end
up with this thing.",00:44:35.370,00:44:38.480
"That's a very nice formula because
now you realize, I",00:44:39.050,00:44:41.370
have a bunch of examples.,00:44:41.370,00:44:42.410
"They have different +1 or -1's that
come in here, different x_n's. The",00:44:42.410,00:44:46.540
"same w of my hypothesis contributes
to all of these terms.",00:44:46.540,00:44:51.400
"So now you can find that there
will be a compromise.",00:44:51.400,00:44:53.590
"If I choose w to favor one example,
I'm messing up the others,",00:44:53.590,00:44:56.470
so I have to find a compromise.,00:44:56.470,00:44:57.800
"And the compromise is likely to reflect
that I am catching something",00:44:57.800,00:45:01.310
"for the underlying probability
distribution that generated these",00:45:01.310,00:45:05.120
examples in the first place.,00:45:05.120,00:45:07.320
"Now let's go for what happens when
we maximize this likelihood.",00:45:07.320,00:45:13.520
"We'll write it down, and then we'll
take it, and the maximizing of",00:45:13.520,00:45:17.250
"likelihood will translate to the
minimizing of an error measure as we",00:45:17.250,00:45:20.690
"know it, or as we have
been familiar with.",00:45:20.690,00:45:23.230
"First thing, we're maximizing.",00:45:23.230,00:45:26.700
"Remember, in the error measure,
we are minimizing.",00:45:26.700,00:45:28.220
"So something will happen through this
slide that will make maximization go",00:45:28.220,00:45:31.880
into minimization.,00:45:31.880,00:45:32.860
That shouldn't be too difficult.,00:45:32.860,00:45:34.600
"But let's look at what
we are maximizing.",00:45:34.600,00:45:37.570
"We are maximizing the likelihood of
this hypothesis, under the data set",00:45:37.570,00:45:43.940
that we were given.,00:45:43.940,00:45:44.480
"Given the data set, how likely
is this hypothesis?",00:45:44.480,00:45:47.280
"Which means, what is the probability of
that data set under the assumption",00:45:47.280,00:45:50.880
"that this hypothesis is
indeed the target?",00:45:50.880,00:45:53.560
And maximizing with respect to what?,00:45:53.560,00:45:56.300
To something that will turn purple.,00:45:56.300,00:45:58.970
That is our parameter.,00:45:58.970,00:46:00.120
"We are maximizing this function
with respect to w.",00:46:00.120,00:46:03.950
Now I'm going to play with this.,00:46:03.950,00:46:05.040
"We are maximizing this, right?",00:46:05.040,00:46:07.140
Can I maximize this instead?,00:46:08.720,00:46:09.990
Yeah.,00:46:12.570,00:46:13.350
"First, it's legitimate to do.",00:46:13.350,00:46:14.940
This is the natural logarithm.,00:46:14.940,00:46:16.390
"First, it's legitimate to take it,
because the quantity here is",00:46:16.390,00:46:18.870
"non-negative, right?",00:46:18.870,00:46:20.820
"theta, by nature, is positive.",00:46:20.820,00:46:23.460
"So I'm not taking logarithm of
0, or logarithm of negatives--",00:46:23.460,00:46:27.800
"things that are not allowed, so
first, I am allowed to take it.",00:46:27.800,00:46:30.370
"Second part is that the logarithm happens
to be monotonically increasing of",00:46:30.370,00:46:34.150
its argument.,00:46:34.150,00:46:34.820
"If you maximize it, you maximize
its argument, or vice versa.",00:46:34.820,00:46:37.910
So I am allowed to do that.,00:46:37.910,00:46:39.740
"I like that, so let
me play it further.",00:46:39.740,00:46:43.570
Can I do this?,00:46:43.570,00:46:45.820
Yes.,00:46:45.820,00:46:46.680
That's just proportional.,00:46:46.680,00:46:47.590
The monotonicity,00:46:47.590,00:46:48.710
still goes.,00:46:48.710,00:46:49.830
"But you can see where this
is going, right?",00:46:49.830,00:46:52.146
"I'm trying to get an error measure,
error measure in the training set.",00:46:52.146,00:46:54.970
That used to be what?,00:46:54.970,00:46:55.910
"1 over N summation of errors
on individual guys.",00:46:55.910,00:47:00.050
So you can see that this is shaping.,00:47:00.050,00:47:01.900
One final thing.,00:47:01.900,00:47:03.410
Can I do this?,00:47:03.410,00:47:04.660
No.,00:47:07.810,00:47:10.040
You are maximizing.,00:47:10.040,00:47:11.140
This is not--,00:47:11.140,00:47:12.420
"but then all you need to do is, instead
of maximizing, you minimize.",00:47:12.420,00:47:18.800
We are cool.,00:47:19.640,00:47:20.760
So this is the problem.,00:47:20.760,00:47:22.190
Now let's see what it's--,00:47:22.190,00:47:26.650
"Miraculously, the logarithm-- the product
becomes a sum of the logarithms.",00:47:26.650,00:47:29.940
"and I do this, the minus takes
the guy, and the logarithm",00:47:29.940,00:47:33.220
puts it in the denominator.,00:47:33.220,00:47:34.330
"And after all of this very sophisticated
algebra, we end up with",00:47:34.330,00:47:38.020
"something that looks very
suspiciously familiar--",00:47:38.020,00:47:40.990
"1 over N, summation from n
equals 1 to N.",00:47:40.990,00:47:44.940
"Something that involves the value of the
example, and the parameters that",00:47:44.940,00:47:50.540
I'm trying to learn.,00:47:50.540,00:47:53.160
"Anybody has seen something
like that before?",00:47:53.160,00:47:54.530
"We're going to give it the proper
name in a moment, but",00:47:54.530,00:47:56.600
this is what we have.,00:47:56.600,00:47:57.990
"Now I'd like to reduce this further,
so I'm going to remember what theta",00:47:57.990,00:48:02.390
"was, because theta is a mysterious
quantity. I want to put it in terms",00:48:02.390,00:48:06.710
that I am completely familiar with.,00:48:06.710,00:48:08.640
"So theta was this guy, which was e to
the s divided by e to the s plus 1.",00:48:08.640,00:48:12.880
"Now I can reduce this by dividing both
the numerator and denominator by e to the",00:48:12.880,00:48:16.770
"s, in which case this
will become this.",00:48:16.770,00:48:20.940
No surprise.,00:48:20.940,00:48:22.570
Why is this good?,00:48:22.570,00:48:23.670
"Well, this is good because I have theta
here, so if I substitute it here, the",00:48:23.670,00:48:28.120
1 plus will go in the numerator.,00:48:28.120,00:48:30.190
Good things will happen.,00:48:30.190,00:48:31.060
"So let me substitute and
see what happens.",00:48:31.060,00:48:33.720
"Now because what I'm going to get is
very clean, and very close to the",00:48:33.720,00:48:36.690
"formula, I'm going to officially declare
it the in-sample error of",00:48:36.690,00:48:41.410
logistic regression.,00:48:41.410,00:48:43.590
"I am minimizing it, so it's legitimate,
and it looks like this.",00:48:43.590,00:48:48.870
This is simply substituting,00:48:48.870,00:48:49.950
"in the above formula, I get this.",00:48:49.950,00:48:53.190
Now this is very nice.,00:48:53.190,00:48:54.750
"I have this term, depending
on this example.",00:48:54.750,00:48:57.660
"I'm summing them up, so I'm completely
within my rights, since I'm minimizing",00:48:57.660,00:49:01.600
"this as my in-sample error,
to call this fellow what?",00:49:01.600,00:49:05.242
To call it the error measure.,00:49:05.242,00:49:07.090
Let me magnify it.,00:49:07.090,00:49:09.410
"This is something that depends
on the particular example.",00:49:10.380,00:49:15.610
"I'm going to call it the error measure
between my hypothesis, which depends",00:49:15.610,00:49:21.030
"on w, applied to x_n, and the value you
gave me as a label for that example,",00:49:21.030,00:49:27.510
which is y_n.,00:49:27.510,00:49:28.260
"That is the way we define error
measures on points.",00:49:28.260,00:49:31.020
"So this is my formula for the error
measure, and under that, maximizing",00:49:31.020,00:49:35.030
"the likelihood would be like minimizing
the in-sample error.",00:49:35.030,00:49:39.330
"Now let me leave this for a moment,
just to mention a point.",00:49:40.840,00:49:44.690
"There is an interesting
interpretation here.",00:49:44.690,00:49:47.400
"If you look at w transposed x_n, this
is what we call the risk score.",00:49:47.400,00:49:54.810
"If this is very positive, the guy
is likely to get a heart attack.",00:49:54.810,00:49:58.630
"If it's very negative, the guy is very
unlikely to get a heart attack.",00:49:58.630,00:50:02.780
"Now this one is whether that particular
person, that supplied the",00:50:02.780,00:50:06.460
"data, ended up with a heart
attack or not.",00:50:06.460,00:50:09.570
"Let's see agreement/disagreement,
and how they affect the",00:50:09.570,00:50:12.500
error measure.,00:50:12.500,00:50:14.840
"If this signal is very positive,
and this guy is +1-- so the guy",00:50:14.840,00:50:20.850
"actually, unfortunately,
got a heart attack--",00:50:20.850,00:50:23.000
"then the result is that
this is minus a lot.",00:50:23.000,00:50:26.390
"And therefore, this is a very small
number, and the contribution to the error",00:50:26.390,00:50:29.680
is small.,00:50:29.680,00:50:30.710
I'm already in good shape.,00:50:30.710,00:50:31.750
My predictions are right.,00:50:31.750,00:50:33.900
"However, if the sign is different-- if
I say this is very positive, and this",00:50:33.900,00:50:37.240
"ends up being -1, or if this is very
negative and this ends up",00:50:37.240,00:50:40.740
"being +1,",00:50:40.740,00:50:41.540
"the end result is this will be
a positive exponential, and the error",00:50:41.540,00:50:45.200
will be huge.,00:50:45.200,00:50:46.080
"And I need to do something, in
order to knock it down.",00:50:46.080,00:50:48.480
"So indeed, this is very intuitive",00:50:48.480,00:50:49.880
"under that interpretation, that
this would be an error measure",00:50:49.880,00:50:52.500
that I would be trying to minimize.,00:50:52.500,00:50:54.780
What is this error measure called?,00:50:54.780,00:50:58.690
It is called the cross-entropy error.,00:50:58.690,00:51:00.540
"And I am putting it between quotation,
because the way to get it strictly to",00:51:00.540,00:51:03.850
"be cross-entropy is to interpret
a binary event as if it was",00:51:03.850,00:51:06.660
"a probability, but we will accept that.
It's universally referred to as",00:51:06.660,00:51:09.855
cross-entropy.,00:51:09.855,00:51:10.480
"And we will call this the cross-entropy,
basically between",00:51:10.480,00:51:14.260
supposedly h and f.,00:51:14.260,00:51:15.690
"Not really h and f, but h and
a particular realization of f.",00:51:15.690,00:51:19.400
So this is what we want to do.,00:51:19.400,00:51:21.700
"Now we have defined the model,
and we have defined the error measure.",00:51:21.700,00:51:27.800
"The remaining order is to do
the learning algorithm.",00:51:27.800,00:51:30.650
I know the error measure.,00:51:30.650,00:51:31.715
I just want to minimize it.,00:51:31.715,00:51:33.170
How can I do that?,00:51:33.170,00:51:34.290
It's an easy question.,00:51:34.290,00:51:39.900
"If you look at logistic regression,
we have just developed the",00:51:39.900,00:51:42.500
error measure for it.,00:51:42.500,00:51:44.220
"So now I have this function,
and I want to minimize it with",00:51:44.220,00:51:47.150
respect to w.,00:51:47.150,00:51:48.850
"Let's look at a previously tackled case,
in order to see how we went about that.",00:51:48.850,00:51:53.220
Remember linear regression?,00:51:53.220,00:51:55.990
"We also had an error function that
was derived directly. We just said",00:51:55.990,00:51:58.620
"mean squared, and wrote it down.",00:51:58.620,00:51:59.510
"We didn't have to go through
this long derivation.",00:51:59.510,00:52:02.520
"But then the in-sample error
ended up being this.",00:52:02.520,00:52:05.570
"Very similar to this, except here we
penalize the difference, squared.",00:52:05.570,00:52:09.420
"Here, we penalize it according
to this cross-entropy thing.",00:52:09.420,00:52:12.870
"Now in the case of linear regression, if
we want to minimize this, we found",00:52:12.870,00:52:15.570
"a very simple way to do it, because we
found a closed-form solution for the",00:52:15.570,00:52:19.240
"minimum, that was the pseudo-inverse,
the one-step learning.",00:52:19.240,00:52:23.630
"Unfortunately here, we're
out of luck.",00:52:23.630,00:52:26.300
"If you use the derivative here and you
try to solve it, it is exponential",00:52:26.300,00:52:28.940
"and things will come out, and you
sum it up, and you cannot find",00:52:28.940,00:52:31.820
a closed-form solution.,00:52:31.820,00:52:33.580
"Well, in the absence of a closed-form
solution, we usually go for",00:52:33.580,00:52:36.850
an iterative solution.,00:52:36.850,00:52:38.970
"We're not going to go for
the solution directly.",00:52:38.970,00:52:40.870
"We're going to improve, improve,
improve, improve.",00:52:40.870,00:52:42.990
"Finally, we'll get the good solution.",00:52:42.990,00:52:44.810
This is not a foreign concept to us.,00:52:44.810,00:52:46.350
"This is what we did with perceptrons,
although we didn't explicitly do it at",00:52:46.350,00:52:49.560
"the time in terms of a declared
error function.",00:52:49.560,00:52:52.570
"We went and tried to improve one example
at a time, and kept repeating",00:52:52.570,00:52:55.750
until we got what we want.,00:52:55.750,00:52:57.060
"So that's what we're
going to do here.",00:52:57.060,00:52:58.540
"But here, we are going to do it based on
calculus, and the method that will",00:52:58.540,00:53:02.070
"be used for minimization can be applied
to any error measure, even",00:53:02.070,00:53:07.200
"nonlinear and so on, as long as
there's some smoothness constraint.",00:53:07.200,00:53:10.210
"And it will be the one that will be
applied to neural networks next time.",00:53:10.210,00:53:12.910
Very famous one.,00:53:12.910,00:53:13.910
It's a very simplistic one.,00:53:13.910,00:53:14.950
"And there are more sophisticated
versions of it.",00:53:14.950,00:53:17.350
It's called gradient descent.,00:53:17.350,00:53:19.210
Let's see what is there.,00:53:19.210,00:53:21.270
"First, let me show you what the
error measure for logistic",00:53:21.270,00:53:25.340
regression looks like.,00:53:25.340,00:53:26.120
"As you vary the weight, the value of
the error differs, but it has this",00:53:26.120,00:53:30.480
"great property that it
has one minimum.",00:53:30.480,00:53:34.140
"And otherwise, it goes like that.",00:53:34.140,00:53:36.240
"A function that goes like that--
it's called convex.",00:53:36.240,00:53:40.450
"And it goes with convex optimization,
which is very easy, because obviously",00:53:40.450,00:53:43.890
"wherever you start, you will go to
the same valley.",00:53:43.890,00:53:47.300
"You can imagine a more
sophisticated nonlinear surface,",00:53:47.300,00:53:50.250
where you do this and that.,00:53:50.250,00:53:51.600
"And then depending on where you start,
if you're sliding down, you will end",00:53:51.600,00:53:54.660
up in one minimum or another.,00:53:54.660,00:53:55.910
"There are other issues that arise, and
we will only tackle them when we need",00:53:55.910,00:53:59.130
"them, when we talk about the error
measure for neural networks.",00:53:59.130,00:54:01.630
"Right now, we have a very friendly
guy, so we're going to describe",00:54:01.630,00:54:04.230
"gradient descent in terms
of this friendly guy.",00:54:04.230,00:54:07.300
"So what do you do with
gradient descent?",00:54:07.300,00:54:09.190
"First, we admit that it's a general
method for nonlinear optimization.",00:54:09.190,00:54:13.900
"And what you do, you
start at a point--",00:54:13.900,00:54:16.610
"initialization, pretty much
like you initialize the perceptron.",00:54:16.610,00:54:19.420
"And then you take a step, and you
try to make an improvement",00:54:19.420,00:54:22.430
using that step.,00:54:22.430,00:54:24.280
"The step is to take the step
along the steepest slope.",00:54:24.280,00:54:30.790
"The steepest slope is not an easy
notion to see in two dimensions,",00:54:30.790,00:54:33.880
because I either go right or left.,00:54:33.880,00:54:35.640
There aren't too many directions.,00:54:35.640,00:54:37.060
So let's do the following.,00:54:37.060,00:54:39.110
"Let's say that I'm in
three dimensions.",00:54:39.110,00:54:40.960
"And in this room, I have a very
nonlinear surface, going up and down,",00:54:40.960,00:54:44.920
up and down.,00:54:44.920,00:54:45.680
"I'm going to assume one thing, that
it's twice differentiable.",00:54:45.680,00:54:48.600
"That's what you need to invoke
gradient descent.",00:54:48.600,00:54:51.390
"You can think of hills
and this and that.",00:54:51.390,00:54:53.960
"Now I'm trying to get to the
minimum of that surface.",00:54:53.960,00:54:57.220
"First thing to remember in optimization,
you don't get to see the",00:54:57.220,00:55:01.210
surface.,00:55:01.210,00:55:02.690
"You don't have a bird's eye
view and you look at it--",00:55:02.690,00:55:05.430
That region looks good.,00:55:05.430,00:55:06.730
Let's go there.,00:55:06.730,00:55:07.700
That doesn't happen.,00:55:07.700,00:55:08.990
"You only have local information
at the point you evaluated.",00:55:08.990,00:55:13.200
"So the best thing to imagine is that
you are sitting on the surface, and",00:55:13.200,00:55:17.170
then you close your eyes.,00:55:17.170,00:55:19.230
"And all you do is feel around you, and
then decide this is a more promising",00:55:19.230,00:55:24.110
direction than this.,00:55:24.110,00:55:26.100
That's all you do at one step.,00:55:26.100,00:55:27.930
"And then when you go to the new point,
repeat, repeat, repeat until",00:55:27.930,00:55:31.380
you get to the minimum.,00:55:31.380,00:55:32.700
"These are all the iterative method
that you're going to use.",00:55:32.700,00:55:37.720
"So we go back, and we start at w(0).",00:55:37.720,00:55:41.760
"And then we look at
a fixed step size.",00:55:41.760,00:55:45.310
I am at a point.,00:55:45.310,00:55:46.340
"I am going to move in w space, by
a certain amount, and I'm going to take",00:55:46.340,00:55:50.650
it to be fixed and small.,00:55:50.650,00:55:52.660
"The reason I'm doing that is because I
am going to apply local approximations",00:55:52.660,00:55:57.880
"based on calculus, Taylor series.",00:55:57.880,00:55:59.900
"And I know that this will apply well
if the move is not that big.",00:55:59.950,00:56:04.830
"If I go very far, the higher-order terms
kick in, and I'm not sure that",00:56:04.830,00:56:08.650
"the conclusion that I got locally will
apply if I just get a 1st order, or",00:56:08.650,00:56:12.280
a 2nd order as in the other methods.,00:56:12.280,00:56:14.830
"So for the fixed step size, I'm going to
say, I'm moving in the w space.",00:56:14.830,00:56:19.000
"I'm moving by a unit vector v.
v hat is a unit vector.",00:56:19.000,00:56:23.700
So this tells me just the direction.,00:56:23.700,00:56:25.140
"Should I go this way, that way?",00:56:25.140,00:56:26.490
If I'm doing the sensing--,00:56:26.490,00:56:27.610
"oh, this is steep, so I'm
going to go this way.",00:56:27.610,00:56:29.570
"The unit vector in this direction
would be my v hat.",00:56:29.570,00:56:34.000
"And I'm going to modulate the amount
of move by a step size, which I'm",00:56:34.000,00:56:40.010
going to call eta.,00:56:40.010,00:56:42.180
So this is the amount of move.,00:56:42.180,00:56:43.380
"The only unknown I have is what is v.
I already decided on the size, but",00:56:43.380,00:56:47.770
"I want to know which direction
to go.",00:56:47.770,00:56:50.310
"And the formula would be: the next
weight, which is w(1), will be the",00:56:50.310,00:56:56.170
current weight plus the move.,00:56:56.170,00:56:58.390
And I already decided on the move.,00:56:58.390,00:57:00.740
"So now under this condition, you're
trying to derive what is v hat?",00:57:00.740,00:57:06.500
That's the direction.,00:57:06.500,00:57:07.690
"If you solve for it with one method
or another, that gives",00:57:07.690,00:57:10.060
you gradient descent.,00:57:10.060,00:57:11.150
"In another method, it will give you
conjugate gradient, which has",00:57:11.150,00:57:13.280
"2nd-order stuff in it, and so on.",00:57:13.280,00:57:14.960
So that is always the question.,00:57:14.960,00:57:16.790
Let's actually try to solve for it.,00:57:16.790,00:57:20.350
"We said that we are going to go
into the direction of the steepest",00:57:20.350,00:57:26.260
"descent, so we are really talking
about change in the",00:57:26.260,00:57:30.190
value of the error.,00:57:30.190,00:57:31.570
"The change of the value of the error,
if I move from w(0) to w(1) would",00:57:31.570,00:57:35.780
"be E_in at some point minus
E_in at another point.",00:57:35.780,00:57:40.130
Which two points?,00:57:40.130,00:57:41.090
It's w(0) and w(1).,00:57:41.090,00:57:44.430
That is OK.,00:57:44.430,00:57:44.850
"If I decide to move to this guy,
this is the amount.",00:57:44.850,00:57:47.550
"What I want to do, I want
this guy to be negative--",00:57:47.550,00:57:50.420
"as negative as possible because
I want to go down,",00:57:50.420,00:57:53.250
by the proper choice of w(1).,00:57:53.250,00:57:54.550
But w(1) is not free.,00:57:54.550,00:57:55.850
"It's dictated by the method, and it has
the very specific form that it is",00:57:55.850,00:57:59.910
the original guy plus the move I made.,00:57:59.910,00:58:02.140
So this is what I would like to make.,00:58:02.140,00:58:03.600
"I would like to make this
as small as possible.",00:58:03.600,00:58:05.990
"Now if I can write this down using
the Taylor series expansion with one",00:58:05.990,00:58:12.020
"term, this is E of the original point
plus a move, minus the original point.",00:58:12.020,00:58:17.220
"That will also be the derivative
times the difference, right?",00:58:17.220,00:58:20.030
"So the derivative times the difference
here will be the gradient transposed",00:58:20.030,00:58:23.470
times the vector times eta.,00:58:23.470,00:58:25.525
"And I just took eta outside
here, to make it clear.",00:58:25.525,00:58:28.250
"So this would be the move
according to the 1st-order",00:58:28.250,00:58:30.930
approximation of the surface.,00:58:30.930,00:58:32.340
"If the surface was linear, this
would be exact, but the",00:58:32.340,00:58:34.990
surface is not linear.,00:58:34.990,00:58:36.440
"And therefore, I have other terms
which are of the order",00:58:36.440,00:58:39.900
eta squared and up.,00:58:39.900,00:58:41.090
"And the assumption for gradient descent
is that I'm going to neglect",00:58:41.090,00:58:44.420
"this fellow, as if it
didn't exist.",00:58:44.420,00:58:46.830
"When you go to conjugate gradient, you
will have the second guy, and you",00:58:46.830,00:58:50.505
will neglect the third.,00:58:50.505,00:58:53.110
And you can see the idea.,00:58:53.110,00:58:55.350
"So now, how do I chose the direction
in order to make this",00:58:55.350,00:59:00.290
as negative as possible?,00:59:00.290,00:59:02.120
"By simple observation, I realize
that this quantity, for any",00:59:02.120,00:59:08.390
"choice of v hat, will be greater than
or equal to this fellow.",00:59:08.390,00:59:14.380
"This guy is gone, right?",00:59:14.380,00:59:15.670
I'm only dealing with this guy.,00:59:15.670,00:59:17.060
"So I'm taking the inner product
between a vector and",00:59:17.060,00:59:20.090
a unit vector.,00:59:20.090,00:59:23.130
"The unit vector could be aligned with
that vector, or could be opposed",00:59:23.130,00:59:26.100
"to that vector, or could be orthogonal
to that vector, but it doesn't",00:59:26.100,00:59:28.570
contribute magnitude.,00:59:28.570,00:59:29.560
Its magnitude is 1.,00:59:29.560,00:59:31.710
"The most I can get is the norm of
this, and the least I can get is",00:59:31.710,00:59:36.850
"negative the norm of this,
if they are opposed.",00:59:36.850,00:59:39.740
"So this would be the least I can get,
and I inherit eta from here.",00:59:39.740,00:59:43.800
"This will be true for
any choice of v hat.",00:59:43.800,00:59:46.980
"Knowing that, if I choose the v hat
that achieves this, this will be my v hat,",00:59:46.980,00:59:51.800
"because it gives me the most negative
value that I can get.",00:59:51.800,00:59:56.230
"Not that difficult to do, because
this is a unit vector, so I'm",00:59:56.230,01:00:00.230
trying to get it.,01:00:00.230,01:00:00.760
"Clearly what I want is a unit vector
opposed to the original guy.",01:00:00.760,01:00:04.180
"So I end up with this fellow,
the formula for it.",01:00:04.180,01:00:06.860
"The formula for it, I am getting
minus the gradient.",01:00:06.860,01:00:11.060
"But this is a unit vector, and therefore
I have to normalize by that",01:00:11.060,01:00:14.060
in order to make it a unit vector.,01:00:14.060,01:00:15.530
So that's the solution.,01:00:15.530,01:00:17.350
"And you can see now why it's called
gradient descent, because you descend",01:00:17.350,01:00:20.770
along the gradient of your error.,01:00:20.770,01:00:25.690
"Now we have said it's a fixed
size, and that was a way for us to",01:00:25.690,01:00:32.700
"make sure that the linear
approximation holds.",01:00:32.700,01:00:35.520
"We're going to modulate
eta.",01:00:35.520,01:00:38.050
"But you can see that there's
a compromise.",01:00:38.050,01:00:39.690
"I can get close-to-perfect
approximation for linear, by taking the",01:00:39.690,01:00:46.280
"size to be very small, but then
it will take me forever",01:00:46.280,01:00:48.600
to get to the minimum.,01:00:48.600,01:00:49.310
I will be moving [MAKING NOISES],01:00:49.310,01:00:51.120
"et cetera, or I could be taking bigger
step, which looks very promising.",01:00:51.120,01:00:55.180
"But then, the linear approximation
may not apply.",01:00:55.180,01:00:57.680
So there is a compromise.,01:00:57.680,01:00:58.720
"Let's look at how eta
affects the algorithm.",01:00:58.720,01:01:04.580
This is the case I talked about.,01:01:04.580,01:01:06.020
"If eta is very small, you will get
there, but it will take you forever.",01:01:06.020,01:01:10.300
"And in optimization, it's
a very simple game.",01:01:10.300,01:01:13.000
I charge you for two things:,01:01:13.000,01:01:14.830
"the value you arrived at, and how
long it took you to get there.",01:01:14.830,01:01:18.990
I don't care how you do it.,01:01:18.990,01:01:20.100
Don't bother me.,01:01:20.100,01:01:21.220
"If you gave me a beautiful thing-- I'm
computing the Hessian and taking the",01:01:21.220,01:01:24.660
inverse of it--,01:01:24.660,01:01:25.480
that's your business.,01:01:25.480,01:01:26.330
"All I care about is how long it took you
to do that, and what is the value",01:01:26.330,01:01:29.450
you're going to deliver.,01:01:29.450,01:01:30.980
"So just by this, this is not good,
because it took me forever.",01:01:30.980,01:01:35.230
"Now I go to the other extreme and make
eta large, and all of sudden, I am",01:01:35.230,01:01:38.790
really doing this.,01:01:38.790,01:01:39.820
You can even do worse.,01:01:39.820,01:01:40.610
Let's say that eta is really large.,01:01:40.610,01:01:42.110
"You start here, and our
next step is here.",01:01:42.110,01:01:45.260
"You end up with the error
surface being up there.",01:01:45.260,01:01:47.330
"So you went up instead of down,
obviously because the 2nd order and",01:01:47.330,01:01:50.650
3rd order dominate that.,01:01:50.650,01:01:53.140
So that is not good.,01:01:53.140,01:01:55.450
Pointer? OK.,01:02:05.000,01:02:06.990
"If you look at it, you realize that,
the best compromise is to",01:02:06.990,01:02:10.540
"have initially a large eta, because the
thing is very steep, and I want to",01:02:10.540,01:02:14.760
"take advantage of it, and just become
more careful when I'm closer to the",01:02:14.760,01:02:18.440
minimum so that I don't bounce.,01:02:18.440,01:02:21.350
So a rule of thumb--,01:02:21.350,01:02:22.480
"it's not like a mathematically
proved thing, it's",01:02:22.480,01:02:24.550
an observation in surfaces.,01:02:24.550,01:02:26.400
"It looks like a very good idea,
instead of having a fixed step, to have",01:02:26.400,01:02:30.090
eta increase with the slope.,01:02:30.090,01:02:32.040
"If I'm in a very high slope, I just go
a lot because I'm doing that.",01:02:32.040,01:02:35.500
"And then, if I'm now close to the
minimum, I'd better be careful in order",01:02:35.500,01:02:38.440
not to miss the minimum and overshoot.,01:02:38.440,01:02:40.760
"And because of this, here is an easy
implementation of this idea.",01:02:40.760,01:02:45.740
"Instead of taking the direction,
which will not change--",01:02:45.740,01:02:51.812
"here's the direction, and we're
going to eta, and this is",01:02:51.812,01:02:55.350
the formula for it.,01:02:55.350,01:02:56.320
This is what we have for fixed size.,01:02:56.320,01:02:58.080
"Now, I'm going to try to make eta
proportional to the size of the",01:02:58.080,01:03:00.990
"gradient, so it's bigger when
the slope is bigger.",01:03:00.990,01:03:04.000
"That's very convenient, because
I have here the size of the",01:03:04.000,01:03:06.540
gradient sitting there.,01:03:06.540,01:03:08.180
"So if I make eta proportional to the
size of our gradient, something",01:03:08.180,01:03:10.700
"will nicely cancel out, and then
I will get another eta--",01:03:10.700,01:03:16.100
"purple eta, which is the new constant.",01:03:16.100,01:03:18.430
"And this guy completely canceled out,
and I have a very simple formula.",01:03:18.430,01:03:22.580
"Now it's not a fixed step anymore, but
a fixed learning rate, eta now being",01:03:22.580,01:03:27.630
the learning rate.,01:03:27.630,01:03:28.640
"You just compute the gradient, and use
that learning rate, and that will",01:03:28.640,01:03:31.760
"take care of the previous
observation.",01:03:31.760,01:03:34.910
So that's what we have.,01:03:34.910,01:03:36.320
"That's all I'm going to say about
gradient descent for this case.",01:03:36.320,01:03:40.550
"And then we're going to go to the more
complicated issues of it, when we talk",01:03:40.550,01:03:43.960
about neural networks next time.,01:03:43.960,01:03:46.290
So this is how to minimize.,01:03:46.290,01:03:47.960
And now we have the logistic regression,01:03:47.960,01:03:50.200
"algorithm, written in language.",01:03:50.200,01:03:54.840
"You iterate, you start
by initializing at w(0).",01:03:54.840,01:03:58.780
You iterate for every step.,01:03:58.780,01:04:00.280
What do you do?,01:04:00.280,01:04:00.880
You compute the gradient.,01:04:00.880,01:04:01.900
How do I do that?,01:04:01.900,01:04:02.640
"Oh, I have a formula for
the in-sample error.",01:04:02.640,01:04:04.965
"All I need to do is to
differentiate it.",01:04:04.965,01:04:06.880
Differentiating it will not be difficult.,01:04:06.880,01:04:08.380
You're going to get this--,01:04:08.380,01:04:10.060
you can verify it.,01:04:10.060,01:04:11.030
"And now, you are going to take the next
weight to be the current weight",01:04:11.030,01:04:15.860
"minus your learning rate
times the gradient.",01:04:15.860,01:04:18.530
That's the formula we had.,01:04:18.530,01:04:20.380
"And then, you go to the next iteration,
next iteration, until it's",01:04:20.380,01:04:23.440
time to stop.,01:04:23.440,01:04:24.440
"And then we return
the final weight.",01:04:24.440,01:04:26.820
That is the algorithm.,01:04:26.820,01:04:28.260
"Now let me spend two minutes
summarizing all the linear models in",01:04:28.260,01:04:34.290
"one slide, and then we will be done
completely with that model.",01:04:34.290,01:04:40.750
"We had three models, right?",01:04:40.750,01:04:42.850
"We had the perceptron--
linear classification.",01:04:42.850,01:04:48.230
We had linear regression.,01:04:48.230,01:04:50.540
"And we today added logistic
regression.",01:04:50.540,01:04:53.680
"Let's take one application domain, which
is credit, and see how each of",01:04:53.680,01:04:58.260
them contributes.,01:04:58.260,01:05:00.500
We have credit analysis.,01:05:00.500,01:05:01.980
"If you apply each of these to credit
analysis, what type of thing do",01:05:01.980,01:05:05.640
you implement?,01:05:05.640,01:05:07.860
"For the linear classification, the
perceptron, you accept or deny. That",01:05:07.860,01:05:11.465
"was our very first example, right?",01:05:11.465,01:05:15.620
"If you use linear regression, you are
trying to decide the credit line.",01:05:15.620,01:05:19.580
We have seen that example as well.,01:05:19.580,01:05:22.870
"If you're applying logistic regression,
you are computing the",01:05:22.870,01:05:26.140
"probability of default-- just the
reliability, and then let the bank",01:05:26.140,01:05:29.490
decide what to do with it.,01:05:29.490,01:05:31.480
So this is from an application domain.,01:05:31.480,01:05:34.070
Let's look from tools point of view.,01:05:34.070,01:05:36.730
They had different error measures.,01:05:36.730,01:05:41.550
"The perceptron had the binary
classification error, right?",01:05:41.550,01:05:47.100
Linear regression has squared error.,01:05:47.100,01:05:50.380
"And finally, logistic regression
had cross-entropy error.",01:05:50.380,01:05:54.930
"Different errors that had different
plausibility motivations to them.",01:05:54.930,01:06:00.080
And we have tackled all three of them.,01:06:00.080,01:06:02.720
"And then there was the learning
algorithm that goes with them, that is",01:06:02.720,01:06:05.120
"very dependent on the error
measure you choose.",01:06:05.120,01:06:07.780
"For the case of the classification
error, which was a combinatorial",01:06:07.780,01:06:10.850
"quantity, and we went for something like
the perceptron learning algorithm,",01:06:10.850,01:06:14.240
"or the pocket version if the
thing is non-separable.",01:06:14.240,01:06:16.540
"And there are other, more sophisticated
methods to do that.",01:06:16.540,01:06:20.810
How about squared error?,01:06:20.810,01:06:22.330
That was the easiest.,01:06:22.330,01:06:23.470
"That was the one-step learning, where we
had the pseudo-inverse and you have",01:06:23.470,01:06:26.880
your solution.,01:06:26.880,01:06:28.200
"And finally with the cross-entropy, we
had the gradient descent, which is",01:06:28.200,01:06:32.770
a very general method.,01:06:32.770,01:06:34.190
"All we needed to do is that it's
a twice differentiable surface, and we",01:06:34.190,01:06:37.670
are ready to go with that.,01:06:37.670,01:06:38.770
"And this was particularly friendly
because it happens to be convex, so it",01:06:38.770,01:06:41.810
"avoids a lot of the traps that we will
see next time when we talk about",01:06:41.810,01:06:44.600
neural networks.,01:06:44.600,01:06:45.760
I'll stop here.,01:06:45.760,01:06:47.020
"And then we will continue
after the short break.",01:06:47.020,01:06:49.940
Let's start the Q&amp;A.,01:06:58.040,01:07:02.020
"MODERATOR: The first question is,
for the algorithm, the termination",01:07:02.020,01:07:06.690
"time means the error does not change, or
what criterion do you usually use?",01:07:06.690,01:07:12.580
"PROFESSOR: When I discuss
gradient descent, there are several",01:07:15.830,01:07:19.570
"aspects to the algorithm that
need to be discussed.",01:07:19.570,01:07:23.320
"There is the question of
the learning rate.",01:07:23.320,01:07:26.920
"And this is what I focused on today,
because it's the most relevant to the",01:07:26.920,01:07:30.930
"particular application, which
is logistic regression.",01:07:30.930,01:07:33.340
That has a very good-behaving one.,01:07:33.340,01:07:35.820
There are other questions.,01:07:35.820,01:07:37.110
One of them is the initialization.,01:07:37.110,01:07:38.960
"For example, if you look at the top
here, I said set it to w(0).",01:07:38.960,01:07:46.350
What is w(0)?,01:07:46.350,01:07:47.430
How do I initialize?,01:07:47.430,01:07:48.390
That's a question.,01:07:48.390,01:07:49.440
"It's not critical in logistic
regression.",01:07:49.440,01:07:52.110
"If you initialize it to 0,
this will be fine.",01:07:52.110,01:07:54.740
"And if you think, initializing it to 0
means that you initialize the probability",01:07:54.740,01:07:57.930
"to a half, the most uncertainty about
any example before you learn anything.",01:07:57.930,01:08:02.020
"So it looks like a reasonable
thing to start.",01:08:02.020,01:08:04.540
"Then also, there is a question
of termination.",01:08:04.540,01:08:07.220
"And termination is an issue here, but
it's less of an issue here than in",01:08:07.220,01:08:10.750
"other cases, because of a reason
that I'm going to explain.",01:08:10.750,01:08:14.370
"But in general, the termination
is tricky, and you have",01:08:14.370,01:08:19.200
a combination of criteria.,01:08:19.200,01:08:21.420
So what do I want?,01:08:21.420,01:08:22.200
I want to minimize the error.,01:08:22.200,01:08:24.430
"So one of them is to say, if the
thing gets flat and flat to",01:08:24.430,01:08:29.640
"the level where, when I move from one point
to another, I'm not really making",01:08:29.640,01:08:32.929
"a lot of improvement, then I must
be close to the minimum and",01:08:32.929,01:08:35.710
should stop.,01:08:35.710,01:08:38.080
"That turns out to be OK,
may be reasonable.",01:08:38.080,01:08:40.729
But sometimes--,01:08:40.729,01:08:41.430
"not in the convex case-- but sometimes
you have a surface like this-- it goes",01:08:41.430,01:08:45.529
"down, and then flattens,
and then goes down.",01:08:45.529,01:08:50.109
"So if you do this criterion by itself,
you may stop prematurely.",01:08:50.109,01:08:54.040
"And you may think this
is pathological.",01:08:54.040,01:08:56.240
It happens more often than you think.,01:08:56.240,01:08:59.350
"So now you say, let me set a target
error. That is, not only that the",01:08:59.350,01:09:03.350
"changes are small, but also if I didn't
get to the target error that I",01:09:03.350,01:09:06.744
"want, I'm not going to stop.",01:09:06.744,01:09:08.380
"Now that will get you over this hump,
until you get to the other guy,",01:09:08.380,01:09:10.870
"and maybe that will achieve
your target error.",01:09:10.870,01:09:13.370
"That's very nice, except for the problem
that if your target error",01:09:13.370,01:09:16.240
"is not achievable, you will
continue forever.",01:09:16.240,01:09:19.200
"So you patch this up and say, I'm
going to have a limit on the number of",01:09:19.200,01:09:22.470
iterations anyway.,01:09:22.470,01:09:23.450
"I'm going to do this for
10,000 epochs,",01:09:23.450,01:09:26.319
"regardless of what happens, and
then I'm going to stop.",01:09:26.319,01:09:29.370
"In practice, some combination
of the above works.",01:09:29.370,01:09:33.160
"But the main thing here is that termination,
as a properly analyzed thing, is a bit",01:09:33.160,01:09:42.020
"tricky because of so many unknowns
in the error surface that",01:09:42.020,01:09:45.250
we are dealing with.,01:09:45.250,01:09:46.620
"But it is something that will become
more of an issue in neural networks",01:09:46.620,01:09:49.850
than it is here.,01:09:49.850,01:09:51.510
"And then another issue in gradient
descent, that I didn't talk about, is",01:09:51.510,01:09:54.084
"the question of local minima
versus global minima.",01:09:54.084,01:09:57.160
"When you do gradient descent, I said
you close your eyes, and you",01:09:57.160,01:10:00.670
roll down the surface.,01:10:00.670,01:10:01.800
"And then when you get to the minimum,
you know you are at a minimum.",01:10:01.800,01:10:04.830
"If you have a surface that goes like
this, then goes up, and then goes down",01:10:04.830,01:10:10.360
to a better minimum.,01:10:10.360,01:10:12.070
"If you start at the original
one, you can go, go, go.",01:10:12.070,01:10:14.920
"Once you get to that minimum, you have
absolutely no reason to leave,",01:10:14.920,01:10:17.890
"according to the prescription of the
gradient descent, because you will be",01:10:17.890,01:10:21.250
"going up a hill, and that looks
like a bad idea.",01:10:21.250,01:10:24.540
"So you will be in a local minimum,
rather than a global minimum.",01:10:24.540,01:10:27.990
"I didn't mention it this time, again
because I have a convex function,",01:10:27.990,01:10:30.900
so there's only one.,01:10:30.900,01:10:31.610
"You will get there, and
everybody is happy.",01:10:31.610,01:10:33.400
"When we get to neural networks,
this is an issue, and this should be",01:10:33.400,01:10:35.960
addressed.,01:10:35.960,01:10:36.440
"So the short answer to the question:
termination is tricky.",01:10:36.440,01:10:41.200
"A combination of criteria
is the best way.",01:10:41.200,01:10:44.440
"And my coverage of gradient descent,
this one, only covers a part of it that",01:10:44.440,01:10:50.190
"is most relevant to logistic
regression.",01:10:50.190,01:10:52.920
"And the rest of the story will come up
when we talk about neural networks.",01:10:52.920,01:10:55.840
"MODERATOR: There are questions about
why was gradient descent",01:11:01.360,01:11:04.030
"picked? Isn't it usually very slow--
a slow method for convergence?",01:11:04.030,01:11:10.840
PROFESSOR: Think of it this way.,01:11:10.840,01:11:13.980
"If I can see the surface, I obviously
can go for the minimum directly.",01:11:13.980,01:11:18.270
"But I'm doing something here that
depends on 1st order.",01:11:18.270,01:11:21.180
Let's say that you're playing golf.,01:11:21.180,01:11:22.960
You want to get to the hole.,01:11:22.960,01:11:23.840
That's your minimum.,01:11:23.840,01:11:25.530
Gradient descent would be doing this--,01:11:25.530,01:11:27.580
"take this, and just this one--",01:11:27.580,01:11:29.100
"Nobody in their right
mind would do that.",01:11:31.830,01:11:34.660
"When you go to the 2nd-order thing,
what you are doing actually-- your",01:11:34.660,01:11:38.910
first guy is a swing.,01:11:38.910,01:11:41.550
"You may not land exactly at the
hole, but you will land close.",01:11:41.550,01:11:44.550
"You have a 2nd-order approximation of
the surface, and you get close.",01:11:44.550,01:11:48.110
And then you try to get there.,01:11:48.110,01:11:50.610
"Having said that, it is a remarkably
efficient algorithm to use, especially",01:11:50.610,01:11:55.682
"the stochastic version of it--
gradient descent, that is.",01:11:55.682,01:11:58.450
"That is, in many applications, you just
apply gradient descent in a very",01:11:58.450,01:12:02.860
"simple way, and you often get
very, very good result.",01:12:02.860,01:12:07.410
"And conjugate gradient, which is
the king of the derivative",01:12:07.410,01:12:12.390
"based methods, is a very
attractive one.",01:12:12.390,01:12:14.860
"And in some optimizations, it completely
trumps the alternatives.",01:12:14.860,01:12:20.400
"On the other hand, in many ways, the
stochastic version of this, and the",01:12:20.400,01:12:24.000
"simplicity of it, makes it the
algorithm of choice in many",01:12:24.000,01:12:29.000
applications.,01:12:29.000,01:12:30.250
"MODERATOR: Although it's not the case for
this error function, what happens",01:12:34.060,01:12:39.240
"for gradient descent if there
are local minima?",01:12:39.240,01:12:44.200
"PROFESSOR: If there are local
minima, and you're applying the",01:12:44.200,01:12:49.070
"algorithm faithfully, you are going to
get to the nearest local minimum to",01:12:49.070,01:12:52.310
where you started.,01:12:52.310,01:12:54.390
"There is a huge amount of research
in optimization about local minima,",01:12:54.390,01:12:57.965
"and how to do it, and there are
algorithms right and left.",01:12:57.965,01:13:02.060
"From a practical point of view,
here is my experience.",01:13:02.060,01:13:05.370
"Let's say I'm using neural networks, and
the local minima are abundant in",01:13:05.370,01:13:10.630
"neural networks, and therefore
it looks, on face value,",01:13:10.630,01:13:13.240
like a serious problem.,01:13:13.240,01:13:16.350
"If all you do is, do the learning
a number of times starting from",01:13:16.350,01:13:22.440
different initial conditions--,01:13:22.440,01:13:24.740
"that is, do a session starting from
this point, do another session",01:13:24.740,01:13:28.060
"starting for this point, et cetera.",01:13:28.060,01:13:29.900
"So each of them will go to its
nearest local minimum.",01:13:29.900,01:13:34.930
If you do this enough times--,01:13:34.930,01:13:36.540
"I'm not talking a million times.
Even a hundred times.",01:13:36.540,01:13:39.770
"And then after all of these sessions,
you pick the one that gave you the",01:13:39.770,01:13:42.810
"best minimum, and you know the best
minimum by evaluating the error which",01:13:42.810,01:13:46.405
is accessible to you.,01:13:46.405,01:13:48.550
That usually gets you what you want.,01:13:48.550,01:13:51.190
"It will not get you the
global minimum.",01:13:51.190,01:13:52.750
"Formally, you can prove that
getting to the global",01:13:52.750,01:13:55.060
minimum is NP-hard.,01:13:55.060,01:13:57.170
"If you insist on getting to it in
every case, this is simply not",01:13:57.170,01:14:01.690
"tractable in terms of
computational time.",01:14:01.690,01:14:03.850
"But some very simple heuristic, as just
repeating from different points and",01:14:03.850,01:14:08.410
"then picking the best, works actually
pretty good in most of the cases that",01:14:08.410,01:14:12.010
I've seen.,01:14:12.010,01:14:13.150
"And if this becomes a real issue in your
application, there is no shortage of",01:14:13.150,01:14:17.540
"methods in optimization that explicitly
deal with local minima.",01:14:17.540,01:14:22.590
"The only thing to remember is that
avoiding local minima, versus not",01:14:22.590,01:14:26.690
"avoiding them, has almost
nothing to do with the",01:14:26.690,01:14:31.330
order of your algorithm.,01:14:31.330,01:14:32.250
"I could be using 1st order, 2nd order,
et cetera, and all of them will",01:14:32.250,01:14:36.030
"be very happy when you get to the
minimum, even if it's local.",01:14:36.030,01:14:38.730
"So this is an added layer that will make
you, in spite of the fact that",01:14:38.730,01:14:42.690
"you are at the minimum,",01:14:42.690,01:14:43.590
you explore further.,01:14:43.590,01:14:44.910
"Sometimes you have a temperature, and
you escape the local minimum to",01:14:44.910,01:14:48.090
"a better one if it's a shallow
local minimum.",01:14:48.090,01:14:50.250
"And there are others that
deliberately look for that.",01:14:50.250,01:14:52.770
"So if your application calls for it,
there will be methods to help.",01:14:52.770,01:14:57.530
"But there will be no method to actually
solve your problem, because",01:14:57.530,01:15:00.550
the problem is NP-hard.,01:15:00.550,01:15:03.220
"MODERATOR: Can you quickly explain what
the stochastic gradient descent is?",01:15:03.220,01:15:07.010
"PROFESSOR: I will do that at
the beginning of the next lecture.",01:15:07.010,01:15:09.370
"It's basically, instead of taking the
whole training set at once, you take",01:15:09.370,01:15:12.170
one example at a time.,01:15:12.170,01:15:13.140
"But I'll do that, because that will be
the part that is applicable to neural",01:15:13.140,01:15:17.260
networks in general.,01:15:17.260,01:15:19.570
"MODERATOR: Can you explain a little
bit more the notion of cross-entropy?",01:15:19.570,01:15:26.380
"PROFESSOR: The formal
definition of cross-entropy--",01:15:26.380,01:15:29.320
"so entropy, you get a function based on
a probability, and it's basically",01:15:29.320,01:15:34.540
"the expected value of logarithm
1 over the probability.",01:15:34.540,01:15:36.950
"That will be your classical
definition of an entropy.",01:15:36.950,01:15:39.890
"When you have two different
probabilities, you can get",01:15:39.890,01:15:43.830
"a cross-entropy between them, by getting
the expected value with one-- the",01:15:43.830,01:15:49.690
"expected value of, and you take a ratio
of them one way or the other.",01:15:49.690,01:15:53.170
"And there are a number of them in the
literature, that have different",01:15:53.170,01:15:58.200
definitions and different scopes.,01:15:58.200,01:16:00.650
"But basically, you are getting
a relationship between two probability",01:16:00.650,01:16:04.240
"distributions, using a logarithmic,
and expected values.",01:16:04.240,01:16:08.030
That's the common thread.,01:16:08.030,01:16:09.660
"The reason why I put this between
quotation is that, you really are",01:16:09.660,01:16:15.270
"getting the cross-entropy between the
h that you're trying to learn, and",01:16:15.270,01:16:20.830
"a binary event, so something like
a probability 1 or 0 at a time.",01:16:20.830,01:16:24.420
"So it's a little bit of a loose thing,
because that's the way it's defined.",01:16:24.420,01:16:27.320
"But again, it is referred
to as cross-entropy.",01:16:27.320,01:16:28.980
"MODERATOR: A question is, why
a method like binary search wouldn't",01:16:34.930,01:16:39.500
work to find minima quickly?,01:16:39.500,01:16:42.460
PROFESSOR: OK.,01:16:42.460,01:16:44.160
"Binary search works well once you
decide on the direction.",01:16:44.160,01:16:47.730
"I am in a space, and you think of
binary search, that's very nice.",01:16:47.730,01:16:52.180
"Let's talk about 1000 dimensional
space.",01:16:52.180,01:16:55.440
Where do I move?,01:16:55.440,01:16:57.700
"If you decide on a direction, and
you say that it's very good to go",01:16:57.700,01:17:03.080
"along this direction, then it
becomes a legitimate question.",01:17:03.080,01:17:06.250
"That's the direction.
How far should I go?",01:17:06.250,01:17:08.680
"We used very crude method, like
using a fixed step, or maybe by",01:17:08.680,01:17:12.570
"looking at it closely,",01:17:12.570,01:17:13.430
maybe it shouldn't really be fixed.,01:17:13.430,01:17:14.500
"It should be proportional to
the gradient, so we had the",01:17:14.500,01:17:16.500
fixed learning rate.,01:17:16.500,01:17:17.920
"But one can become more sophisticated
and say, let me explore that",01:17:17.920,01:17:20.710
"direction until I get to the minimum
along this direction.",01:17:20.710,01:17:24.640
"And then there are binary search
methods for doing that.",01:17:24.640,01:17:28.300
"Again, when you judge one method versus
the other in optimization, don't",01:17:28.300,01:17:33.430
"be excited about the sophisticated
method, because you will be charged",01:17:33.430,01:17:37.540
for it.,01:17:37.540,01:17:38.850
"If I have to evaluate a lot of
stuff, I have to show for it that I",01:17:38.850,01:17:44.340
got to a much better value.,01:17:44.340,01:17:46.170
"If I evaluate many more values, and get
that much of a difference, then I",01:17:46.170,01:17:50.510
"lose in the optimization game, because
I used CPU cycles, and I didn't",01:17:50.510,01:17:54.270
improve the error as much.,01:17:54.270,01:17:57.430
"So whenever you are looking at a method,
it's a very practical question",01:17:57.430,01:18:01.360
whether it will work or not.,01:18:01.360,01:18:02.620
"For example, 2nd-order methods.",01:18:02.620,01:18:05.470
"Hands down, approximating the surface
as a 2nd order is better than the",01:18:05.470,01:18:08.890
1st order.,01:18:08.890,01:18:10.630
"And the problem, why don't we do this?",01:18:10.630,01:18:12.040
"Because if you approximate it as
2nd order, you have to compute the",01:18:12.040,01:18:15.280
2nd-order derivatives.,01:18:15.280,01:18:17.510
"And that's a full matrix
called Hessian.",01:18:17.510,01:18:20.560
"If I do that outright, I will get
a better minimum, and I will get it",01:18:20.560,01:18:26.200
"quickly in terms of the number
of steps, but each step",01:18:26.200,01:18:28.270
becomes very expensive.,01:18:28.270,01:18:29.880
"And conjugate gradient, that I mentioned
very quickly, is a way to do",01:18:29.880,01:18:33.460
"the 2nd order without actually
explicitly computing the",01:18:33.460,01:18:35.960
"Hessian, which is effective and
that's why it's a famous method.",01:18:35.960,01:18:38.940
"So it is used, but be careful
where to use it.",01:18:38.940,01:18:42.540
"MODERATOR: Can logistic regression be
applied for a multi-class setting?",01:18:45.930,01:18:49.900
PROFESSOR: Yeah.,01:18:49.900,01:18:51.150
Let's look at the full--,01:18:56.600,01:18:57.850
"Now this is the linear model
as we covered it.",01:19:00.470,01:19:04.410
"On the other hand, if you see",01:19:04.410,01:19:05.685
"what type of functions
am I getting?",01:19:05.685,01:19:07.550
"I seem to be getting either binary,
real-valued, or bounded real-valued,",01:19:07.550,01:19:12.870
specifically probability.,01:19:12.870,01:19:14.310
"There are obviously other classes, but
in many cases, the other classes can be",01:19:14.310,01:19:18.010
derived in terms of those.,01:19:18.010,01:19:19.110
"Let's look at, for example,
the multi-class, the thing",01:19:19.110,01:19:21.650
being asked about.,01:19:21.650,01:19:23.360
"Remember recognizing the
digits, that we talked about.",01:19:23.360,01:19:26.120
How many digits did we have?,01:19:26.120,01:19:27.350
We had 10 digits--,01:19:27.350,01:19:28.210
"0, 1, 2, up to 9 in the ZIP codes.",01:19:28.210,01:19:30.750
"And we wanted to be able
to classify them.",01:19:30.750,01:19:33.710
What did we do for that?,01:19:33.710,01:19:34.980
We used perceptron.,01:19:34.980,01:19:36.160
"Wait a minute, perceptron
does a binary thing.",01:19:36.160,01:19:38.020
How did we do that?,01:19:38.020,01:19:39.350
"We did what we usually do for
multi-class problems.",01:19:39.350,01:19:43.770
"Instead of taking 1 versus 2 versus 3
versus 4, et cetera, we either take",01:19:43.770,01:19:49.940
one versus one--,01:19:49.940,01:19:51.190
one class versus another class.,01:19:51.190,01:19:53.520
"Like the 1 versus 5, and
2 versus 3, et cetera, and",01:19:53.520,01:19:56.590
then combine the decisions.,01:19:56.590,01:19:58.300
"Or sometimes, you do one versus all.",01:19:58.300,01:20:00.970
"So I want to recognize 1 from the rest,
2 from the rest, 3 from the",01:20:00.970,01:20:04.865
"rest, et cetera.",01:20:04.865,01:20:05.910
And there are other methods.,01:20:05.910,01:20:07.080
"So many of the multi-class approaches
deal with a tree, based on binary",01:20:07.080,01:20:14.730
"decisions, and that is the
way it is applied.",01:20:14.730,01:20:16.920
"But there are some others which--
ordinal regression, for example, where",01:20:16.920,01:20:20.380
"there is a specific order to them,
it is dealt with differently.",01:20:20.380,01:20:23.930
"So there are modifications to these guys
that accommodate other methods,",01:20:23.930,01:20:27.740
"the easiest of which is multi-class,
because it's ready for us, and we have",01:20:27.740,01:20:30.670
seen an example of it.,01:20:30.670,01:20:33.540
"MODERATOR: What other sigmoid functions
that can be used instead of that?",01:20:33.540,01:20:39.670
"PROFESSOR: We will
use one next time.",01:20:39.670,01:20:40.840
I'm glad you asked.,01:20:40.840,01:20:41.640
"So the logistic function
is between 0 and 1.",01:20:41.640,01:20:44.070
"We are going to use the tanh
which will be between",01:20:44.070,01:20:45.810
-1 and +1.,01:20:45.810,01:20:47.310
"And that will be the neuronal function
in neural networks next time.",01:20:47.310,01:20:52.490
"And at that time, I will discuss it
a little bit, so you get a feel for it.",01:20:52.490,01:20:56.540
"But it's also based on exponential-- the
tanh has an exponential in it.",01:20:56.540,01:20:59.445
It's very close to the sigmoid.,01:20:59.445,01:21:01.290
There is a scale and shift.,01:21:01.290,01:21:03.870
"On the other hand, you can
use other functions.",01:21:03.870,01:21:07.740
"It turns out that, from an analytic
point of view, using the exponential",01:21:07.740,01:21:11.830
"based soft thresholds has
advantages, as we see.",01:21:11.830,01:21:15.100
"So we got a number
of advantages here--",01:21:15.100,01:21:18.180
a simple formula for the error.,01:21:18.180,01:21:19.820
"And also, the error measure that
resulted from it was convex.",01:21:19.820,01:21:25.250
"That may not be guaranteed in every
choice, so therefore, there is",01:21:25.250,01:21:28.450
a criterion for choice.,01:21:28.450,01:21:29.860
It's not just a good-looking formula.,01:21:29.860,01:21:32.610
"It's a formula that will go through
the chain of the processes that we go",01:21:32.610,01:21:38.080
through when we do the learning.,01:21:38.080,01:21:39.431
"MODERATOR: There's a conceptual
question about how is eta derived?",01:21:45.203,01:21:48.010
It's not really derived.,01:21:48.010,01:21:51.262
PROFESSOR: The learning rate?,01:21:51.262,01:21:52.440
MODERATOR: Yeah.,01:21:52.440,01:21:52.860
"PROFESSOR: The way I described it, I got the
zeroth order, which is fixed eta-- fixed step.",01:21:52.860,01:21:59.730
"And then I got the 1st order, which
is to make the step proportional to",01:21:59.730,01:22:05.590
the gradient.,01:22:05.590,01:22:06.110
"And then you got the fixed
the learning rate.",01:22:06.110,01:22:08.170
"You can take that and make it more
sophisticated, and you have",01:22:08.170,01:22:10.350
an adaptive learning rate.,01:22:10.350,01:22:12.410
"It's a very simple heuristic, that
you take a learning rate and",01:22:12.410,01:22:16.170
you do a step.,01:22:16.170,01:22:18.230
"And if the step is successful, you
say maybe I can afford a bigger",01:22:18.230,01:22:20.820
"learning rate, so you increase
the learning rate.",01:22:20.820,01:22:23.180
"And you keep doing that until you hit
a point where-- oh, I actually used too",01:22:23.180,01:22:26.990
"big a learning rate, because after
doing my minimization in one step, I",01:22:26.990,01:22:30.880
"ended up with a bigger value,
so obviously I went too far.",01:22:30.880,01:22:33.290
"And in that case, you shrink
the learning rate.",01:22:33.290,01:22:35.650
So you can do this adaptively.,01:22:35.650,01:22:37.150
And it does buy you time.,01:22:37.150,01:22:38.590
"There are lots of heuristics that are
add-on's to the plain-vanilla gradient",01:22:38.590,01:22:42.650
"descent, and one of them has to
do with the learning rate.",01:22:42.650,01:22:45.600
"When you go to conjugate gradient,
which is a 2nd-order one, because",01:22:45.600,01:22:49.970
"there is a 2nd-order thing, you can
look at it eventually, and interpret",01:22:49.970,01:22:53.330
"it as if it was having a principled
direction, and",01:22:53.330,01:22:56.830
a principled learning rate.,01:22:56.830,01:22:57.980
This is one way to look at it.,01:22:57.980,01:22:59.400
"But for the gradient descent, it's
really a heuristic that will choose",01:22:59.400,01:23:02.610
the learning rate.,01:23:02.610,01:23:03.940
I have a rule for it.,01:23:03.940,01:23:05.420
I'll mention it as a rule of thumb.,01:23:05.420,01:23:07.150
"There are certain values that
work in many cases.",01:23:07.150,01:23:09.540
"And in the case of eta, there is
a particular value that works.",01:23:09.540,01:23:12.340
I'm going to mention it.,01:23:12.340,01:23:13.320
"But again, this is just a practical
observation, and other people may have",01:23:13.320,01:23:18.650
different experience.,01:23:18.650,01:23:20.831
"MODERATOR: Going back to the first
part of the lecture, and a few",01:23:20.831,01:23:25.180
"lectures back, you made the example of
character recognition, and you chose",01:23:25.180,01:23:31.680
"features like symmetry and I don't
remember something else.",01:23:31.680,01:23:37.230
"But how much are you charged in terms
of d_VC for getting those features?",01:23:37.230,01:23:43.816
"PROFESSOR: I'm glad
this question was asked.",01:23:43.816,01:23:45.680
This was the nonlinear transformation.,01:23:45.685,01:23:47.520
"And we called the space Z the feature
space, and these guys are features.",01:23:47.520,01:23:53.000
"And there are basically
two types of features.",01:23:53.000,01:23:55.540
"Here, I'm trying to find just
generically more sophisticated",01:23:55.540,01:24:00.170
"surface, because I realize that the
points will not be linearly separable.",01:24:00.170,01:24:03.230
so I cannot do it here.,01:24:03.230,01:24:04.750
"And the other one is along the lines we
started with, which are meaningful",01:24:04.750,01:24:08.960
"features, like symmetry in the case
of the digits, and in the",01:24:08.960,01:24:14.220
"case of, let's say, years
in residence--",01:24:14.220,01:24:16.430
"if you want this as an input to the
credit, you may not want it as",01:24:16.430,01:24:19.100
"a linear thing, but you say: am
I bigger than five years or",01:24:19.100,01:24:21.700
less than five years?,01:24:21.700,01:24:22.210
So those are meaningful features.,01:24:22.210,01:24:23.830
"The key distinction you need to
make in your mind is that, did I",01:24:23.830,01:24:30.630
"choose the feature by understanding
the problem, or did I choose the",01:24:30.630,01:24:34.490
"feature by looking at the specific
data set that was given to me?",01:24:34.490,01:24:38.960
The latter is the problem.,01:24:38.960,01:24:41.390
"If I look at the data and then choose
features, then I am doing the learning",01:24:41.390,01:24:45.850
"myself-- at least the first stage of it--
and therefore, I have a bigger",01:24:45.850,01:24:49.030
"hypothesis set than what I'm
going to end up with.",01:24:49.030,01:24:51.670
"And therefore, as I said, the VC
warranty is forfeited in this case.",01:24:51.670,01:24:56.460
"If you look at the problem, and then
derive features that are meaningful,",01:24:56.460,01:25:00.350
"but that's not depending on the data set
that you're going to learn from--",01:25:00.350,01:25:03.190
"depending on general understanding.
We look at the credit.",01:25:03.190,01:25:06.840
"Looks like years in residence are fine,
but I don't think it's just",01:25:06.840,01:25:09.840
"proportional to these guys,
without looking at data.",01:25:09.840,01:25:12.420
"I think the thresholds are five years
for stability, less than one year for",01:25:12.420,01:25:16.350
"not so much stability, and so on.",01:25:16.350,01:25:18.310
So I'm going to derive those.,01:25:18.310,01:25:19.440
"You're absolutely charged
nothing for doing that.",01:25:19.440,01:25:22.220
More power to you.,01:25:22.220,01:25:23.195
"You may have helped the learning
algorithm by taking properties of the",01:25:23.195,01:25:26.800
"learning problem that you're working
on, and got a better representation",01:25:26.800,01:25:30.790
"for it. And this is an art-- purely
an art, and it depends on",01:25:30.790,01:25:33.930
the application domain.,01:25:33.930,01:25:35.330
"The thing I warned about, very explicitly,
is to try to derive",01:25:35.330,01:25:38.620
features from looking at the data.,01:25:38.620,01:25:41.300
And the warning is not absolute.,01:25:41.300,01:25:43.630
"Try do derive features based on the
data, and still think that the final",01:25:43.630,01:25:47.650
"hypothesis set you ended up with
is what will dictate the",01:25:47.650,01:25:50.880
generalization behavior.,01:25:50.880,01:25:51.770
That is where the fallacy lies.,01:25:51.770,01:25:54.800
MODERATOR: There's a question.,01:26:01.590,01:26:02.580
"Is it possible to choose parameters
automatically?",01:26:02.580,01:26:06.390
"I'm guessing they're referring
to the learning rate.",01:26:06.390,01:26:10.500
PROFESSOR: OK--,01:26:10.500,01:26:13.331
"MODERATOR: Oh, no wait, sorry.",01:26:13.331,01:26:14.790
They corrected it.,01:26:14.790,01:26:15.410
"How to select the features
automatically, so it's back to--",01:26:15.410,01:26:20.220
PROFESSOR: It's selected,01:26:20.220,01:26:21.470
"automatically, that's what
we are in business for.",01:26:21.470,01:26:24.360
It's machine learning.,01:26:24.360,01:26:25.090
Things are automatic.,01:26:25.090,01:26:26.600
"But then, this becomes
part of learning.",01:26:26.600,01:26:29.660
"Now here, we had an explicit
nonlinear transformation.",01:26:29.660,01:26:33.830
"When we go to neural networks, we'll
find out that they choose",01:26:33.830,01:26:38.380
features automatically.,01:26:38.380,01:26:39.820
"But that is part of learning,
and you are charged for it",01:26:39.820,01:26:41.940
in terms of VC dimension.,01:26:41.940,01:26:43.330
"So probably, the question will be
better answered and understood in the",01:26:43.330,01:26:47.440
"context when I talk about neural networks
and hidden layers, and then",01:26:47.440,01:26:50.330
we'll see what that means.,01:26:50.330,01:26:51.580
MODERATOR: I think that's it.,01:26:54.190,01:26:55.300
PROFESSOR: Very good.,01:26:55.310,01:26:56.220
Then we'll see you on Thursday.,01:26:56.230,01:26:57.480

text,start,stop
[MUSIC],00:00:00.000,00:00:04.839
&gt;&gt; Stanford University.,00:00:04.839,00:00:08.390
"&gt;&gt; About the full back
propagation algorithm.",00:00:08.390,00:00:11.990
"And I promise you as the last bit of
super heavy math- After that you can have",00:00:11.990,00:00:17.240
"a warm fuzzy feeling around most of the
state of the art deep learning techniques.",00:00:17.240,00:00:22.730
"Both for natural language processing and
even a little bit for",00:00:22.730,00:00:24.860
computer vision a lot of other places.,00:00:24.860,00:00:26.400
"So, I know this can be a lot if you're not
super familiar with multivariate calculus.",00:00:26.400,00:00:32.290
"And so, I'll actually describe
backprop in four different ways today.",00:00:32.290,00:00:36.710
"And hopefully bring most
of you into the backprop",00:00:36.710,00:00:41.320
"the group of people who know
back propagation really well.",00:00:41.320,00:00:46.870
"So 4 different descriptions of
essentially the same thing.",00:00:46.870,00:00:49.930
"But hopefully, some will resonate
more with some folks than others.",00:00:49.930,00:00:55.800
"And so to show you that afterwards
we can have a lot more fun.",00:00:55.800,00:00:58.980
"Well, actually then the second sort of,
well, not quite half, but",00:00:58.980,00:01:02.260
"maybe the last third,
talk about the projects and",00:01:02.260,00:01:05.440
"encourage you to get
started on the project.",00:01:05.440,00:01:07.630
Give you some advice on what the projects,00:01:07.630,00:01:10.780
"will likely entail if you choose to do
a project instead of the last problem set.",00:01:10.780,00:01:15.510
Maybe one small hint for problem set one.,00:01:15.510,00:01:18.810
"Again, it's super important to understand
the math and dimensionality, and if you do",00:01:19.840,00:01:23.570
"that on paper, and then you still have
some trouble in the implementation.",00:01:23.570,00:01:27.610
"It can be very helpful to
essentially set break points and",00:01:27.610,00:01:31.970
"then print out the shape of all the
various derivatives you may be computing,",00:01:31.970,00:01:36.500
"and that can help you in your
debugging process a little bit.",00:01:36.500,00:01:40.050
"All right, are there any questions around
organization, problem sets, one, no?",00:01:41.640,00:01:44.960
"All right, my project,",00:01:46.230,00:01:49.470
"my office hours going to be a half
hour after the class ends today.",00:01:49.470,00:01:51.990
"So if you have project questions,
I'll be there after the class.",00:01:51.990,00:01:57.510
"All right, let's go to explanation
number 1 for back propagation.",00:01:59.280,00:02:02.650
"And again, just to motivate you of
why we want to go through this.",00:02:02.650,00:02:06.310
"Why do I torture some of you
with all these derivatives?",00:02:06.310,00:02:10.540
"It is really important to have an actual
understanding of the math behind",00:02:10.540,00:02:14.190
most of deep learning.,00:02:14.190,00:02:15.840
"And in many cases, in the future, you will
kind of abstract the way backpropagation.",00:02:15.840,00:02:20.640
"You'll just kind of assume it
works based on a framework,",00:02:20.640,00:02:23.950
software package that you might use.,00:02:23.950,00:02:25.890
"But that sometimes leads you to not
understand why your model might not",00:02:25.890,00:02:29.630
"be working, right?",00:02:29.630,00:02:30.710
"In theory,
you say it's just abstracted away,",00:02:30.710,00:02:33.390
I don't have to worry about it anymore.,00:02:33.390,00:02:35.050
"But really in practice, in the
optimization you might run into problems.",00:02:35.050,00:02:41.270
"And if you don't understand
the actual back propagation,",00:02:41.270,00:02:43.500
"you don't know why you
will have these problems.",00:02:43.500,00:02:45.230
"And so in addition to that, we kinda wanna
prepare you to not just be a user of",00:02:45.230,00:02:50.090
"deep learning, but
maybe even eventually do research.",00:02:50.090,00:02:53.240
"In this field and maybe think of and
implement and be very,",00:02:53.240,00:02:57.820
"very good at debugging
completely new kinds of models.",00:02:57.820,00:03:00.480
"And you'll observe that depending on which
software package you'll use in the future,",00:03:00.480,00:03:04.640
"not everything is de facto supported
in some of these frameworks.",00:03:04.640,00:03:08.530
"So if you want to create a completely new
model that's sort of outside the convex",00:03:08.530,00:03:12.775
"Known things, you will need to implement,
the forward and",00:03:12.775,00:03:15.770
"the backward propagation for a new
sub-module that you might have invented.",00:03:15.770,00:03:20.650
"So, you just have to trust me
a little bit in why it's useful.",00:03:20.650,00:03:24.760
Hopefully this helps.,00:03:24.760,00:03:26.390
"So last time we ended with
this kind of neural network,",00:03:27.390,00:03:31.700
where we had a single hidden layer.,00:03:31.700,00:03:34.240
And we derive all the gradients for,00:03:34.240,00:03:38.150
"all the different parameters of this
model, namely the word vectors here.",00:03:38.150,00:03:41.780
"The W, the weight matrix for
our single hidden layer and",00:03:41.780,00:03:47.330
the U for the simple linear layer here.,00:03:47.330,00:03:51.900
"And we defined this objective function and
we ended up writing,",00:03:51.900,00:03:57.060
"for instance, one such derivative here
fully out where we have the indicator",00:03:57.060,00:04:01.320
"function whether we're in this regime or
if it's zero.",00:04:01.320,00:04:03.830
"And if it's above zero,
then this was the derivative.",00:04:03.830,00:04:07.055
"In here, I just rewrote the same
derivative twice, essentially showing you",00:04:07.055,00:04:11.710
"that instead of having to recompute,
this if you had basically",00:04:11.710,00:04:16.550
"stored during forward propagation
the activations of this,",00:04:18.060,00:04:21.650
"this is exactly the same thing,
so f(Wx + b) we defined",00:04:21.650,00:04:27.440
"as our hidden activation, a, then you
could reuse those to compute derivatives.",00:04:27.440,00:04:31.550
"Alright, now we're going to take it up
a notch and add an additional hidden",00:04:34.330,00:04:39.130
"layer to that exact same model, and it's
the same kind of layer but in out that",00:04:39.130,00:04:44.080
"we have 2, we have to be very careful here
about our superscript which will indicate.",00:04:44.080,00:04:48.800
The layers that we're in.,00:04:48.800,00:04:50.150
"So it's the same kind
of window definition,",00:04:51.220,00:04:52.980
"we'll go over corpus, we'll select
samples for our positive class and",00:04:52.980,00:04:57.970
"everything that doesn't for instance have
an entity will be our negative class.",00:04:57.970,00:05:02.200
"Everything else is the same, but
we're adding one hidden layer to this.",00:05:02.200,00:05:06.430
"And so, let's go through the definition.",00:05:07.920,00:05:09.830
"We'll define x here, our Windows and
our word vectors that we concatenated,",00:05:09.830,00:05:15.710
"as our first activations,
our first hidden layer.",00:05:15.710,00:05:19.140
"And now to compute to intermediate
representation for our second layer,",00:05:19.140,00:05:23.850
"just a linear part of that,
we basically have here W1.",00:05:23.850,00:05:28.660
A superscript matrix times x plus b1.,00:05:28.660,00:05:30.820
"And then to compute the activations A,
superscript two of that will apply",00:05:30.820,00:05:35.790
"the element-wise nonlinearities,
such as the sigmoid function.",00:05:35.790,00:05:39.680
"All right, and
then we'll define this here as z3,",00:05:39.680,00:05:45.420
"same idea but this could potentially have
different dimensionalities, w1, w2 for",00:05:45.420,00:05:49.780
"instance don't have to have
exactly the same dimensionality.",00:05:49.780,00:05:52.280
"Do the same thing again,
element wise nonlinearity and",00:05:53.280,00:05:55.400
we have the same linear layer at the top.,00:05:55.400,00:05:57.380
"All right, are there any questions
around the definition of this here?",00:05:57.380,00:06:02.582
"&gt;&gt; [INAUDIBLE]
&gt;&gt; The question is do those two element",00:06:02.582,00:06:05.420
wise functions here have to be the same?,00:06:05.420,00:06:06.870
And the answer is they do not.,00:06:06.870,00:06:08.350
"And if fact this is something
that you could cross-validate and",00:06:08.350,00:06:12.710
"try as different
hyper-parameters of the model.",00:06:12.710,00:06:15.260
"We so far have only introduced
to you the sigmoid.",00:06:16.510,00:06:18.990
"So let's assume for now, it's the same.",00:06:18.990,00:06:21.600
"But in a later lecture, I think next week,
we'll describe a lot of other",00:06:21.600,00:06:25.616
kinds of non-linearities.,00:06:25.616,00:06:30.640
Yeah.,00:06:30.640,00:06:31.741
"How do you choose which
of these functions.",00:06:35.348,00:06:37.410
"We'll go into all of that once
we know that we have which or",00:06:37.410,00:06:40.100
what the options are.,00:06:40.100,00:06:41.120
"The best answer usually is,
you let your data speak for",00:06:41.120,00:06:44.140
"yourself and you run experiments
with a lot of different options.",00:06:44.140,00:06:48.800
"Once you do that, after a while you gain,
again, certain intuitions.",00:06:48.800,00:06:51.380
And you don't have to redo it every time.,00:06:51.380,00:06:53.640
"Especially if you have ten layers, you
don't wanna go through the cross-product",00:06:53.640,00:06:56.810
of five different nonlinearities.,00:06:56.810,00:06:58.990
And then all the different variations.,00:06:58.990,00:07:00.940
"Usually, you get diminishing returns for
some of those type of parameters.",00:07:00.940,00:07:04.020
&gt;&gt; Question.,00:07:04.020,00:07:04.520
"&gt;&gt; Yeah?
[INAUDIBLE]",00:07:05.520,00:07:10.743
"&gt;&gt; Sorry, I didn't hear you.",00:07:16.418,00:07:17.140
Right.,00:07:19.915,00:07:22.268
So the question,00:07:22.268,00:07:28.387
"is,could we put the b into the w?",00:07:28.387,00:07:34.820
"And if that's confusing,",00:07:34.820,00:07:35.720
"you could essentially assume
that b is this biased term here.",00:07:35.720,00:07:41.380
"Is another element of this W matrix,",00:07:41.380,00:07:44.230
"if we add a single one to every
activation that we have here.",00:07:44.230,00:07:49.138
"So, if a two frame,",00:07:49.138,00:07:54.010
"since we just added one here, then we
could get rid of this bias term and",00:07:54.010,00:07:56.920
"we'd have an additional Row or
column depending on what we have in W.",00:07:56.920,00:08:01.869
"So yes, we could fold b into W
to simplify the notation, but",00:08:01.869,00:08:05.254
"then as we're taking derivatives we want
to keep everything separate and clear.",00:08:05.254,00:08:09.955
"And you'll usually back propagate
through these activations,",00:08:09.955,00:08:12.830
"whereas you don't back
propagate through b.",00:08:12.830,00:08:14.875
"So for this math,
it's better to keep them separate.",00:08:14.875,00:08:17.236
Yeah.,00:08:17.236,00:08:18.735
"So U transpose is our last,
the question is what's U transposed?",00:08:22.068,00:08:26.245
"And U transpose is our last layer,
if you will.",00:08:26.245,00:08:29.811
"But there's no non-linearity with it,
and it's just a single vector.",00:08:29.811,00:08:34.738
"And so because by default here in
the notation of the class we assume these",00:08:34.738,00:08:39.260
are column vectors.,00:08:39.260,00:08:40.604
"We transpose it so
that we have a simple inner product.",00:08:40.604,00:08:43.320
"So it's just another set of
parameters that will score",00:08:43.320,00:08:46.770
"the final activations to be
high if they're a named entity,",00:08:46.770,00:08:50.110
"if there's a named entity of
the center of this window.",00:08:50.110,00:08:53.790
And scored low if not.,00:08:53.790,00:08:55.250
That's correct.,00:08:58.565,00:08:59.240
"It is just a score that
we're trying to maximize and",00:08:59.240,00:09:01.430
"we compute that final score
with this inner product.",00:09:01.430,00:09:04.640
"And so these activations
are now something that we",00:09:04.640,00:09:07.598
"compute in this pretty complex
neural network function, yeah.",00:09:07.598,00:09:11.480
"Here everything is a column vector,
that's correct.",00:09:20.021,00:09:22.013
All the x's are column vectors.,00:09:22.013,00:09:24.375
"So the question is,",00:09:34.576,00:09:35.825
"is there a particular reason of why we
chose a linear layer as the last layer?",00:09:35.825,00:09:40.720
"And the answer here is to
simplify the math a little bit.",00:09:40.720,00:09:43.415
"And because and to introduce to you
another kind of objective function,",00:09:43.415,00:09:48.266
"not everything has to be normalized and
basically summed to 1 as probabilities.",00:09:48.266,00:09:54.130
"If you just care about finding one
thing versus a lot of other things,",00:09:54.130,00:09:58.306
"like I just want to find named entities
that are locations as center words.",00:09:58.306,00:10:02.927
"And if it's high, then that's likely one.",00:10:02.927,00:10:05.018
"And if it's low,
then it's likely not a center location.",00:10:05.018,00:10:08.860
Then that's all you need to do.,00:10:08.860,00:10:10.040
"And in some sense, you could add here
a sigmoid after this, and then call it",00:10:10.040,00:10:15.084
"a probability, and then use standard
cross entropy loss to, in your model.",00:10:15.084,00:10:20.311
"There's no reason of why
you shouldn't do it.",00:10:20.311,00:10:23.331
"That's something you have to do in the
problem sets, trying to combine, we derive",00:10:23.331,00:10:28.613
"that, we help you derive the softmax and
cross entropy pair optimization.",00:10:28.613,00:10:33.302
"And then we going through this and
hopefully you can combine the two and",00:10:33.302,00:10:37.706
you'll see how both work.,00:10:37.706,00:10:39.700
"But it's essentially
a modeling decision and",00:10:39.700,00:10:42.493
it's not wrong to apply a sigmoid here.,00:10:42.493,00:10:44.933
"And then call this a probability,
instead of a score.",00:10:44.933,00:10:47.280
"All right, so now,
we have this two layer neural network, and",00:10:49.385,00:10:53.914
"we essentially did most of the work
already to derive the final things here.",00:10:53.914,00:10:59.820
"We already knew how to
derive our U gradients.",00:10:59.820,00:11:04.370
"And what used to be just
W is not W superscript 2,",00:11:04.370,00:11:08.510
"but just because we add the superscript
all the math is the same.",00:11:08.510,00:11:11.670
"So here, same derivation that we did for",00:11:12.680,00:11:15.960
"W, it's just now sitting
on a2 instead of on just a.",00:11:15.960,00:11:20.280
"And so what we did here, basically,
follows directly to what we now have.",00:11:20.280,00:11:25.730
"It's the same thing, but we now have
to be careful to add these superscripts",00:11:25.730,00:11:30.470
"depending on where we
are in the neural network.",00:11:30.470,00:11:32.700
"And we'll have the same definition
here when we multiply Ui and",00:11:34.640,00:11:39.310
"f prime of zi superscript 3, we'll
just call that delta superscript 3 and",00:11:39.310,00:11:43.960
"subscript i, for the ith element.",00:11:43.960,00:11:45.450
"And this is going to give us the partial
derivative with respect to Wij,",00:11:45.450,00:11:50.140
the i jth element of the W matrix.,00:11:50.140,00:11:52.100
"So this one we've already
derived in all its glory.",00:11:53.460,00:11:55.970
"I'm just putting here again
with the right superscripts.",00:11:55.970,00:12:01.290
"Now, the total function
that we have is this one.",00:12:03.050,00:12:07.030
"And, again, we have here this same
derivative, I just copied it over.",00:12:07.030,00:12:12.000
"And in matrix notation, we have to
find this as the outer product here.",00:12:12.000,00:12:16.110
"That would give us the cross product,
all the pairs of i and",00:12:16.110,00:12:20.130
"j to have the full
gradient of the W2 matrix.",00:12:20.130,00:12:23.720
"So this one was exactly as before, except
that we now add the superscript a2 here.",00:12:25.010,00:12:28.990
"Now, in terms of the notation,",00:12:30.090,00:12:32.048
"we defined this delta i in
terms of all these elements.",00:12:32.048,00:12:35.393
"And these are basically,
if you think about it, two vectors.",00:12:35.393,00:12:39.010
This Ui we could write as the full U.,00:12:39.010,00:12:42.010
All the elements of the u vector.,00:12:42.010,00:12:44.090
"And f prime of zi we could
write as f prime of z3 where",00:12:44.090,00:12:48.140
"we basically drop the index and assume
this is just one vector of a bunch of",00:12:48.140,00:12:52.000
"element wise applications of this gradient
function of this derivative here.",00:12:52.000,00:12:57.670
"So we'll introduce now this notation
which will come in very handy.",00:12:57.670,00:13:01.400
"And we call the Hadamard product or
element-wise product.",00:13:01.400,00:13:05.135
Sometimes you'll see it as little circles.,00:13:05.135,00:13:07.465
"Sometimes it's a circle with a cross or
with a dot inside.",00:13:07.465,00:13:10.970
"Whenever you see these
in backprop derivatives,",00:13:10.970,00:13:12.933
it's usually means the same thing.,00:13:12.933,00:13:14.390
"Which we just element-wise multiply all
the elements of the two vectors with one",00:13:14.390,00:13:19.329
another.,00:13:19.329,00:13:19.990
"So this is how we'll define
from now on this delta,",00:13:21.440,00:13:25.120
"the air signal that's
coming in at this layer.",00:13:25.120,00:13:28.850
"So the last missing piece for
back propagation and",00:13:28.850,00:13:31.970
"to understand it is essentially
the gradient with respect to W1,",00:13:31.970,00:13:37.980
"the second layer now,
that we're moving through.",00:13:37.980,00:13:41.570
"Any questions around the Hadamard product,
the outer product from the W?",00:13:44.092,00:13:48.500
Yeah?,00:13:52.062,00:13:52.616
It is no longer what?,00:13:59.539,00:14:00.416
Sorry it's.,00:14:05.310,00:14:06.210
"Associated, so yes.",00:14:08.285,00:14:10.236
"So the question is once you use the
Hadamard product, how is this related to",00:14:10.236,00:14:14.524
"the matrix multiplication here or
the vector, outer product?",00:14:14.524,00:14:18.260
"And so
you basically first have to compute this.",00:14:18.260,00:14:21.770
"And then you have the full
delta definition.",00:14:21.770,00:14:24.350
"And then you can multiply these and
outer product to get the gradient.",00:14:24.350,00:14:30.436
Yeah.,00:14:30.436,00:14:31.186
"Sure, so the question is, could you
assume that these are diagonal matrices?",00:14:38.230,00:14:42.813
"And yes, it's this kind of the same thing.",00:14:42.813,00:14:45.680
"But in terms of the multiplication
you have to then make sure your",00:14:45.680,00:14:50.170
"diagonal matrix is efficiently implemented
when it's multiplied with another vector.",00:14:50.170,00:14:54.940
"And as you write this out,
if this is confusing,",00:14:54.940,00:14:58.572
"write out what it means to
have a matrix times this.",00:14:58.572,00:15:02.389
"And what if this is
just a diagonal matrix?",00:15:02.389,00:15:04.830
"And what do you get versus just
multiplying each of these elements with",00:15:04.830,00:15:09.349
one another?,00:15:09.349,00:15:10.275
"So just write out the definitions
of the matrix product and",00:15:10.275,00:15:13.092
"then you'll observe that you
could think of it this way.",00:15:13.092,00:15:15.864
"But then really this f prime here,",00:15:15.864,00:15:19.539
"is just as a single vector
why apply the derivative",00:15:19.539,00:15:24.863
"function to a bunch of zeros,
in this case, zero, two so.",00:15:24.863,00:15:31.232
"All right, so the last missing piece,
W1, the gradient for it.",00:15:31.232,00:15:37.140
"And so the main thing we have to figure
out now is what's the bottom layer's",00:15:37.140,00:15:40.915
error message delta 2 that's coming in?,00:15:40.915,00:15:44.385
"And I'm not going to go
through all the indices again.",00:15:44.385,00:15:47.205
"It would take a while and
it's kind of repetitive.",00:15:47.205,00:15:49.235
"And it's very, very similar to what
we've done in the last lecture.",00:15:49.235,00:15:52.615
"But essentially,
we had already arrived at this expression.",00:15:52.615,00:15:56.750
As the next lower update.,00:15:56.750,00:15:59.790
"And in our previous model,
we would just arrive,",00:15:59.790,00:16:03.250
"that would be the signal that
arrives at the word vectors.",00:16:03.250,00:16:07.310
"So our final word vector
update was defined this way.",00:16:07.310,00:16:12.860
"And what we now basically
have to do is once more,",00:16:12.860,00:16:15.753
"just apply the chain rule because instead
of having coming up at the word vectors.",00:16:15.753,00:16:20.670
"Instead, we're actually
coming up at another layer.",00:16:20.670,00:16:23.170
"So basically, you can kind of call
it a local gradient also, but",00:16:25.710,00:16:29.770
"it's when you multiply whatever error
signal comes from the top layer,",00:16:29.770,00:16:35.880
"you multiply that with your local error
signal, in this case, f prime here.",00:16:35.880,00:16:40.440
"Then together, you'll get the update for
either the weights that are at that layer,",00:16:40.440,00:16:47.000
"or the intermediate term for
the gradient for lower layers.",00:16:47.000,00:16:51.860
So that's what we mean by our signal.,00:16:51.860,00:16:53.720
"And it might help in the next definition,",00:16:53.720,00:16:57.081
"it might give you a better explanation
of this in backprop number two.",00:16:57.081,00:17:02.760
"All right, so almost there.",00:17:02.760,00:17:04.571
"Basically, we apply the chain rule again.",00:17:04.571,00:17:06.870
"And if the chain rule for such a complex
function is maybe less intuitive,",00:17:06.870,00:17:10.750
"so one thing that helped
me many years ago,",00:17:10.750,00:17:13.520
"is to essentially assume all of these
are scalars, just single variable.",00:17:13.520,00:17:18.810
"And then derive all this,
assuming it just, U, W,",00:17:18.810,00:17:23.110
and x are all just single numbers.,00:17:23.110,00:17:25.940
"And then derive it,
that will help you gain some intuition.",00:17:25.940,00:17:30.200
"And then you'll observe in the end that
the final delta 2 is essentially similar",00:17:30.200,00:17:35.650
"to what we had derived in a very detailed
way, which is W2 transposed times delta 3,",00:17:35.650,00:17:41.520
"and then Hadamard product times f
prime of Z2 which is that layer here.",00:17:41.520,00:17:47.500
"And this is basically it,
if you understand these two equations, and",00:17:49.170,00:17:53.770
"you feel you can derive them now,
then you will know all the updates for",00:17:53.770,00:17:58.360
all standard multilayer neural networks.,00:17:58.360,00:18:00.680
"You will, in the end,
always arrive at these two equations.",00:18:00.680,00:18:05.010
"And that is, if you wanna
compute the error signal that's",00:18:05.010,00:18:09.849
"coming into a new layer,
then you'll have some form of W,",00:18:09.849,00:18:14.686
"of the high layer transposed times
the error signal that's coming in there.",00:18:14.686,00:18:21.143
"Hadamard product with element y's
derivatives here of f in the f prime.",00:18:21.143,00:18:28.810
"And in the final update for each W,
will always be this outer product",00:18:28.810,00:18:33.640
"of delta error signal times
the activation at that layer.",00:18:33.640,00:18:37.450
"And here, I include also our
standard regularization term.",00:18:39.390,00:18:43.410
"And you can even describe the top and
bottom layers this way.",00:18:46.590,00:18:51.880
"And then lead to word vectors and
the linear layer, but",00:18:51.880,00:18:53.980
they just have a very simple delta.,00:18:53.980,00:18:56.090
"All right, now, for some of you, just like
all right, now I understand everything,",00:18:57.280,00:19:01.452
"and it's great that I fully
understand back propagation.",00:19:01.452,00:19:04.426
But judging from Piazza and,00:19:04.426,00:19:07.890
"just from previous years, it's also
quite a lot to wrap your head around.",00:19:07.890,00:19:12.890
"And so I will go through three
additional explanations now,",00:19:12.890,00:19:17.344
of this exact same algorithm.,00:19:17.344,00:19:19.623
"And there, we're going through much
simpler functions not full neural",00:19:19.623,00:19:24.045
"networks, but
much simpler kinds of functions.",00:19:24.045,00:19:27.100
"But maybe for some, it will help to wrap
their heads around sort of the general",00:19:27.100,00:19:31.915
"idea of these error signals through
these simpler kinds of functions.",00:19:31.915,00:19:36.380
"So instead of having a crazy neural
network with lots of matrices and",00:19:36.380,00:19:40.220
hidden layers.,00:19:40.220,00:19:42.108
"We'll just kinda look at
a simple function like this, and",00:19:42.108,00:19:45.367
we'll arrive at a similar kind of idea.,00:19:45.367,00:19:47.860
"Namely, recursively applying and
computing these error signals or",00:19:47.860,00:19:52.555
"local gradients as we
move through a network.",00:19:52.555,00:19:56.690
"Now, the networks in this idea seen
function as circuits are going to be much,",00:19:56.690,00:20:01.540
much simpler.,00:20:01.540,00:20:02.905
"And these are examples from another
lecture on Git learning for",00:20:02.905,00:20:08.410
convolutional neural networks and,00:20:08.410,00:20:09.920
"computer vision, and we're basically
copying here some of their slides.",00:20:09.920,00:20:15.350
"And so let's take, for example, this very
simple function, f of three variables.",00:20:15.350,00:20:22.499
"And this simple function
is just x plus y times z.",00:20:22.499,00:20:26.230
"And let's assume we start with
some random initial values for",00:20:27.720,00:20:32.410
"x, y, and z from which we start and
wanna compute derivatives.",00:20:32.410,00:20:35.960
"Now, just as before with a complex neural
network, we can define intermediate",00:20:37.040,00:20:41.840
"terms but now, the intermediate
terms are very, very simple.",00:20:41.840,00:20:45.800
"So we'll just take q, for instance,
and we define q as x plus y,",00:20:45.800,00:20:49.713
this local computation.,00:20:49.713,00:20:51.320
"And now,
we can look at the partial derivatives",00:20:51.320,00:20:56.044
"here of q with respect to x and
with respect to y.",00:20:56.044,00:21:00.780
"They're very simple,
it's just addition, right, just one.",00:21:01.840,00:21:05.080
"And we can also define f now,
in terms of q times z,",00:21:05.080,00:21:07.950
"where we use our intermediately
defined function here.",00:21:07.950,00:21:12.840
"And here, we're kind of simplifying q, it
should be q is a function of x and y, but",00:21:12.840,00:21:17.080
we just drop that.,00:21:17.080,00:21:18.530
"And we can also define our partials of f,
our overall function with respect to q.",00:21:20.110,00:21:25.948
"Now, again, to connect that
to what we looked at before.",00:21:25.948,00:21:29.030
"F could be our lost function, x, y, z
could be parameters of this and we wanna,",00:21:29.030,00:21:33.665
"for instance, minimize our lost function.",00:21:33.665,00:21:36.310
"So now, what we want is, we want the final
updates to update these variables.",00:21:37.550,00:21:43.490
So we'll start with at the very top.,00:21:44.750,00:21:47.418
"Just a df by df which is just 1,
so it's not much there.",00:21:47.418,00:21:54.490
We usually start with that.,00:21:54.490,00:21:57.540
"And now, we want to update and
learn how do we update our z vectors?",00:21:57.540,00:22:02.260
"So we look at dfdz, and what is that?",00:22:02.260,00:22:08.240
"Well, we wrote down here all our different
derivatives, so df by dz is just q.",00:22:08.240,00:22:16.996
"And we define q as x + y, and x and",00:22:16.996,00:22:21.628
y is minus 2 and 5.,00:22:21.628,00:22:24.671
"And so the gradient or
the partial derivative here is just 3.",00:22:24.671,00:22:28.323
"All right, so far we're just very
simple q times derivative z, that's it.",00:22:30.834,00:22:36.385
"All right, now,
we can move also through this circuit.",00:22:36.385,00:22:41.578
"And are there questions around just
the description of this circuit,",00:22:41.578,00:22:44.805
of this function in terms of the circuit?,00:22:44.805,00:22:46.728
"All right, so now, let's look at the dfdq
which is the element here of the circuit,",00:22:51.607,00:22:59.323
"this node in the circuit
description of this function.",00:22:59.323,00:23:04.525
"Now, the dfdq is again, quite simple
we already wrote it right here.",00:23:04.525,00:23:10.820
"It's just z, and z is just minus 4.",00:23:10.820,00:23:14.657
"But now, the chain rule,
we have to multiply and",00:23:14.657,00:23:18.253
"this is essentially a delta
kind of error message.",00:23:18.253,00:23:22.260
"We multiply what we have from
the higher node in the circuit, but",00:23:22.260,00:23:26.390
"that's in this case, is just 1.",00:23:26.390,00:23:28.619
"And so the overall is just z times 1,
and z is minus 4, z is minus 4.",00:23:28.619,00:23:33.878
"And now,
we're going to move through this plus",00:23:33.878,00:23:38.586
"node to compute the next
lower derivatives here.",00:23:38.586,00:23:43.540
"And this is, we end up at the final
nodes here, the final leaf nodes if you",00:23:43.540,00:23:49.591
"will of this tree structure,
and we wanna compute the dfdy.",00:23:49.591,00:23:54.668
"Now dfdy,
We basically wanna use the chain rule,",00:23:54.668,00:24:00.428
"and we're going to multiply what
we have in the previous one, dfdq,",00:24:00.428,00:24:05.617
"which is the error signal coming from here
times dqdy, which is the local error,",00:24:05.617,00:24:11.699
"the local gradient,
it's not really the full gradient right,",00:24:11.699,00:24:16.529
"this is the local part
of the gradient dqdy.",00:24:16.529,00:24:19.873
"So we multiply these two terms,
the dfdq we wrote down here,",00:24:19.873,00:24:24.345
"that says z minus 4, times dqdy,
as you wrote down here.",00:24:24.345,00:24:28.740
"It's just one, so
minus 4 times 1, we got minus 4.",00:24:28.740,00:24:32.360
"And we can do the same thing for
x, again, apply the chain rule.",00:24:34.610,00:24:37.627
"All right, so in general, in this way of
seeing all these functions as circuits,",00:24:37.627,00:24:43.648
"we basically always have
some kind of input so",00:24:43.648,00:24:46.964
"each node in the circuit and
we compute some kind of output.",00:24:46.964,00:24:52.290
"And what's important is we can
compute our local gradients here",00:24:52.290,00:24:56.060
directly during the forward propagation.,00:24:56.060,00:24:59.050
"We don't need to know this
local part of the gradient.",00:24:59.050,00:25:03.030
We don't need to know what's up before.,00:25:03.030,00:25:04.870
"But in general, we will run this forward.",00:25:04.870,00:25:07.650
We'll around some of these values.,00:25:07.650,00:25:10.028
"And then in back propagation,
we get the gradient signals from any",00:25:10.028,00:25:14.920
"element upstream from each of
these nodes in the circuits.",00:25:14.920,00:25:20.430
"And essentially then,
use the chain rule and",00:25:20.430,00:25:23.873
"multiply all of these
to compute the updates.",00:25:23.873,00:25:27.413
"All right, any questions around
the definition of the circuits for",00:25:27.413,00:25:32.002
simple functions?,00:25:32.002,00:25:33.380
"It's very hard to take this
kind of abstraction, and",00:25:33.380,00:25:35.710
then get all the way to this full update.,00:25:35.710,00:25:37.705
"Therefore, a full near
layer neural network, but",00:25:37.705,00:25:41.995
"it's very good to gain intuition of
what's really going on, on a high level.",00:25:41.995,00:25:48.594
"&gt;&gt; All right, so now,",00:25:56.347,00:25:57.718
"let's go through a little more complex
example of what this looks like.",00:25:57.718,00:26:02.664
"And I think of at the end of that you
kind of gain some good intuition of how",00:26:02.664,00:26:07.446
"we basically do forward propagation, and",00:26:07.446,00:26:10.319
"recursively call these kinds of
circuits to compute the full update.",00:26:10.319,00:26:15.850
"So here,
we have a little bit more of a complex",00:26:15.850,00:26:20.400
"function namely actually our sigmoid
function that we had before.",00:26:20.400,00:26:24.680
"Usually when we have our sigmoid function,",00:26:24.680,00:26:26.260
"this was one activation
of one hidden layer.",00:26:26.260,00:26:29.100
"In most cases, x was our input and
w were the weights.",00:26:30.190,00:26:34.635
"So we defined this already, and now,
let's assume we just want to compute",00:26:34.635,00:26:39.239
"the partial derivatives with respect
to all the elements, w and x.",00:26:39.239,00:26:43.421
"And let's assume x and w are just,
x is two-dimensional, and",00:26:43.421,00:26:47.661
w is three-dimensional.,00:26:47.661,00:26:49.530
"And we have here the bias term
as just an extra element of w.",00:26:49.530,00:26:53.200
"So now, if you take this whole function,
we're gonna now compute or",00:26:54.440,00:26:59.050
define this as a circuit.,00:26:59.050,00:27:00.590
"That one description that's the most
detailed description of this function",00:27:00.590,00:27:05.802
"as a circuit would look like this,
where you basically recursively",00:27:05.802,00:27:10.372
"divide this function into all
the separate actions that you might take.",00:27:10.372,00:27:15.276
And you can compute gradients and,00:27:15.276,00:27:17.641
"the local gradients at each
note in this kind of circuit.",00:27:17.641,00:27:21.704
"So the last operation to compute
the final output f here of this function,",00:27:21.704,00:27:26.893
is 1 over whatever is in here.,00:27:26.893,00:27:29.080
"And so that's our last element of
the circuit, and from the bottom it",00:27:29.080,00:27:34.248
"starts with multiplying these two numbers,
multiplying these two numbers,",00:27:34.248,00:27:40.132
"and then adding to their summation
this w2 install, all right?",00:27:40.132,00:27:45.057
"Are there any questions around
the description of the circuit?",00:27:45.057,00:27:48.387
"All right, so now, let's assume we
start with these simple numbers here,",00:27:54.293,00:27:59.477
"so w2, w0 starts at 2, x0 starts at minus
1, minus 3, minus 2, and minus 3 here.",00:27:59.477,00:28:05.650
"So we just move forward through
the circuit to compute our forward",00:28:06.840,00:28:10.561
"propagation, right?",00:28:10.561,00:28:11.920
"So this is a relatively simple
concatenation of functions.",00:28:11.920,00:28:18.500
"And now, we wanna compute all our partial",00:28:18.500,00:28:21.970
"derivatives with respect to all
these different elements here.",00:28:21.970,00:28:26.260
"So we'll now go backwards and recursively
backwards through this circuit, and",00:28:26.260,00:28:31.290
apply the chain rule every time.,00:28:31.290,00:28:34.080
"So let's start the final value to the
forward propagation numbers here in green,",00:28:34.080,00:28:39.290
"at the top the final
value of this is 0.73.",00:28:39.290,00:28:43.490
"And again, the first delta derivative of
just the function with itself, is just 1.",00:28:43.490,00:28:51.080
"And now, we hit this node in a circuit,
and we want to now compute",00:28:51.080,00:28:56.330
"the derivative of this function,
and the function's 1 over x.",00:28:56.330,00:28:59.110
"And so the derivative is
just minus 1 over x squared.",00:28:59.110,00:29:02.688
"x is 1.73, and so we basically compute",00:29:02.688,00:29:07.968
"minus 1 divided by 1.37,
sorry, 1.37 squared.",00:29:07.968,00:29:15.311
"And then we multiply using a chain rule,",00:29:15.311,00:29:19.753
"the gradient signal here from
the top that goes into this node.",00:29:19.753,00:29:26.688
"So now, you multiple these two,
and you get the number minus 0.53.",00:29:26.688,00:29:31.861
"Now, we're moved to the next node,
so this node here,",00:29:35.686,00:29:42.454
"we just sum up a constant
with the value x,",00:29:42.454,00:29:47.494
and so the derivative of that is just 1.,00:29:47.494,00:29:52.390
"So we multiply, use the chain rule,
multiply these two elements,",00:29:52.390,00:29:57.130
"the error signal or
gradient signal from the top as it moves",00:29:57.130,00:30:01.840
"through this element of the circuit,
which is just minus 0.3 times 1 so",00:30:01.840,00:30:07.440
"we get again minus 0.53, sorry.",00:30:07.440,00:30:12.342
"Now, we move through the exponent.",00:30:12.342,00:30:14.260
It's a little more interesting.,00:30:14.260,00:30:15.690
"So here, derivative of e to
the x is just e to the x.",00:30:15.690,00:30:19.580
"And we have the incoming value
which is minus 1, so that's our x.",00:30:19.580,00:30:25.000
"So we have e to the minus
1 times minus 0.53,",00:30:26.140,00:30:30.487
"the gradient signal from
the higher node in this circuit.",00:30:30.487,00:30:35.693
"And we basically continue like this for
a while, and",00:30:39.000,00:30:43.212
"compute the same for plus,
similar to this plus and so on.",00:30:43.212,00:30:47.917
"And that the end, we arrive right here.",00:30:47.917,00:30:50.785
"And our error signal is 0.2, and
we have this multiplication here.",00:30:50.785,00:30:55.350
"And we know in multiplication, the partial",00:30:55.350,00:31:00.394
"of w0 times x0 partial with
respect to x0 is just w0.",00:31:00.394,00:31:06.811
"And so we multiply 0.2 times the value
here which is 2 and we get 0.4.",00:31:06.811,00:31:14.548
"And now, we have an update for
this parameter after we've moved",00:31:14.548,00:31:19.718
"recursively through the circuit
all the way to where it was used.",00:31:19.718,00:31:24.996
"And this is essentially the same thing
that we've done for the very complex",00:31:27.600,00:31:31.537
"neural network, but sort of one step
at a time for a very simple function.",00:31:31.537,00:31:35.425
"Any questions around this
sort of circuit description",00:31:41.033,00:31:45.887
of the same back propagation at year.,00:31:45.887,00:31:49.268
"Namely reusing the derivatives,
multiplying local error signals",00:31:49.268,00:31:54.400
"with the global error signals from
higher Layers, where here, the layer",00:31:54.400,00:31:59.749
"definition's a bit stretched, it's very,
very simple kinds of operations.",00:31:59.749,00:32:03.209
Yeah?,00:32:06.620,00:32:07.218
"That's right, so here,
each time the sort of gradient,",00:32:12.645,00:32:16.934
"the local gradient times the global or
above higher layer gradient signal.",00:32:16.934,00:32:22.550
"When you multiply them,
you get an actual gradient.",00:32:22.550,00:32:24.940
"So they're not really gradients, right,",00:32:24.940,00:32:26.975
"they're sort of intermediate
values of a gradient.",00:32:26.975,00:32:29.453
Yep.,00:32:32.874,00:32:33.643
"So the question is we're
using this kind of circuit",00:32:53.706,00:32:57.824
"interpretation to compute derivatives and
that's correct.",00:32:57.824,00:33:03.105
"If you were to just do
standard math on this equation",00:33:04.125,00:33:08.310
"you would end up with something
that looks exactly like this.",00:33:08.310,00:33:11.070
"And you would also have
similar kinds of numbers.",00:33:11.070,00:33:14.650
"But we're making it a little
more complicated, in some ways,",00:33:14.650,00:33:18.630
"to compute the derivatives here,
of each of the elements of this function.",00:33:18.630,00:33:23.480
We're kind of push the chain rule to its,00:33:23.480,00:33:27.540
"maximum by defining every single
operation as a separate function.",00:33:27.540,00:33:32.650
"And then computing gradients at
every single separate function.",00:33:32.650,00:33:35.650
"And when you do that, even for this kind
of simple function, you usually wouldn't",00:33:35.650,00:33:40.950
"write out this complex thing and take
a derivative with respect to this node,",00:33:40.950,00:33:45.080
"which is just plus,
cuz we all know how to do that.",00:33:45.080,00:33:47.378
"And usually we just move
through this very quickly but",00:33:47.378,00:33:49.650
"the circuit definitions can help you
understand the idea that at each node what",00:33:49.650,00:33:54.810
"you end up getting is the local gradient
times the gradient signal from the top.",00:33:54.810,00:33:59.870
"So in the end you get the exact same
updates as if you had just taken",00:34:01.710,00:34:06.310
"the derivatives using
the chain rule like this.",00:34:06.310,00:34:08.300
"And in fact, the definition of the circuit
can be arbitrary too and sometimes",00:34:09.620,00:34:14.810
"it's a lot more work to write out all the
different sub components of a function.",00:34:14.810,00:34:21.090
"So for instance,
we know if we just described",00:34:21.090,00:34:23.850
"sigma of x as our sigmoid function, we
could kind of combine all these different",00:34:24.910,00:34:29.920
"elements of the circuit as
just one node in the circuit.",00:34:29.920,00:34:35.760
"And we know, with this one little
trick here, the derivative",00:34:35.760,00:34:40.300
"of sigma x with respect to x can actually
be described in terms of sigma x.",00:34:40.300,00:34:44.880
"So we don't need to do any extra
computation like we did internally here,",00:34:44.880,00:34:48.380
take another exponent and so on.,00:34:48.380,00:34:50.580
"We actually can just know, well if that
was our value here of sigma x then",00:34:50.580,00:34:55.482
"the derivative that will come out here
is just 1- sigma x times sigma x.",00:34:55.482,00:35:00.479
"And so we could, in theory, also define
our circuit differently, and in fact",00:35:00.479,00:35:04.752
"the circuits we eventually define are this
whole thing is one neural network layer.",00:35:04.752,00:35:09.382
"And internally we know exactly the kinds
of messages that pass through such",00:35:09.382,00:35:14.094
"a layer, or the error signals, or
again, elements of the final gradients.",00:35:14.094,00:35:18.980
Yeah?,00:35:20.080,00:35:20.580
"That's a good question, sorry, yes.",00:35:26.314,00:35:28.270
"So the question is, we're talking
about back propagation here, and",00:35:28.270,00:35:30.350
what is forward propagation?,00:35:30.350,00:35:31.300
"Yeah, forward propagation just means
computing the value of your overall",00:35:31.300,00:35:34.485
function.,00:35:34.485,00:35:34.997
"The relationship between the two is
forward propagation is what you compute,",00:35:37.828,00:35:42.039
"what you do at test time, to compute
the final output of your function.",00:35:42.039,00:35:46.330
"So, you want the probability for
this node to be a location, or for this",00:35:46.330,00:35:52.200
"word to be a location, you'd do forward
propagation to compute that probability.",00:35:52.200,00:35:56.550
"And the you do backward propagation to
compute the gradients if you wanna train",00:35:56.550,00:36:00.294
"and update your model if you have
a training data set and so on.",00:36:00.294,00:36:03.256
"That's right, the red numbers here at
the bottom are all the partial derivatives",00:36:07.457,00:36:11.830
with respect to each of these parameters.,00:36:11.830,00:36:14.148
"And here all the intermediate values
that we use as that gradient flows",00:36:14.148,00:36:19.872
"through the circuit to the parameters that
we might wanna update, great question.",00:36:19.872,00:36:26.769
"All right, so",00:36:30.972,00:36:31.829
"essentially we recursively applied the
chain rule as we moved through this graph.",00:36:31.829,00:36:37.330
"And we end up with a similar
kind of intuition,",00:36:37.330,00:36:40.480
"as we did with the same,
with just using math and",00:36:42.410,00:36:47.125
"multivariate calculus, to arrive at these
final gradients, to update our parameters.",00:36:47.125,00:36:51.980
"All right,
any questions around the circuit?",00:36:53.010,00:36:56.032
"Interpretation of back propagation, yeah.",00:36:56.032,00:36:59.725
"So here w2 is our bias term,
it doesn't depend on the values of x,",00:37:03.642,00:37:09.031
"we just add it, and
w2 down here in the circuit.",00:37:09.031,00:37:13.140
"So that is the last element we add
after adding these two multiplications.",00:37:13.140,00:37:18.997
"All right, so,
now if that was too simple and",00:37:24.206,00:37:27.781
"you wanna get a little
bit high level again,",00:37:27.781,00:37:31.264
"you can essentially think of these
circuits also as flow graphs.",00:37:31.264,00:37:36.460
"And circuit is the terminology that
Andrej Karpathy used in 231 and",00:37:36.460,00:37:41.714
"Yoshua Bengio, for instance,
another very famous researcher in",00:37:41.714,00:37:46.701
"deep learning uses the terminology
of flow graphs, but, again,",00:37:46.701,00:37:51.687
we have the very similar kind of idea.,00:37:51.687,00:37:54.560
"You start with some input x,",00:37:54.560,00:37:56.231
"you do forward propagation to
compute some kind of value.",00:37:56.231,00:37:59.653
"You go through some intermediate variables
y, and then, in back propagation,",00:37:59.653,00:38:04.516
"you compute your gradients going backwards
in the reverse order to what you've",00:38:04.516,00:38:09.309
done during forward propagation.,00:38:09.309,00:38:11.470
"And so this is if you just have one
intermediate value now if x, and",00:38:13.534,00:38:18.123
"this is something else important to
know it for the circuits it's the same,",00:38:18.123,00:38:23.464
"if x modifies two paths in
your flow graph you end up,",00:38:23.464,00:38:27.135
based on the multiple variable Chain Rule.,00:38:27.135,00:38:30.930
"You have to sum up the local air signals
for both from both of the paths.",00:38:30.930,00:38:37.160
"And in general,
again you move backwards through them.",00:38:37.160,00:38:40.880
"So usually as long as you have some
kind of directed basically graph or",00:38:40.880,00:38:46.040
"tree structure,
you can always compute these flows and",00:38:46.040,00:38:49.870
these elements of your gradient.,00:38:49.870,00:38:53.660
"And in general, if x goes through multiple
different elements in your flow graph,",00:38:55.030,00:39:00.040
you just sum up all the partials this way.,00:39:00.040,00:39:03.690
"And so this is another interpretation much
more high level without defining exactly",00:39:05.550,00:39:10.165
"what kinds of computation
you have here at each node.",00:39:10.165,00:39:13.098
"But in general you can define
these kind of flow graphs and",00:39:13.098,00:39:17.201
"each node is some kind
of computational result.",00:39:17.201,00:39:20.722
"And each arc here is some kind
of dependency, so you need,",00:39:20.722,00:39:23.915
"in order to compute this, you needed this.",00:39:23.915,00:39:26.680
"And you can define more complex
things where you have so",00:39:26.680,00:39:29.854
"called short circuit connections, we'll
define those much later in the class, but",00:39:29.854,00:39:34.822
"in general,
you move forward through your node.",00:39:34.822,00:39:37.720
"So this is a more realistic example
where we may have some input x,",00:39:37.720,00:39:41.625
"we have some probability, or
sorry some class y for our train data set.",00:39:41.625,00:39:45.893
"And in forward propagation,
we'll move these through a sigmoid neural",00:39:45.893,00:39:50.814
"network layer here such
as h is just sigma of Vx.",00:39:50.814,00:39:54.050
We dropped here The bias term.,00:39:54.050,00:39:56.720
"And so, you can also describe your
v as part of this flow graph.",00:39:56.720,00:40:01.640
"You move through a next layer, and
then you may have a softmax layer here,",00:40:01.640,00:40:06.150
"similar to the one that you
derived in problem set one.",00:40:06.150,00:40:09.690
"And then you have your
negative log likelihood, and",00:40:09.690,00:40:12.420
"you compute that final cost function for
this pair xy, for this training element.",00:40:12.420,00:40:18.100
"And then back propagation again,
you move backwards through the flow graph.",00:40:18.100,00:40:21.597
"And you update your parameters as
you move through the flow graph.",00:40:21.597,00:40:26.871
"Now, before I go through the last and
final explanation, the good news is you",00:40:35.548,00:40:40.294
"won't actually have to do that for
very complex neural networks.",00:40:40.294,00:40:44.250
It would be close to impossible for,00:40:44.250,00:40:45.790
"the kinds of large complex neural
networks to do this by hand.",00:40:45.790,00:40:50.020
"Many years ago, when I had started my PhD,",00:40:50.020,00:40:52.640
"there weren't any software packages
with automatic differentiation.",00:40:52.640,00:40:55.660
So you did have to do that.,00:40:55.660,00:40:57.210
And it slowed us down a little bit.,00:40:57.210,00:40:59.610
"But, nowadays,
you can essentially automatically",00:40:59.610,00:41:03.370
"infer your back propagation updates
based on the forward propagation.",00:41:03.370,00:41:07.580
"It's a completely deterministic process,
and",00:41:07.580,00:41:10.230
"so can use symbolic expressions for
your forward prop.",00:41:10.230,00:41:14.470
"And then have algorithms automatically
determine your gradient, right?",00:41:14.470,00:41:19.030
"The gradients always exist for
these kinds of functions.",00:41:19.030,00:41:21.070
"And so that will allow us
to much faster prototyping.",00:41:22.150,00:41:25.140
"And you'll get introduced
next week to a tensor flow,",00:41:25.140,00:41:29.280
"which is one such package that essentially
takes all these headaches away from you.",00:41:29.280,00:41:34.250
"But with this knowledge,",00:41:34.250,00:41:35.973
"you'll actually know what's going on
under the hood of these packages.",00:41:35.973,00:41:40.842
"All right, any question around the flow
graph interpretation of back propagation?",00:41:40.842,00:41:44.530
Yes?,00:41:45.950,00:41:46.450
It's actually in closed form.,00:41:51.209,00:41:53.180
"Yeah, it's not numerically solved.",00:41:53.180,00:41:54.580
"So sorry, the question was, the automatic
differentiation, is it numeric or",00:41:54.580,00:41:59.225
symbolic?,00:41:59.225,00:41:59.931
It's usually symbolic.,00:41:59.931,00:42:00.710
"All right, now, for the last and
final explanation of the same idea.",00:42:03.310,00:42:07.320
"But combining the idea of the flow graph
with the math that you've seen before,",00:42:08.800,00:42:14.690
and hopefully that will help.,00:42:14.690,00:42:17.030
"So, let's bring back this complex
two layer neural network.",00:42:17.030,00:42:21.840
"Now, how can we describe this
at a much simplified kind of",00:42:21.840,00:42:27.110
"flow graph or circuit where we can combine
in a lot of different elements instead of",00:42:27.110,00:42:32.240
"writing every multiplication, summation,
exponent, negation, and so and out?",00:42:32.240,00:42:37.680
"This is the kind of flow
graph that kind of yeah,",00:42:37.680,00:42:40.540
kind of combines these two worlds.,00:42:40.540,00:42:42.880
So we assumed here we had our delta,00:42:42.880,00:42:46.340
"error signal coming from
the simple score that we have.",00:42:47.980,00:42:52.380
"And let's say that our final, we want all
the updates, essentially, to W(2) and",00:42:52.380,00:42:57.071
W(1).,00:42:57.071,00:42:57.669
"Now W(2), as we move through this
linear score, the delta doesn't change.",00:42:57.669,00:43:04.580
"And so the update that we get for W(2)
here is just this outer product again.",00:43:04.580,00:43:11.116
"And that's kind of, as we move through
this very high level flow graph,",00:43:11.116,00:43:17.017
"we basically now update W(2) once we get
the error message from the layer above.",00:43:17.017,00:43:23.820
"Now, as we move through W(2),",00:43:24.900,00:43:27.642
"this kind of circuit will essentially
just multiply the affine,",00:43:27.642,00:43:32.844
"like as we move through this simple affine
transformation this matrix vector product,",00:43:32.844,00:43:39.844
"we're just required to transpose
the forward propagation matrix.",00:43:39.844,00:43:45.356
"And we arrived why this is before,
but this is kind of the interpretation",00:43:45.356,00:43:50.435
"of this flow graph in terms of a complex
and large realistic neural network.",00:43:50.435,00:43:55.780
"And so notice also that
the dimensions here line up perfectly.",00:43:56.930,00:44:00.557
"So the output here, we multiply this delta
that has the dimensionality of the output.",00:44:00.557,00:44:07.856
"With the transpose, we get exactly
the dimensionality of the input of this W.",00:44:07.856,00:44:12.834
"So it's quite intuitive, right?",00:44:12.834,00:44:14.040
"You have the linear transformation,
affine transformation through this W",00:44:14.040,00:44:19.340
"as you move backwards to this W,
you just multiply it with its transpose.",00:44:19.340,00:44:24.230
"And now, we are hitting this
element wise nonlinearity.",00:44:24.230,00:44:29.613
"And so as we update the next delta,",00:44:29.613,00:44:33.332
"we essentially have also
an element wise derivative",00:44:33.332,00:44:38.979
"here of each of the elements
of this activation.",00:44:38.979,00:44:44.270
"So as we're moving our error vector,
error signal, or",00:44:44.270,00:44:47.942
"global parts of the gradient through
these point-wise nonlinearities, we need",00:44:47.942,00:44:53.104
"to apply point-wise multiplications with
the local gradients of the non-linearity.",00:44:53.104,00:44:58.989
"And now we have this delta
that's arrived at W(1).",00:45:01.110,00:45:04.777
"And so W1 we can now compute
the final gradient with respect to",00:45:04.777,00:45:08.837
"W(1) as just the delta again times
the activation of the previous layer,",00:45:08.837,00:45:13.317
"which is a(1) and
we have this outer product.",00:45:13.317,00:45:17.020
"So this is combining the different
interpretations that we've learned.",00:45:17.020,00:45:20.890
"We arrived through this through
just multivariate calculus.",00:45:21.910,00:45:26.060
"And now this is the flow graph or
circuit interpretation of what's going on.",00:45:26.060,00:45:31.060
Yes?,00:45:32.840,00:45:33.393
"&gt;&gt; If I mean point-wise non linearity,
I mean coordinate wise, yes,",00:45:39.080,00:45:42.894
they are the same.,00:45:42.894,00:45:43.962
"So, whenever we write f(z) here, and",00:45:43.962,00:45:48.589
"z was a vector of z1, z2, for instance,",00:45:48.589,00:45:53.487
then we meant f(z1) and f(z2).,00:45:53.487,00:45:57.858
"And the same is true if
we write it like this.",00:45:57.858,00:46:00.748
And look at the partial derivatives.,00:46:02.360,00:46:05.167
"Yeah?
&gt;&gt; I know.",00:46:05.167,00:46:06.064
"&gt;&gt; That's just, from matrix [INAUDIBLE].",00:46:16.168,00:46:19.539
"It is, yes, so the question is the delta
here the same as in the definition of",00:46:28.926,00:46:32.581
the two layer neural network?,00:46:32.581,00:46:34.080
"And it is, yeah.",00:46:34.080,00:46:35.190
So this delta here is this and,00:46:35.190,00:46:37.158
"you notice here that it's the same
thing that we wrote before.",00:46:37.158,00:46:41.518
We have W(2) transpose times delta(3).,00:46:41.518,00:46:44.455
"And then you have the Hadamard product
with the element-wise derivatives here.",00:46:44.455,00:46:49.846
"All right, congratulations!",00:46:56.574,00:46:59.370
"You've done it.
So",00:46:59.370,00:47:00.200
"now, understand the inner workings of
most deep learning models out there.",00:47:00.200,00:47:05.220
"And this was literally
the hardest part of the class.",00:47:05.220,00:47:06.950
"I think it's gonna go all uphill
from here for many of you.",00:47:06.950,00:47:10.520
"And everything from now on is really
just more matrix multiplications and",00:47:10.520,00:47:15.690
this kind of back propagation.,00:47:15.690,00:47:17.380
"It's really 90% of the state of
the art models out there right now and",00:47:17.380,00:47:21.320
"top new papers that
are coming out this year.",00:47:21.320,00:47:23.920
"You now can have a warm, fuzzy feeling,",00:47:23.920,00:47:26.582
"as you look through the forward
propagation definitions.",00:47:26.582,00:47:30.508
"All right, with that, let's have a little
intermission and look at a paper.",00:47:30.508,00:47:37.996
"Take it away.
&gt;&gt; Hi everyone.",00:47:37.996,00:47:40.021
"So yeah, so let's take a break from neural
networks, and let's talk about this",00:47:40.021,00:47:44.565
"paper which came out from Facebook ARV
search just this past summer.",00:47:44.565,00:47:48.698
"So text classification is
a really important topic in NLP.",00:47:48.698,00:47:53.931
"Given a piece of text, we may wanna say,
is this a positive sentiment or",00:47:53.931,00:47:57.514
does it have negative sentiment?,00:47:57.514,00:48:00.160
"Is this spam or ham, or
did JK Rowling actually write this?",00:48:00.160,00:48:03.498
"And so
this one's particular from a website and",00:48:03.498,00:48:07.229
"it's basing [COUGH] an example
of sentiment analysis.",00:48:07.229,00:48:11.527
"And so if you recall from your
problem set in problem four.",00:48:11.527,00:48:16.653
"An easy way to featurize a sentence
is to just average out all the word",00:48:16.653,00:48:21.612
vectors in a sentence.,00:48:21.612,00:48:23.580
"And that's basically what
the model from this paper does.",00:48:23.580,00:48:27.510
"And so they use really low
dimensional word vectors.",00:48:27.510,00:48:30.930
"Take the average of them, kind of you
know you lose the ordering of it and",00:48:30.930,00:48:34.230
"then you get this low dimensional text
vector which represents the sentence.",00:48:34.230,00:48:39.650
"In order to kind of get some of
the ordering back, they also use n-grams.",00:48:39.650,00:48:44.864
"And so now that we have the text vector
that's kind of like in the hidden layer.",00:48:44.864,00:48:49.471
"We then feed it through a linear
classifier which uses softmax compute",00:48:49.471,00:48:52.802
"the probability over all
the predictive classes.",00:48:52.802,00:48:55.240
"The hidden representation is also
shared by all the classifiers for",00:48:56.490,00:48:59.521
all the different categories.,00:48:59.521,00:49:00.969
"Which helps the classifier use information
about words learned from one category for",00:49:00.969,00:49:05.937
another category.,00:49:05.937,00:49:07.110
"And so
will look a little bit more familiar",00:49:09.052,00:49:12.010
"to you now that you guys have gone
through all the costs and whatnot.",00:49:12.010,00:49:15.300
"So we minimize the negative flaws
likelihood over all the classes, and",00:49:15.300,00:49:19.310
"the model's trying to using
stochastic gradient descent and",00:49:19.310,00:49:22.535
a linear decaying learning rate.,00:49:22.535,00:49:26.055
"Another thing that makes it really fast
is the use of the hierarchical softmax.",00:49:26.055,00:49:29.695
"And so by using this, the classes
are organized in like this tree kind of",00:49:29.695,00:49:32.995
fashion instead of just like in a list.,00:49:32.995,00:49:34.818
"And so this also helps with the timing, so",00:49:34.818,00:49:39.193
"we go from linear time
to logarithmic time.",00:49:39.193,00:49:43.940
"Because also the costs are organized
in terms of how frequent they are.",00:49:43.940,00:49:47.701
"So in case, we have maybe like a lot
of class, but less of one class.",00:49:47.701,00:49:50.812
"This helps kind of balance that out so
NLP is really hot right now.",00:49:50.812,00:49:54.395
"So in here the depth is much smaller,
so we can access that cost a lot faster.",00:49:54.395,00:49:59.714
"But maybe for some less popular topics,
I just made some up here,",00:49:59.714,00:50:02.741
that's not actually my opinion.,00:50:02.741,00:50:04.404
"But they have a much deeper depth
because they are much more infrequent.",00:50:04.404,00:50:10.271
"And so especially in this day and age when
we're really crazy about neural networks,",00:50:10.271,00:50:13.166
"the question is like how well
does this stack up against them?",00:50:13.166,00:50:15.313
"Because it uses a linear classifier, it
doesn't really have all those layers for",00:50:15.313,00:50:18.835
neural network.,00:50:18.835,00:50:20.270
"And as it turns out,
this actually performs really well.",00:50:20.270,00:50:23.530
"It's not only really fast, but it performs
just as well if not sometimes better than",00:50:23.530,00:50:28.445
neural networks which is pretty crazy.,00:50:28.445,00:50:30.060
And so just a quick summary.,00:50:31.230,00:50:32.309
"FastText, which is what they call their
model is often on par with deep learning",00:50:32.309,00:50:36.963
classifiers.,00:50:36.963,00:50:37.872
"It takes seconds to train,
instead of days,",00:50:37.872,00:50:40.118
"thanks to their use of low dimensional
word vectors in the hierarchical softmax.",00:50:40.118,00:50:43.972
"And another side bit, is that it can also
learn vector representations of words in",00:50:43.972,00:50:48.055
"different languages,
with performs even better than word2vec.",00:50:48.055,00:50:51.624
Thank you.,00:50:51.624,00:50:52.250
"&gt;&gt; [APPLAUSE]
&gt;&gt; All right, and you know what's awesome?",00:50:52.250,00:50:59.483
"Like this kind of equation you could
totally derive all the gradients now too.",00:50:59.483,00:51:02.823
"&gt;&gt; [LAUGH]
&gt;&gt; Just another day in the office.",00:51:02.823,00:51:07.499
"All right, so class project.",00:51:07.499,00:51:11.932
"This is for many, the most lasting and
fun part of the class.",00:51:11.932,00:51:18.554
"But some people also don't
have a research agenda or",00:51:18.554,00:51:22.219
"some kind of interesting data set,
so you don't have to do the project.",00:51:22.219,00:51:27.310
"If you do a project,
we want you to have a mandatory mentor.",00:51:28.610,00:51:33.506
"The mentors that are pre-approved are all
the PhD students, and Chris and me.",00:51:33.506,00:51:39.867
"So we wanna really give
you good advice and",00:51:39.867,00:51:43.148
"we want you to meet your
mentors frequently.",00:51:43.148,00:51:46.725
"So think I'll have 25, Chris has 25, and",00:51:46.725,00:51:50.302
"then I guess each of the PhD TAs
also has at most 25 groups.",00:51:50.302,00:51:54.906
It's a very large class.,00:51:54.906,00:51:56.986
"But yeah, so
basically your class projects,",00:51:56.986,00:52:00.500
"if you do decide to do it,
is 30% of your final grade.",00:52:00.500,00:52:04.747
"And sometimes real paper
submissions come out from these.",00:52:04.747,00:52:09.476
"It's really exciting, you get to travel.",00:52:09.476,00:52:11.044
"You get probably paid,
depending on who you're working with.",00:52:11.044,00:52:14.208
"If you're a grad student and
you write a paper,",00:52:14.208,00:52:17.568
to go to some fun places in the world.,00:52:17.568,00:52:20.181
"And something that's really helpful for
people's careers.",00:52:20.181,00:52:24.330
"Sometimes these papers,",00:52:24.330,00:52:25.822
"people get contacted from various
companies once we put these papers up.",00:52:25.822,00:52:30.035
"If you do a really good job,",00:52:30.035,00:52:31.737
"it can have really lasting impact
on the kinda work that you do.",00:52:31.737,00:52:35.664
"So on the choice of doing assignment four,
the final project.",00:52:35.664,00:52:39.527
"We don't wanna force you
to do the final project,",00:52:39.527,00:52:41.905
"cuz some people just wanna learn
the concepts and then move on with life.",00:52:41.905,00:52:45.371
"And it can be a little painful to
try to come up with something.",00:52:45.371,00:52:49.341
"So there is a final project, and",00:52:49.341,00:52:51.627
"we will ask you to sort of define
your project with your mentor.",00:52:51.627,00:52:56.128
And then we might encourage you or,00:52:56.128,00:52:57.752
"discourage you from moving
forward with that project.",00:52:57.752,00:53:00.481
"Some projects might be too large in
scope or too small in scope, and so on.",00:53:00.481,00:53:04.230
"And so do check with the TAs of whether
the project is the right thing for you.",00:53:04.230,00:53:10.506
"If you do a project, and if you decide
to do it you really have to start early.",00:53:10.506,00:53:15.750
"Ideally you will start meeting me today,
or",00:53:15.750,00:53:20.346
"latest like next week or
two weeks and or the other TAs.",00:53:20.346,00:53:26.074
"We write out a lot of the sort of
organizational things on the website.",00:53:26.074,00:53:32.953
So let's look at the website really quick.,00:53:32.953,00:53:35.220
It's now linked from our main page.,00:53:35.220,00:53:37.689
"So you can get a couple of different
ideas from these top conferences.",00:53:37.689,00:53:41.605
"So one project idea and
we'll go into that a little bit later,",00:53:41.605,00:53:45.416
"is to take one of these newest
papers from the various groups or",00:53:45.416,00:53:49.227
"various conferences and
just try to replicate the results.",00:53:49.227,00:53:53.470
"You will notice that despite having in
theory, everything written in the paper,",00:53:53.470,00:53:57.802
"if it's a nontrivial model
there's a lot of subtle detail.",00:53:57.802,00:54:01.022
"And it's hard to squeeze all of
those details in eight pages.",00:54:01.022,00:54:04.466
"So usually the maximum page them in so
replicating sometimes,",00:54:04.466,00:54:08.653
"this paper is sufficient enough for
most papers in most projects.",00:54:08.653,00:54:13.083
"So here, here's some very concrete
papers that you can look at and",00:54:16.107,00:54:21.553
to get adheres from others.,00:54:21.553,00:54:23.894
"And what's kind of interesting and
new these days, this is by no means and",00:54:23.894,00:54:29.846
"exclusive list,
there a lot more other interesting papers.",00:54:29.846,00:54:35.280
"So again here there sort of pre
proofed mentors for projects.",00:54:35.280,00:54:39.431
"You'll have to contact
us through office hours.",00:54:39.431,00:54:42.234
"And if you do a project
in your project proposal,",00:54:42.234,00:54:47.277
you have to write out who the mentor is.,00:54:47.277,00:54:51.341
"A lot of other mentors, we'll
actually list probably next week now.",00:54:51.341,00:54:55.168
"A list of potential projects that
are coming from people who spend all their",00:54:55.168,00:54:59.074
time thinking about deep learning and NLP.,00:54:59.074,00:55:02.300
"So if you don't have an idea, but you
really do wanna do some interesting novel",00:55:02.300,00:55:06.341
"research project,
we'll post that link internally.",00:55:06.341,00:55:09.232
"So that not the whole world sees it,
but only the students in this class.",00:55:09.232,00:55:13.410
"Cuz sometimes, the PhD students
have some interesting novel idea.",00:55:13.410,00:55:16.647
"They don't want it to get scooped and
have some other researchers do that idea,",00:55:16.647,00:55:20.112
"but they do wanna collaborate
with students and youths.",00:55:20.112,00:55:22.607
"So we'll keep those
ideas under wraps here.",00:55:22.607,00:55:27.897
"So yeah, this is your project proposal.",00:55:27.897,00:55:29.274
"You have to define all these things, and",00:55:29.274,00:55:32.119
"we'll go through that now in
some of the details here.",00:55:32.119,00:55:36.890
"And then you have a final submission,
you have to write a report.",00:55:36.890,00:55:39.557
"And then we'll also have
a poster presentation,",00:55:39.557,00:55:42.571
"where all the projects
are basically being described.",00:55:42.571,00:55:45.957
"You'll have to print a little poster,
and we'll walk around.",00:55:45.957,00:55:49.518
It's usually quite fun.,00:55:49.518,00:55:50.413
"Maybe we'll even come up with a prize for
best poster, and best paper, and so on.",00:55:50.413,00:55:55.858
"All right, so
these are the organizational, Tips.",00:55:55.858,00:55:59.550
"Posters and projects by the way
I have maximum of three people.",00:55:59.550,00:56:04.340
"If you have some insanely,
well thought out plan,",00:56:04.340,00:56:08.830
we may make an exception and go to four.,00:56:08.830,00:56:10.830
But the standard default is three.,00:56:10.830,00:56:12.920
"So the exception kind of has to be
mailed to the TAs or Aston Piazza.",00:56:12.920,00:56:17.540
"Any questions around the organizational
aspects of the project?",00:56:20.530,00:56:23.520
Groups.,00:56:24.720,00:56:25.370
"You can do groups of one, two, or three.",00:56:25.370,00:56:28.060
So it doesn't have to be three.,00:56:28.060,00:56:29.520
"The bigger your group,
the more we expect from the project.",00:56:29.520,00:56:33.150
"And you have to also write out exactly
what each person in the project has done.",00:56:33.150,00:56:38.980
"You can actually use any kind of open
source library and code that you want.",00:56:38.980,00:56:47.300
It's just a realistic research project.,00:56:47.300,00:56:49.690
"But if you just take Kaldi,
which is a speech recognition system, and",00:56:49.690,00:56:54.100
you say I did speech recognition.,00:56:54.100,00:56:55.470
"And then really all you did
was download the package and",00:56:55.470,00:56:57.900
"run it, then that's not very impressive.",00:56:57.900,00:57:00.730
"So the more you use,
the more you also have to be careful and",00:57:00.730,00:57:05.500
"say exactly what parts
you actually implemented.",00:57:05.500,00:57:09.230
"And in the code,
you also have to submit your code, so",00:57:11.030,00:57:14.550
"that we understand what you've done and
the results are real.",00:57:14.550,00:57:18.718
"So this year we do want
some language in there.",00:57:30.368,00:57:34.860
Some natural human language.,00:57:34.860,00:57:36.620
Last year I was a little more open.,00:57:38.270,00:57:39.600
"It could be the language of music and
so on now.",00:57:39.600,00:57:41.470
But this year it's [INAUDIBLE].,00:57:41.470,00:57:42.990
"So we've got to have some
natural language in there, yeah.",00:57:42.990,00:57:46.948
"But other than that,
that can be done quite easily so",00:57:48.984,00:57:51.876
"we'll go through the types of
projects you might want to do.",00:57:51.876,00:57:55.760
"And if you have a more theoretically
inclined project where you",00:57:55.760,00:57:59.430
"really are just faking out some clever
way of doing a sarcastic ready to sent or",00:57:59.430,00:58:04.380
"using different kinds of
optimization functions.",00:58:04.380,00:58:07.100
"An optimizers that we'll talk
about leading the class to",00:58:07.100,00:58:10.390
"then as long as you at least
applied it in one experiment",00:58:10.390,00:58:13.300
"to a natural language processing data set
that would still be a pretty cool project.",00:58:13.300,00:58:17.270
"So you can also apply
it to genomics data and",00:58:18.870,00:58:21.520
"to text data if you wanna have
a little bit of that flavor.",00:58:21.520,00:58:24.890
"But there is gonna be at least one
experiment where you apply it to a text",00:58:24.890,00:58:28.414
data set.,00:58:28.414,00:58:28.967
"All right, so now let's walk through the
different kinds of projects that you might",00:58:32.183,00:58:37.701
"wanna consider, and what might be entailed
in such project to give you an idea.",00:58:37.701,00:58:42.920
"Unless there are any other questions
around the organization of the projects,",00:58:45.070,00:58:48.517
deadlines and so on.,00:58:48.517,00:58:49.450
"So, let's start with
the kind of simplest and",00:58:51.491,00:58:54.575
"all the other ones are sort of bonuses
on top of that simple kind of project.",00:58:54.575,00:58:59.700
"And this is actually, I think generally,
good advice, not just for a class project,",00:58:59.700,00:59:05.810
"but in general, how to apply a deep
learning algorithm to any kind of problem,",00:59:05.810,00:59:10.210
"whether in academia or
in industry, or elsewhere.",00:59:10.210,00:59:13.620
"So, let's assume you want to",00:59:13.620,00:59:16.720
"apply an existing neural
network to an existing task.",00:59:17.960,00:59:21.150
"So in our case, for instance,
let's take summarization.",00:59:22.370,00:59:25.290
"So you want to be able to
take a long document and",00:59:26.320,00:59:29.470
summarize into a short paragraph.,00:59:29.470,00:59:32.380
Let's say that was your goal.,00:59:32.380,00:59:33.210
"Now step one, after you define your task,
is you have to define your dataset.",00:59:34.540,00:59:39.360
"And that is actually, sadly,
in many cases in both industry and",00:59:39.360,00:59:44.120
"in academia,
an incredibly time intensive problem.",00:59:44.120,00:59:48.420
"And so, the simplest solution
to that is you just search for",00:59:48.420,00:59:52.200
an existing academic dataset.,00:59:52.200,00:59:54.120
"There's some people who've
worked in summarization before.",00:59:54.120,00:59:58.220
"The nice thing is if you use
an existing data set, for instance,",00:59:58.220,01:00:01.110
"from the Document Understanding
Conference, DUC here, then other people",01:00:01.110,01:00:05.450
"have already applied some algorithms
to it, you'll have some base lines,",01:00:05.450,01:00:08.770
"you know what kind of metric or evaluation
is reasonable versus close to random.",01:00:08.770,01:00:13.980
"And so on,
cuz sometimes that's not always obvious.",01:00:13.980,01:00:16.060
"We don't always us just accuracy for
instance.",01:00:16.060,01:00:19.020
"So in that case, using an existing
academic data set gets rid",01:00:19.020,01:00:23.200
of a lot of complexity.,01:00:24.470,01:00:28.590
"However, it is really fun if you actually
come up with your own kind of dataset too.",01:00:28.590,01:00:34.300
"So maybe you're really excited about food,
and you want to prowl Yelp, or",01:00:34.300,01:00:38.810
"use a Yelp dataset for restaurant review,
or something like that.",01:00:38.810,01:00:43.750
"So, however,
when you do decide to do that,",01:00:43.750,01:00:46.780
"you definitely have to check in with your
mentor, or with Chris and me, and others.",01:00:46.780,01:00:51.070
Because I sadly have seen several projects,01:00:52.350,01:00:56.250
"in the last couple of years where
people have this amazing idea.",01:00:56.250,01:00:58.900
"I'm excited, they're excited.",01:00:58.900,01:01:00.580
"And then they spent 80% of the time
on their project on a web crawler",01:01:00.580,01:01:05.910
"getting not blocked from IP addresses,",01:01:05.910,01:01:08.870
"writing multiple IP addresses,
having multiple machines, and crawling.",01:01:08.870,01:01:13.465
"And so on, then they realize,
all right, it's super noisy.",01:01:13.465,01:01:16.255
"Sometimes it's just the document
they were hoping to get and",01:01:16.255,01:01:19.155
"crawl, it's just a 404 page.",01:01:19.155,01:01:20.075
And now they've filtered that.,01:01:20.075,01:01:21.875
"And then they realize HTML,
and they filter that.",01:01:21.875,01:01:25.215
"And before you know it,",01:01:25.215,01:01:25.925
"it's like, they have like three more days
left to do any deep learning for NLP.",01:01:25.925,01:01:29.670
"And so, it has happened before so
don't fall into that trap.",01:01:31.090,01:01:35.150
"If you do decide to do that, check with us
and try to, before the milestone deadline.",01:01:35.150,01:01:41.810
"For sure have the data set ready so
you can actually do deep learning for NLP,",01:01:41.810,01:01:45.740
"cuz sadly we just can't give you a good
grade for a deep learning for NLP class if",01:01:45.740,01:01:50.330
"you spend 95% of your time writing a web
crawler and explaining your data set.",01:01:50.330,01:01:54.590
"So in this case, for instance, you might
say all right, I want to use Wikipedia.",01:01:54.590,01:01:59.440
Wikipedia slightly easier to crawl.,01:01:59.440,01:02:01.430
"You can actually download sort of
already pre-crawled versions of it.",01:02:01.430,01:02:05.270
Maybe you want to say my intro paragraph,01:02:05.270,01:02:08.040
"is the summary of the whole
rest of the article.",01:02:08.040,01:02:11.450
"Not completely crazy to
make that assumption, but",01:02:11.450,01:02:15.350
really you can be creative in this part.,01:02:15.350,01:02:17.985
"You can try to connect it to your
own research or your own job if your",01:02:17.985,01:02:21.555
"a [INAUDIBLE] student, or
just any kind of interest that you have.",01:02:21.555,01:02:25.425
"Song lyrics come up from time
to time it's really fun NLP",01:02:25.425,01:02:29.085
"combine with language of music
with natural language and so on.",01:02:29.085,01:02:32.115
"So you can be creative here, and
we kind of value a little bit of",01:02:32.115,01:02:36.565
"the creativity this is like a task of
data set we had never seen before and",01:02:36.565,01:02:39.985
"you actually gain some interesting
Linguistic insights or something.",01:02:39.985,01:02:44.300
"That is the cool part of the project,
right.",01:02:44.300,01:02:47.620
Any questions around defining a data set?,01:02:47.620,01:02:49.405
"All right, so
then you wanna define your metric.",01:02:53.762,01:02:59.290
This is also super important.,01:02:59.290,01:03:00.940
"For instance, you have maybe
have crawled your dataset and",01:03:02.260,01:03:05.390
"let's say you did something simpler like
restaurant star rating classification.",01:03:05.390,01:03:10.110
"This is a review and I want to
classify if this a four star review or",01:03:10.110,01:03:14.050
a one star review or a two or three.,01:03:14.050,01:03:17.790
"And now you may have a class
distribution where this is one star,",01:03:17.790,01:03:24.464
"this is two stars, three and four,
and now the majority are three.",01:03:24.464,01:03:31.523
Maybe that you troll kind of funny and,01:03:31.523,01:03:33.574
"so really most of the reviews
are three star reviews.",01:03:33.574,01:03:37.060
"So this is just like number
of reviews per star category.",01:03:37.060,01:03:42.226
"And maybe 90% of the things you
called are in the third class.",01:03:42.226,01:03:49.550
"And then you write your report, you're
super excited, it was a new data set,",01:03:49.550,01:03:52.510
"you did well, you crawled it quickly.",01:03:52.510,01:03:54.460
"And then all you give us
is an accuracy metric, so",01:03:54.460,01:03:57.100
"accuracy is total correct
divided by total.",01:03:57.100,01:04:00.390
"And now, let's say your accuracy is 90%.",01:04:00.390,01:04:02.738
"It's 90% accurate, 90% of the cases
gives you the ride star rating.",01:04:02.738,01:04:08.580
"Sadly, it just always gives three.",01:04:08.580,01:04:11.600
It never gives any other result.,01:04:11.600,01:04:14.830
"You're essentially overfit
to your dataset and",01:04:14.830,01:04:18.011
"your evaluation metric
was completely bogus.",01:04:18.011,01:04:21.123
"It's hard to know whether they basically
could have implemented a one line",01:04:21.123,01:04:24.161
"algorithm that's just as accurate as yours
which is just, no matter what the input,",01:04:24.161,01:04:27.630
return three.,01:04:27.630,01:04:28.270
"So hard to give a good grade on that and
it's a very tricky trap to fall into.",01:04:29.480,01:04:33.780
"I see it all the time in industry and
for young researchers and so on.",01:04:33.780,01:04:38.518
"So in this case, you should've used,",01:04:38.518,01:04:41.141
"does anybody know what kind
of metric you should've used?",01:04:41.141,01:04:45.118
"F1, that's right.",01:04:45.118,01:04:46.087
"So, and we'll go through some of
these as we go through the class but",01:04:46.087,01:04:49.852
"it's very important to
define your metric well.",01:04:49.852,01:04:52.640
"Now, for something as tricky as
summarization, this isn't where you're",01:04:53.650,01:04:56.290
"really just like, this is the class,
this is the final answer.",01:04:56.290,01:04:58.900
"You have to actually either extract or
generate a longer sequence.",01:04:58.900,01:05:04.228
"And there are a lot of different
kinds of metrics you can use.",01:05:04.228,01:05:07.328
"BLEU's n-gram overlap or Rouge share
which is a Recall-Oriented Understudy for",01:05:07.328,01:05:12.998
"Gisting Evaluation which essentially
is just a metric to weigh differently",01:05:12.998,01:05:18.182
"how many n-grams are correctly overlapping
between a human generated summary.",01:05:18.182,01:05:23.779
"For instance,
your Wikipedia paragraph number one, and",01:05:23.779,01:05:26.727
whatever output your algorithm gives.,01:05:26.727,01:05:28.780
"So, Rouge is the official metric for",01:05:30.420,01:05:32.405
"summarization in different sub-communities
and NOP have their own metrics and",01:05:32.405,01:05:36.762
"it's important that you know
what you're optimizing.",01:05:36.762,01:05:39.850
"So, the machine translation, for
instance, you might use BLEU scores,",01:05:39.850,01:05:44.371
"BLEU scores are essentially also
a type of n-gram overlap metric.",01:05:44.371,01:05:48.486
"If you have a skewed data set,
you wanna use F1.",01:05:48.486,01:05:50.570
"And in some cases,
you can just use accuracy.",01:05:50.570,01:05:52.690
"And this is generally useful
even if you're in industry and",01:05:54.150,01:05:57.080
"later in life, you always wanna
know what metric you're optimizing.",01:05:57.080,01:05:59.610
"It's hard to do well if you don't know
the metric that you're optimizing for,",01:05:59.610,01:06:04.040
both in life and deep learning projects.,01:06:04.040,01:06:05.926
"All right so,
let's say you defined your metric now,",01:06:05.926,01:06:10.140
you need to split your dataset.,01:06:10.140,01:06:11.960
And it's also very important step and,01:06:11.960,01:06:14.480
"it's also something that you can
easily make sort of honest mistakes.",01:06:14.480,01:06:20.550
"Again, in advantage of taking pre-existing
academic dataset is that in many cases,",01:06:20.550,01:06:25.370
it's already pre-split but not always.,01:06:25.370,01:06:28.500
And you don't wanna look at your,01:06:29.690,01:06:31.800
"final test split until around
1 week before the deadline.",01:06:32.820,01:06:36.710
"So, let's say you have downloaded
a lot of different articles and",01:06:36.710,01:06:41.028
"now you basically have 100% of
some articles you wanna summarize.",01:06:41.028,01:06:45.740
"And normal split would be take 80% for
training,",01:06:45.740,01:06:49.436
"you take 10% for your validation and
your development.",01:06:49.436,01:06:53.900
"So, oftentimes this is called
the validation split, or",01:06:53.900,01:06:57.116
"the development split, or
dev split, or various other terms.",01:06:57.116,01:07:00.741
And 10% for your final test split.,01:07:00.741,01:07:02.890
"And so, the final one, you ideally get a
sense of how your algorithm would work in",01:07:02.890,01:07:08.510
"real life, on data you've never
seen before, you didn't try to chew",01:07:08.510,01:07:13.540
"on your model like, how many layers should
I use, how wide should each layer be?",01:07:13.540,01:07:18.191
"You'll try a lot of these things,
we'll describe these in the future.",01:07:18.191,01:07:22.848
"But it's very important to correctly split
and why do I make such a fuss about that?",01:07:22.848,01:07:30.168
"Well, there too, you might make mistakes.",01:07:30.168,01:07:33.090
"So let's say,
you have unused text and let's say,",01:07:33.090,01:07:36.114
"you crawled it in such a way there's a lot
of mistakes that you can make if you try",01:07:36.114,01:07:40.649
"to predict the soft market for instance,
don't do that, it doesn't work.",01:07:40.649,01:07:45.152
"But in many cases, you might say,
or there some temporal sequence.",01:07:45.152,01:07:51.030
"And now, you basically have all your
dataset and the perfect thing to do",01:07:51.030,01:07:56.520
"is actually do it like this, you take 80%
of let's say, month January to May or",01:07:56.520,01:08:02.240
"something and then,
your final test split is from November.",01:08:02.240,01:08:06.210
That way you know there's no overlap.,01:08:06.210,01:08:07.930
"But maybe you made a mistake and
you said well, I crawled it this way, but",01:08:09.000,01:08:13.060
now I'm just randomly sample.,01:08:13.060,01:08:15.130
"So, as sample an article from here,
and one from here, and one from here.",01:08:15.130,01:08:18.110
"And then the random sample goes
to the 80% of my training data.",01:08:18.110,01:08:22.220
"And now, the test data and the development
data might actually have some overlap.",01:08:22.220,01:08:26.815
"Cuz if you're depending on how you
chose your dataset maybe the another",01:08:26.815,01:08:32.500
"article which just like a slight addition,
like some update to an emerging story.",01:08:32.500,01:08:37.920
"And now the summary is
almost exact same but",01:08:37.920,01:08:40.743
"the input document just
changed a tiny bit.",01:08:40.743,01:08:43.654
"And you have one article in your training
set and another one in your test set.",01:08:43.654,01:08:50.256
"But the test set article is really only
one extra paragraph on an emerging story",01:08:50.256,01:08:53.952
and the rest is exactly the same.,01:08:53.952,01:08:55.868
"So now you have an overlap of your
training and your testing data.",01:08:55.868,01:08:58.710
"And so in general,",01:08:59.980,01:09:01.810
"if this is your training data and
this should be your test data.",01:09:01.810,01:09:07.180
It should be not overlapping at all.,01:09:07.180,01:09:09.280
"And whenever you do really well, you run
your first experiment and you get 90 F1.",01:09:10.770,01:09:14.657
"And things look just too good to be
true sadly in many cases they are and",01:09:14.657,01:09:19.016
"you made some mistake where maybe your
test set had some overlap for instance,",01:09:19.016,01:09:23.979
with your training data.,01:09:23.979,01:09:25.650
"It's very important to be a little
paranoid about that when your first couple",01:09:27.070,01:09:31.650
"of experiments turn out just
to be too good to be true.",01:09:31.650,01:09:34.830
"That can mean either your training,
your task is too simple,",01:09:34.830,01:09:39.721
"or you made a mistake in splitting and
defining your dataset.",01:09:39.721,01:09:44.624
"All right, any questions around defining
a metric or your dataset, yeah?",01:09:46.080,01:09:49.528
"So, if we split it temporally, wouldn't
we learn a different distribution?",01:10:00.975,01:10:04.700
"That is correct,
we would learn a different distribution,",01:10:04.700,01:10:08.097
these are non-stationary.,01:10:08.097,01:10:09.772
"And that is kinda true for a lot of texts,
but if you, ideally, when you built",01:10:09.772,01:10:14.322
"a deep learning system for an LP you
want it to built it so that it's robust.",01:10:14.322,01:10:18.940
It's robust to sum such changes over time.,01:10:18.940,01:10:21.429
"And you wanna make sure that when
you run it in a real world setting,",01:10:21.429,01:10:25.277
"on something you've never seen before,
you've shipped your software,",01:10:25.277,01:10:29.612
"it's doing something, it will still work.",01:10:29.612,01:10:32.635
And this was the most realistic way,01:10:32.635,01:10:35.155
"to capture how well it
would work in real life.",01:10:35.155,01:10:37.329
"Would it be appropriate to run both
experiments as in both where you subsample",01:10:44.400,01:10:47.961
"randomly, and
then you subsample temporally for your?",01:10:47.961,01:10:51.020
"You could do that, and the intuitive
thing that is likely going to happen",01:10:51.020,01:10:56.029
"is if you sample randomly from all
over the place, then you will probably",01:10:56.029,01:11:01.035
"do better than if you have this
sort of more strict kind of split.",01:11:01.035,01:11:05.492
"But running an additional experiment will
rarely ever get you points subtracted.",01:11:05.492,01:11:11.020
"You can always run more experiments,
and we're trying really",01:11:11.020,01:11:16.097
"hard to help you get computing
infrastructure and Cloud compute.",01:11:16.097,01:11:21.481
"So you don't feel restricted with
the number of experiments you run.",01:11:21.481,01:11:24.850
"All right, now, number 5,
establish a baseline.",01:11:30.145,01:11:34.065
"So, you basically wanna implement
the simplest model first.",01:11:34.065,01:11:37.475
"This could just be a very simple logistic
regression on unigrams or bigrams.",01:11:37.475,01:11:41.871
"Then, compute your metrics on your train
data and your development data, so",01:11:41.871,01:11:45.776
"you understand whether you're
overfitting or underfitting.",01:11:45.776,01:11:49.615
"If, for instance, you're training Metric.",01:11:49.615,01:11:54.260
"Let's say your loss is very,
very low on training.",01:11:54.260,01:11:57.450
"You do very well on training, but
you don't do very well on testing,",01:11:57.450,01:12:01.220
then you're in an over fitting regime.,01:12:01.220,01:12:03.310
"If you do very well on training and well
on testing, you're done, you're happy.",01:12:03.310,01:12:07.130
"But if your training loss can't be lower,
so you're not even doing well on your",01:12:07.130,01:12:11.756
"training, that often means your
model is not powerful enough.",01:12:11.756,01:12:16.070
"So it's very important to compute
both the metrics on your training and",01:12:16.070,01:12:19.610
your development split.,01:12:19.610,01:12:20.418
"And then, and this is something
we value a lot in this class too.",01:12:20.418,01:12:24.230
"And it's something very important for
you in both research and",01:12:24.230,01:12:27.560
"industries like you wanna analyze your
errors carefully for that baseline.",01:12:27.560,01:12:31.110
"And if the metrics are amazing and
there are no errors, you're done,.",01:12:32.200,01:12:36.510
Probably a problem was too easy and,01:12:36.510,01:12:37.740
"you may wanna restart unless it's really
a valuable problem for the world.",01:12:37.740,01:12:41.000
"And then maybe you can just really
describe it carefully and you're done too.",01:12:41.000,01:12:45.480
"All right, now, any questions
around establishing your baseline?",01:12:45.480,01:12:50.207
"It is very important to not just go in and
add lots of bells and",01:12:50.207,01:12:52.960
"whistles that you'll learn about in
the next couple of weeks in this class and",01:12:52.960,01:12:56.542
create this monster of a model.,01:12:56.542,01:12:58.650
"You want to start with something simple,",01:12:58.650,01:13:00.621
"sanity check, make sure you didn't
make mistakes in splitting your data.",01:13:00.621,01:13:04.400
You have the right kind of metric.,01:13:04.400,01:13:06.180
"And in many cases, it's a good indicator
for how successful your final project",01:13:06.180,01:13:11.400
"is if you can get this baseline
In the first half of the quarter.",01:13:11.400,01:13:17.540
"Cuz that means you figured out a lot
of these potential issues here.",01:13:17.540,01:13:22.800
And you kind of have your right data set.,01:13:22.800,01:13:24.940
"You know what the metric is, you know what
you're optimizing, and everything is good.",01:13:24.940,01:13:28.200
"So try to get to this point
as quickly as possible.",01:13:28.200,01:13:30.785
"Cuz that is also not as interesting, and",01:13:30.785,01:13:33.101
"you can't really use that much
knowledge from the class.",01:13:33.101,01:13:36.450
Now then it gets more interesting.,01:13:36.450,01:13:39.170
"And now you can implement some
existing neural network model that",01:13:39.170,01:13:42.530
we taught you in class.,01:13:42.530,01:13:44.390
"For instance, this Window-based model if
your task is named entity recognition.",01:13:44.390,01:13:48.080
"You can compute your metric
again on your train AND dev set.",01:13:48.080,01:13:51.390
"Hopefully you'll see some interesting
patterns such as usually train",01:13:51.390,01:13:56.680
"neural nets is quite easy in a sense
that we lower the loss very well.",01:13:56.680,01:14:02.800
"And then we might not generalize
as well in the development set.",01:14:02.800,01:14:05.920
"And then you'll play around
with regularization techniques.",01:14:05.920,01:14:10.000
"And don't worry if some of the stuff
I'm saying now is kind of confusing.",01:14:10.000,01:14:13.570
"If you want to do this,",01:14:13.570,01:14:14.710
"we'll walk you through that as we're
mentoring you through the project.",01:14:14.710,01:14:17.380
"And that's why each project has to
have an assigned mentor that we trust.",01:14:17.380,01:14:23.158
"All right, then you analyze your
output and your errors again.",01:14:23.158,01:14:26.585
"Very important, be close to your data.",01:14:26.585,01:14:27.985
"You can't give too many
examples usually ever.",01:14:27.985,01:14:32.465
"And this is kind of the minimum bar for
this class.",01:14:32.465,01:14:34.255
So if you've done this well and,01:14:34.255,01:14:35.685
"there's an interesting dataset, then
your project is kind of in a safe haven.",01:14:35.685,01:14:40.890
"Now again it's very important
to be close to your data.",01:14:40.890,01:14:44.800
"Once you have a metric and
everything looks good,",01:14:44.800,01:14:46.750
"we still want you to visualize the kind
of data, even if it's a known data set.",01:14:46.750,01:14:51.170
"We wanted you to visualize it,
collect summary statistics.",01:14:51.170,01:14:54.530
"It's always good to know the distribution
if you have different kinds of classes.",01:14:54.530,01:14:58.450
"You want to, again very important, look
at the errors that your model is making.",01:14:58.450,01:15:02.385
"Cuz that can also give you
intuitions of what kinds of",01:15:02.385,01:15:05.290
"patterns can your deep learning
algorithm not capture.",01:15:05.290,01:15:08.760
"Maybe you need to add
a memory component or",01:15:08.760,01:15:11.210
"maybe you need to have longer temporal
kind of dependencies and so on.",01:15:11.210,01:15:16.010
"Those things you can only figure out
if you're close to your data and",01:15:16.010,01:15:19.040
"you look at the errors that your
baseline models are making.",01:15:19.040,01:15:22.030
"And then we want you to analyze
also different hyperparameters.",01:15:23.970,01:15:26.790
"A lot of these models
have lots of choices.",01:15:26.790,01:15:28.560
"Did we add the sigmoid to that score or
not?",01:15:28.560,01:15:30.760
"Is the second layer 100 dimensional or
200 dimensional?",01:15:30.760,01:15:34.020
"Should we use 50 dimensional word vectors
or 1,000 dimensional word vectors?",01:15:34.020,01:15:38.138
There are a lot of choices that you make.,01:15:38.138,01:15:40.400
"And it's really good in your first
couple projects to try more and",01:15:40.400,01:15:44.800
gain that intuition yourself.,01:15:44.800,01:15:46.910
"And sometimes, if you're running
out of time, and only so much, so",01:15:46.910,01:15:50.560
"many experiments you can run, we can help
you, and use our intuition to guide you,.",01:15:50.560,01:15:54.350
"But it's best if you do
that a little bit yourself.",01:15:54.350,01:15:57.360
"And once you've done all of that, now you
can try different model variants, and",01:15:57.360,01:16:01.870
"you'll soon see a lot of
these kinds of options.",01:16:01.870,01:16:04.350
"We'll talk through all
of them in the class.",01:16:04.350,01:16:07.240
So now another,01:16:08.920,01:16:11.780
"kind of class project is you actually
wanna implement a new fancy model.",01:16:11.780,01:16:15.830
"Those are the kinds of things that
will put you into potentially writing",01:16:15.830,01:16:19.650
"an academic paper, peer review,
and at a conference, and so on.",01:16:19.650,01:16:22.710
"The tricky bit of that is you kinda have
to do all the other steps that I just",01:16:23.890,01:16:27.390
described first.,01:16:27.390,01:16:28.900
"And then, on top of that,
you know the errors that you're making.",01:16:28.900,01:16:32.360
"And now you can gain some intuition of
why the existing models are flawed.",01:16:32.360,01:16:36.640
And you come up with your own new model.,01:16:36.640,01:16:39.190
"If you do that, you really wanna be in
close contact with your mentor and some",01:16:40.480,01:16:45.770
"researchers, unless you're a researcher
yourself, and you earned your PhD.",01:16:45.770,01:16:50.000
"But even then,
you should chat with us from the class.",01:16:50.000,01:16:53.260
You want to basically try to set up,01:16:53.260,01:16:56.450
"an infrastructure such that
you can iterate quickly.",01:16:56.450,01:16:58.540
"You're like, maybe I should add this new
layer type to this part of my model.",01:16:58.540,01:17:04.250
"You want to be able to quickly iterate and
see if that helps or not.",01:17:04.250,01:17:07.850
So it's important and,01:17:07.850,01:17:09.020
"actually require a fair amount of software
engineering skills to set up efficient",01:17:09.020,01:17:13.150
"experimental frameworks that
allow you to collect results.",01:17:13.150,01:17:17.980
"And again you want to start with
simple models and then go to more and",01:17:17.980,01:17:21.000
more complex ones.,01:17:21.000,01:17:21.890
"So for instance, in summarization you
might start with something super simple",01:17:21.890,01:17:25.308
"like just average all your
word vectors in the paragraph.",01:17:25.308,01:17:27.815
"And then do a greedy search of
generating one word at a time.",01:17:27.815,01:17:31.470
"Or even greedily searching for
just snippets from the existing",01:17:31.470,01:17:36.110
"article in Wikipedia and
you're just copying certain snippets over.",01:17:38.570,01:17:42.940
"And then stretch goal is something more
advanced would be lets you actually",01:17:44.190,01:17:47.195
generate that whole summary.,01:17:47.195,01:17:48.440
And so here are a couple of project ideas.,01:17:48.440,01:17:51.310
"But, again, we'll post the whole
list of them with potential mentors",01:17:51.310,01:17:56.600
"from the NOP group and the vision group
and various other groups inside Stanford.",01:17:56.600,01:18:01.730
Sentiment is also a fun data set.,01:18:01.730,01:18:03.440
You can look at this URL here for,01:18:03.440,01:18:06.250
"one of the preexisting data sets
that a lot of people have worked on.",01:18:06.250,01:18:09.497
"All right, so
next week we'll look at some fun and",01:18:09.497,01:18:12.800
"fundamental linguistic tasks
like syntactic parsing.",01:18:12.800,01:18:15.120
"And then you'll learn TensorFlow and
have some great tools under your belt.",01:18:15.120,01:18:18.920
Thank you.,01:18:18.920,01:18:19.420

text,start,stop
[MUSIC],00:00:00.000,00:00:04.830
Stanford University.,00:00:04.830,00:00:09.109
"&gt;&gt; Okay, hi everyone.",00:00:09.109,00:00:12.683
"It's exciting, right?",00:00:12.683,00:00:13.640
"We're in week ten, everyone's really
excited for the end of the class.",00:00:13.640,00:00:17.880
"Well, thank you so much for",00:00:17.880,00:00:19.170
"the people who did turn up in person to
see this last week's lecture I guess.",00:00:19.170,00:00:24.010
"Rich and me both have one lecture each,
with me doing the Tuesday and Thursday.",00:00:24.010,00:00:29.528
"I suspect that there probably a few
people who we won't be seeing today.",00:00:29.528,00:00:34.472
"Who are trying to catch up
on their final projects.",00:00:34.472,00:00:37.186
"I'm sure every one of them
will watch the video and",00:00:37.186,00:00:40.210
"catch up on what they
miss during spring break.",00:00:40.210,00:00:43.230
"But, for those of you who are here or
watching.",00:00:43.230,00:00:46.527
Here's what we're gonna have today.,00:00:46.527,00:00:49.712
"I wanted to sort of try and
say a little bit about",00:00:49.712,00:00:54.064
"sort of bigger picture
solving needs of language.",00:00:54.064,00:00:58.870
"And where things might be heading or
should be heading.",00:00:58.870,00:01:02.151
I wanted to kind of come back and,00:01:02.151,00:01:04.041
"say a bit more about some of
the tree structured model ideas.",00:01:04.041,00:01:08.010
"And how those might be reinterpreted in
a different context at the high level of",00:01:08.010,00:01:12.762
language.,00:01:12.762,00:01:13.482
"And then something that's in
the same vein as that is for",00:01:13.482,00:01:16.243
our research highlight today.,00:01:16.243,00:01:17.868
"Zhedi is then gonna be talking about this
kind of interesting model out of Berkeley.",00:01:17.868,00:01:22.270
"For learning to compose for
question answering.",00:01:22.270,00:01:25.471
"And then for the end part of it, there
are kind of a couple of things that we",00:01:25.471,00:01:30.268
haven't said very much about in the class.,00:01:30.268,00:01:33.189
"Which I thought I should just say a bit
more about before the class ends so",00:01:33.189,00:01:37.704
we'll talk about character based models.,00:01:37.704,00:01:40.500
"Okay, quickly the reminders.",00:01:41.990,00:01:43.940
"Well, best of luck in finishing your
assignment four or your final project.",00:01:43.940,00:01:48.090
"So, I mean,
there's sort of a subtle balance.",00:01:49.350,00:01:52.774
"But in terms of what you're doing, I mean,",00:01:52.774,00:01:55.031
"you should be making sure you kind of have
solid baselines and well-trained models.",00:01:55.031,00:01:59.430
"It's obviously good to be
investigating fancy architectures.",00:01:59.430,00:02:03.372
"But if they're sort of not working at
all or it seems like they're working",00:02:03.372,00:02:08.999
"very badly it's sort of always
hard to work out where things are.",00:02:08.999,00:02:14.007
"So especially if it's a different problem,
well you sort of wanna know, well.",00:02:14.007,00:02:17.610
"How much would you get if you just ran
a bigram logistic regression on this?",00:02:17.610,00:02:21.032
"And then, is there reasonable evidence
that this neural model was well trained?",00:02:21.032,00:02:25.530
"So it's not that you have to sort of
infinitely tune your hyper parameters.",00:02:25.530,00:02:28.581
"But we sort of hope that you have
sort of reasonably solid models.",00:02:28.581,00:02:33.340
"If you're having any problems do try and
get some help.",00:02:33.340,00:02:36.200
"I know there are a lot of people
going to office hours today.",00:02:36.200,00:02:40.250
"So tonight Richard is again having
infinite project office hours so",00:02:40.250,00:02:45.280
feel free to come by.,00:02:45.280,00:02:47.440
"Depending, you can either go home and have
dinner and come back at 10 PM probably and",00:02:47.440,00:02:51.520
he'll still be there.,00:02:51.520,00:02:53.308
"[LAUGH]
Make sure you're in the queue.",00:02:53.308,00:02:59.090
"And then sort of, finally,
for Microsoft Azure, so",00:02:59.090,00:03:03.940
"I mean Microsoft's really been super
generous at giving us more GPU hours.",00:03:03.940,00:03:07.900
"And so we've got a fresh,
new allotment of GPU hours.",00:03:07.900,00:03:11.603
"And I guess this afternoon I was trying
to top up various people's accounts.",00:03:11.603,00:03:16.050
But in general it's kind of easiest for,00:03:16.050,00:03:18.558
"everybody if your account
doesn't run out of credits.",00:03:18.558,00:03:22.133
"Because then it gets cancelled,
and you get locked out, and",00:03:22.133,00:03:25.419
"we have to reactivate and
blah, blah, blah, blah, blah.",00:03:25.419,00:03:28.713
"Really if you sort of know you want to do
something that takes more credits than",00:03:28.713,00:03:33.540
you have.,00:03:33.540,00:03:34.220
"Feel free to get in contact
on piazza beforehand.",00:03:34.220,00:03:37.910
"Preferably don't just email
me cuz in the good case,",00:03:37.910,00:03:41.750
"one of the TAs, James or
Nish do it rather than me.",00:03:41.750,00:03:45.690
"Okay, so",00:03:47.200,00:03:47.921
"a kind of interesting thing that's
happened in the last couple of years.",00:03:47.921,00:03:53.522
"Is that a lot of the key deep learning
people have really sort of redirected",00:03:53.522,00:03:58.314
their mission.,00:03:58.314,00:03:59.630
"With this idea they could
sort of solve language.",00:03:59.630,00:04:02.416
"So historically, most of deep
learning came out of vision, and",00:04:02.416,00:04:07.367
vision really dominated in deep learning.,00:04:07.367,00:04:11.110
"But for a couple of interesting reasons,",00:04:11.110,00:04:13.876
"sort of the same people that were the sort
of most key deep learning people.",00:04:13.876,00:04:18.865
"Such as Yann LeCun and
the others, and Geoff Hinton.",00:04:18.865,00:04:22.029
"Yoshua Bengio,",00:04:22.029,00:04:22.568
"really they've all considerably redirected
their research programs towards language.",00:04:22.568,00:04:28.630
"And so here's just sort of one
random sort of business mag quote",00:04:28.630,00:04:32.605
from businessinsider.com.,00:04:32.605,00:04:34.564
"Which shows how this is sort
of panning out in practice.",00:04:34.564,00:04:38.123
So Yann LeCun is quoted as saying.,00:04:38.123,00:04:40.630
"The role of FAIR, Facebook AI Research, is
to advance the science and the technology",00:04:40.630,00:04:45.581
"of AI and do experiments that demonstrate
that technology for new applications like",00:04:45.581,00:04:50.467
"computer vision, dialogue systems,
virtual assistants, speech recognition,",00:04:50.467,00:04:55.354
"natural language understanding,
translation, things like that.",00:04:55.354,00:04:59.449
"And the thing that's sort of
really surprising about that list.",00:04:59.449,00:05:02.740
"Is, well okay the first application
he mentions is computer vision.",00:05:02.740,00:05:07.868
"And he is a 20 year computer vision guy so
that may be not too surprising.",00:05:07.868,00:05:12.020
"But if you keep on reading
after the first application.",00:05:12.020,00:05:15.730
"Every other application he mentions
is a natural language application.",00:05:15.730,00:05:20.290
"Dialogue systems, virtual assistants,
speech recognition,",00:05:20.290,00:05:23.794
"natural language understanding,
translation.",00:05:23.794,00:05:26.543
They're all language applications.,00:05:26.543,00:05:30.808
"And I think coming off of that
that sort of many people who",00:05:30.808,00:05:35.529
haven't been long term language people.,00:05:35.529,00:05:39.750
"Kind of feel that language will
have a kind of an image net moment.",00:05:39.750,00:05:43.835
"Where somehow someone builds a big
enough complex enough neural network.",00:05:43.835,00:05:50.420
"That kind of the task will just be
sort of measurably solved in one step.",00:05:50.420,00:05:55.738
"I kind of think actually that's
probably not gonna happen.",00:05:55.738,00:05:58.890
"Because I kinda think there are so many
different language phenomena and tasks.",00:05:58.890,00:06:03.795
That there's sort of just no one task.,00:06:03.795,00:06:06.020
"That can really sort of have
the same kind of seminal, okay,",00:06:06.020,00:06:09.013
"we've solved natural
language understanding.",00:06:09.013,00:06:11.484
"And so I wanted to,
with respect to that, sort of mention",00:06:11.484,00:06:16.486
"a couple of things of what has
been lost from old NLP work.",00:06:16.486,00:06:21.506
"So I think there's actually kind
of an interesting contrast.",00:06:21.506,00:06:25.024
"If you go back to the sort of good old
fashioned AI work in natural language",00:06:25.024,00:06:29.364
"processing versus a lot of
what you see more recently.",00:06:29.364,00:06:32.980
"And that is if you sort of look back and
sort of 80s, 70s,",00:06:32.980,00:06:38.750
"natural language processing work,
that these people had really lofty goals.",00:06:38.750,00:06:43.690
"That their goal was to sort of really
have human level language understanding.",00:06:43.690,00:06:49.304
"Now, actually what they could do
is extremely, extremely modest.",00:06:49.304,00:06:53.990
"But nevertheless the contrast is,
it sort of seems like here we are today.",00:06:53.990,00:06:58.230
"We have much better realities as
to the kind of things we can do.",00:06:58.230,00:07:03.389
"But it's not always clear that
everyone's reaching for the stars.",00:07:03.389,00:07:06.820
"As opposed to just thinking, okay, well
we can run an LSTM on this language data.",00:07:06.820,00:07:11.203
"And that'll sort of work reasonably
well and not try to get beyond that.",00:07:11.203,00:07:15.260
"So I thought it might be good just to
have a few slides at the beginning.",00:07:15.260,00:07:18.456
"Sort of looking at a bit
of older NLP work.",00:07:18.456,00:07:20.690
"And seeing if there are things that we
could kind of see and learn from that.",00:07:20.690,00:07:25.713
"And so I chose as my example
Peter Norvig's Ph.D thesis.",00:07:25.713,00:07:30.346
"So probably most of you vaguely at
least know who Peter Norvig is.",00:07:30.346,00:07:34.945
"Because of the textbook on what
is Artificial Intelligence and",00:07:34.945,00:07:40.323
"the Modern Approach,
Russell and Norvig's book.",00:07:40.323,00:07:44.792
"He's worked for many years as
a director of research at Google.",00:07:44.792,00:07:48.449
"And so it's just past the 30th anniversary
of Peter's PhD thesis which was in",00:07:48.449,00:07:52.680
natural language understanding.,00:07:52.680,00:07:54.885
"It was called A Unified Theory of
Inference for Text Understanding.",00:07:54.885,00:07:58.755
"Well, what you'll find if
you look in his thesis?",00:08:00.919,00:08:04.266
"I mean, in terms of the actual
language processing and",00:08:04.266,00:08:08.335
"language understanding
what's in the thesis.",00:08:08.335,00:08:11.972
"I mean, in some sense,
there's shockingly little of that.",00:08:11.972,00:08:16.176
"So for the work for his entire thesis,
there is only one piece of real,",00:08:16.176,00:08:23.026
"natural language that's analyzed
in the entire dissertation.",00:08:23.026,00:08:29.537
"All the rest of it is sort of toy
examples like Bill had a bicycle.",00:08:29.537,00:08:34.168
"John wanted it, he gave it to him,
and trying to analyze stuff like that.",00:08:34.168,00:08:38.691
"Actually, a bit reminiscent of the kind
of baby sentences that Facebook AI",00:08:38.691,00:08:43.421
"research lately came out with, but this is
the once piece of real natural language",00:08:43.421,00:08:48.600
"that's analyzed in the dissertation
which comes from a children's story.",00:08:48.600,00:08:53.505
"In a poor fishing village built on
an island not far from the coast of China,",00:08:53.505,00:08:57.915
"a young boy named Chang Lee
lived with his widowed mother.",00:08:57.915,00:09:01.351
"Every day, little Chang bravely set off
with his net, hoping to catch a few fish",00:09:01.351,00:09:05.939
"from the sea which they could sell and
have a little money to buy bread.",00:09:05.939,00:09:10.001
"But the interesting thing is that these
are sort of some of things that Norvig",00:09:10.001,00:09:15.302
"said, we should be able to get out of
this text and I think many of these or",00:09:15.302,00:09:20.116
"at least half of these are things
that we're still not very good at.",00:09:20.116,00:09:25.100
"Actually, the first half.",00:09:25.100,00:09:26.674
"So, he's hoping to get out.",00:09:26.674,00:09:29.120
"There is a sea, which surrounds
the island is used by the villagers for",00:09:29.120,00:09:33.906
"fishing and
forms part of the coast of China.",00:09:33.906,00:09:36.999
"So, there's a sort of interpretive",00:09:36.999,00:09:39.869
"understanding of the relations
that are involved.",00:09:39.869,00:09:44.089
"And in particular, the relationship
between the sea and China is quite",00:09:44.089,00:09:49.565
"distant in the text and sort of
mediated by the existence of the coast.",00:09:49.565,00:09:54.960
"That's the kind of stuff
we're still not very good at.",00:09:54.960,00:09:57.285
"Chang intends to trap fish in his net,
which is a fishing net.",00:09:57.285,00:10:01.906
"Again, that sort of interpretive stuff.",00:10:01.906,00:10:06.450
"So, there's sort of this leap of he's
set off with his net hoping to catch",00:10:06.450,00:10:11.060
a few fish from the sea.,00:10:11.060,00:10:13.090
"So it never actually says that he's
gonna catch fish with the net, but",00:10:13.090,00:10:18.030
"any human being would
interpret it that way.",00:10:18.030,00:10:21.222
"And again, that's sort of the kind of
interpretive language that we still aren't",00:10:21.222,00:10:25.859
very good at dealing with.,00:10:25.859,00:10:27.394
The other two actually much easier.,00:10:27.394,00:10:30.902
"The word which refers to the fish, the
word they refers to Chang and his mother.",00:10:30.902,00:10:36.208
We talked about things like that.,00:10:36.208,00:10:38.020
"So in terms of how he's gonna do this,
I mean,",00:10:39.290,00:10:42.392
"interestingly, the kind of perspective
in the 80s was just completely",00:10:42.392,00:10:47.330
"diametrically opposed to
what it is these days.",00:10:47.330,00:10:50.613
"I mean, in the 1980s,
it was sort of believed,",00:10:50.613,00:10:53.670
"sort of just as an assumption that was
beyond question that the only way that you",00:10:53.670,00:10:58.255
"were going to do any kind of natural
language understanding was to have",00:10:58.255,00:11:02.354
"a knowledge base that you
could work with and reason on.",00:11:02.354,00:11:05.640
"So on page four of the thesis, he takes
it as established as we have just seen.",00:11:05.640,00:11:12.241
"A suitable knowledge base
is a prerequisite for",00:11:12.241,00:11:15.179
"making proper inferences and that was sort
of just no more of an argument than look,",00:11:15.179,00:11:20.323
"we need to make inferences
between these facts to understand",00:11:20.323,00:11:24.070
"that the net is gonna be
used to catch the fish.",00:11:24.070,00:11:26.949
"Therefore, we need a knowledge
base to do it over.",00:11:26.949,00:11:30.328
"Where it's sort of what's happened in
the last two decades is to sort of show",00:11:30.328,00:11:34.497
"that there's actually a lot of natural
language that you can understand and",00:11:34.497,00:11:38.802
"do without having any knowledge base, and
just working on fairly surface forms.",00:11:38.802,00:11:43.461
"So in his thesis,
he outlines six forms of inference and",00:11:43.461,00:11:46.961
"tries to build a natural language
understanding system that",00:11:46.961,00:11:50.759
"embodies them and will be able to
make inferences over that passage.",00:11:50.759,00:11:55.093
"Two of them come in pairs, so
there are four basic types.",00:11:55.093,00:11:58.225
"So, one is elaboration.",00:11:58.225,00:12:00.552
"And so, that's working out
how to connect to entities.",00:12:00.552,00:12:05.400
"So this example is John got a piggy
bank for the reason of having money for",00:12:05.400,00:12:10.560
"the reason of buying a present, but
the example from the previous slide",00:12:10.560,00:12:15.721
"was he had the net with him for
the reason of helping catch the fish.",00:12:15.721,00:12:20.560
"And so, you have to sort of do
this contextual elaboration.",00:12:20.560,00:12:24.733
The second one is reference resolution.,00:12:24.733,00:12:27.417
"That's really the only one of
these that we've talked about and",00:12:27.417,00:12:30.780
"we know how to do that one, and
we can do that one pretty well today.",00:12:30.780,00:12:34.279
"The third one is related
to the interpretation",00:12:34.279,00:12:39.127
"of metaphor and
abstract forms from language.",00:12:39.127,00:12:44.107
"So that if you have a sentence like
the Red Sox killed the Yankees,",00:12:44.107,00:12:47.927
"that doesn't mean that they went out
murdering them with knifes and guns.",00:12:47.927,00:12:52.780
"It means that they defeated
them convincingly.",00:12:52.780,00:12:56.753
"And so,
language is full of that kind of metaphor.",00:12:56.753,00:12:59.915
"But all the time,
we're sort of using physical",00:12:59.915,00:13:02.785
"metaphors which are having these
sort of abstract interpretations.",00:13:02.785,00:13:07.036
"And so another part of his dissertation
was sort of trying to work out how you",00:13:07.036,00:13:12.022
"derived those abstract interpretations,
then the third one is sort of",00:13:12.022,00:13:16.774
"an interesting one that a lot of the time
that I think we don't think of as much,",00:13:16.774,00:13:21.998
"but is actually I think quite
important and that's concretization.",00:13:21.998,00:13:26.701
"And so, that this is you solve goal
from a general description to much",00:13:26.701,00:13:31.605
more specific form of that.,00:13:31.605,00:13:33.644
"So if you know someone is traveling in
automobile, then you know that the kind of",00:13:33.644,00:13:38.548
"traveling that at least one of the people
is doing is driving the automobile,",00:13:38.548,00:13:43.260
"at least until our autonomous
cars get a bit better.",00:13:43.260,00:13:46.453
"And so, there's that kind of
concretization that you'll need to be able",00:13:46.453,00:13:51.259
"to do more fine grained inferences
in the state of knowledge.",00:13:51.259,00:13:55.300
"So, that's kind of interesting.",00:13:55.300,00:13:58.621
"There are ways in which we've
made enormous progress.",00:13:58.621,00:14:02.830
"So in Norvig's thesis, the situation
is essentially that they don't",00:14:02.830,00:14:07.658
"have syntactic parsers that's good
enough that they can actually parse",00:14:07.658,00:14:12.566
"sentences they'd like to
parse like that story.",00:14:12.566,00:14:15.957
"So he describes how Wilensky,
that's his dissertation advisor's.",00:14:15.957,00:14:20.515
"PHRAN program was used where
possible to pause sentences.",00:14:20.515,00:14:24.491
"But for some input,
PHRAN was not up to the task, so",00:14:24.491,00:14:27.657
"we constructed the representation
to sentence by hand instead.",00:14:27.657,00:14:31.721
"Well, you know we're
actually better than that.",00:14:31.721,00:14:33.189
"We can actually parse
sentences pretty well now.",00:14:33.189,00:14:36.579
"But once you're starting to do the kind
of elaborations to do the sort of things",00:14:36.579,00:14:41.904
"that were being talked about as other
goals, these are kind of things that",00:14:41.904,00:14:46.909
"most of NLP still hasn't gotten to, and
maybe should be starting to get to.",00:14:46.909,00:14:52.098
"So, I think there's still sort of big
things that we still need to do in NLP.",00:14:52.098,00:14:56.744
"On the one hand, I mean,
it's just true that in the last few years,",00:14:56.744,00:15:00.734
"there's been these exciting times and a
lot of systems have gone a lot better and",00:15:00.734,00:15:05.424
"there's sort of this sense that BiLSTMs
with attention are sort of taking over",00:15:05.424,00:15:10.044
"the field of natural language processing,
because you can try them on any task and",00:15:10.044,00:15:14.874
"they work better than what people used
to do and you can even use them for",00:15:14.874,00:15:19.004
surprising things.,00:15:19.004,00:15:20.274
You can just use a BiLSTM for attention.,00:15:20.274,00:15:23.810
"As that's gonna be your natural language
parser, it works surprisingly well.",00:15:23.810,00:15:28.429
"So, they seem good.",00:15:28.429,00:15:30.211
"Another thing that's really exciting
that's happened is these neural",00:15:30.211,00:15:34.564
"methods have just clearly led to a
renaissance in language generation tasks.",00:15:34.564,00:15:39.361
"So any task where you actually have to
generate pieces of natural language where",00:15:39.361,00:15:43.635
"that's the sort of generation
side machine translation,",00:15:43.635,00:15:46.824
the generation side of dialogue.,00:15:46.824,00:15:49.190
"Answering a question, doing summarization.",00:15:49.190,00:15:53.050
"That all of those feels the generation
sides of things were sort of",00:15:53.050,00:15:57.650
"Fairly moribund in the first decade of
the 2000s where now it just seems like,",00:15:57.650,00:16:02.344
"we have these fantastically
good neural language models.",00:16:02.344,00:16:05.760
"That we can configure in different
ways and we can do really exciting and",00:16:05.760,00:16:09.420
nice language generation.,00:16:09.420,00:16:10.960
"All those fields have sort of
been springing to life lately.",00:16:10.960,00:16:13.680
"And it's also sorta super
interesting time, scientifically,",00:16:15.410,00:16:19.730
"because I think sorta most of
natural language processing",00:16:19.730,00:16:24.130
"has just sort of been this assumption
that we need to be building particular",00:16:24.130,00:16:28.550
"kinds of representations of language and
working with them.",00:16:28.550,00:16:31.710
"So we want to have sort of
syntactic representations.",00:16:31.710,00:16:35.150
"And often we want to have above those
things like semantic frames to represent",00:16:35.150,00:16:39.890
events and relations in language.,00:16:39.890,00:16:42.010
"And so we're sort of building
explicit localist language knowledge",00:16:42.010,00:16:45.954
representations.,00:16:45.954,00:16:47.114
"Where it's what some of the recent work
such as the stuff that Richard was talking",00:16:47.114,00:16:51.906
"about last week was showing is that it now
seems that there are a lot of situations",00:16:51.906,00:16:56.626
"in which we can build these end to end
deep learning systems that don't have any",00:16:56.626,00:17:01.276
"of these kind of explicit localist
knowledge representations that actually",00:17:01.276,00:17:05.859
work very nicely.,00:17:05.859,00:17:08.000
"But on the other hand,
there are a lot of things that we",00:17:08.000,00:17:11.260
"still I think have barely
scratched the surface off.",00:17:11.260,00:17:14.610
"So one of those is we still kind
of have very primitive means for",00:17:14.610,00:17:20.600
"building and
accessing memories or knowledge.",00:17:20.600,00:17:23.450
"So yes, there's been a lot of work
with LSTMs where the M is memory and",00:17:23.450,00:17:28.240
"memory networks and
other things like that.",00:17:28.240,00:17:30.540
"But really all of those are models of
sort of very recent short term memory.",00:17:30.540,00:17:36.028
That's the ST in the LSTM.,00:17:36.028,00:17:38.560
"They're not really models of
how like human beings that we",00:17:38.560,00:17:42.631
"can have years of experience of
our lives stored in our heads.",00:17:42.631,00:17:46.960
"And we can flexibly sort of bring forth
relevant facts at the right time,",00:17:46.960,00:17:51.853
"that all they're doing is a sort of
linear scan of what's happened in",00:17:51.853,00:17:56.506
the last 100 words or something like that.,00:17:56.506,00:17:59.659
So that's pretty poor.,00:17:59.659,00:18:01.379
"Another thing that's pretty poor
is we don't really have much",00:18:01.379,00:18:05.257
"in the way of models that let us
formulate goals or formulate plans.",00:18:05.257,00:18:09.440
"And really, if you're going to do a lot
of things in conversation like have",00:18:09.440,00:18:13.645
"a meaningful dialogue, you sort of have
to have some goals and plans that yes,",00:18:13.645,00:18:18.050
you can just be shooting the breeze.,00:18:18.050,00:18:20.150
"But a lot of the time,",00:18:20.150,00:18:21.040
"you wanna accomplish things which leads
to sub tasks, and things like that.",00:18:21.040,00:18:24.730
"Another area where we're still really
bad is we're just sort of pretty",00:18:26.160,00:18:30.371
bad at inter-sentential relationships.,00:18:30.371,00:18:32.981
"So once we're inside one sentence
the structure is usually pretty clear and",00:18:32.981,00:18:37.856
we can work with that.,00:18:37.856,00:18:39.595
"But once we sort of try and reason
between sentences or between clauses and",00:18:39.595,00:18:44.585
"understand the relationships,
we're usually pretty poor at that still.",00:18:44.585,00:18:48.710
"And we still can't do many of
those things that Peter Norvig was",00:18:48.710,00:18:53.490
"talking about, right?",00:18:53.490,00:18:54.470
"That if we wanna do
elaborations in the situation",00:18:54.470,00:18:58.000
"using common sense knowledge that's not
really the kind of thing that we've been",00:18:58.000,00:19:01.673
"able to build deep learning
systems to do so far.",00:19:01.673,00:19:03.680
"Okay, so that's the end of part one.",00:19:06.020,00:19:08.730
"So, for part two of today,",00:19:08.730,00:19:10.781
"I wanted to sort of say a bit more
about tree-structured models.",00:19:10.781,00:19:15.700
"And then sort of talk about a bit of
recent work that was done by a recent",00:19:15.700,00:19:20.250
"student of mine, Sam Bowman,
along with Jean Gaultier",00:19:20.250,00:19:23.939
"on sort of having more efficient ways
of doing tree-structured models.",00:19:23.939,00:19:28.825
"So for my linguistic self,
I still think sort of having these sort of",00:19:28.825,00:19:33.240
"constituent pieces that you can
build representations of which gives",00:19:33.240,00:19:37.810
"you kind of a tree structure is
roughly the right kind of model.",00:19:37.810,00:19:42.250
"And so up until now, I'd sort of
shown some examples of bits of syntax",00:19:42.250,00:19:47.800
"and looking inside
the sentiment of clauses.",00:19:47.800,00:19:51.150
"Here's another nice
example that was some work",00:19:51.150,00:19:53.640
"that was done at Maryland by
Mojita Iya and his fellow students.",00:19:53.640,00:19:57.880
"And so what they were wanting to do,
is the learning models of",00:19:57.880,00:20:02.570
"the political ideology
of pieces of language.",00:20:02.570,00:20:06.310
"And so pieces of language could either
be mutual which is shown in gray or",00:20:06.310,00:20:10.800
"they could have liberal or conservative
pieces of political ideology and",00:20:10.800,00:20:17.140
"that wasn't just done at sort of
a whole paragraph or sentence level.",00:20:17.140,00:20:22.550
"In particular,
they were able to sort of build this",00:20:22.550,00:20:25.930
"hierarchical constituent model where
you could label pieces of rhetoric or",00:20:25.930,00:20:30.755
"ideology as conservative or
liberal inside sentences.",00:20:30.755,00:20:34.790
So we have examples like this one.,00:20:34.790,00:20:36.800
They dubbed it the death tax and,00:20:36.800,00:20:39.040
"created a big lie about its adverse
effects on small businesses.",00:20:39.040,00:20:43.260
"And so the model is picking out death
tax as a term of conservative ideology,",00:20:43.260,00:20:49.212
"and its adverse effects
on small businesses.",00:20:49.212,00:20:52.839
So that's conservative ideology language.,00:20:52.839,00:20:56.340
"But then when you're sort of putting that
together, interestingly it seems to have",00:20:56.340,00:21:01.272
"learned that putting scare
quotes around death tax,",00:21:01.272,00:21:04.397
"it then regards that as
a piece of liberal ideology.",00:21:04.397,00:21:07.466
"[LAUGH] But beyond that once you get up
to the whole sentence representation,",00:21:07.466,00:21:12.031
"when it's they dubbed it the death tax,
that that's then being regarded as",00:21:12.031,00:21:16.595
"sort of a piece of liberal ideology, of
representing the opposite point of view.",00:21:16.595,00:21:22.010
"Here's one other example that
shows the same kind of thing.",00:21:22.010,00:21:25.550
"But taxpayers do know already that TARP,
so that was the recovery",00:21:25.550,00:21:30.140
"program beginning of the Obama
administration, was designed in a way that",00:21:30.140,00:21:33.990
"allowed the same corporations who were
saved by huge amounts of taxpayer money to",00:21:33.990,00:21:38.580
"continue to show the same arrogant traits
that should've destroyed their companies.",00:21:38.580,00:21:43.428
"So this bit here,",00:21:43.428,00:21:44.612
"the huge amounts of taxpayer money is
being identified as conservative ideology.",00:21:44.612,00:21:50.182
"And then it's being embedded in this
sentence which is again showing liberal",00:21:50.182,00:21:54.249
ideology.,00:21:54.249,00:21:55.570
"Yeah I cannot explain that,",00:21:55.570,00:22:03.922
"I mean it seems like it should've
been colored gray, yeah.",00:22:03.922,00:22:06.944
"[LAUGH] These models
aren't always perfect,",00:22:06.944,00:22:11.155
they try with some inference or something.,00:22:11.155,00:22:15.260
"Yeah, I don't know.",00:22:15.260,00:22:16.200
"And even saved, it's not very clear,
that's what we get out.",00:22:17.980,00:22:22.450
"Okay, so, those were our kind of
tree recursive neural networks.",00:22:22.450,00:22:26.370
So I think that theoretically appealing.,00:22:26.370,00:22:28.840
"They can be empirically
competitive especially if",00:22:28.840,00:22:31.740
"you don't have 100 million
words of data to train on.",00:22:31.740,00:22:34.780
"But in most circles they've
sort of fallen out of favor,",00:22:34.780,00:22:38.850
"and that's because they have
some big disadvantages.",00:22:38.850,00:22:41.840
So they're often prohibitively slow.,00:22:41.840,00:22:45.560
"Most often they've been used
with an external parser,",00:22:45.560,00:22:48.180
"although you can use them
to parse as you go, but",00:22:48.180,00:22:50.680
"I guess that contributes to
them being prohibitively slow.",00:22:50.680,00:22:55.190
"And you could also think that although
there's something nice about these",00:22:55.190,00:22:59.260
"tree-structured models,",00:22:59.260,00:23:00.812
"you might actually wonder if
are they missing out on something.",00:23:00.812,00:23:04.313
"Cuz even though it makes sense that
there's sort of this tree structure of",00:23:04.313,00:23:08.948
"language, language does also
have a linear structure.",00:23:08.948,00:23:12.550
"You do sort of say these strings
of words that go left to right.",00:23:12.550,00:23:16.620
"And something that people have
often observed is if you only",00:23:16.620,00:23:20.440
"have tree-structured models, you then
have words that should be very close to",00:23:20.440,00:23:24.730
"each other that end up very distant from
each other in any kind of tree structure.",00:23:24.730,00:23:29.820
"And so actually the model I'm about to
talk of ends up having both tree structure",00:23:29.820,00:23:33.850
and linear structure.,00:23:33.850,00:23:34.910
"Why are the tree-structured models so
badly performing?",00:23:38.230,00:23:43.830
"And essentially, the reason is
they're not well suited to the kind of",00:23:43.830,00:23:47.590
"batch computations on GPUs, which is
really the sort of centerpiece of what's",00:23:47.590,00:23:52.510
"allowed sort of efficient training
of large deep learning models.",00:23:52.510,00:23:57.530
"So if you have a sequence model,",00:23:57.530,00:24:01.070
"a sequence model can
only have one structure.",00:24:01.070,00:24:03.880
"You're going from left to right, and",00:24:03.880,00:24:05.350
"you're computing hidden stuff
above each word in turn.",00:24:05.350,00:24:08.470
"And because of that, you can take a whole
bunch of sentences, preferably of",00:24:08.470,00:24:12.410
"similar lengths, and run them through and
lock step in the sequence model.",00:24:12.410,00:24:17.330
And that's really efficient.,00:24:17.330,00:24:18.515
"And the problem is if you wanna do
that with TreeRNNs, that you get this",00:24:18.515,00:24:23.616
"input specific structure, that every
sentence has a different structure.",00:24:23.616,00:24:29.253
"And so that undermines your ability to do
batched computation because you're still",00:24:29.253,00:24:33.696
"trying to construct different structural
units in the different sentences.",00:24:33.696,00:24:38.560
"And what happens with GPU code,
if you've got a batch of sentences and",00:24:38.560,00:24:43.260
"each one has different structure,
what it does is of your 32 threads,",00:24:43.260,00:24:48.340
"one of them is computing and
the other one of 31 are idle.",00:24:48.340,00:24:52.538
"Every time that there's
something different being done",00:24:52.538,00:24:55.290
"on one thread versus the other threads,
and so that kills all your efficiency.",00:24:55.290,00:24:59.320
"So Sam and John and co were sort
of trying to then work out well,",00:25:00.430,00:25:05.440
"could we come up with
a different form of model,",00:25:05.440,00:25:09.300
"which is at least closer to
the efficiency of a sequence model,",00:25:09.300,00:25:14.350
"while still giving us the benefits
of tree-structured representation.",00:25:14.350,00:25:18.670
"And that led to the Shift-reduce
Parser-Interpreter Neural Network or",00:25:18.670,00:25:22.626
SPINN model.,00:25:22.626,00:25:23.990
"So the base model is
equivalent to a TreeRNN, but",00:25:23.990,00:25:28.130
"because it's better for batch computation,",00:25:28.130,00:25:30.450
"you can't completely get rid of the fact
that they're different structures.",00:25:30.450,00:25:34.110
It's better for batch computation.,00:25:34.110,00:25:35.683
"It can be sort of 25 times faster or
more, depending on the kind of data.",00:25:35.683,00:25:41.082
"And it also provide an opportunity
to do a sort of a mixed linear and",00:25:41.082,00:25:45.035
"tree-structured model that can
be used alone without a parser.",00:25:45.035,00:25:49.540
"So, it's kind of a nice integrated model,
and I just wanna show you a bit of that.",00:25:49.540,00:25:53.740
"And the starting point of it is
essentially exactly the same as",00:25:53.740,00:25:58.265
"the dependency parsers that we saw
in assignment two and in class.",00:25:58.265,00:26:03.146
"So for any piece of tree structure,
you can describe a tree structure",00:26:03.146,00:26:08.116
"as uniquely as a sequence of shifts and
reduces.",00:26:08.116,00:26:11.720
"So for this structure on the left,
all right, you're gonna take the and cat,",00:26:11.720,00:26:15.890
shift on twice.,00:26:15.890,00:26:17.430
"Then you reduce them once
to put them together.",00:26:17.430,00:26:20.060
"You shift,
shift to get sat down on the stack.",00:26:20.060,00:26:23.730
"You reduce once,
you reduce the second time.",00:26:23.730,00:26:26.470
"And so this sequence of shifts and reduces
corresponds to this tree structure.",00:26:26.470,00:26:30.870
"And every other sequence
of shifting reduces,",00:26:30.870,00:26:34.190
"well, either corresponds to a different
tree structure or is invalid.",00:26:34.190,00:26:38.272
"All right, if you start off
trying to reduce before you've",00:26:38.272,00:26:40.250
"got nothing on the stack, you can't do it.",00:26:40.250,00:26:42.240
"And so what we can do is that we
can build a model that's sort of",00:26:42.240,00:26:47.222
"acting like a transition based
parser in shifting and reducing.",00:26:47.222,00:26:52.623
"And so it's gonna have a sequence
model in the middle of it.",00:26:52.623,00:26:55.190
"But that sequence model is then gonna
be looking at a buffer of words yet",00:26:55.190,00:26:59.836
to be dealt with and maintaining a stack.,00:26:59.836,00:27:02.610
"And inside the stack,
there's gonna be composition,",00:27:02.610,00:27:06.620
"kind of like a TreeRNN, or not really,
""kinda like"" exactly like a TreeRNN,",00:27:06.620,00:27:11.990
which will build TreeRNN representations.,00:27:11.990,00:27:15.500
"So at each point,
our LSTM model is tracking along, and",00:27:15.500,00:27:19.620
"it's predicting which thing to do,
reduce or",00:27:19.620,00:27:22.615
"shift, kind of just like
the dependency parsers that you build.",00:27:22.615,00:27:26.320
"And so depending on what it
does when things reduce, you're",00:27:26.320,00:27:30.160
"then doing a composition operation on
the stack, which is reshaping the stack.",00:27:30.160,00:27:34.830
"And while having this tracking
LSTM is both a simple parser and",00:27:36.795,00:27:41.387
"it gives us this kind of sequence
context that we can just,",00:27:41.387,00:27:45.712
"sort of, also model the sequence of words.",00:27:45.712,00:27:49.000
"So the essence of getting
this to work well is,",00:27:50.000,00:27:53.570
"how can you implement
the stack efficiently?",00:27:53.570,00:27:56.660
"Because if the stack was just like this,
and you've sort of got a stack for",00:27:56.660,00:28:01.541
"each time step, and the stack changes.",00:28:01.541,00:28:04.315
"Well, then you'd use a vast amount
of memory in the stack because",00:28:04.315,00:28:09.658
"you've got a different complete
stack at each time step.",00:28:09.658,00:28:14.798
"And there would also be bad for
achieving this lock step advance that will",00:28:14.798,00:28:19.690
make computation efficient on a GPU.,00:28:19.690,00:28:23.610
"So that's sort of bad in various ways, so
the secret of getting it to work pretty",00:28:23.610,00:28:27.830
"well is to say actually, for each
sentence, we're only gonna have one stack.",00:28:27.830,00:28:33.300
"And we're gonna incrementally
sort of build up representations",00:28:33.300,00:28:36.260
on the stack as we go.,00:28:36.260,00:28:38.110
"And so it's using an efficient
kind of data structure,",00:28:38.110,00:28:40.966
"which is similar to data
structures used elsewhere.",00:28:40.966,00:28:43.716
"They're sometimes called zipper data
structures and things like that.",00:28:43.716,00:28:46.707
"They're also using programming
language techniques.",00:28:46.707,00:28:49.580
"So, the idea is like this, so we wanna
build up the structure of Spot sat down.",00:28:49.580,00:28:53.510
"And so when we start shifting,
we have this one stack,",00:28:53.510,00:28:57.913
"which we start shifting words on to,
Spot sat down.",00:28:57.913,00:29:02.520
"And so there's only ever one stack, but",00:29:02.520,00:29:04.970
"then on this side,
we maintain some backpointers.",00:29:04.970,00:29:08.798
"And these backpointers
read from right to left is",00:29:08.798,00:29:12.030
"sort of telling us what's
on top of the stack.",00:29:12.030,00:29:14.290
"So at the moment,
3 is on top of the stack, followed by 2.",00:29:14.290,00:29:17.372
"So then when do a reduce operation,
we don't delete things",00:29:17.372,00:29:22.547
"off the stack,
we just write a new thing on to the stack.",00:29:22.547,00:29:27.470
"So we compute using our composition
operation in the TreeRNN style.",00:29:27.470,00:29:33.320
"A representation of the sat down, and we
simply change our backpointers, and say,",00:29:33.320,00:29:37.628
"okay, now, we've got 1, 4 on the stack.",00:29:37.628,00:29:39.950
"So 4 is the top of a stack, and
1 is the other thing on the stack.",00:29:39.950,00:29:44.600
"And so then when we, again, do a reduce,",00:29:44.600,00:29:46.933
"we compute a representation
of the (Spot (sat down)).",00:29:46.933,00:29:50.291
"And our backpointers say 5 is
the only active part of the stack.",00:29:50.291,00:29:54.810
"And the crucial thing is in these
backpointers allow us to know what's where",00:29:54.810,00:29:59.202
on the stack when we do operations.,00:29:59.202,00:30:01.272
"But in terms of our vectors that we're
using inside our forward pass and",00:30:01.272,00:30:06.582
"our backward pass,
all of the vectors are in this array here.",00:30:06.582,00:30:11.200
"And so we only have sort
of one array per sentence",00:30:11.200,00:30:14.000
"that we're sort of
incrementally building up.",00:30:14.000,00:30:17.530
Yes?,00:30:17.530,00:30:18.030
"Yes, it does.",00:30:24.626,00:30:25.440
Yeah.,00:30:26.730,00:30:27.360
"So, well I mean, it clearly needs to
back propagate through it, right?",00:30:30.723,00:30:38.370
"Cuz it's wanting to learn
composition functions, right?",00:30:38.370,00:30:40.890
So it's going to be sort of,00:30:40.890,00:30:43.420
"learning a matrix representation as
to how to combine these two words.",00:30:43.420,00:30:48.260
"And then your question is how,
and at that point,",00:30:48.260,00:30:52.750
"I have to give a sort of an answer of,
gee, this was done with supervision.",00:30:52.750,00:30:58.790
"So the reason you'd think it wouldn't be
differentiable is, you'd think, okay,",00:30:58.790,00:31:03.340
there's a choice at different points.,00:31:03.340,00:31:04.860
That's where there's a shift or reduce.,00:31:04.860,00:31:06.176
"And if that's a hard decision, then
that introduces a non-differentiability.",00:31:06.176,00:31:12.720
"Now, the way we did things in this
was sort of to take it in roundabout",00:31:12.720,00:31:17.035
round that.,00:31:17.035,00:31:17.910
"So if we train the model at training
time on sentences that had parses,",00:31:17.910,00:31:23.056
we could compute.,00:31:23.056,00:31:24.460
"Then there's no non-differentiability,
and we could",00:31:24.460,00:31:27.966
"compute the composition functions, and
we could learn to predict the actions.",00:31:27.966,00:31:32.940
"And so in some sense,
that's the easy way to do it,",00:31:32.940,00:31:37.292
which avoids the non-differentiability.,00:31:37.292,00:31:41.170
"If you didn't wanna do that and
you actually sorta wanted to say,",00:31:41.170,00:31:43.988
"let's have this uncertainty
while you're learning,",00:31:43.988,00:31:46.397
"then you'd have to do
something more complex.",00:31:46.397,00:31:48.460
"And that then leads into ideas,",00:31:48.460,00:31:50.593
"like reinforcement learning or Or
some of the other estimators that",00:31:50.593,00:31:55.159
"have been tried recently like the straight
through estimator where effectively,",00:31:55.159,00:32:00.015
"although the model makes hard
decisions on the forward pass.",00:32:00.015,00:32:03.638
"You're using a kind of a soft
logistic function in the backward",00:32:03.638,00:32:07.719
pass to do the differentiation.,00:32:07.719,00:32:09.879
"But doing this kind of model compared
to a traditional LSTM, sorry,",00:32:24.375,00:32:28.737
"compared to a traditional
tree with recurrent",00:32:28.737,00:32:31.521
"neural network is actually
sort of super efficient.",00:32:31.521,00:32:34.925
"So, this sort of shows for
different batch sizes.",00:32:34.925,00:32:39.387
"And obviously, it's going well out in
the batch sizes, but sort of shows",00:32:39.387,00:32:44.347
"the general point that traditionally
sort of tree recursive models and",00:32:44.347,00:32:49.147
"just sort of really inefficient that
you don't kind of get a good batch",00:32:49.147,00:32:53.867
"speed up effect, because you got
different operations at every time",00:32:53.867,00:32:58.507
"whereas the dotted line is just
an LSTM which is super efficient.",00:32:58.507,00:33:03.260
"And although this model starts
to curve up to in the blue and",00:33:03.260,00:33:06.666
"it has to curve up, because you are doing
different operations on the stack.",00:33:06.666,00:33:11.199
"It's sort of just way, way more efficient
out to quite large batch sizes.",00:33:11.199,00:33:15.230
"So Sam was then working to use this
model on natural language inference and",00:33:17.750,00:33:22.049
"I think we haven't talked much about
natural language inference, but",00:33:22.049,00:33:25.643
"I know it's come up for some of you
that look at the fake news challenge and",00:33:25.643,00:33:29.941
"things like that, cuz I and
others have pointed people out of it.",00:33:29.941,00:33:33.845
"So the idea of the natural language
inference was you have a piece of text and",00:33:33.845,00:33:38.372
"then you have a hypothesis following
it and you're wanting to say,",00:33:38.372,00:33:42.535
"whether the hypothesis follows from
the piece of text is an entailment.",00:33:42.535,00:33:47.310
"So for
a man rides a bike on a snow covered road,",00:33:47.310,00:33:50.105
"then a man is outside,
that's an entailment.",00:33:50.105,00:33:52.991
"By the way, it's a contradiction.",00:33:52.991,00:33:54.880
"So for
man in an apron is shopping at a market,",00:33:54.880,00:33:57.845
"that contradicts a man in
an apron is preparing dinner or",00:33:57.845,00:34:01.513
"whether it's neutral which means it's
neither an entailment or contradiction.",00:34:01.513,00:34:06.990
"So two female babies eating
chips is neutral with respect",00:34:06.990,00:34:10.979
"to two female babies are enjoying chips,
cuz they may or",00:34:10.979,00:34:14.883
"may not be enjoying them
even if they're eating them.",00:34:14.883,00:34:18.720
"So we collected this large corpus
SNLI of these kind of sentences and",00:34:18.720,00:34:23.465
"the way we constructed that was we
started off with one of the vision and",00:34:23.465,00:34:28.464
"language databases, MS COCO where there
is a picture and the sentence describe.",00:34:28.464,00:34:34.337
"A man rides a bike on a snow covered road
and we wanted to sorta have something",00:34:34.337,00:34:38.624
"where the scene was a description
of a picture, cuz that meant there",00:34:38.624,00:34:42.502
"was sort of a concrete thing that
the sentences were described.",00:34:42.502,00:34:46.200
"So that would hope to avoid there
be uncertainty of reference, but",00:34:46.200,00:34:50.104
"then we collected this sort of
hypothesis from Turkers and",00:34:50.104,00:34:53.521
"the Turkers weren't
actually shown the photo.",00:34:53.521,00:34:56.391
"They were just shown the passage and then
they were meant to generate sentences or",00:34:56.391,00:35:01.677
"entailments, neutrals or
contradictions and",00:35:01.677,00:35:04.787
"then we're trying to build systems that
do that and this has been a quite good",00:35:04.787,00:35:09.762
"task that a whole bunch of other
groups that then try to do better on.",00:35:09.762,00:35:14.064
"For what we were doing, we're kind of
interested in this idea of coming up with",00:35:14.064,00:35:18.337
"sentence representations by building
them as recursive neural networks.",00:35:18.337,00:35:22.750
"And so, the models we were building
was using spin model rules to build",00:35:22.750,00:35:27.311
"representation for each sentence and
then running it through comparison neural",00:35:27.311,00:35:32.572
"network layers to find the whole sentence
meaning at the center's relationships.",00:35:32.572,00:35:38.082
"So, we have one spin model for
representation.",00:35:38.082,00:35:42.006
This sentence one for that one and,00:35:42.006,00:35:43.799
"then we're sticking it
through a neural network,",00:35:43.799,00:35:46.556
"which is then giving a probability
distribution of the three actions.",00:35:46.556,00:35:50.419
"So after having being through the spin
model, we have a couple of fully connected",00:35:50.419,00:35:54.890
"neural network layers and then you've
got a Softmax over the three decisions.",00:35:54.890,00:35:59.310
"And so,
here are a couple of results from this.",00:36:00.810,00:36:03.408
"So there's sort of some previous results,
but",00:36:03.408,00:36:07.570
"the sort of things that are kind
of interesting here is this",00:36:07.570,00:36:12.667
"is LSTM RNN sequence model
that has accuracy of 80.6%.",00:36:12.667,00:36:17.987
"Some of surprisingly and
somewhat disappointingly,",00:36:17.987,00:36:21.726
"over this quite large dataset, so
it sort of half a million samples.",00:36:21.726,00:36:26.449
"The tree model actually barely does better
than that, so it's performance was 80.9%.",00:36:26.449,00:36:33.333
"Whereas relative my argument,
you'd hope for more gains.",00:36:33.333,00:36:36.879
"Something that's interesting is having
both at once, the sequence model and",00:36:36.879,00:36:40.945
the tree structure model.,00:36:40.945,00:36:42.620
That actually seems to be rather nice and,00:36:42.620,00:36:45.492
"give you kind of a nice
gain on this model.",00:36:45.492,00:36:48.453
"I should mention having whole sentence
representation isn't the best way",00:36:48.453,00:36:54.341
"to do task like the sort of SNLI,
entailment contradiction task.",00:36:54.341,00:36:59.675
"I mean, as comes up In a whole bunch of
these tasks in a lot of recent work that",00:36:59.675,00:37:04.175
"if you wanna do even
better at these tasks,",00:37:04.175,00:37:06.747
"you'd just do better using attention
models to make alignments at the word",00:37:06.747,00:37:11.250
level between the different sentences.,00:37:11.250,00:37:13.990
"And if you do that,",00:37:13.990,00:37:15.287
"you can do several percent better as
people have shown in recent results.",00:37:15.287,00:37:20.333
"But nevertheless, I still do believe
in the sort of tree structured idea and",00:37:20.333,00:37:25.727
"you can sort of see places where
having the tree representations allows",00:37:25.727,00:37:30.787
"you to get things right that you just
don't get right in the LSTM model.",00:37:30.787,00:37:35.890
"So when there are finer grain
semantic facts, such as negation,",00:37:35.890,00:37:40.540
"gymnast completes her floor exercise,
gymnast cannot finish her exercise.",00:37:40.540,00:37:46.140
"Well, then then tree
structure model is better.",00:37:46.140,00:37:48.429
"It also turns out to be differentially
better when you just have very long",00:37:48.429,00:37:53.335
"examples, which the LSTM
gets less good at.",00:37:53.335,00:37:56.463
I finished part one.,00:37:56.463,00:37:57.780
"And so, we now hand over
to the research highlight.",00:37:57.780,00:38:01.730
[INAUDIBLE] Zhedi.,00:38:01.730,00:38:02.697
"&gt;&gt; So today, we'll discuss how to compose
neural networks for question answering.",00:38:07.056,00:38:12.290
"So as a high-level overview,
the papers talk about a compositional,",00:38:12.290,00:38:17.162
"attentional for answering questions about
a variety of world representations,",00:38:17.162,00:38:22.766
"including images and
also structured knowledge bases.",00:38:22.766,00:38:26.685
"The model has two components,
trained jointly.",00:38:26.685,00:38:29.790
"The first complement is
a collection of neural",00:38:29.790,00:38:32.538
modules that can be freely composed.,00:38:32.538,00:38:34.799
"The figure shows actually four modules,
a lookup module,",00:38:34.799,00:38:38.815
"a relate module, an and
module and also a find module and",00:38:38.815,00:38:42.588
"the second component is a network
layout predictor that assembles",00:38:42.588,00:38:47.089
"modules into complete deep networks
tailored to each question.",00:38:47.089,00:38:52.130
"So, our current query is
what cities are in Georgia?",00:38:52.130,00:38:55.729
"And the figure shows the network
layout for that particular query.",00:38:55.729,00:39:02.752
"Essentially, the model
has two distributions.",00:39:02.752,00:39:05.995
"A layout model that chooses a layout for
sentence and",00:39:05.995,00:39:09.398
"also an execution model that
applies the network specified",00:39:09.398,00:39:13.267
"by a particular layout to
a world representation.",00:39:13.267,00:39:16.610
We'll start from the Layout Model.,00:39:17.920,00:39:20.210
"So in order to obtain from the Layout
Model, there are three steps to take.",00:39:20.210,00:39:24.960
"First, we want to represent the input
sentence as a dependency tree.",00:39:24.960,00:39:29.770
"So the figure shows that dependency tree
for the query, what cities are in Georgia?",00:39:29.770,00:39:33.580
"The second step is to associate
fragments of the dependency parse",00:39:35.020,00:39:38.990
with appropriate modules.,00:39:38.990,00:39:41.020
"So as we can see in this figure, the find
module is associated with city, the relate",00:39:41.020,00:39:47.140
"module is associated with in and the
lookup module is associated with Georgia.",00:39:47.140,00:39:51.990
"And the last step is to assemble
fragment into full layouts.",00:39:53.250,00:39:57.310
"It should be noted that each sentence
could have multiple layouts.",00:39:57.310,00:40:01.380
"So for our example,
one candidate layout is shown there.",00:40:01.380,00:40:05.900
"It's a tree structure with the and
module as a root module.",00:40:05.900,00:40:10.120
"And now, we will just talk about
how to score the kind of layouts.",00:40:11.280,00:40:16.530
"So, in order to score a kind of layout,",00:40:16.530,00:40:19.230
"we need to produce an LSTM
representation of the question.",00:40:19.230,00:40:23.250
"A feature based representation
of the query and",00:40:23.250,00:40:26.277
"pass both representations
through a multilayer perceptron.",00:40:26.277,00:40:30.299
"And then, the update to the layout
scoring model at each time step",00:40:30.299,00:40:34.485
"is simply the gradient of
the log-probability of the chosen layout,",00:40:34.485,00:40:38.897
"scaled by the accuracy of
that layout's predictions.",00:40:38.897,00:40:42.430
"And now,
just talk about the execution model.",00:40:44.220,00:40:47.470
"So, given the layout as shown in figure b,",00:40:47.470,00:40:50.187
we could basically assemble,00:40:50.187,00:40:55.420
"the corresponding modules into a full
neural network, as showing in figure c.",00:40:55.420,00:41:01.270
"And we'll just apply it to a knowledge
source as shown in figure d.",00:41:01.270,00:41:05.149
"And basically, we should note that
immediate results flow between",00:41:06.170,00:41:09.875
"modules until your answer
is produced at the root.",00:41:09.875,00:41:12.870
"So in this case,
Atlanta is produced as the root",00:41:12.870,00:41:15.740
"which is also the answer to our query,
what cities are in Georgia?",00:41:15.740,00:41:18.958
"So essentially,
modules are just like small neural",00:41:18.958,00:41:23.535
"components that take inputs or
intermediate results.",00:41:23.535,00:41:28.443
"The slide now is showing a lookup
module,whose expression is",00:41:28.443,00:41:33.490
also coloured in red.,00:41:33.490,00:41:34.900
"And the lookup module basically
just produces an attention focused",00:41:34.900,00:41:39.405
entirely at the index f(i).,00:41:39.405,00:41:41.467
"Where you can just think of
the relationship f between words and",00:41:41.467,00:41:45.403
"positions in the input map as some sort
of string matches on database fields.",00:41:45.403,00:41:50.245
"And, we also have a relate module.",00:41:51.355,00:41:54.495
"So, the relate module is
a softmax that directs focus from",00:41:54.495,00:41:58.925
one region of the input to another.,00:41:58.925,00:42:00.905
"The find module is sort of
similar to the relate module.",00:42:02.300,00:42:05.320
"It's also a softmax but it computes
a distribution or an indices by",00:42:05.320,00:42:09.790
"concatenating the parameter argument with
each position of the input feature map.",00:42:09.790,00:42:14.960
"And passing the concatenated to the
vectors through a multilayer perceptron.",00:42:14.960,00:42:18.060
"So, the last module is the and Module.",00:42:19.210,00:42:22.470
"Typically the and Module is at
the root of our dependency part tree.",00:42:22.470,00:42:27.471
"And for the and Module is sort of
similar to a set intersection but",00:42:27.471,00:42:32.202
it's used for attentions.,00:42:32.202,00:42:34.740
"And is actually probabilities
are multiplied together.",00:42:34.740,00:42:38.040
For the and Module.,00:42:38.040,00:42:38.890
"In order to train an execution model,
we need to maximize the sum of log",00:42:40.470,00:42:45.890
"probability of answer labels given a world
representation over a particular layout.",00:42:45.890,00:42:51.190
"And so our model actually achieves
State of the art performance,",00:42:53.110,00:42:57.827
"on both images and
also structure knowledge basis.",00:42:57.827,00:43:01.614
"So, we'll start from looking at how it
performs on a visual question answering",00:43:01.614,00:43:06.628
dataset.,00:43:06.628,00:43:07.620
"So, the model is actually able to
figure out what's in the sheep's ear",00:43:07.620,00:43:11.818
"is actually a tag, it's gonna small but
it's actually a tag it's also able to",00:43:11.818,00:43:16.515
"figure out the color of
the robe the woman is wearing.",00:43:16.515,00:43:19.746
"And for the third image,the model
is not able to figure out that",00:43:19.746,00:43:24.470
"a man is dragging a boat, but
it says the man is dragging",00:43:24.470,00:43:28.570
"a board,which is fairly close,
so it's pretty amazing.",00:43:28.570,00:43:32.806
"And in terms of the numbers, so we can
see the model out of all the approaches",00:43:32.806,00:43:38.040
"it has the highest test set accuracy on
the available question answering dataset.",00:43:38.040,00:43:43.000
"And the model can also do
like general knowledge-based",00:43:44.700,00:43:48.440
type of question answering.,00:43:48.440,00:43:50.280
"So in this case, the model is actually
able to figure out what national parks and",00:43:50.280,00:43:55.980
"in Florida, and
also whether Key Largo is an island.",00:43:55.980,00:44:00.490
"So, also in terms of numbers,
the model does really well.",00:44:00.490,00:44:05.440
"It actually beat every other
approach by at least 3%.",00:44:05.440,00:44:09.320
"So, this is really amazing model.",00:44:09.320,00:44:11.320
"And I will definitely recommend you to
take a look at the paper, and that's it.",00:44:11.320,00:44:15.900
Thanks.,00:44:15.900,00:44:16.465
"[APPLAUSE]
&gt;&gt; Okay, thank Zhedi.",00:44:16.465,00:44:24.410
"Okay, so then for my remaining
time on the stage this quarter,",00:44:24.410,00:44:29.270
"I wanted to just sort of say a bit
more about a couple of things that",00:44:29.270,00:44:34.219
"I think of vaguely come up
at some point or another,",00:44:34.219,00:44:38.161
"but haven't really very
prominently come up.",00:44:38.161,00:44:41.857
"I mean, the first one is just
a very brief cameo appearance.",00:44:41.857,00:44:47.670
"But, I just so wanted to say a fraction
more about pointer copying models,",00:44:47.670,00:44:52.690
just so that that idea's in your head.,00:44:52.690,00:44:55.060
"So, Richard sort of talked about a version
of these a long time ago, when he",00:44:55.060,00:44:59.956
"sort of told a bit about his group's
work on Pointer Sentinel mixture models.",00:44:59.956,00:45:05.119
"And I just want to sort of repeat
the idea of this as sort of one of",00:45:05.119,00:45:09.175
"the other ideas that people have been
using a bit in recent neural networks.",00:45:09.175,00:45:14.173
"So, one of the central papers is this
one on pointing the unknown words.",00:45:14.173,00:45:19.890
"And this diagram is a bit
confusing to read because for",00:45:19.890,00:45:24.370
some reason they sort of did it backwards.,00:45:24.370,00:45:27.060
"So the source is on the right side,
and the target is on the left side.",00:45:27.060,00:45:31.830
"But, the idea here is so
that for the source sequence,",00:45:31.830,00:45:35.418
"right, that we're assuming that
we've run a bidirectional LSTM, or",00:45:35.418,00:45:40.020
"something like that,
over the source words.",00:45:40.020,00:45:43.064
"And then, we're in the kind of
usual kind of generational LSTM,",00:45:43.064,00:45:47.630
"where what we've done is sort of,
we've got some hidden state.",00:45:47.630,00:45:51.980
"And then, we go to be sorta starting
to generate the next word based on it.",00:45:51.980,00:45:58.000
And then how is that gonna be done?,00:45:59.190,00:46:02.700
"So, we've had already the ideas
that you can just sort of",00:46:02.700,00:46:06.910
"generate from the hidden state,
based on a softmax.",00:46:06.910,00:46:11.150
"And we've had the idea that you could kind
of use an attention model to sort of look",00:46:11.150,00:46:16.100
"back at the source and coding, and
sort of use that as input to your softmax.",00:46:16.100,00:46:21.790
"And those are both good ideas, but in both
those cases at the end of the day you're",00:46:21.790,00:46:27.070
"so doing a softmax over your vocabulary,
based on some hidden state over here.",00:46:27.070,00:46:33.710
"And the suggestion that's come up is that,
at least in some applications,",00:46:33.710,00:46:38.970
and those include machine translation.,00:46:38.970,00:46:42.300
But also things like text summarization.,00:46:42.300,00:46:45.370
"Another thing that might be a good
idea is if you could just decide",00:46:45.370,00:46:49.840
"that what you wanna do is copy some
word that came from the source sequence.",00:46:49.840,00:46:54.820
"So, rather than having to have a hidden
state from which you can generate it from",00:46:54.820,00:46:58.380
"your softmax, you could just say
a good idea in this position",00:46:58.380,00:47:02.270
"would just be to copy word
17 from the source sequence.",00:47:02.270,00:47:05.240
"And that might be appropriate if
it's something like a name that you",00:47:05.240,00:47:08.920
"might have a rare name in the input or
some kind of lone word or",00:47:08.920,00:47:13.810
"anything like that, that just makes
sense to copy towards the output.",00:47:13.810,00:47:18.310
"And so, from the hidden state, you're
sort of having a binary logistic model,",00:47:18.310,00:47:24.230
"which is saying to what
extent do you want to",00:47:24.230,00:47:28.040
"generate from the vocabulary softmax,
versus do you want to point and copy?",00:47:28.040,00:47:33.640
"And so this choice here, you could
make it a hard choice, which would",00:47:33.640,00:47:38.420
"have the same kind of complications for
making it differentiable.",00:47:38.420,00:47:42.120
"But commonly, the way it's been done
is just being made as a soft choice.",00:47:42.120,00:47:45.600
"So with probability p,
you're generating for",00:47:45.600,00:47:48.649
the vocabulary softmax of probability 1-p.,00:47:48.649,00:47:51.704
"You're kind of gonna look in to do another
attention distribution over the source and",00:47:51.704,00:47:57.620
"then you're gonna sort of just be copying
a word where you're placing attention.",00:47:57.620,00:48:01.550
"And so, that's been kind of
quite an effective mechanism",00:48:01.550,00:48:05.372
"to sort of just be able
to just copy words for",00:48:05.372,00:48:08.180
"the input to the output which has sort
of helped for several applications.",00:48:08.180,00:48:12.710
"So in their paper, where they
are using it for machine translation,",00:48:12.710,00:48:17.765
"having this kind of pointer model
was giving them about three and",00:48:17.765,00:48:22.472
"a half BLEU points of performance,
which is a ton, right?",00:48:22.472,00:48:26.850
"A lot of the time in MT,
if you can get a BLEU point,",00:48:26.850,00:48:29.240
you think you're doing really well.,00:48:29.240,00:48:30.510
"So, that was sort of
a very big improvement.",00:48:30.510,00:48:33.780
"Another place where it's very effective
is in summarization models, cuz a lot of",00:48:33.780,00:48:38.525
"the time in the summarization models you
are wanting to sort of copy names of",00:48:38.525,00:48:42.856
"people, and places and things like that
straight from the input to the output.",00:48:42.856,00:48:47.491
"But just to give a cautionary
note on the other side.",00:48:47.491,00:48:51.990
"Interestingly, in the sort of, the Google
Neural Machine Translation paper that came",00:48:51.990,00:48:56.660
"out recently going along with
their sort of recent release of",00:48:56.660,00:49:01.900
"their big Neural Machine Translation
Models on the live servers,",00:49:01.900,00:49:06.530
"that they sort of state that they
weren't having much success from that.",00:49:06.530,00:49:11.750
"So they say in principle, you can train
a copy model that this approach is both",00:49:11.750,00:49:16.660
"unreliable at scale, the attention
mechanism is unstable when the network",00:49:16.660,00:49:21.000
"is deep, and copying may not always
be the best strategy for rare words.",00:49:21.000,00:49:25.610
"So, they don't seem to found
successful for that method.",00:49:25.610,00:49:28.150
"So I guess like all things you can try and
see if it works for you.",00:49:28.150,00:49:32.630
"Okay, so then the final thing I
wanted to talk about was a bit about",00:49:33.870,00:49:38.880
work that goes below the word level.,00:49:38.880,00:49:43.070
"So, right from the beginning of this
course where we started off with",00:49:43.070,00:49:47.450
is that we had words and,00:49:47.450,00:49:49.540
"we were gonna want to build
distributed representations for words.",00:49:49.540,00:49:53.840
"And that then led into Word2Vec and
all the stuff we talked about there.",00:49:53.840,00:49:59.100
"And I think it's fair
to say that sort of for",00:49:59.100,00:50:03.370
"around the sort of period of 2011,
12, 13, 14, that's what everybody",00:50:03.370,00:50:08.200
"was doing, and that's where we
still start for this course.",00:50:09.250,00:50:13.070
"But in the last couple of years,
there's now started to be a lot of work",00:50:13.070,00:50:16.920
"where people are going
below the word level.",00:50:16.920,00:50:19.580
"And so, I just wanted to
show a bit more about that.",00:50:19.580,00:50:22.240
"Before doing it,",00:50:22.240,00:50:23.220
"just sort of a couple of slides of
sort of context of how languages work.",00:50:23.220,00:50:29.000
"So most of the time in NLP,
with deep learning,",00:50:29.000,00:50:32.860
"we're sort of working with
language in its written form.",00:50:32.860,00:50:36.420
"It's easily processed, found data.",00:50:36.420,00:50:38.930
"And so, something that you're at least
sort of vaguely have in your head,",00:50:38.930,00:50:42.130
"that human writing systems aren't
all the same kind of system.",00:50:42.130,00:50:46.160
"So, that they sort of,
various to their nature.",00:50:46.160,00:50:49.720
"So, that there are many writing
systems that are alphabetic, and",00:50:49.720,00:50:54.340
basically phonemic.,00:50:54.340,00:50:55.790
"Which is sorta if you see what
the letter is how to pronounce that.",00:50:55.790,00:50:59.430
"So, sorta something like Italian is
also a fairly phonemic writing system.",00:50:59.430,00:51:06.520
"This example here is from
an Australian language, Wambaya.",00:51:06.520,00:51:09.540
"But sort of it's jiyawu ngabulu,",00:51:09.540,00:51:12.100
"that it's sort of the sounds
are just read off the letters.",00:51:12.100,00:51:16.080
"So, that contrasts with something like
English which is an alphabetic writing",00:51:16.080,00:51:19.620
"system that as anyone who's spent
time learning English knows, that",00:51:19.620,00:51:24.840
"the correspondence between letters and
sounds is much more convoluted in English.",00:51:24.840,00:51:30.270
"So when you have a word like
thorough It's sort of not really",00:51:30.270,00:51:33.950
"spelled out how it sounds, that sort of
this complex historical stuff going on.",00:51:33.950,00:51:38.110
"But then,",00:51:38.110,00:51:38.720
"there are other language systems that
sort of represent slightly bigger units.",00:51:38.720,00:51:42.570
"So some languages represent syllabic units
or moraic units, so you've kind of got",00:51:42.570,00:51:47.560
"a syllable Like yawn or gargle, something
that's being represented by one letter.",00:51:47.560,00:51:52.560
And this is Inuktitut from Canada.,00:51:52.560,00:51:55.320
"And then, there are sort of
the syllabic kind of languages.",00:51:55.320,00:51:59.930
"There sort of two kinds there are ones
where you're having a sort of individual",00:51:59.930,00:52:05.660
"letter is being used still on the basis
of it's sound like an alphabetic form.",00:52:05.660,00:52:10.800
"And then,
you have ideographic languages for",00:52:10.800,00:52:13.300
"which by far the dominant
sample is Chinese, where",00:52:13.300,00:52:16.860
"you're having the individual character
representing a semantic components.",00:52:16.860,00:52:21.370
"So that also has a pronunciation, but
there will be lots of characters,",00:52:21.370,00:52:25.396
"which have the same pronunciation, but
have different meanings attached to them.",00:52:25.396,00:52:30.083
"And then, you get writing systems
that are sorta a mixture of these.",00:52:30.083,00:52:33.760
"So something like modern Japanese, you
both have characters like these two and",00:52:33.760,00:52:38.170
"that one, which are, well, moraic,",00:52:38.170,00:52:41.050
"that they're sorta being kind
of either just a vowel or",00:52:41.050,00:52:45.530
"just a syllabic consonant or
a vowel-consonant-vowel form.",00:52:45.530,00:52:50.320
"But then, you also get the kind of
idiographic characters that Japanese",00:52:50.320,00:52:54.770
borrows from Chinese.,00:52:54.770,00:52:55.770
"So, you get these sort of different
forms of writing systems.",00:52:55.770,00:52:59.680
"And then there's this question of, well do
your writing systems differentiate words?",00:52:59.680,00:53:06.100
"So, some writing systems differentiate
words explicitly and some don't.",00:53:06.100,00:53:12.810
"So, again, Chinese is a famous
example of a language that does not",00:53:12.810,00:53:17.130
"have any word boundaries marked in the
text, you just get a string of characters.",00:53:17.130,00:53:22.870
"Interestingly something that
most people are less aware of,",00:53:22.870,00:53:29.050
"is that if you actually go back to Ancient
Greek as it was written by Ancient Greeks.",00:53:29.050,00:53:34.170
"Ancient Greek was a language that was
written with no word segmentation.",00:53:34.170,00:53:39.109
"That the letters were
just a continuous stream.",00:53:39.109,00:53:41.990
"So it's sort of putting
spaces between words and",00:53:41.990,00:53:44.974
"Ancient Greek was actually something
that was first done by medieval Monks to",00:53:44.974,00:53:49.706
make ancient Greek easier to understand.,00:53:49.706,00:53:52.348
"So, it was also a no word
segmentation language.",00:53:52.348,00:53:55.207
"So the question, was Latin segmented?",00:54:01.263,00:54:03.880
"So, it actually sort of varied.",00:54:03.880,00:54:07.320
"You can certainly find unsegmented
Latin but it started, they didn't use",00:54:07.320,00:54:12.723
"spacers but it started to become common to
sort of chisel a little dot between words.",00:54:12.723,00:54:18.828
"And so then, there was
a representation of word segmentation.",00:54:18.828,00:54:22.538
Yeah.,00:54:22.538,00:54:23.038
"Okay, and
then when you do have word segmentation,",00:54:26.087,00:54:28.911
"there's still sort of some variety
as to how much you segment things.",00:54:28.911,00:54:32.940
"And so, there are kind of a couple of
big parameters of variation of that.",00:54:32.940,00:54:37.030
"So, a lot of languages sort of have little",00:54:37.030,00:54:40.080
"words that have sort
of functional meaning.",00:54:40.080,00:54:42.460
"And languages vary, how much they
separate them, or keep them together.",00:54:42.460,00:54:47.540
"So, a language like French has
these sort of little clitics for",00:54:47.540,00:54:52.650
the first sort of pronouns.,00:54:52.650,00:54:55.118
"Je vous ai, but they sort of the vous for
you is represented with a space, even",00:54:55.118,00:55:01.170
"though it sort of joins on phonologically,
whereas other languages, such as Arabic,",00:55:01.170,00:55:06.370
"will sort of take the similar kinds
of clitic pronouns, like we and it.",00:55:06.370,00:55:12.160
"And sort of just glum them all together
and produce a slightly larger word",00:55:12.160,00:55:16.430
"even though it's sort of different content
words are space separated in Arabic.",00:55:16.430,00:55:20.920
"And the other languages vary
is when you have compounds.",00:55:20.920,00:55:24.690
"So, all languages pretty much have
lots of compounds like life insurance,",00:55:24.690,00:55:28.890
company employee.,00:55:28.890,00:55:30.440
"But in English, we handily still
have the spaces between the words",00:55:30.440,00:55:34.715
"in Life Insurance Company employees,
whereas, once you go to German,",00:55:34.715,00:55:39.216
"that's then being written as one
big unsegmented word like that.",00:55:39.216,00:55:43.510
"Okay, so that's the sorta context for
then going below the word level.",00:55:45.526,00:55:51.930
"And so, there are lots of reasons
to want to go below the level,",00:55:51.930,00:55:55.130
"so if you'd like to handle a very large,",00:55:55.130,00:55:57.670
"open vocabulary It's kind of unappealing
if you need a word vector for every word.",00:55:57.670,00:56:02.560
"And that's especially true in a lot
of languages where they have a lot of",00:56:02.560,00:56:07.410
"different word forms that represent
different derivational morphologies,",00:56:07.410,00:56:13.100
"that's sort of relationships where
you sort of have causatives or",00:56:13.100,00:56:17.520
possessives and other things.,00:56:17.520,00:56:18.790
"Things that joined onto words,
an inflectional relationship.",00:56:18.790,00:56:22.160
"So you have different forms,
a person, number, and agreement and",00:56:22.160,00:56:25.662
things like that.,00:56:25.662,00:56:27.088
"So here's a very long
check word to the worst",00:56:27.088,00:56:32.250
"farmable one, which has a lot
of morphology join together.",00:56:32.250,00:56:36.990
"So if you have languages like that,",00:56:36.990,00:56:38.800
"it's sort of unappealing to have
word vectors for every word.",00:56:38.800,00:56:41.870
"Another place that you find things
happening everywhere over modern social",00:56:41.870,00:56:46.370
"meeting, is people use creative spellings
to express a little bit more emotion.",00:56:46.370,00:56:52.410
"And so then you have words like good,
with a lot of o's in it.",00:56:52.410,00:56:56.580
"Which probably isn't in
the vocabulary of your system.",00:56:56.580,00:57:01.202
"And then when you want to do
other tasks like translation,",00:57:01.202,00:57:05.404
"you often would like to go
below the word level, so",00:57:05.404,00:57:09.004
"if you'd like to Christopher into Czech,
you might want it to know it",00:57:09.004,00:57:13.977
"sort of translates into something
that's sort of related, Krystof.",00:57:13.977,00:57:19.685
"And that sort of makes sense if
you're at the character level, but",00:57:19.685,00:57:23.105
"not if you just have sort
of these individual words.",00:57:23.105,00:57:25.485
"And so
there's a question of how you can start to",00:57:25.485,00:57:28.585
deal with some of these phenomena.,00:57:28.585,00:57:30.215
"So in traditional linguistics, the
smallest semantic units were morphemes.",00:57:31.590,00:57:36.810
"So big words would be divided up
into their individual morphemes, so",00:57:36.810,00:57:40.650
"if you had a word like unfortunately,
so if it has a root of fortune and",00:57:40.650,00:57:46.100
"then you add on the derivational
ending ate to get fortunate.",00:57:46.100,00:57:50.020
"You add on another derivational ending
to get, unfortunate and then you add on",00:57:50.020,00:57:54.510
"a final derivational ending, ly, to turn
into an adverb and you get, unfortunately.",00:57:54.510,00:58:00.330
"So this kind of use of morphology has been
very little studied in deep learning,",00:58:00.330,00:58:06.040
"though actually Mitchell and me had
a paper with Tom Luong a few years back.",00:58:06.040,00:58:10.880
"Or we try to use the same kind
of tree structured models to",00:58:10.880,00:58:14.830
"build up representations of
morphologically complex words.",00:58:14.830,00:58:19.299
"Most of the time people haven't done that,
but have done simple things.",00:58:19.299,00:58:24.180
"So a common alternative is to
work with character n-grams.",00:58:24.180,00:58:28.300
"That's something that actually
has a very long history, so",00:58:28.300,00:58:33.180
"back in the earlier age of
neural networks, Rumelhart and",00:58:33.180,00:58:37.000
"McClelland proposed a representation
that they humorously called",00:58:37.000,00:58:41.940
"wickelphones, and what wickelphones were,
were sort of triples of letters.",00:58:41.940,00:58:47.260
"And so they proposed this model in
some of their language models for",00:58:47.260,00:58:51.950
"representing inflections, and learning
inflectional forms of verbs in English,",00:58:51.950,00:58:57.060
"which was a model that at the time
linguists reacted to very negatively.",00:58:57.060,00:59:01.940
"But it's an idea that's kind of lived on,
so",00:59:01.940,00:59:05.120
"there's much more recent
work from Microsoft.",00:59:05.120,00:59:07.445
"Where essentially they're sort of
using the same kind of letter triples.",00:59:07.445,00:59:12.175
"The picture from the bottom is
actually from Rumelhart and",00:59:12.175,00:59:15.375
"McClelland's work, and so
when you start off with a word,",00:59:15.375,00:59:18.675
"they represent it internally
as a set of letter triples.",00:59:18.675,00:59:23.780
"So effectively the intermediate layer
encoder is sort of turning any word",00:59:23.780,00:59:29.910
"into just the set of letter triples
that are contained inside it and",00:59:30.920,00:59:35.740
"so that gives kind of a flat
representation without needing a sequence.",00:59:35.740,00:59:39.920
"The nimbleness captures most of
the structure of the word and",00:59:39.920,00:59:43.310
"it sort of works from there and
can then generate another word.",00:59:43.310,00:59:45.850
And this is sort of doing it explicitly,00:59:45.850,00:59:49.030
"with character trigrams which
are then given vector encoding.",00:59:49.030,00:59:53.500
"But in some sense the idea is sort of
related to when we looked at convolutional",00:59:53.500,00:59:58.270
"layers briefly because they were also kind
of combining together multiple letters.",00:59:58.270,01:00:03.530
"But it was sort of doing it, after it
being turned into a continuous space.",01:00:03.530,01:00:07.910
"Rather than just separately learning
continuous vector representation for",01:00:07.910,01:00:12.350
each letter trigram.,01:00:12.350,01:00:14.250
"And it seems to have been shown that
using these kind of character n-gram",01:00:14.250,01:00:19.629
"ideas whether like these wickelphones or
using convolutions can in practice",01:00:19.629,01:00:25.362
"give you a lot of the gains of morphemes
with perhaps less suffering, okay.",01:00:25.362,01:00:31.480
"And so what people have found
is that you can generate",01:00:31.480,01:00:36.410
"word embeddings by building them
up from character embeddings.",01:00:36.410,01:00:40.630
"And so, if you're able to do that,
you can then generate an embedding for",01:00:40.630,01:00:44.790
"any new word you see when someone sort of
has some weird sequence of letters and",01:00:44.790,01:00:49.590
your social media text.,01:00:49.590,01:00:51.340
"You can just go letter by letter and
generate a word representation for it.",01:00:51.340,01:00:57.415
"And in a way that should work better,
because it has some of the gains",01:00:57.415,01:01:02.590
"that in general people argue for
deep learning.",01:01:04.070,01:01:06.735
"That since you can then kind
of have words with similar",01:01:06.735,01:01:10.360
"spellings should have similar embeddings,
so to the extent that they're so",01:01:10.360,01:01:14.070
"different morphological forms or
related words by derivation.",01:01:14.070,01:01:18.470
"You should be able to
capture that commonality",01:01:18.470,01:01:21.240
into your character embeddings.,01:01:21.240,01:01:23.210
"And if you're using these kind
of character level models",01:01:23.210,01:01:26.030
"then you kind of don't have any
problems ever with unknown words",01:01:26.030,01:01:29.880
because you can just represent them all.,01:01:29.880,01:01:32.310
"And so using character level
models in the last couple of years",01:01:32.310,01:01:36.670
"has just proven to work super,
super successfully.",01:01:36.670,01:01:40.990
"If I make an admission now, I mean, when
the idea first came up of using character",01:01:40.990,01:01:45.920
"level models, I was really pretty
skeptical as to whether it would work.",01:01:45.920,01:01:51.630
"So from the traditional
linguistic perspective but",01:01:51.630,01:01:55.310
"the idea had always been, well,
yeah, we have sort of morphemes.",01:01:55.310,01:01:59.080
"We have those units like fortune,
and fortunate, and unfortunate,",01:01:59.080,01:02:03.440
"that those morphemes like un,
fortune, and ate, they have meaning.",01:02:03.440,01:02:08.050
"But if you just have letters like a ""u"".",01:02:08.050,01:02:11.720
"Or a ""f"" they don't seem
to have any meaning.",01:02:11.720,01:02:15.900
"So, it sort of seemed a little
bit difficult to imagine",01:02:15.900,01:02:20.760
"that you can learn a vector
representation for f and",01:02:20.760,01:02:23.840
"a vector representation for u and
a vector representation for n.",01:02:23.840,01:02:27.250
"And then you can start
composing them together, and",01:02:27.250,01:02:30.220
"getting useful semantic
representations for words.",01:02:30.220,01:02:34.030
"But what people have found is actually,",01:02:34.030,01:02:37.170
"some of these modern models like these
LSTM models have sufficiently powerful",01:02:37.170,01:02:42.860
"composition functions that actually you
can learn very good word representations.",01:02:42.860,01:02:47.760
By building them up letter by letter.,01:02:47.760,01:02:50.430
"So, here's a kind of
a clean version of this.",01:02:50.430,01:02:53.110
"And so this was some work that was
done by a bunch of people at CMU.",01:02:53.110,01:02:59.330
Chris Dyer and colleagues.,01:02:59.330,01:03:01.040
"And so, in some sense you're
doing the obvious thing.",01:03:01.040,01:03:05.180
"So for word,
what you're doing is you're running",01:03:05.180,01:03:09.710
"a bidirectional LSTM over
the characters of the words.",01:03:09.710,01:03:14.010
"And so you're learning
character representations and",01:03:14.010,01:03:17.470
"then sort of hidden representations
using the LSTM above those characters.",01:03:17.470,01:03:22.560
"Then you append the two representations
at the end of those sequences and",01:03:22.560,01:03:27.920
"just say that's your word
representation for unfortunately.",01:03:27.920,01:03:31.730
"And then, to sort of train this whole
model you're then embedding it and another",01:03:31.730,01:03:36.320
"LSTM which is then going to give you
your sequence over words for some task.",01:03:36.320,01:03:41.700
"So we then have a word level LSTM
which is doing something like",01:03:41.700,01:03:46.070
"predicting the sequence of
words on the bank was closed.",01:03:46.070,01:03:49.480
So you sort of have these.,01:03:49.480,01:03:50.800
"Doubly recurrent models that are nested
hierarchically inside each other's,",01:03:50.800,01:03:56.080
"and so they tested out this model for
two tasks.",01:03:56.080,01:03:58.950
"One was just the language
modeling task and",01:03:58.950,01:04:01.734
"one was the part of speech tagging task,
and it just worked super successfully.",01:04:01.734,01:04:07.003
"So in particular, they were able to show
better results for part of speech tagging",01:04:07.003,01:04:12.989
"than people had shown with word level
neural part of speech tagging models.",01:04:12.989,01:04:18.950
"And so that makes sense, if you're sort
of say well, this is because we can share",01:04:18.950,01:04:23.307
"the similarities between words in the
character level model that makes sense.",01:04:23.307,01:04:27.620
"And it worked though initially as I say,
I was kind of surprised.",01:04:27.620,01:04:32.066
"Cuz it actually just sort surprised me,",01:04:32.066,01:04:33.952
"you can learn effective enough character
level embeddings to be able to do this.",01:04:33.952,01:04:37.730
"I think that's a nice clean version
of this model that works very nicely.",01:04:39.620,01:04:44.840
"I mean effectively different people
have put character level models",01:04:44.840,01:04:48.880
in in all sorts of ways.,01:04:48.880,01:04:50.920
"So there were slightly earlier
work that was calculating",01:04:50.920,01:04:54.590
"convolutions over characters
to generate word embeddings.",01:04:54.590,01:04:59.410
"I think this paper
the character-aware neural",01:04:59.410,01:05:03.291
"language models came up
earlier as a spotlight paper.",01:05:03.291,01:05:07.990
"And so this is a more recent and
very complex model where they're first",01:05:07.990,01:05:12.551
"of all doing convolutions and
then they've got a highway network.",01:05:12.551,01:05:16.969
"And then they've got an LSTM network,
cuz it's sort of very complex, but again,",01:05:16.969,01:05:21.029
it's a character level model.,01:05:21.029,01:05:22.596
"You can also do simple things,
so just going right back to",01:05:22.596,01:05:26.807
"the word2vec beginnings that you
can sort of start off with a model",01:05:26.807,01:05:31.722
"that has exactly the same objective and
loss function as word2vec.",01:05:31.722,01:05:37.330
"And just say well I'm going to at
the top level train my word2vec model.",01:05:37.330,01:05:41.980
But rather than storing a vector for,01:05:41.980,01:05:44.690
"each word and updating that,
I'm going to kind of like the CMU work.",01:05:44.690,01:05:49.830
"Say, I'm generating the representation for
each word using a character level LSTM.",01:05:49.830,01:05:56.467
"And then I'm feeding that into my skip
gram negative sampling algorithm and",01:05:56.467,01:06:01.911
that works very nicely as well.,01:06:01.911,01:06:04.220
"So lots of stuff of that sort, and
so yeah, so these days kind of,",01:06:05.380,01:06:09.806
"there's just a lot of action and
use of these character level models.",01:06:09.806,01:06:14.560
"And sort of many people are thinking
it's sort of less necessary to do word",01:06:14.560,01:06:18.714
level stuff.,01:06:18.714,01:06:19.520
"And so for my final bits, I just sort of
thought I'd show you again then back to",01:06:21.270,01:06:26.161
"a bit of neural machine translation
of a couple of the ways that people",01:06:26.161,01:06:30.527
"are incorporating these character
level models or sub word level models.",01:06:30.527,01:06:35.296
"So there are sort of two trends really,
one weight is to sort of build neural",01:06:35.296,01:06:41.058
"machine translation models which have
sub-word units, but the same architecture.",01:06:41.058,01:06:48.040
"And the other way is to have sort
of architectures explicitly put in",01:06:48.040,01:06:51.690
characters.,01:06:51.690,01:06:52.910
"And so I just wanna show you
one example of both of those.",01:06:52.910,01:06:56.400
"So one of idea that's
been quite prominent,",01:06:58.310,01:07:01.430
"which sort of gets back more to
having something like morphology,",01:07:01.430,01:07:05.630
"is this notion that's referred
to as byte pair encoding.",01:07:05.630,01:07:09.530
"And the name byte pair encoding
is kind of a misnomer, but",01:07:09.530,01:07:13.360
"it sort of comes from
the inspiration of this algorithm.",01:07:13.360,01:07:16.510
"So byte pair encoding is
a compression algorithm that has been",01:07:16.510,01:07:20.940
"developed quite separately,
just as a way to compress stuff.",01:07:20.940,01:07:24.240
"And the idea of byte pair encoding is
that you're learning a code book for",01:07:24.240,01:07:28.830
"compression by allocating
codes to common sequences.",01:07:28.830,01:07:33.730
"So you look for common pairs of bytes and
you allocate a code book place to them.",01:07:33.730,01:07:40.969
"And so someone had the idea,
maybe we could run this algorithm, but",01:07:40.969,01:07:45.213
"do it with character
ngrams rather than byte.",01:07:45.213,01:07:48.880
"And so this is how it works, so you
stop with the vocabulary of characters.",01:07:48.880,01:07:52.843
"And then you replace the most frequent
character ngram with the new ngram.",01:07:52.843,01:07:57.150
"So if our dictionary is like this we have
the words low, lower, newest, wildest.",01:07:57.150,01:08:02.108
"And they occur that often,",01:08:02.108,01:08:03.912
"we can then say well we start with a basic
character in the vocab, that's them.",01:08:03.912,01:08:09.570
"And so now, we're gonna look for
the commonest character bigram, and",01:08:09.570,01:08:13.679
"their allocated as a new
thing in our vocabulary.",01:08:13.679,01:08:16.571
"So here,
the commonest character bigram is es,",01:08:16.571,01:08:20.159
"that occurs nine times, so
we add that to our vocabulary.",01:08:20.159,01:08:24.454
"And then we look again and
say est that also occurs nine times,",01:08:24.454,01:08:28.592
let's add that to our vocabulary.,01:08:28.592,01:08:31.410
Then we ask what's still commonest?,01:08:31.410,01:08:33.971
"That's l, o seven times,
add that to our vocabulary, and so",01:08:33.971,01:08:38.891
you keep on doing this up to some limit.,01:08:38.891,01:08:42.028
"So you sort of say, okay, the size
vocabulary I wanna learn is 30,000 words.",01:08:42.028,01:08:49.315
"And so some of the things that's in your
vocabulary will actually end up as words.",01:08:49.315,01:08:54.208
"Cuz you will have sort of vocabulary
items like the and in and of,",01:08:54.208,01:08:57.972
if you're doing English.,01:08:57.972,01:08:59.850
"But the other things that you're
getting are just letters and",01:08:59.850,01:09:03.895
"word pieces that are kind of things
that are pieces of morphology.",01:09:03.895,01:09:08.262
"So it's kind of an empirical
way to learn a vocab.",01:09:08.262,01:09:11.467
"And again you have problem with unknown
words, cuz at the end of the day you",01:09:11.467,01:09:16.179
"have these sort of individual letter
that are part of your vocabulary.",01:09:16.179,01:09:20.750
"And so you can always do
things just with the letters.",01:09:20.750,01:09:24.191
"And so that kind of automatically
decide of vocab which is sort of",01:09:24.191,01:09:29.056
"no longer word base in
the conventional linguistic way.",01:09:29.056,01:09:33.394
"When you wanna translate
the piece of text,",01:09:33.394,01:09:36.577
"you just use that vocab and
you greedily chop from the left.",01:09:36.577,01:09:40.801
"You chop off pieces that
you can find in your vocab,",01:09:40.801,01:09:44.862
preferring the longest ones first.,01:09:44.862,01:09:47.798
"So it's a very simple way
to maintain a small vocab.",01:09:47.798,01:09:51.728
"But it was actually employed very
successfully by these people",01:09:51.728,01:09:54.881
at the university of Edinborough.,01:09:54.881,01:09:56.674
"And so at the 2016 workshop on machine
translation a number of the language",01:09:56.674,01:10:02.746
"pairs were won by the Edinborough
team using this byte pair encoding.",01:10:02.746,01:10:08.185
"And actually, it turns out that again,
if we say, gee,",01:10:08.185,01:10:11.575
"what is Google doing in their
neural machine translation system,",01:10:11.575,01:10:15.532
"that they're actually essentially using
a variant of byte pairing encoding?",01:10:15.532,01:10:20.284
"So they've got a slightly different
criterion of when to join letter sequences",01:10:20.284,01:10:24.804
"together that's more probabilistic
rather than just count based.",01:10:24.804,01:10:28.725
"And it's essentially the same kind
of byte pairing encoding idea.",01:10:28.725,01:10:33.910
"Okay, and so then the final bit that
I wanted to show you is some work",01:10:33.910,01:10:38.597
that Thang Luong and me did last year.,01:10:38.597,01:10:41.292
"Which was trying to get the benefits
of a character level system",01:10:41.292,01:10:46.229
"while also still allowing user
translate at the word level.",01:10:46.229,01:10:50.995
"And so the hope of this was to
gain the best of both worlds by",01:10:50.995,01:10:55.588
"having the performance of
a character level system while",01:10:55.588,01:11:00.187
"having the efficiency
of a word level system.",01:11:00.187,01:11:03.943
"So something I haven't mentioned so far,
is that even though you can get very good",01:11:03.943,01:11:08.499
"performance results by working
directly at the character level.",01:11:08.499,01:11:12.208
"A problem of it is it tends to
make things much, much slower.",01:11:12.208,01:11:16.820
"So if you sort of think back to
that example of saying when I said,",01:11:16.820,01:11:20.611
you can train a word2vec style model.,01:11:20.611,01:11:22.795
"But instead of having word vectors
you'd have a little LSTM where you",01:11:22.795,01:11:27.261
"build a word representation
from characters.",01:11:27.261,01:11:30.580
"You can do that, but",01:11:30.580,01:11:31.737
"the difference is we have to go
doing standard word2vec training.",01:11:31.737,01:11:35.631
"You're just looking up a word's
representation then computing with that.",01:11:35.631,01:11:40.076
"If you have to run a whole LSTM to
calculate the words representation,",01:11:40.076,01:11:44.647
"that's a much more
computationally expensive thing.",01:11:44.647,01:11:48.000
"So purely character-based models
tend to take well over an order of",01:11:48.000,01:11:52.800
"magnitude longer to train and
well over And a similar kind of slow",01:11:52.800,01:11:57.798
"down at runtime as well, because you sort
of are going character by character.",01:11:57.798,01:12:02.100
Yeah.,01:12:04.420,01:12:05.150
"So the question is how can you
evaluate character based models,",01:12:26.732,01:12:30.554
"can you sort of do the equivalent
of word similarity?",01:12:30.554,01:12:33.760
"I think there's nothing
that you can really",01:12:33.760,01:12:37.726
"do that directly does
it on the characters.",01:12:37.726,01:12:41.890
"But I mean,
you don't have to go all the way to say,",01:12:41.890,01:12:44.928
"let's run them in neural machine
translation system and see if it helps.",01:12:44.928,01:12:49.336
"I mean, the thing that people have done.",01:12:49.336,01:12:51.430
"And again, this has proven that
character based encoding works pretty",01:12:51.430,01:12:56.061
"successfully is to do word
similarity based evaluations.",01:12:56.061,01:12:59.934
"So you can say, okay,",01:12:59.934,01:13:01.447
"let's build up representations of
words using the character, LSTM and",01:13:01.447,01:13:06.312
"then see how good the result is as a word
similarity measure using the kind of word",01:13:06.312,01:13:11.654
"similarity metrics that we talked about
when we're doing things like word2vec.",01:13:11.654,01:13:17.343
"And I think the results are that it
doesn't work quite as well as the best",01:13:17.343,01:13:22.100
"results that people have gotten
from word-level models, but",01:13:22.100,01:13:26.303
it actually works pretty close.,01:13:26.303,01:13:28.840
"And so that's again, where it sort of
seems to having done this successfully and",01:13:28.840,01:13:32.533
"gives you the advantage to
having this open vocab.",01:13:32.533,01:13:34.885
"So, the question is how
well do the character",01:13:34.885,01:13:41.877
vectors generalize across tasks.,01:13:41.877,01:13:47.400
"Well, I think they generalize,",01:13:47.400,01:13:49.836
"as well as the kind of word vectors that
we train by something like word2vec.",01:13:49.836,01:13:55.500
"I mean, certainly people have used them
across tasks by doing different things",01:13:55.500,01:13:59.265
"like language modeling versus
part of speech tagging and",01:13:59.265,01:14:01.962
that's worked completely fine.,01:14:01.962,01:14:03.600
"So, this hybrid model.",01:14:03.600,01:14:10.000
"Again, it was a kind of a hierarchical
character and word based model.",01:14:10.000,01:14:13.300
"So, the heart of it in the middle
is that there's a word-level",01:14:13.300,01:14:18.157
neural machine translation system.,01:14:18.157,01:14:21.100
"And so, it was working over
a fixed moderate size vocabulary.",01:14:21.100,01:14:25.800
"And so,
that's going to have unknowns in it.",01:14:25.800,01:14:28.900
"So when you can sort of generate
an unknown vocabulary item and",01:14:28.900,01:14:32.773
"you can feed that through in
the generation time and encoding time,",01:14:32.773,01:14:37.018
"you're gonna have words for which there
are no word vectors which here we're",01:14:37.018,01:14:41.858
"having it be for cute though,
maybe cute would be in your vocabulary.",01:14:41.858,01:14:46.280
"So if you had a word not
in your word vocabulary,",01:14:46.280,01:14:49.970
"then what you're doing is then saying,
okay,",01:14:49.970,01:14:53.661
"well, we can do that by
having character level LSTMs.",01:14:53.661,01:14:57.930
"And so, there are two cases.",01:14:57.930,01:14:59.781
"So if a word is not in your input
vocabulary, you can just as",01:14:59.781,01:15:03.582
"a pre-processing step run a character LSTM
that generates a word representation for",01:15:03.582,01:15:09.171
"it and then you'll just say that's
it's word representation and",01:15:09.171,01:15:13.519
you can run it through the encoder.,01:15:13.519,01:15:16.470
"The situation was slightly
different on the decoder side.",01:15:16.470,01:15:19.490
"So on the decoder side,",01:15:19.490,01:15:21.131
"what would happen is the word-level LSTM
could chose to generate an unknown word.",01:15:21.131,01:15:26.886
"And if it generated an unknown word,
you then sort of took the hidden state",01:15:26.886,01:15:32.262
"representation and used that as
a starting point of another character",01:15:32.262,01:15:37.379
"level LSTM which could then generate
a word character by character.",01:15:37.379,01:15:42.432
And something that we didn't do for,01:15:42.432,01:15:44.337
"efficiency is you might think you'd want
to take the resulting meaning here and",01:15:44.337,01:15:48.590
"feed that back into the next timestamp and
that might have been a good idea, but",01:15:48.590,01:15:52.779
"we were interested in trying to
make this as fast as possible.",01:15:52.779,01:15:56.101
"So actually in this model,",01:15:56.101,01:15:57.964
"that representation was only
used to generate the word and",01:15:57.964,01:16:01.933
"it was still this sort of unk that was
being fed back into the next timestamp.",01:16:01.933,01:16:07.120
"So, standard kind of word-level
beam search and then you're doing",01:16:07.120,01:16:12.142
"a char-level beam search for
when you're spilling that one out.",01:16:12.142,01:16:17.004
"And so let's again, sort of showed
the strength of character level models.",01:16:17.004,01:16:22.630
"So, this shows some results
from English to Czech.",01:16:22.630,01:16:25.810
"And if you're wanting to
do character level models,",01:16:25.810,01:16:28.314
"Czechs are really good
language to look at,",01:16:28.314,01:16:30.371
"because of the fact that has a lot of
morphology and complex words forms.",01:16:30.371,01:16:33.780
"So, here are some results from machine
translation systems on English-Czech for",01:16:33.780,01:16:39.456
the 2015 workshop on machine translation.,01:16:39.456,01:16:43.110
"So the system that won the competition
in 2015, it got 18.8 BLEU.",01:16:43.110,01:16:49.896
"There had been an entry that did
sort of word-level neural machine",01:16:49.896,01:16:55.358
"translation in WMT 2015,
which didn't do quite as well.",01:16:55.358,01:17:00.540
"That's maybe not a condemnation of
word-level neural machine translation,",01:17:01.540,01:17:06.070
"cuz it turns out the winning system had
been trained on 30 times as much data and",01:17:06.070,01:17:10.598
was an ensemble of three systems.,01:17:10.598,01:17:12.570
"So, maybe the word level NMT
was pretty good by comparison.",01:17:12.570,01:17:17.828
"But nicely by putting in
this sort of hybrid word and",01:17:17.828,01:17:21.254
"character level system that, that was
able to sort of boost the performance of",01:17:21.254,01:17:26.512
"the NMT system trained on the same amount
of data as this one by 2.5 BLEU points,",01:17:26.512,01:17:32.009
"putting it well above
the performance of the best system.",01:17:32.009,01:17:35.932
"So, this then is just showing you one
example of the kind of places you can win",01:17:35.932,01:17:40.568
in Czech.,01:17:40.568,01:17:41.226
"So the source sentence is her 11-year
old daughter, Shani Bart said,",01:17:41.226,01:17:45.490
it felt a little bit weird.,01:17:45.490,01:17:47.440
"And so in Czech,",01:17:47.440,01:17:48.625
"11-year-old is turning into this
one morphologically complex word.",01:17:48.625,01:17:53.980
"And so if you're sort of running with
a medium size vocabulary in a neural MT",01:17:53.980,01:17:58.968
"system, well,
you're just gonna unks full of this stuff.",01:17:58.968,01:18:03.020
"And so, that's not very good.",01:18:03.020,01:18:04.410
"Well, one solution to that would be say,
hey, maybe we can copy and",01:18:04.410,01:18:08.138
sometimes that's useful.,01:18:08.138,01:18:09.746
"Cuz if you can copy Shani,
that's kinda good,",01:18:09.746,01:18:13.126
"cuz then you just get the right
word copied across, but",01:18:13.126,01:18:17.011
"doing copying isn't a good strategy
if you're copying across 11-year old.",01:18:17.011,01:18:22.790
"Because that sort of fails badly,
generating what you want.",01:18:22.790,01:18:27.394
"But nicely, if you're running
this hybrid word character model,",01:18:27.394,01:18:31.699
"you could actually get this word correct,
because it could actually just generate",01:18:31.699,01:18:36.985
"it character by character as an unknown
word and give you the correct answer.",01:18:36.985,01:18:41.935
"Tada, that is my central result for
the day.",01:18:41.935,01:18:48.695
"And so remember, get help on projects.",01:18:48.695,01:18:51.774
"Hope you have good luck finishing,
last lecture which is on Thursday.",01:18:51.774,01:18:56.240

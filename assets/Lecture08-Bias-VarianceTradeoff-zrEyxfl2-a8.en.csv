text,start,stop
"ANNOUNCER: The following program
is brought to you by Caltech.",00:00:00.570,00:00:03.270
YASER ABU-MOSTAFA: Welcome back.,00:00:15.410,00:00:18.410
"Last time, we finished the VC analysis.",00:00:18.410,00:00:22.370
And that took us three full lectures.,00:00:22.370,00:00:25.950
"The end result was the definition of the
VC dimension of a hypothesis set.",00:00:25.950,00:00:31.800
"It was defined as the most points that
the hypothesis set can shatter.",00:00:31.800,00:00:38.820
"And we used the VC dimension in
establishing that learning is",00:00:38.820,00:00:43.060
"feasible, on one hand, and then in
estimating the example resources that",00:00:43.060,00:00:47.540
are needed in order to learn.,00:00:47.540,00:00:50.540
"One of the important aspects of
the VC analysis is the scope.",00:00:50.540,00:00:55.430
"The VC inequality, and the generalization
bound that corresponds",00:00:55.430,00:00:59.690
"to it, describe the generalization
ability of the final hypothesis you",00:00:59.690,00:01:05.379
are going to pick.,00:01:05.379,00:01:07.320
"It describes that in terms of the VC
dimension of the hypothesis set, and",00:01:07.320,00:01:14.290
"makes a statement that is true for
all but delta of the data sets",00:01:14.290,00:01:19.560
that you might get.,00:01:19.560,00:01:20.820
So this is where it applies.,00:01:20.820,00:01:23.340
"And the most important part of the
application are the disappearing",00:01:23.340,00:01:27.910
"blocks, because it gives the generality
that the VC inequality has.",00:01:27.910,00:01:32.840
"So the VC bound is valid for any
learning algorithm, for any input",00:01:32.840,00:01:39.040
"distribution that may take place, and
also for any target function that you",00:01:39.040,00:01:44.960
may be able to learn.,00:01:44.960,00:01:47.530
So this is the most theoretical part.,00:01:47.530,00:01:49.120
"And then we went into a little bit of
a practical part, where we are asking",00:01:49.120,00:01:52.920
"about the utility of the VC
dimension in practice.",00:01:52.920,00:01:55.850
"You have a learning problem-- someone
comes with a problem, and you would",00:01:55.850,00:01:59.420
"like to know how many examples. What is
the size of the data set you need,",00:01:59.420,00:02:03.360
"in order to be able to achieve
a certain level of performance.",00:02:03.360,00:02:06.840
"The way we did this analysis is by
plotting the core aspect of the delta,",00:02:06.840,00:02:11.980
"the probability of error
in the VC bound.",00:02:11.980,00:02:15.060
"And we found that it's
behaving regularly.",00:02:15.060,00:02:17.790
"We focused on a certain aspect of these
curves, which correspond to",00:02:17.790,00:02:21.860
different VC dimensions.,00:02:21.860,00:02:23.960
"And the main aspect is
below this line.",00:02:23.960,00:02:26.720
"This line designates
the probability 1.",00:02:26.720,00:02:29.340
"We want the probability of the bad event
to be small, so we are working",00:02:29.340,00:02:33.080
in this region.,00:02:33.080,00:02:34.570
"And the x-axis here is the number
of examples-- the size",00:02:34.570,00:02:39.320
of your data set.,00:02:39.320,00:02:40.930
"And we don't particularly care about
the shape of these guys.",00:02:40.930,00:02:43.770
"They could be a little bit
nonlinear, et cetera.",00:02:43.770,00:02:46.280
"But the quantity we are looking for is,
if we cut through this way, what is",00:02:46.280,00:02:54.440
"the behavior of the x-axis, the number
of examples, in terms of the VC",00:02:54.440,00:02:59.280
"dimension, which is the label
for the colored curves?",00:02:59.280,00:03:02.640
"And we realized that, given this analysis,
it is very much proportional.",00:03:02.640,00:03:07.780
"And we were able to say that,
theoretically, the bound will give us",00:03:07.780,00:03:11.620
"that the number of examples needed
would be proportional to the VC",00:03:11.620,00:03:14.000
"dimension, more or less.",00:03:14.000,00:03:16.380
"And although the constant of
proportionality, if you go for the",00:03:16.380,00:03:21.450
"bound, will be horrifically
pessimistic--",00:03:21.450,00:03:24.480
"you will end up requiring tens of
thousands of examples for something",00:03:24.480,00:03:27.660
"for which you really need
only maybe 50 examples--",00:03:27.660,00:03:30.890
"the good news is that the actual
quantity behaves in the",00:03:30.890,00:03:33.890
same way as the bound.,00:03:33.890,00:03:35.330
"So the number of examples needed
is, in practice, as a practical",00:03:35.330,00:03:38.440
"observation, indeed proportional
to the VC dimension.",00:03:38.440,00:03:41.950
"And furthermore, as a rule of thumb,
in order to get to the interesting",00:03:41.950,00:03:45.670
"part, or interesting delta and epsilon,
you need the number of",00:03:45.670,00:03:49.660
"examples to be 10 times
the VC dimension.",00:03:49.660,00:03:52.870
More will be better.,00:03:52.870,00:03:54.110
Less might work.,00:03:54.110,00:03:55.240
"But the ballpark of it is that you have
a factor of 10, in order to start",00:03:55.240,00:03:58.990
"getting interesting generalization
properties.",00:03:58.990,00:04:03.050
"We ended with summarizing the entire
theoretical analysis into a very",00:04:03.050,00:04:07.020
"simple bound, which we are referring
to as the generalization bound, that",00:04:07.020,00:04:11.110
"tells us a bound on the out-of-sample
performance given the in-sample",00:04:11.110,00:04:14.510
performance.,00:04:14.510,00:04:16.060
"And that involved adding
a term, capital Omega.",00:04:16.060,00:04:20.930
"And Omega captures all the
theoretical analysis we had.",00:04:20.930,00:04:24.210
"It's a function of N, function of the
hypothesis set through the VC",00:04:24.210,00:04:26.920
"dimension, function of your tolerance
for probability of error, which is delta.",00:04:26.920,00:04:33.640
"And although this is a bound, we keep
saying that, in reality, E_out will",00:04:33.640,00:04:38.860
"be equal to E_in plus something
that behaves like Omega.",00:04:38.860,00:04:42.240
"And we will take advantage of that,
when we get a technique like",00:04:42.240,00:04:44.750
regularization.,00:04:44.750,00:04:46.090
"So that's the end of the VC analysis,
which is the biggest part of the",00:04:46.090,00:04:50.060
theory here.,00:04:50.060,00:04:51.600
"And we are going to switch today to
another approach, which is the",00:04:51.600,00:04:55.110
bias-variance tradeoff.,00:04:55.110,00:04:57.060
It's a stand-alone theory.,00:04:57.060,00:04:59.320
"It gives us a different angle
on generalization.",00:04:59.320,00:05:02.340
"And I am going to cover it, beginning
to end, during this lecture.",00:05:02.340,00:05:06.540
This is the plan.,00:05:06.540,00:05:09.060
The outline is very simple.,00:05:09.060,00:05:10.540
"We are going to talk about the bias
and variance, define them, see the",00:05:10.540,00:05:13.610
"tradeoff, take a very detailed example--
one particular example-- in",00:05:13.610,00:05:18.530
"order to demonstrate what the
bias and variance are.",00:05:18.530,00:05:21.950
"And then we are going to introduce
a very interesting tool for illustrating",00:05:21.950,00:05:26.310
"learning, which are called
learning curves.",00:05:26.310,00:05:28.360
"And we are going to contrast the
bias-variance analysis versus the VC",00:05:28.360,00:05:31.920
"analysis on these learning curves, and
then apply them to the linear",00:05:31.920,00:05:35.270
"regression case that we
are familiar with.",00:05:35.270,00:05:37.460
So this is the plan.,00:05:37.460,00:05:38.486
"The first part is the
bias and variance.",00:05:38.486,00:05:42.690
"In the big picture, we have been trying
to characterize a tradeoff.",00:05:42.690,00:05:47.790
"And roughly speaking, the tradeoff
is between approximation and",00:05:47.790,00:05:52.740
generalization.,00:05:52.740,00:05:53.620
"So let me discuss this for a moment,
before we put bias and variance into",00:05:53.620,00:05:58.320
the picture.,00:05:58.320,00:05:59.940
We would like to get to small E_out.,00:05:59.940,00:06:02.640
That's the purpose of learning.,00:06:02.640,00:06:03.960
"If E_out is small, then
you have learned.",00:06:03.960,00:06:05.970
"You have a hypothesis that approximates
the target function well.",00:06:05.970,00:06:10.060
"There are two components to this, and
we are very familiar with them now.",00:06:10.060,00:06:14.340
"We are looking for a good
approximation of f.",00:06:14.340,00:06:17.600
That's the approximation part.,00:06:17.600,00:06:19.440
"But we would like that approximation
to hold out-of-sample.",00:06:19.440,00:06:24.960
"We are not going to be happy if we
approximate f well in-sample, and we",00:06:24.960,00:06:29.910
behave badly out-of-sample.,00:06:29.910,00:06:31.510
These are the two components.,00:06:31.510,00:06:34.870
"In the case of a more complex
hypothesis set, you are going to have",00:06:34.870,00:06:41.550
"a better chance of approximating
f, obviously.",00:06:41.550,00:06:44.830
I have more hypotheses to run around.,00:06:44.830,00:06:46.400
"I'll be able to find one of
them that is closer to the",00:06:46.400,00:06:49.220
target function I want.,00:06:49.220,00:06:50.970
"The problem is that, if you have the
bigger hypothesis set, you are going",00:06:50.970,00:06:57.020
"to have a problem identifying
the good hypothesis.",00:06:57.020,00:07:01.620
"That is, if you have fewer hypotheses,
you have a better chance of",00:07:01.620,00:07:06.130
generalization.,00:07:06.130,00:07:07.160
"And one way to look at it is
that, I'm trying to",00:07:07.160,00:07:10.010
approximate the target function.,00:07:10.010,00:07:11.190
You give me a hypothesis set.,00:07:11.190,00:07:13.120
"Now, if I tell you I have
good news for you.",00:07:13.120,00:07:16.470
"The target function is actually
in the hypothesis set.",00:07:16.470,00:07:20.340
"You have the perfect approximation
under your control.",00:07:20.340,00:07:24.870
"Well, it's under your hand, but not
necessarily under your control.",00:07:24.870,00:07:29.280
"Because you still have to navigate
through the hypothesis set in order to",00:07:29.280,00:07:32.670
find the good candidate.,00:07:32.670,00:07:34.390
"And the way you navigate is
through the data set.",00:07:34.390,00:07:37.650
"That is your only resource for finding
one hypothesis versus the other.",00:07:37.650,00:07:42.360
"So the target function could be
sitting there calling for you.",00:07:42.360,00:07:46.650
"Please, I am the target
function, come.",00:07:46.650,00:07:48.360
But you can't see it.,00:07:48.360,00:07:49.560
"You're just navigating with the training
set, you have very limited resources,",00:07:49.560,00:07:52.770
"and you end up with something
that is really bad.",00:07:52.770,00:07:54.880
"Having f in the hypothesis set
is great for approximation.",00:07:59.140,00:08:01.850
"But having a big hypothesis set, that is
big enough to include that, may be",00:08:01.850,00:08:05.500
"bad news, because you will
not be able to get it.",00:08:05.500,00:08:09.080
"Now if you think about it, what is the
ideal hypothesis set for learning?",00:08:09.080,00:08:14.030
"If I only had a hypothesis set that has
a singleton hypothesis, which",00:08:14.030,00:08:18.100
"happens to be the target function, then
I have the best of both worlds.",00:08:18.100,00:08:21.020
The perfect approximation.,00:08:21.020,00:08:22.450
"I will zoom in directly, because
it is only one.",00:08:22.450,00:08:25.170
"Well, you might as well go
and buy a lottery ticket.",00:08:25.170,00:08:29.920
That's the equivalent.,00:08:29.920,00:08:32.090
"We don't know the target function, so
we will have to make the hypothesis set",00:08:32.090,00:08:35.559
big enough to stand a chance.,00:08:35.559,00:08:37.020
"And once we do that, then the question
of generalization kicks in.",00:08:37.020,00:08:40.270
This is this big picture.,00:08:40.270,00:08:41.340
"So let's try to fit the VC analysis in
it, and then fit the bias-variance",00:08:41.340,00:08:46.000
"analysis in it, before we even know what
the bias-variance analysis is, in",00:08:46.000,00:08:49.730
"order to see where we
are going with this.",00:08:49.730,00:08:52.740
So we are quantifying this tradeoff.,00:08:52.740,00:08:55.060
"And the quantification, in the case
of the VC analysis, was what?",00:08:55.060,00:08:59.320
Was the generalization bound.,00:08:59.320,00:09:02.760
E_in is approximation.,00:09:04.560,00:09:05.990
"Because I am actually trying to fit the
target function-- I am just fitting",00:09:05.990,00:09:09.850
them on the sample.,00:09:09.850,00:09:11.310
That's the restriction here.,00:09:11.310,00:09:13.030
"So if I get this well, then I'm
approximating f well, at",00:09:13.030,00:09:15.980
least on some points.,00:09:15.980,00:09:18.130
This guy is purely generalization.,00:09:18.130,00:09:20.520
"The question is, how do you generalize
from in-sample to out-of-sample?",00:09:20.520,00:09:24.750
So this is a way of quantifying it.,00:09:24.750,00:09:27.530
"Now the bias-variance analysis
has another approach.",00:09:27.530,00:09:30.950
"It also decomposes E_out, as you did
in the generalization bound.",00:09:30.950,00:09:36.850
"But it does decompose it into
two different entities.",00:09:36.850,00:09:41.730
"The first one is an approximation
entity, how well H can approximate f.",00:09:41.730,00:09:46.590
"Well, what is the difference then?",00:09:46.590,00:09:48.580
"The difference is that the bias-variance
asks the question, how",00:09:48.580,00:09:51.680
"can H approximate f, overall?",00:09:51.680,00:09:55.170
"Not on your sample. In reality. As if
you had access to the target function,",00:09:55.170,00:10:00.600
"and you are stuck with this hypothesis
set, and you are eagerly looking for",00:10:00.600,00:10:04.800
"which hypothesis best describes
the target function.",00:10:04.800,00:10:08.240
"And then you quantify how well that best
hypothesis performs, and that is",00:10:08.240,00:10:13.370
"your measure of the approximation
ability.",00:10:13.370,00:10:17.270
Then what is the other component?,00:10:17.270,00:10:19.650
"The other component is exactly
what I alluded to.",00:10:19.650,00:10:22.490
Can you zoom in on it?,00:10:22.490,00:10:25.660
"So this is the best hypothesis, and it
has a certain approximation ability.",00:10:25.660,00:10:30.130
"Now I need to pick it, so I have to use
the examples in order to zoom in",00:10:30.130,00:10:34.230
"into the hypothesis set, and
pick this particular one.",00:10:34.230,00:10:37.080
Can I zoom in on it?,00:10:37.080,00:10:38.610
"Or do I get something that is a poor
approximation of the approximation?",00:10:38.610,00:10:44.180
"And that decomposition will
give us the bias-variance.",00:10:44.180,00:10:46.600
"And we'll be able to put them at the
end of the lecture, side by side, in",00:10:46.600,00:10:49.930
"order to compare: here is what the VC
analysis does, and here is what",00:10:49.930,00:10:53.020
bias-variance does.,00:10:53.020,00:10:54.270
"The analysis, from a mathematical
point of view, applies",00:10:59.080,00:11:01.330
to real-valued targets.,00:11:01.330,00:11:02.760
And that's good news.,00:11:02.760,00:11:03.570
"Because remember, in the VC analysis, we
were confined to binary functions",00:11:03.570,00:11:08.210
in the particular analysis that I did.,00:11:08.210,00:11:10.020
"You can extend it, but
it's very technical.",00:11:10.020,00:11:12.340
"So it's a good idea to see the same
tradeoff, and the same generalization",00:11:12.340,00:11:16.050
"questions, apply to real-valued functions.",00:11:16.050,00:11:18.180
"Now we have regression, and we are
able to make a statement about",00:11:18.180,00:11:21.710
"generalization on regression, which we
will apply very specifically to linear",00:11:21.710,00:11:25.820
"regression, the model that we already
studied that has real-valued outputs.",00:11:25.820,00:11:32.060
"And we are going to confine the analysis
here to squared error.",00:11:32.060,00:11:37.790
"The reason we are doing this is that, for
the math to go through in such",00:11:37.790,00:11:41.930
"a way that these two guys decompose
cleanly-- there are no cross terms, we",00:11:41.930,00:11:47.210
will need the squared error.,00:11:47.210,00:11:49.480
"So this is a restriction
of the analysis.",00:11:49.480,00:11:51.770
There are ways to extend it.,00:11:51.770,00:11:53.240
"They are not as clean, so this
is the simplest form that we",00:11:53.240,00:11:56.150
are going to use.,00:11:56.150,00:11:57.270
Let's start.,00:11:57.560,00:12:00.050
Our starting point is E_out.,00:12:00.050,00:12:01.670
So let me put it--,00:12:01.670,00:12:02.920
Don't worry about the gap.,00:12:05.360,00:12:06.640
The gap here will be filled.,00:12:06.640,00:12:09.250
What do we have?,00:12:09.250,00:12:10.095
We have E_out.,00:12:10.095,00:12:11.250
"E_out depends on the hypothesis
you pick.",00:12:11.250,00:12:13.280
"E_out is E_out of your
final hypothesis.",00:12:13.280,00:12:15.710
"How does it perform on
the overall space?",00:12:15.710,00:12:19.690
"And in order to do that, since we are
talking about squared error, you are",00:12:19.690,00:12:23.150
"going to take the value of your
hypothesis, and compare it to the value",00:12:23.150,00:12:26.990
"of the target function,
and take that squared.",00:12:26.990,00:12:28.700
And that will be your error.,00:12:28.700,00:12:30.480
"So this is the building block for
getting the out-of-sample performance.",00:12:30.480,00:12:34.910
"Now the gap here comes from the fact
that, if you look at the final",00:12:34.910,00:12:39.660
"hypothesis, the final hypothesis
depends on a number of things.",00:12:39.660,00:12:43.670
"Among other things, it does depend
on the data set that I'm",00:12:43.670,00:12:47.080
"going to give you, right?",00:12:47.080,00:12:49.550
"Because if I give you a different data
set, you'll find a different final",00:12:49.550,00:12:52.760
hypothesis.,00:12:52.760,00:12:54.220
"That dependency is quite important
in the bias-variance analysis.",00:12:54.220,00:12:58.190
"Therefore, I am going to make
it explicit in the notation.",00:12:58.190,00:13:00.980
"It has always been there, but I didn't
need to carry ugly notation throughout,",00:13:00.980,00:13:05.100
when I'm not using it.,00:13:05.100,00:13:06.170
"Here I'm using it, so we'll
have to live with it.",00:13:06.170,00:13:08.770
"So now I'll make that
dependency explicit.",00:13:08.770,00:13:12.180
"I'm having now a superscript, which
tells me that this g comes from that",00:13:12.180,00:13:16.530
particular data set.,00:13:16.530,00:13:18.120
"If you give me another data set, this
will be a different g.",00:13:18.120,00:13:20.940
"And you take the same g, apply it to x,
compare it to f, and this is your error.",00:13:20.940,00:13:26.280
"And finally, in order for it to be
genuinely out-of-sample error, you",00:13:26.280,00:13:29.740
"need to get the expected value of that
error over the entire space.",00:13:29.740,00:13:33.730
So this is what we have.,00:13:33.730,00:13:35.640
"Now what we would like to do, we would
like to see a decomposition of this",00:13:35.640,00:13:39.720
"quantity into the two conceptual
components, approximation and",00:13:39.720,00:13:44.780
"generalization, that we saw.",00:13:44.780,00:13:46.740
So here's what we are going to do.,00:13:46.740,00:13:50.270
"We are going to take this quantity,
which equals this quantity, as I",00:13:50.270,00:13:55.170
"mentioned here, and then realize
that this depends on the",00:13:55.170,00:13:59.510
particular data set.,00:13:59.510,00:14:02.420
"I would like to rid this from the
dependency on the specific data set",00:14:02.420,00:14:07.420
that I give you.,00:14:07.420,00:14:08.710
"So I'm going to play
the following game.",00:14:08.710,00:14:11.160
"I am going to give you a budget of
N examples, training examples",00:14:11.160,00:14:16.110
to learn from.,00:14:16.110,00:14:18.350
"If I give you that budget N, I could
generate one D and another D and",00:14:18.350,00:14:22.360
"another D, each of them
with N examples.",00:14:22.360,00:14:25.620
"Each of them will result in a different
hypothesis g, and each of",00:14:25.620,00:14:30.220
"them will result in a different
out-of-sample error.",00:14:30.220,00:14:33.180
Correct?,00:14:33.180,00:14:34.460
"So if I want to get rid of the
dependency on the particular sample",00:14:34.460,00:14:37.860
"that I give you, and just know the
behavior-- if I give you N data points,",00:14:37.860,00:14:41.640
"what happens?-- then I would
like to integrate D out.",00:14:41.640,00:14:45.720
"So I am going to get the expected value
of that error, with respect to D.",00:14:45.720,00:14:51.606
"This is not a quantity that
you are going to encounter",00:14:51.606,00:14:53.900
in any given situation.,00:14:53.900,00:14:55.110
"In any given situation, you have
a specific data set to work with.",00:14:55.110,00:14:59.280
"However, if I want to analyze the
general behavior--",00:14:59.280,00:15:03.020
"someone comes to my door, how many
examples do you have, and they tell me",00:15:03.020,00:15:06.410
100. I haven't seen the examples yet.,00:15:06.410,00:15:08.930
"So it stands to logic that I say,
for 100 examples, the following",00:15:08.930,00:15:12.350
behavior follows.,00:15:12.350,00:15:13.370
"So I must be taking an expected value
with respect to all possible",00:15:13.370,00:15:16.550
realizations of 100 examples.,00:15:16.550,00:15:18.770
"And that is, indeed, what
I am going to do.",00:15:18.770,00:15:20.910
"I'm going to get the expected
value of that.",00:15:20.910,00:15:22.670
"And this is the quantity that
I am going to decompose.",00:15:22.670,00:15:25.350
"And this obviously happens to be the
expected value of the other guy, and",00:15:25.350,00:15:28.670
we have that.,00:15:28.670,00:15:29.920
"Now I am going to take this quantity,
the expression for the quantity that",00:15:32.200,00:15:36.260
"I'm interested in, and keep deriving
stuff until I get to the",00:15:36.260,00:15:40.870
decomposition I want.,00:15:40.870,00:15:42.690
"The first order of business,
I have two expectations.",00:15:42.690,00:15:46.020
"The first thing I'm going to do, I am
going to reverse the order of the",00:15:46.020,00:15:48.600
expectations.,00:15:48.600,00:15:51.310
Why can I do that?,00:15:51.310,00:15:52.830
I am integrating.,00:15:52.830,00:15:54.160
"So now I change the order
of integration.",00:15:54.160,00:15:55.750
"I am allowed to do that, because the
integrand is strictly non-negative.",00:15:55.750,00:16:01.150
So I get this.,00:16:01.150,00:16:02.520
"And the reason for that is because
I am really interested in the",00:16:02.520,00:16:05.345
"expectation with respect to D, and I'd
rather not carry the expectation with",00:16:05.345,00:16:09.020
respect to x throughout.,00:16:09.020,00:16:10.710
"So I am going to get rid of that
expectation for a while, until I get",00:16:10.710,00:16:14.170
a clean decomposition.,00:16:14.170,00:16:15.430
"And when I get the clean decomposition,
I'll go back and get",00:16:15.430,00:16:17.920
"the expectation, just to
keep the focus clear.",00:16:17.920,00:16:21.050
You focus on the inside quantity.,00:16:24.540,00:16:27.530
"If I give you the expression for the
inside quantity for any x, then all",00:16:27.530,00:16:30.920
"you need to do in order to get the
quantity that you need, is get the",00:16:30.920,00:16:33.310
"expected value of what I
said, with respect to x.",00:16:33.310,00:16:36.270
"So this is the quantity that we are
going to carry to the next slide.",00:16:36.270,00:16:40.000
Let's do that.,00:16:40.000,00:16:42.350
"And the main notion, in order to evaluate
this quantity, is the notion",00:16:42.350,00:16:46.690
of an average hypothesis.,00:16:46.690,00:16:48.210
It's a pretty interesting idea.,00:16:48.210,00:16:50.880
Here is the idea.,00:16:50.880,00:16:53.530
"You have a hypothesis set, and you are
learning from a particular data set.",00:16:53.530,00:16:58.400
"And I am going to define
a particular hypothesis.",00:16:58.400,00:17:02.120
"I am going to call it the
average hypothesis.",00:17:02.120,00:17:04.619
"And because it's average, I am going
to give it a bar notation.",00:17:04.619,00:17:08.319
So what is this fellow?,00:17:08.319,00:17:10.790
"Well, this fellow is
defined as follows.",00:17:10.790,00:17:15.410
You learn from a data set.,00:17:15.410,00:17:17.230
You get a hypothesis.,00:17:17.230,00:17:18.319
"Someone else learns from
another data set.",00:17:18.319,00:17:20.150
"They get another hypothesis, et cetera.",00:17:20.150,00:17:22.480
"So how about getting the expected
value of these hypotheses?",00:17:22.480,00:17:26.849
What does that formally mean?,00:17:26.849,00:17:28.900
We have x fixed.,00:17:28.900,00:17:30.220
"So we actually are in a good position,
because g of x is really just a random",00:17:30.220,00:17:34.860
variable at this point.,00:17:34.860,00:17:35.920
"It's a random variable, determined
by the choice of your data.",00:17:35.920,00:17:39.250
The data is the randomization source.,00:17:39.250,00:17:41.080
"x is fixed, so you think I have one
test point in the space, that I'm",00:17:41.080,00:17:44.480
interested in.,00:17:44.480,00:17:46.130
"Maybe you are playing the stock market,
and now you are only interested",00:17:46.130,00:17:49.190
in what's going to happen tomorrow.,00:17:49.190,00:17:50.600
"So you take the inputs, and these are
the only inputs you're interested in",00:17:50.600,00:17:53.360
performing on.,00:17:53.360,00:17:54.320
That's your x.,00:17:54.320,00:17:55.770
"And all of the questions
now pertain to this.",00:17:55.770,00:17:57.610
You are learning from other data.,00:17:57.610,00:17:58.650
"And then you ask yourself, how am
I performing on this point?",00:17:58.650,00:18:00.920
That is the point x.,00:18:00.920,00:18:02.880
Now you are looking at this point.,00:18:02.880,00:18:03.900
"And you say, if you give me a data set
versus another, I am going to",00:18:03.900,00:18:06.740
"get different values for the
hypothesis on that point.",00:18:06.740,00:18:10.510
"It stands to logic that, if I take the
average with respect to all possible",00:18:10.510,00:18:14.280
"data sets, that would be awesome.",00:18:14.280,00:18:16.790
"Because now I am getting the benefit
of an infinite number of data sets.",00:18:16.790,00:18:20.940
"I am using them in the capacity
of one data set at a time.",00:18:20.940,00:18:24.040
But I am getting value.,00:18:24.040,00:18:25.000
"Maybe the correct value
should be here.",00:18:25.000,00:18:27.030
"But since I am getting fluctuations
because of the data set, sometimes I'm",00:18:27.030,00:18:30.070
"here, sometimes I'm here, et cetera.",00:18:30.070,00:18:31.670
"If you get the expected value,
you will get it right.",00:18:31.670,00:18:34.740
"So this looks like a great
quantity to have.",00:18:34.740,00:18:36.610
"And in reality, we will
never have that.",00:18:36.610,00:18:39.340
"Because if you give me an infinite
number of examples, I'm not going to",00:18:39.340,00:18:41.610
"divide them neatly into N and N
and N, and learn from these, and",00:18:41.610,00:18:45.500
then take the average.,00:18:45.500,00:18:46.230
"I'm just going to take all your examples,
and learn all through and get",00:18:46.230,00:18:48.860
the target function almost perfectly.,00:18:48.860,00:18:51.570
"So this is just a conceptual tool
for us to do the analysis.",00:18:51.570,00:18:54.550
But we understand what it is.,00:18:54.550,00:18:55.850
"If you now vary x, your test 
point in general, then you take that",00:18:56.440,00:19:00.800
"random variable and the expected value
of it, and the function that is",00:19:00.800,00:19:04.050
"constituted by the expected values at
different points is your g bar.",00:19:04.050,00:19:09.640
So this is understood.,00:19:09.640,00:19:10.970
Why do I need this for the analysis?,00:19:10.970,00:19:12.540
"Because if you look at the top thing,
I have here squared, so I'm",00:19:12.540,00:19:15.950
probably going to expand it.,00:19:15.950,00:19:18.910
"And in expanding it, I am just going
to get a linear term of this.",00:19:18.910,00:19:22.750
And I have an expected value.,00:19:22.750,00:19:24.300
"So you can see that I'm going to
get something that requires me",00:19:24.300,00:19:27.220
to define g bar.,00:19:27.220,00:19:28.910
That's the technical utility here.,00:19:28.910,00:19:31.010
"But the conceptual utility
is very important.",00:19:31.010,00:19:33.210
"And if you want to tell someone
what g bar is, think that you",00:19:33.210,00:19:36.460
"have many, many data sets.",00:19:36.460,00:19:38.220
"And the game is such that you learn from
one data set at a time, and you",00:19:38.220,00:19:41.630
"want to make the most of
it after you learn.",00:19:41.630,00:19:43.580
What do you do?,00:19:43.580,00:19:44.210
You take votes.,00:19:44.210,00:19:45.040
You take just the average.,00:19:45.040,00:19:46.330
You have this.,00:19:46.330,00:19:47.870
"There is 1 over K here,
the size of those.",00:19:47.890,00:19:52.550
So this is the average.,00:19:52.550,00:19:53.800
"Now let's see how we can use
g bar, in order to get the",00:19:56.210,00:19:59.480
decomposition we want.,00:19:59.480,00:20:00.740
"Here, this is again the quantity I'm
passing from one slide to another,",00:20:04.730,00:20:08.970
in order not to forget.,00:20:08.970,00:20:10.390
"This is the quantity that I'd like
to decompose.",00:20:10.390,00:20:12.550
"The first thing I am going to do, I am
going to make it longer, by the simple",00:20:12.550,00:20:17.200
"trick of adding g bar
and subtracting it.",00:20:17.200,00:20:21.260
"I'm allowed to do that, right?",00:20:21.260,00:20:23.010
"Doing that, I am going to
consolidate these two terms,",00:20:23.800,00:20:27.890
"and I'm going to consolidate
these two terms. And then",00:20:27.890,00:20:31.990
expand with the squared.,00:20:31.990,00:20:33.280
So let's do that.,00:20:33.280,00:20:36.180
You get this.,00:20:36.180,00:20:37.450
"This is the first consolidated
guy with the squared.",00:20:37.450,00:20:42.040
"This is the second consolidated
guy with the squared.",00:20:42.040,00:20:44.930
Am I missing something?,00:20:44.930,00:20:46.590
"Yes, I am missing the cross terms.",00:20:46.590,00:20:48.760
So let's add the cross terms.,00:20:48.760,00:20:51.170
And I get twice the product.,00:20:51.170,00:20:53.760
This equals that.,00:20:57.360,00:20:58.610
"So the expected value here applies
to the whole thing.",00:21:01.180,00:21:06.020
"The first order of business is to look
at the cross terms, because they are",00:21:06.020,00:21:08.780
"annoying, and see if I
can get rid of them.",00:21:08.780,00:21:12.090
"That's where the benefit of
the squared error comes in.",00:21:12.090,00:21:18.070
"I am getting the expected value
with respect to D, right?",00:21:18.070,00:21:22.390
So this fellow is a constant.,00:21:22.390,00:21:28.620
"Therefore, when I get the expected value
of this whole thing, all I need",00:21:28.620,00:21:32.470
"to do is get the expected value of
this part, because this one will",00:21:32.470,00:21:35.540
factor out.,00:21:35.540,00:21:37.730
"Now, if I get the expected value of this,
the expected value of the sum is",00:21:37.730,00:21:40.660
"the sum of the expected values-- one of
the few universal rules that you can",00:21:40.660,00:21:44.980
"apply, without asking any
other questions.",00:21:44.980,00:21:47.360
So I get the expected value of this.,00:21:47.360,00:21:49.080
What is the expected value of g^D?,00:21:49.080,00:21:52.030
Wait a minute.,00:21:52.030,00:21:52.850
"That was g bar, by definition.",00:21:52.850,00:21:54.340
"That's how we defined it, right?",00:21:54.340,00:21:56.260
"So I get g bar, minus the expected
value of a constant, which",00:21:56.260,00:21:59.950
happens to be g bar.,00:21:59.950,00:22:01.015
"So this goes to 0, and happily
this guy goes away.",00:22:01.015,00:22:06.280
Now I have only these two guys.,00:22:06.280,00:22:07.740
So let's write them.,00:22:07.740,00:22:11.880
"I have the expected value of this whole
thing, which again is the sum",00:22:11.880,00:22:15.450
of the expected values.,00:22:15.450,00:22:17.720
"The first guy is a genuine expected
value of these guys.",00:22:17.720,00:22:21.150
"When I apply expected value to this
guy, again this is just a constant, so",00:22:21.150,00:22:24.840
the expected value of it is itself.,00:22:24.840,00:22:27.660
"The second guy I add without bothering
with the expected value, because it's",00:22:27.660,00:22:30.950
just a constant.,00:22:30.950,00:22:32.410
"So this is what I have as
the expression for the",00:22:32.410,00:22:34.970
quantity that I want.,00:22:34.970,00:22:37.350
"Now let's take this and look at it
closely, because this will be the bias",00:22:37.350,00:22:40.520
and variance.,00:22:40.520,00:22:41.770
"This is the quantity again,
and it equals this fellow.",00:22:44.930,00:22:50.960
"Now let's look beyond the math,
and understand what's going on.",00:22:50.960,00:22:54.710
"This measure-- this quantity-- tells you
how far your hypothesis, that you got",00:22:54.710,00:23:02.520
"from learning on a particular
data set, differs from the",00:23:02.520,00:23:06.540
"ultimate thing, the target.",00:23:06.540,00:23:10.070
"And we are decomposing
this into two steps.",00:23:10.070,00:23:13.230
"The first step is to ask you, how far
is your hypothesis that you got from",00:23:13.230,00:23:18.800
"that particular data set, from the best
possible you can get using your",00:23:18.800,00:23:24.950
hypothesis set?,00:23:24.950,00:23:27.890
"Now there is a leap here, because
I don't know whether this is",00:23:27.890,00:23:33.240
the best in the hypothesis set.,00:23:33.240,00:23:34.610
I got it by the averaging.,00:23:34.610,00:23:35.150
"But since I'm averaging from several
data sets, it looks like a pretty good",00:23:35.150,00:23:38.840
hypothesis.,00:23:38.840,00:23:39.920
"I am not even sure that it's actually
in the hypothesis set.",00:23:39.920,00:23:42.770
"It's the average of guys that came
from the hypothesis set.",00:23:42.770,00:23:45.790
"But I can definitely construct
hypothesis sets, where the average of",00:23:45.790,00:23:49.090
"hypotheses does not necessarily
belong there.",00:23:49.090,00:23:51.240
So there are some funny stuff.,00:23:51.240,00:23:52.860
"But just think of it that, this
is an intermediate step.",00:23:52.860,00:23:56.150
"Instead of going all the way to the
target function, here is your",00:23:56.150,00:23:58.420
hypothesis set.,00:23:58.420,00:23:59.460
It restricts your resources.,00:23:59.460,00:24:01.170
"Now I am getting the best possible
out of it, based on some formula.",00:24:01.170,00:24:03.870
"I'm learning from infinite
number of data sets.",00:24:03.870,00:24:06.940
This is a pretty good hypothesis.,00:24:06.940,00:24:08.300
"So how far are you from
that hypothesis?",00:24:08.300,00:24:10.030
That's the first step.,00:24:10.030,00:24:11.710
"The second step is how far that
hypothesis, that great hypothesis, is",00:24:11.710,00:24:17.520
from the ultimate target function.,00:24:17.520,00:24:19.980
"So hopping from your guy to the target,
goes into a small hop from your guy to",00:24:19.980,00:24:26.630
"the best hypothesis, and another
hop from the best hypothesis",00:24:26.630,00:24:29.800
to the target function.,00:24:29.800,00:24:31.010
"And the neat thing is that they
decomposed cleanly.",00:24:31.010,00:24:33.230
"And we found that they decomposed
cleanly because the cross term",00:24:33.230,00:24:35.340
disappeared.,00:24:35.340,00:24:36.190
"That's the advantage of the particular
measure that we have.",00:24:36.190,00:24:39.900
"Now we need to give names
to these guys.",00:24:39.900,00:24:43.460
They will be the bias and variance.,00:24:43.460,00:24:45.050
"I'd like you to think for five seconds,
and you don't have to even",00:24:45.050,00:24:48.780
answer the question.,00:24:48.780,00:24:49.410
"Which will be the bias, and which
will be the variance?",00:24:49.410,00:24:51.530
"Just look at which would be
a better description to them.",00:24:51.530,00:24:54.220
I'm not going to ask.,00:24:56.810,00:24:57.500
"This is not a quiz, like last time.",00:24:57.500,00:25:00.330
This is the bias.,00:25:00.330,00:25:01.010
Why is it the bias?,00:25:01.010,00:25:01.940
"Because what I'm saying is that,
learning or no learning, your",00:25:01.940,00:25:06.410
"hypothesis set is biased away
from the target function.",00:25:06.410,00:25:10.110
"Because this is the best I could
do, under fictitious scenario.",00:25:10.110,00:25:13.120
"You have infinite data sets, and you
are doing all of this, and you're",00:25:13.120,00:25:15.820
"taking the average, and that's the
best you could come up with.",00:25:15.820,00:25:18.210
"And you are still far away
from the target.",00:25:18.210,00:25:20.090
"So it must be a limitation
of your hypothesis set.",00:25:20.090,00:25:23.300
"I'm going to measure that limitation and
say that your hypothesis set, which",00:25:23.300,00:25:27.090
"is represented at its best by
this guy, is biased away",00:25:27.090,00:25:30.880
from the target function.,00:25:30.880,00:25:33.180
So this is the bias term.,00:25:33.180,00:25:34.690
"And again, bias applies to that
particular point x, the test point in",00:25:34.690,00:25:38.520
"the input space that
I'm interested in.",00:25:38.520,00:25:41.230
The other guy must be the variance.,00:25:41.230,00:25:43.810
Why is that?,00:25:43.810,00:25:44.970
"Because if I knew everything, if I could
zoom in perfectly, I would zoom",00:25:44.970,00:25:49.380
"in onto the best, assuming
this is there,",00:25:49.380,00:25:52.020
so I have this guy. But you don't.,00:25:52.020,00:25:54.260
You have one data set at a time.,00:25:54.260,00:25:56.330
"When you get one data set,
you get this guy.",00:25:56.330,00:25:58.220
"You get another data set,
you get another guy.",00:25:58.220,00:25:59.950
These are different from that.,00:25:59.950,00:26:01.280
"So you are away from that, and I'm
measuring how far you are away.",00:26:01.280,00:26:04.950
"But because the expected value
of this fellow is g bar.",00:26:04.950,00:26:07.840
"And I am comparing the difference
squared with this.",00:26:07.840,00:26:10.960
It is properly called variance.,00:26:10.960,00:26:12.800
"This is the variance of what I am
getting, due to the fact that I get",00:26:12.800,00:26:17.480
a finite data set.,00:26:17.480,00:26:18.780
"Every time I get a data set, I get
a different one, and I am measuring the",00:26:18.780,00:26:23.110
"distance from the core
that I get here.",00:26:23.110,00:26:26.370
"So this we call the bias, and
this we call the variance.",00:26:26.370,00:26:30.100
This is very clean.,00:26:30.100,00:26:31.110
"Now let's go back, and put it
into the original form.",00:26:31.110,00:26:35.610
Remember this guy?,00:26:35.610,00:26:36.710
This is where we started.,00:26:36.710,00:26:38.640
"We got the other expression, and then
we neglected to take the expected",00:26:38.640,00:26:42.310
"value with respect to x, in order
to simplify the analysis.",00:26:42.310,00:26:44.910
"We would like to get that back,
so we'll look at this.",00:26:44.910,00:26:48.460
"This was the expected value, with respect
to x, of the quantity we just",00:26:48.460,00:26:54.130
decomposed.,00:26:54.130,00:26:56.220
"Now I take the decomposition and put it
back, in order to get the expected",00:26:56.220,00:27:00.900
"value of the out-of-sample error, in
terms of the bias and variance.",00:27:00.900,00:27:04.570
So this will be what?,00:27:04.570,00:27:05.720
"This will be the expected value with
respect to x, of bias plus variance",00:27:05.720,00:27:09.540
with x.,00:27:09.540,00:27:11.370
"And the expected value of the bias with
respect to x, I'm just going to",00:27:11.370,00:27:14.610
call it bias.,00:27:14.610,00:27:16.120
"The expected here, I'm going to call it
variance, and that's what",00:27:16.120,00:27:20.550
you get.,00:27:20.550,00:27:21.440
"And this is the bias-variance
decomposition.",00:27:21.440,00:27:24.990
"Now I have a single number that
describes the expected out-of-sample.",00:27:24.990,00:27:29.750
So I give you a full learning situation.,00:27:29.750,00:27:31.780
"I give you a target function, and
an input distribution, and a hypothesis",00:27:31.780,00:27:35.860
"set, and a learning algorithm.",00:27:35.860,00:27:37.510
And you have all the components.,00:27:37.510,00:27:39.220
"You go about, and learn
for every data set.",00:27:39.220,00:27:41.830
"And you get-- someone else learned
from another data set.",00:27:41.830,00:27:44.400
"And get the expected value of
the out-of-sample error.",00:27:44.400,00:27:46.470
"And I'm telling you if this
out-of-sample error is 0.3, well, 0.05",00:27:46.470,00:27:50.130
"of it is because of bias, and
0.25 is because of variance.",00:27:50.130,00:27:55.500
"So 0.05 means that your hypothesis set
is pretty good in approximation, but",00:27:55.500,00:27:59.180
maybe it's too big.,00:27:59.180,00:27:59.970
"Therefore, you have a lot of
variance, which is 0.25.",00:27:59.970,00:28:03.150
This is the decomposition.,00:28:03.150,00:28:06.420
"Now let's look at the tradeoff of
generalization versus approximation, in",00:28:06.420,00:28:11.340
terms of this decomposition.,00:28:11.340,00:28:12.700
That was the purpose.,00:28:12.700,00:28:13.950
"Here is the bias, explicitly
written as a formula.",00:28:16.530,00:28:20.530
And here is the variance.,00:28:20.530,00:28:24.590
"We would like to argue that there is
a tradeoff, that when you change your",00:28:24.590,00:28:27.310
"hypothesis set-- you make it bigger,
more complex, or smaller.",00:28:27.310,00:28:30.460
"One of these guys goes up, and
one of these guys goes down.",00:28:30.460,00:28:33.790
"So I'm going to argue
about it informally.",00:28:33.790,00:28:35.910
"And then we'll take a specific
example, where we are going",00:28:35.910,00:28:38.930
to get exact numbers.,00:28:38.930,00:28:40.530
"But this is just to realize
that this decomposition actually",00:28:40.530,00:28:43.930
"captures the tradeoff of approximation
versus generalization.",00:28:43.930,00:28:47.190
Why is that?,00:28:47.190,00:28:49.630
Let's look at this picture.,00:28:49.630,00:28:51.750
"Here, I have a small hypothesis set.",00:28:51.750,00:28:55.370
"One function, if you want, but, in
general, let's call it small.",00:28:55.370,00:28:59.140
"This one, I have a huge
hypothesis set.",00:28:59.140,00:29:02.520
"So I have here the black points are
hypotheses, that are candidates.",00:29:02.520,00:29:06.030
"Someone gives me a data set, and
I learn, and choose something.",00:29:06.030,00:29:09.870
"Now the target function
is sitting here.",00:29:09.870,00:29:12.390
"If I use this guy, obviously I am far
away from the target function.",00:29:12.390,00:29:16.930
"And therefore, the bias is big.",00:29:16.930,00:29:19.885
"If I have a big hypothesis set-- this
is big enough that it actually",00:29:19.885,00:29:22.650
includes the f.,00:29:22.650,00:29:24.220
"Then when I learn, on average,
I would be very close to f.",00:29:24.220,00:29:27.610
"Maybe I won't hit f exactly, because
of the nonlinearity of the regime.",00:29:27.610,00:29:30.960
"The regime, I get N examples, learn and
keep it, another N example, learn",00:29:30.960,00:29:35.480
"and keep it, and then
take the average.",00:29:35.480,00:29:36.850
"I might have lost some because
of the nonlinearity.",00:29:36.850,00:29:38.840
"I might not get f, but I'll
get pretty close.",00:29:38.840,00:29:41.540
"So the bias here is very,
very small, close to 0.",00:29:41.540,00:29:44.800
"In terms of the variance here,
there is no variance.",00:29:44.800,00:29:46.940
"If I have one target function, I don't
care what data set you give me.",00:29:46.940,00:29:49.600
I will always give you that function.,00:29:49.600,00:29:51.860
"So there's nothing to lose here
in terms of variance.",00:29:51.860,00:29:54.050
"Here, I have so many varieties
that, depending on the examples you",00:29:54.050,00:29:57.910
"give me, I may pick this.",00:29:57.910,00:29:58.910
"And in another example-- because I'm
fitting your data, so I get",00:29:58.910,00:30:02.350
a red cloud around this.,00:30:02.350,00:30:04.030
"And their centroid will be g bar,
the one that is good, but I may",00:30:04.030,00:30:08.580
get one or the other.,00:30:08.580,00:30:09.750
"And the size of this guy
measures the variance.",00:30:09.750,00:30:12.250
This is the price I pay.,00:30:12.250,00:30:14.320
"Now you can see that if I go from
a small hypothesis to a bigger",00:30:14.320,00:30:17.020
"hypothesis, the bias goes down,
and the variance goes up.",00:30:17.020,00:30:22.120
"The idea here, if I make the hypothesis
set bigger, I am making the",00:30:22.120,00:30:28.950
"bias smaller, because I
am making this bigger.",00:30:28.950,00:30:30.910
"I'm getting it closer to f, and being
able to approximate it better, so the",00:30:30.910,00:30:33.950
bias is diminishing.,00:30:33.950,00:30:36.330
"But I am making this-- so
the bias goes down.",00:30:36.330,00:30:40.020
And here the variance goes up.,00:30:40.020,00:30:41.470
Why is the variance going up?,00:30:41.470,00:30:42.230
"Because the red cloud becomes
bigger and bigger.",00:30:42.230,00:30:43.950
"If I have this thing, then I have more
variety to choose from, and I am",00:30:43.950,00:30:47.520
getting bigger variance to work with.,00:30:47.520,00:30:49.770
So this is the nature of the tradeoff.,00:30:49.770,00:30:52.950
"You may not believe this, because
I just drew a picture and argued",00:30:52.950,00:30:55.730
very informally.,00:30:55.730,00:30:56.820
"So now let's take a very concrete
example, and we will solve it",00:30:56.820,00:31:00.340
beginning to end.,00:31:00.340,00:31:01.700
"And if you understand this example
fully, you will have understood bias",00:31:01.700,00:31:05.000
and variance perfectly.,00:31:05.000,00:31:07.490
So let's see.,00:31:07.490,00:31:08.510
"I took that simplest possible example
that I can get a solution of, fully.",00:31:08.510,00:31:14.680
My target is a sinusoid.,00:31:14.680,00:31:18.330
That's an easy function.,00:31:18.330,00:31:19.330
"And I just wanted to restrict
myself to -1, +1.",00:31:19.330,00:31:22.690
So I'm going to get sine pi x.,00:31:22.690,00:31:24.690
"Just to scale it so that it's
from -1 to +1, gets",00:31:24.690,00:31:27.130
me the whole action.,00:31:27.130,00:31:28.680
"Therefore, the target function
formally defined, is from",00:31:28.680,00:31:32.960
"-1, +1, to the real numbers.",00:31:32.960,00:31:35.050
The co-domain is the real numbers.,00:31:35.050,00:31:36.250
"But obviously, the function would
be restricted from -1",00:31:36.250,00:31:38.105
"to +1, as a range.",00:31:38.105,00:31:41.290
Now the target function is unknown.,00:31:41.290,00:31:44.800
"That's what we have been preaching
for several lectures now.",00:31:44.800,00:31:47.760
"And now I am just giving you
the target function.",00:31:47.760,00:31:50.660
"Again, this is an illustration.",00:31:50.660,00:31:52.100
"When we come to learning, we will try
to blank it out, so that it becomes",00:31:52.100,00:31:55.100
unknown in our mind.,00:31:55.100,00:31:56.750
"But in order to understand the analysis
of the bias-variance, we",00:31:56.750,00:31:59.910
"would like to know what target
function we are working with.",00:31:59.910,00:32:02.250
"We're going to get things in terms of
it, and then you will understand why",00:32:02.250,00:32:05.520
the tradeoff exists.,00:32:05.520,00:32:08.160
"So the function looks like this.
Surprise-- just like a sinusoid.",00:32:08.160,00:32:13.000
Now the catch is the following.,00:32:13.000,00:32:14.350
You are going to learn this function.,00:32:14.350,00:32:15.990
I am going to give you a data set.,00:32:15.990,00:32:17.460
How big is the data set?,00:32:17.460,00:32:18.710
"I am not in a generous mood today,
so I am just going to",00:32:22.360,00:32:25.490
give you two examples.,00:32:25.490,00:32:26.930
"And from the two examples, you need to
learn the whole target function.",00:32:26.930,00:32:31.230
I'll try.,00:32:31.230,00:32:32.680
N equals 2.,00:32:32.680,00:32:34.750
"The next item is to give
you the hypothesis set.",00:32:34.750,00:32:37.670
"I'm going to give you two hypothesis
sets to play with.",00:32:37.670,00:32:40.680
"So one of you gets one, and another gets
another, and you try to learn and",00:32:40.680,00:32:43.740
then compare the results.,00:32:43.740,00:32:46.410
"Well, I have two examples.",00:32:46.410,00:32:47.590
"So I cannot give you
a 17th-order polynomial.",00:32:47.590,00:32:50.940
"So I am just going to give
you the following.",00:32:50.940,00:32:53.530
The two models are H_0 and H_1.,00:32:53.530,00:32:57.480
H_0 happens to be the constant model.,00:32:57.480,00:33:03.040
Just give me a constant.,00:33:03.040,00:33:04.280
"I am going to approximate the sine
function with a constant.",00:33:04.280,00:33:07.760
"OK, this doesn't look good.",00:33:07.760,00:33:10.410
But that's what we are working with.,00:33:10.410,00:33:12.570
"And the other one is far
more sophisticated.",00:33:12.570,00:33:14.830
"It's so elaborate, you will love it.",00:33:14.830,00:33:17.660
It's linear.,00:33:17.660,00:33:19.550
"Looks good now, having seen the
constant already, right?",00:33:19.550,00:33:22.910
These are your two hypothesis sets.,00:33:22.910,00:33:24.340
"And we would like to see
which one is better.",00:33:24.340,00:33:28.240
Better for what?,00:33:31.130,00:33:33.120
That's the key issue.,00:33:33.120,00:33:35.910
"Let's start to answer the question of
approximation first, and then go to the",00:33:35.910,00:33:40.110
question of learning.,00:33:40.110,00:33:42.080
"Here is the question of approximation,
H_0 versus H_1.",00:33:42.080,00:33:45.980
"When I talk about approximation, I
am not talking about learning.",00:33:45.980,00:33:48.840
"I am giving you the target
function, outright.",00:33:48.840,00:33:50.900
It's a sinusoid.,00:33:50.900,00:33:52.390
"If it's a sinusoid, why don't I
say it's just a sinusoid, and",00:33:52.830,00:33:55.500
have E_out equal 0?,00:33:55.500,00:33:57.030
"Oh, because the rule of the game is that
you're using one of the models.",00:33:57.030,00:34:00.020
"You have use either the constant
or the linear.",00:34:00.020,00:34:02.940
Do your best.,00:34:02.940,00:34:03.830
Use all the information you have.,00:34:03.830,00:34:05.300
"But if you use the constant,
return a constant.",00:34:05.300,00:34:08.320
"If you use the linear,
return a line.",00:34:08.320,00:34:10.340
"You are not going to be able to return
a bigger hypothesis than those.",00:34:10.340,00:34:13.320
That's the game.,00:34:13.320,00:34:14.380
OK?,00:34:14.380,00:34:15.199
Let's see what happens with H_1.,00:34:15.199,00:34:16.880
Here is the target.,00:34:19.560,00:34:20.840
"I am trying to fit it with
a line, an arbitrary line.",00:34:20.840,00:34:25.010
Can you think of what it looks like?,00:34:25.010,00:34:27.440
"Line is not much, but at least I can
get something like this, right?",00:34:27.440,00:34:31.900
"Try to get part of the
slope, et cetera.",00:34:31.900,00:34:34.070
I can solve this.,00:34:34.070,00:34:35.449
"I get a line in general, calculate
the mean squared error.",00:34:35.449,00:34:38.900
It will be a function of 'a' and 'b'.,00:34:38.900,00:34:40.190
"Differentiate with respect to 'a'
and 'b', and get the optimal.",00:34:40.190,00:34:42.980
It's not a big deal.,00:34:42.980,00:34:45.440
So you end up with this.,00:34:45.440,00:34:47.070
That's your best approximation.,00:34:47.070,00:34:48.670
"This is not a learning situation, but
this is the best you can do using the",00:34:48.670,00:34:51.469
linear model.,00:34:51.469,00:34:53.630
"Under those conditions,
you made errors.",00:34:53.630,00:34:56.900
And these are your errors.,00:34:56.900,00:34:59.570
"You didn't get it right, and these
regions tell you how far you are from",00:34:59.570,00:35:02.770
the target.,00:35:02.770,00:35:05.180
Let's do it with the other guy.,00:35:05.180,00:35:07.770
Now I have a constant.,00:35:07.770,00:35:09.690
"I want to approximate this
guy with a constant.",00:35:09.690,00:35:12.850
What is the constant?,00:35:12.850,00:35:15.610
I guess I have to work with 0.,00:35:15.610,00:35:17.455
That's the best I have.,00:35:17.455,00:35:18.490
"Remember, it's mean squared error.",00:35:18.490,00:35:19.860
"So if I move the 0, the big error will
contribute a lot, because it's squared.",00:35:19.860,00:35:23.820
"So I just put it in the middle,
and this is your hypothesis.",00:35:23.820,00:35:29.560
And how much is your error?,00:35:29.560,00:35:31.660
Big.,00:35:31.660,00:35:32.090
The whole thing is your error.,00:35:32.090,00:35:34.730
Let's quantify it.,00:35:34.730,00:35:35.520
"If you get the expected values of mean
squared error, you'll get a number,",00:35:35.520,00:35:38.010
"which here will be 0.5, and here
will be approximately 0.2.",00:35:38.010,00:35:44.290
So the linear model wins.,00:35:44.290,00:35:46.080
"Yeah, I'm approximating.",00:35:46.080,00:35:47.540
"I have more parameters, sure.",00:35:47.540,00:35:48.750
"If you give me third order, I
will be able to do better.",00:35:48.750,00:35:50.800
"If you give me 17th order, I'll
be able to do better.",00:35:50.800,00:35:53.260
But that's the game.,00:35:53.260,00:35:55.110
"In terms of approximation,
the more the merrier.",00:35:55.110,00:35:57.210
"Because you have all the information.
There's no question of zooming in.",00:35:57.210,00:36:01.670
Now let's go for learning.,00:36:01.670,00:36:04.080
"This course is about machine learning, right?
Not about approximation.",00:36:04.080,00:36:07.080
So this is the important part for us.,00:36:07.080,00:36:09.370
"Let's play the same game with
a view to learning.",00:36:09.370,00:36:12.220
You have two examples.,00:36:12.220,00:36:13.240
You are going to learn from them.,00:36:13.240,00:36:14.560
"You are restricted to one hypothesis
set or the other.",00:36:14.560,00:36:18.510
"So let's start with H_1, and
I'll go to H_0 again.",00:36:18.510,00:36:22.690
This is your target function.,00:36:22.690,00:36:24.610
Now you get two examples.,00:36:24.610,00:36:25.920
"I'm going to, let's say, uniformly pick
two examples independently, and I",00:36:25.920,00:36:29.790
get these two examples.,00:36:29.790,00:36:31.840
"I'd like you to fit the examples, and
we'll see how well you approximate the",00:36:31.840,00:36:35.160
target function.,00:36:35.160,00:36:36.680
"The first item of business is to
get rid of the target function.",00:36:36.680,00:36:39.640
Because you don't know it.,00:36:39.640,00:36:41.300
You only know the examples.,00:36:41.300,00:36:42.770
"So in a learning situation,
this is what you get.",00:36:42.770,00:36:48.150
Now I ask you to fit a line.,00:36:48.150,00:36:50.660
"Line, two points. I can do that.",00:36:50.660,00:36:54.500
This is what you do.,00:36:54.500,00:36:56.150
"Now that you settled on the final
hypothesis, I'm going to grade you.",00:36:56.150,00:36:58.980
"So I'm going to bring back the target
function, and compare this to that, and",00:36:58.980,00:37:03.800
"give you what is your
out-of-sample error.",00:37:03.800,00:37:07.310
Let's do it for H_0.,00:37:07.310,00:37:09.790
You have the same two points.,00:37:09.790,00:37:11.210
You're fitting them with a constant.,00:37:11.210,00:37:13.770
How would you do that?,00:37:13.770,00:37:15.180
"Probably the midpoint will give
you the least squared error",00:37:15.180,00:37:17.560
"on these two points, right?",00:37:17.560,00:37:19.430
"So this would be your
final hypothesis.",00:37:19.430,00:37:22.030
"And you get back your target function,
in order to evaluate your",00:37:22.030,00:37:24.640
"out-of-sample error, and
this is what you get.",00:37:24.640,00:37:27.790
Now you can see what the problem is.,00:37:27.790,00:37:31.060
"I can compute the error here,
and I can have the error",00:37:31.060,00:37:32.980
"regions, and all of that.",00:37:32.980,00:37:34.030
"But this depends on which
two points I gave you.",00:37:34.030,00:37:36.310
"If I give you another two points, I
give you another two points, et",00:37:36.310,00:37:39.550
"cetera, I am not sure how to really
compare them, because it does depend",00:37:39.550,00:37:42.940
on your data set.,00:37:42.940,00:37:44.330
"That's why we needed the
bias-variance analysis.",00:37:44.330,00:37:46.970
"That's why we got the expected value
of the error, with respect to the",00:37:46.970,00:37:50.060
"choice of the data set, so that we
actually are talking inherently about",00:37:50.060,00:37:53.900
"a linear model learning a target using
two points, regardless of which two",00:37:53.900,00:37:59.220
points I'm talking about.,00:37:59.220,00:38:02.020
"So let's do the bias and variance
decomposition for the constant guy.",00:38:02.020,00:38:10.270
Here is the figure.,00:38:10.270,00:38:11.570
It's an interesting figure.,00:38:11.570,00:38:13.740
"Here I am generating data sets, each
of size 2 points, and then",00:38:13.740,00:38:20.400
fitting a line.,00:38:20.400,00:38:21.400
And the line would be the midpoint.,00:38:21.400,00:38:24.000
"I keep repeating this exercise,
and I am showing you the final",00:38:24.000,00:38:27.900
hypothesis you get.,00:38:27.900,00:38:29.530
"I repeated it a very large
number of times.",00:38:29.530,00:38:31.170
"This is a real simulation, and these
are the hypotheses you get.",00:38:31.170,00:38:34.170
"You can see that when you get this line,
it means that the two points",00:38:34.170,00:38:37.100
were equally distant from here.,00:38:37.100,00:38:38.570
Sometimes I get the points here.,00:38:38.570,00:38:39.800
"Sometimes I get them equal to
that, so I get here.",00:38:39.800,00:38:41.840
"The middle point is
a little bit heavier.",00:38:41.840,00:38:44.870
"Because, obviously, the chance of
getting them on the two lobes is",00:38:44.870,00:38:48.950
"there, and so on.",00:38:48.950,00:38:49.700
"So this is basically the
distribution you get.",00:38:49.700,00:38:52.890
"Each of them will give you
an out-of-sample error.",00:38:52.890,00:38:55.220
"And the interesting thing for us is
the expected out-of-sample error.",00:38:55.220,00:38:58.600
That's what will grade the model.,00:38:58.600,00:39:00.920
"Now what we are going to do, we are
going to get the bias and variance",00:39:00.920,00:39:03.530
decomposition based on that.,00:39:03.530,00:39:05.100
And that is our next figure.,00:39:05.100,00:39:06.350
Look at this carefully.,00:39:09.290,00:39:11.920
"The green guy, the very light
green guy, is g bar of x.",00:39:11.920,00:39:20.040
"This is the average hypothesis
you get.",00:39:20.040,00:39:22.380
How did I get that?,00:39:22.380,00:39:23.610
"I simply added up all of these guys,
and divided by their number.",00:39:23.610,00:39:28.250
"And it is expected obviously, by the
symmetry, that on average, I will get",00:39:28.250,00:39:31.000
something very close to 0.,00:39:31.000,00:39:33.310
"The interesting thing is that you can see
now that g bar, here, happens to be",00:39:33.310,00:39:38.180
also the best approximation.,00:39:38.180,00:39:39.810
"If I keep repeating this, I will
actually get the 0 guy, which I was",00:39:39.810,00:39:43.100
"able to get when I had the full target
function I was approximating.",00:39:43.100,00:39:46.630
"Here I don't have the full
target function.",00:39:46.630,00:39:48.400
I have one hypothesis at a time.,00:39:48.400,00:39:49.520
"I am getting the average,
but I am getting this.",00:39:49.520,00:39:51.880
"So there is a justification for saying
that g bar will be the best",00:39:51.880,00:39:54.590
hypothesis.,00:39:54.590,00:39:55.340
"Because this game of getting one at
a time, and then getting the average,",00:39:55.340,00:39:58.400
does get me somewhere.,00:39:58.400,00:39:59.800
"But do remember, this is not the output
of your learning process.",00:39:59.800,00:40:02.760
I wish it were.,00:40:02.760,00:40:03.840
It isn't.,00:40:03.840,00:40:04.930
"The output of your learning process
is one of those guys, and",00:40:04.930,00:40:07.130
you don't know which.,00:40:07.130,00:40:08.370
"It just happens that, if you repeat
it, this will be your average.",00:40:08.370,00:40:11.190
"And because you are getting different
guys here, there will be a variance",00:40:11.190,00:40:14.990
around this.,00:40:14.990,00:40:15.730
"And the variance, I'm describing it
basically by the standard deviation",00:40:15.730,00:40:19.010
you are going to get.,00:40:19.010,00:40:20.620
"So the error between the green line
and the target function will",00:40:20.620,00:40:24.560
give you the bias.,00:40:24.560,00:40:26.120
"And the width of the gray region
will give you the variance.",00:40:26.120,00:40:30.350
Understood what the analysis is?,00:40:30.350,00:40:32.680
So that takes care of H_0.,00:40:32.680,00:40:35.610
Let's go to H_1.,00:40:35.610,00:40:36.980
"So to remember, the learning
situation for H_0 was this.",00:40:36.980,00:40:40.630
This is when I had the constant model.,00:40:40.630,00:40:42.790
"What will happen if you are actually
fitting the two points, not with",00:40:42.790,00:40:46.730
"a constant, which you do at the midpoint,
but you are fitting them",00:40:46.730,00:40:50.240
with a complete line?,00:40:50.240,00:40:51.660
What will it look like?,00:40:51.660,00:40:54.030
It will look like this.,00:40:54.030,00:40:55.280
Wow.,00:40:58.840,00:40:59.070
You can see where the problem is.,00:40:59.070,00:41:00.580
Talk about variance.,00:41:00.580,00:41:02.310
Take two points.,00:41:02.310,00:41:02.930
You connect them.,00:41:02.930,00:41:03.660
"Wherever the two points, you
get this jungle of lines.",00:41:03.660,00:41:06.560
"This is for exactly the same data sets
that gave me the horizontal lines in",00:41:06.560,00:41:10.420
the previous slide.,00:41:10.420,00:41:12.910
So this is what I get.,00:41:12.910,00:41:14.360
"Now I ask myself, what on
average will I get?",00:41:14.360,00:41:17.510
"You can immediately say, on average,
you'd better get a positive slope.",00:41:17.510,00:41:21.743
"There is a tendency to
get a positive slope.",00:41:21.743,00:41:23.970
"Because when you get the points
split, you will get this.",00:41:23.970,00:41:26.870
"Sometimes you get a negative
slope here, here.",00:41:26.870,00:41:29.310
"But that is balanced by getting
a positive slope here.",00:41:29.310,00:41:32.310
"You can argue this, but
you can do the math.",00:41:32.310,00:41:34.710
"And then you get the bias-variance
decomposition.",00:41:34.710,00:41:38.190
This will be your average.,00:41:38.190,00:41:40.980
This is g bar.,00:41:40.980,00:41:42.940
And this will be the variance you get.,00:41:42.940,00:41:44.760
The variance depends on x.,00:41:44.760,00:41:46.020
This is the way we defined it.,00:41:46.020,00:41:47.070
"And when you want one variance to
describe it, you get the expected",00:41:47.070,00:41:49.780
"value of the width squared
of that gray region.",00:41:49.780,00:41:52.370
"This gray region has the
standard deviation.",00:41:52.370,00:41:55.320
Now you can see exactly that,00:41:55.320,00:41:56.640
"I am getting better approximation
than the previous guy.",00:41:56.640,00:41:59.940
"But I sure am getting very bad variance,
which is expected here.",00:41:59.940,00:42:05.360
Now you can see what the tradeoff is.,00:42:05.360,00:42:08.150
"And the question is, given these
two models, which one wins",00:42:08.150,00:42:13.410
from a learning scenario?,00:42:13.410,00:42:15.570
"You need to ask the question,
to remember what it is.",00:42:15.570,00:42:17.540
I am trying to approximate a sinusoid.,00:42:17.540,00:42:21.210
"Is it better to do it with
a constant or a general line?",00:42:21.210,00:42:26.210
"The answer to that question
is obvious.",00:42:26.210,00:42:28.690
"But that is not the question
I am asking in learning.",00:42:28.690,00:42:31.920
"The question I am asking in learning,
you have two points coming from",00:42:31.920,00:42:37.600
something I don't know.,00:42:37.600,00:42:39.630
"Is it better to use
a constant or a line?",00:42:39.630,00:42:43.310
You notice the difference.,00:42:43.310,00:42:44.560
"I am going to put them side by side,
and then see which is the winner.",00:42:46.790,00:42:51.960
"So this guy has a big bias
and a small variance.",00:42:51.960,00:42:54.990
"This guy has a small bias
and a big variance.",00:42:54.990,00:42:57.960
Let's get quantitative.,00:42:57.960,00:43:00.900
What is the bias here?,00:43:00.900,00:43:03.230
It's actually 0.5.,00:43:03.230,00:43:04.200
"Exactly the same we got when we
were approximating outright.",00:43:04.200,00:43:06.800
It's the 0.,00:43:06.800,00:43:07.360
That's the expected value.,00:43:07.360,00:43:08.310
"You get 0.5, the mean squared error.",00:43:08.310,00:43:12.790
What is the bias here?,00:43:12.790,00:43:16.220
It's 0.21.,00:43:16.220,00:43:17.150
"Interestingly enough, when we did the
approximation, it was about 0.2.",00:43:17.150,00:43:21.280
"And indeed, this is not
exactly the best fit.",00:43:21.280,00:43:24.500
"Remember when I told you there
is a nonlinearity aspect.",00:43:24.500,00:43:26.750
"You are taking two points at the time,
and then taking a fit, and then taking",00:43:26.750,00:43:29.610
the average.,00:43:29.610,00:43:30.120
"And it's conceivable that this will give
you something different from if",00:43:30.120,00:43:33.180
"you have the full curve, and you
are fitting it outright.",00:43:33.180,00:43:35.390
"The difference is usually
very small, and it is.",00:43:35.390,00:43:37.510
"But here you get something which is not
exactly perfect, but is very close",00:43:37.510,00:43:40.160
to perfect.,00:43:40.160,00:43:41.820
"So obviously, here the
bias is much smaller.",00:43:41.820,00:43:44.250
Let's look at the variance.,00:43:44.250,00:43:45.180
What is the variance here?,00:43:45.180,00:43:47.480
The variance here is 0.25.,00:43:47.480,00:43:50.360
It's not too bad.,00:43:50.360,00:43:52.830
"The variance here, we
expect it to bigger.",00:43:52.830,00:43:54.610
But is it big enough to kill us?,00:43:54.610,00:43:56.160
"It's a disaster, complete
and utter disaster.",00:44:01.150,00:44:05.080
"And now, when you see what is the
expected out-of-sample error, you add",00:44:05.080,00:44:07.750
these two numbers.,00:44:07.750,00:44:08.520
"Here I'm going to get 0.75, and
here you are going to get",00:44:08.520,00:44:11.220
something much bigger.,00:44:11.220,00:44:13.170
And the winner is--,00:44:13.170,00:44:14.420
"Now you go to your friends, and tell
them that I learned today that in",00:44:16.630,00:44:19.090
"order to approximate a sine, I am better
off approximating it with",00:44:19.090,00:44:22.240
a constant than with a general line.,00:44:22.240,00:44:24.810
And have a smile on your face.,00:44:24.810,00:44:26.110
"Of course you know what you're talking
about, but they might not really",00:44:26.110,00:44:29.110
appreciate the humor here.,00:44:29.110,00:44:30.850
This is the game.,00:44:32.170,00:44:33.210
I think we understand it well.,00:44:33.210,00:44:34.735
"So the lesson learned, if I want to
articulate it, is that when you are in",00:44:37.510,00:44:40.220
"a learning situation always remember: you
are matching the model complexity",00:44:40.220,00:44:48.210
"to the data resources you have,
not to the target complexity.",00:44:48.210,00:44:55.520
I don't know the target.,00:44:55.520,00:44:57.010
"And even if I knew the level of
complexity it has, I don't have the",00:44:57.010,00:45:01.210
resources to match it.,00:45:01.210,00:45:03.250
"Because if I match it, I will have the
target in my hypothesis set, but I",00:45:03.250,00:45:07.420
will never arrive at it.,00:45:07.420,00:45:09.720
"Pretty much like I'm sitting in my
office, and I want a document of some",00:45:09.720,00:45:13.960
"kind, an old letter.",00:45:13.960,00:45:15.590
"Someone has asked me for a letter of
recommendation, and I don't want to",00:45:15.590,00:45:17.930
rewrite it for you.,00:45:17.930,00:45:19.090
"So I want to take the old guy and just
see what I wrote, and then add the",00:45:19.090,00:45:21.410
update to that.,00:45:21.410,00:45:23.640
"Before everything was archived
in the computers, it used to",00:45:23.640,00:45:27.160
be a piece of paper.,00:45:27.160,00:45:27.870
"So I know the letter of recommendation
is somewhere.",00:45:27.870,00:45:30.750
"Now I face the question, should I
write the letter of recommendation",00:45:30.750,00:45:33.410
from scratch?,00:45:33.410,00:45:34.280
"Or should I look for the letter
of recommendation?",00:45:34.280,00:45:37.740
The recommendation is there.,00:45:37.740,00:45:39.100
It's much easier when I find it.,00:45:39.100,00:45:40.650
"However, finding it is a big deal.",00:45:40.650,00:45:43.980
"So the question is not that the
target function is there.",00:45:43.980,00:45:46.210
"The question is, can I find it?",00:45:46.210,00:45:48.940
"Therefore, when I give you 100 examples,
you choose the hypothesis",00:45:48.940,00:45:52.800
set to match the 100 examples.,00:45:52.800,00:45:54.740
"If the 100 examples are terribly
noisy, that's even worse.",00:45:54.740,00:45:58.480
"Because their information
to guide you is worse.",00:45:58.480,00:46:01.200
"That's what I mean by the
data resources you have.",00:46:01.200,00:46:04.150
"The data resources you have is, what do
you have in order to navigate the",00:46:04.150,00:46:07.650
hypothesis set?,00:46:07.650,00:46:09.090
"Let's pick a hypothesis set that
we can afford to navigate.",00:46:09.090,00:46:12.550
That is the game in learning.,00:46:12.550,00:46:13.800
Done with the bias and variance.,00:46:16.810,00:46:18.230
"Now we are going to take just
an illustrative tool, called",00:46:18.230,00:46:21.330
the learning curves.,00:46:21.330,00:46:22.750
"And then we are going to put the bias
and variance versus the VC analysis on",00:46:22.750,00:46:26.040
those curves.,00:46:26.040,00:46:27.130
So what are the learning curves?,00:46:27.130,00:46:28.950
"They are related to what we think of
intuitively as a learning curve.",00:46:28.950,00:46:31.520
But they are a technical term here.,00:46:31.520,00:46:33.300
"They are basically plotting the expected
value of E_out and E_in.",00:46:33.300,00:46:37.480
We have done E_out already.,00:46:37.480,00:46:38.980
"But here we also plot the expected value
of E_in, as a function of N.",00:46:38.980,00:46:43.330
Let's go through the details.,00:46:43.330,00:46:45.070
"I give you a data set of size N. We know
what the expected value of the",00:46:45.070,00:46:50.590
out-of-sample error is.,00:46:50.590,00:46:51.630
"We have seen that already in the
bias-variance decomposition.",00:46:51.630,00:46:54.110
And this is the quantity.,00:46:54.110,00:46:55.440
"I know this is the quantity that I will
get in any learning situation.",00:46:55.440,00:46:58.770
It depends on the data set.,00:46:58.770,00:47:00.300
"If I want a quantity that describes
just the size of the set, I will",00:47:00.300,00:47:03.620
"integrate this out, and get the expected
value with respect to D.",00:47:03.620,00:47:06.000
That's the quantity I have.,00:47:06.000,00:47:07.500
"And the other one is exactly the
same, except it's in-sample.",00:47:07.500,00:47:10.060
"We didn't use it in the bias-variance
analysis.",00:47:10.060,00:47:12.160
"This one, I am going to get the expected
value of the in-sample.",00:47:12.160,00:47:14.350
"So I want to get, given this situation,
if I give you N examples,",00:47:14.350,00:47:18.090
how well are you going to fit them?,00:47:18.090,00:47:19.240
"Well, it depends on the examples.",00:47:19.240,00:47:20.300
"But on average, this is how well
you are going to fit them.",00:47:20.300,00:47:23.510
"And you ask yourself, how
do these vary with N?",00:47:23.510,00:47:26.650
And here comes the learning curve.,00:47:26.650,00:47:27.960
"As you get more examples,
you learn better.",00:47:27.960,00:47:30.160
"So hopefully, the learning curve--
and we'll see what the learning",00:47:30.160,00:47:32.760
curve looks like.,00:47:32.760,00:47:35.440
Let's take a simple model first.,00:47:35.440,00:47:37.270
So it's a simple model.,00:47:39.950,00:47:41.540
"And because it's a simple model, it
does not approximate your target",00:47:41.540,00:47:44.860
function well.,00:47:44.860,00:47:45.900
"The best out-of-sample error
you can do is pretty high.",00:47:45.900,00:47:49.350
"When you learn, the in-sample will be
very close to the out-of-sample.",00:47:52.460,00:47:56.770
"So let's look first at the behavior as
you increase N. As you increase N,",00:47:56.770,00:48:00.980
"hopefully the out-of-sample
error is going down.",00:48:00.980,00:48:04.200
I have more examples to learn from.,00:48:04.200,00:48:05.940
"I have a better chance of approximating
the target function.",00:48:05.940,00:48:08.010
"And indeed, it goes.",00:48:08.010,00:48:09.280
"And it can go down and down, until it
gets to the absolute limit of your",00:48:09.280,00:48:12.330
hypothesis set.,00:48:12.330,00:48:13.480
Your hypothesis set very simple.,00:48:13.480,00:48:14.660
"It doesn't have a very good
approximation for your target.",00:48:14.660,00:48:16.710
This is the best it can do.,00:48:16.710,00:48:17.830
"The best you can do is
the best you can do.",00:48:17.830,00:48:19.600
So that's what you get.,00:48:19.600,00:48:21.341
"When you look at the in-sample, it
actually goes the other way around.",00:48:21.341,00:48:24.170
"Because here my task is
simpler than here.",00:48:24.170,00:48:27.520
Here I am trying to fit 5 examples.,00:48:27.520,00:48:29.700
Here I am trying to fit 20 examples.,00:48:29.700,00:48:32.490
And I only have the examples to fit.,00:48:32.490,00:48:33.710
"I'm not looking at target function,
or anything like that.",00:48:33.710,00:48:35.860
"So obviously, I can use my degrees of
freedom in the hypothesis set, and fit",00:48:35.860,00:48:40.490
"the 5 examples better, and get
a smaller in-sample error.",00:48:40.490,00:48:43.640
"Whereas if I increase N, I will
get a worse in-sample error.",00:48:43.640,00:48:46.790
"It doesn't bother me, because
the in-sample error is",00:48:46.790,00:48:48.470
not the bottom line.,00:48:48.470,00:48:49.250
The out-of-sample is.,00:48:49.250,00:48:50.310
"And as you can see, although I am
getting worse in-sample, I am getting",00:48:50.310,00:48:52.940
better out-of-sample.,00:48:52.940,00:48:53.980
"And indeed, the discrepancy between
them, which is the generalization error,",00:48:53.980,00:48:57.340
"is getting tighter and tighter
as N increases.",00:48:57.340,00:49:00.340
Completely logical.,00:49:00.340,00:49:01.700
"By the way, this is a real model, so when
we talk about overfitting, I will",00:49:01.700,00:49:05.250
"tell you what that model is, as the
simple model and the complex model.",00:49:05.250,00:49:09.230
"The complex model, exactly the same
behavior, except it's shifted.",00:49:09.230,00:49:11.950
"It's a complex model, so it has
a better approximation",00:49:11.950,00:49:15.750
for your target function.,00:49:15.750,00:49:16.740
"So it can achieve, in principle,
a better out-of-sample error.",00:49:16.740,00:49:20.950
"You have so many degrees of freedom, that
you were able to fit the training",00:49:20.950,00:49:24.530
set perfectly up to here.,00:49:24.530,00:49:26.410
"This corresponds, more or less,
to the VC dimension.",00:49:26.410,00:49:29.290
"The VC dimension can
shatter everything.",00:49:29.290,00:49:31.440
So you can shatter these guys.,00:49:31.440,00:49:32.440
You can fit them perfectly.,00:49:32.440,00:49:33.360
So you get zero error.,00:49:33.360,00:49:34.970
"You start compromising when you have
more guys and you cannot shatter, so",00:49:34.970,00:49:37.550
maybe you have to compromise.,00:49:37.550,00:49:38.520
"And you end up starting
to have in-sample error.",00:49:38.520,00:49:41.150
"And the in-sample error goes up, and the
out-of-sample error goes down.",00:49:41.150,00:49:44.740
"The interesting thing is that in
here, when you have this, I",00:49:44.740,00:49:47.330
fit the examples perfectly.,00:49:47.330,00:49:48.320
I'm so happy.,00:49:48.320,00:49:49.030
What is out-of-sample?,00:49:49.030,00:49:50.760
An utter disaster.,00:49:50.760,00:49:53.360
Absolutely no information.,00:49:53.360,00:49:54.390
We didn't learn anything.,00:49:54.390,00:49:55.080
We just memorized the examples.,00:49:55.080,00:49:58.030
"So here, again, the out-of-sample
error goes down.",00:49:58.030,00:50:03.680
The in-sample error goes up.,00:50:03.680,00:50:05.120
Same argument exactly.,00:50:05.120,00:50:06.230
They get closer together.,00:50:06.230,00:50:07.170
"But obviously the discrepancy between
them is bigger, because I have a more",00:50:07.170,00:50:10.350
complex set.,00:50:10.350,00:50:11.700
"Therefore, the generalization
error is bigger.",00:50:11.700,00:50:14.760
"The bound on it is bigger
in the VC analysis.",00:50:14.760,00:50:17.050
And the actual value is bigger.,00:50:17.050,00:50:19.330
So this is the analysis.,00:50:19.330,00:50:21.090
It's a very simple tool.,00:50:21.090,00:50:22.440
"And the reason I introduced it here is
that I want to illustrate the bias and",00:50:22.440,00:50:26.420
"variance analysis versus the VC analysis,
using the learning curves.",00:50:26.420,00:50:30.800
"It will be very illustrative to
understand how the two theories relate",00:50:30.800,00:50:34.240
to each other.,00:50:34.240,00:50:35.490
"Let's start with the VC analysis
on learning curves.",00:50:37.210,00:50:43.580
These are learning curves.,00:50:43.580,00:50:45.340
"The in-sample error goes
up, as promised.",00:50:45.340,00:50:46.765
The out-of-sample error goes down.,00:50:46.765,00:50:48.760
"There is a best approximation that
corresponds to this level of",00:50:48.760,00:50:52.210
"out-of-sample error, if we
actually knew the thing.",00:50:52.210,00:50:54.880
And what did we do in the VC analysis?,00:50:54.880,00:50:57.840
"We had the in-sample error, which is
this region, the height of this",00:50:57.840,00:51:01.630
"region, and then we had a bound on the
generalization error, which is Omega.",00:51:01.630,00:51:07.030
"And we said that the bound behaves the
same way as the quantity itself.",00:51:07.030,00:51:10.530
"So the bound actually will
not be this thing.",00:51:10.530,00:51:12.230
It will be way bigger.,00:51:12.230,00:51:14.990
"But in proportionality, it will
give us the same proportion.",00:51:14.990,00:51:19.140
"So as you increase N, the generalization
error goes down.",00:51:19.140,00:51:22.520
The bound on it goes down.,00:51:22.520,00:51:23.600
"Omega goes down, which
we already realized.",00:51:23.600,00:51:26.940
"And obviously, you can
take another model.",00:51:26.940,00:51:29.330
"And if the model is very complex, the
discrepancy between them becomes",00:51:29.330,00:51:34.240
"bigger, which agrees with that.",00:51:34.240,00:51:35.840
So this is the decomposition of it.,00:51:35.840,00:51:37.810
"Now I took some liberties, in order
to be able to do that.",00:51:37.810,00:51:40.760
"The VC analysis doesn't
have expected values.",00:51:40.760,00:51:43.210
"So I took expected values
of everything there is.",00:51:43.210,00:51:45.510
"So there is some liberty taken, in
order to put it to fit in that",00:51:45.510,00:51:49.510
"diagram, but the principle holds.",00:51:49.510,00:51:51.295
"The blue region is the in-sample
error, and the red region is",00:51:54.585,00:51:57.610
basically the Omega.,00:51:57.610,00:51:58.620
"That is what happens in
the generalization bound.",00:51:58.620,00:52:01.410
"Think for a moment, which region will be
blue and which region will be red",00:52:01.410,00:52:05.130
in the bias-variance analysis?,00:52:05.130,00:52:06.300
"I'll get exactly the same
curves, the same model.",00:52:06.300,00:52:09.700
So what will it be?,00:52:09.700,00:52:12.310
It will be this.,00:52:12.310,00:52:16.190
That's the difference.,00:52:16.190,00:52:18.740
"In the bias-variance, I got the bias
based on the best approximation.",00:52:18.740,00:52:23.250
"I didn't look at how you
performed in-sample.",00:52:23.250,00:52:26.160
"I assumed hypothetically that you
could look for the best possible",00:52:26.160,00:52:29.770
approximation.,00:52:29.770,00:52:30.950
And I charged the bias for that.,00:52:30.950,00:52:33.100
And this is the bias you have.,00:52:33.100,00:52:35.210
So this is the best you can do.,00:52:35.210,00:52:36.530
And this is the error you are making.,00:52:36.530,00:52:38.090
"Again, there is a liberty taken here.",00:52:38.090,00:52:39.490
"Because this is genuinely the best
approximation in your hypothesis set.",00:52:39.490,00:52:43.800
"The one I am using for the
bias-variance analysis is",00:52:43.800,00:52:46.330
the error on g bar.,00:52:46.330,00:52:48.960
"And we said, g bar will be
close in error to this guy.",00:52:48.960,00:52:51.735
"It may not even be in
the hypothesis set.",00:52:51.735,00:52:53.395
"So there is some liberty, but
it's not a huge liberty.",00:52:53.395,00:52:55.750
"This is very much close to what you
are getting in the bias-variance.",00:52:55.750,00:52:59.140
And the rest of it is the variance.,00:52:59.140,00:53:01.720
"Because you get the bias plus that, and
you will get the expected value of",00:53:01.720,00:53:04.720
the out-of-sample error.,00:53:04.720,00:53:06.870
"Now you can see why they are both
talking about the same thing.",00:53:06.870,00:53:09.350
"Both of them are talking
about approximation.",00:53:09.350,00:53:11.930
That's the blue part.,00:53:11.930,00:53:13.260
Here it's approximation overall.,00:53:13.260,00:53:15.950
And here it's approximation in-sample.,00:53:15.950,00:53:19.750
"And both of them take into consideration
what happens in terms of",00:53:19.750,00:53:22.970
generalization.,00:53:22.970,00:53:24.230
"Well, the red region here
is maybe twice the size.",00:53:24.230,00:53:26.980
Not twice the size,00:53:26.980,00:53:27.850
"in general. It will be twice
the size actually in the",00:53:27.850,00:53:29.440
linear regression example.,00:53:29.440,00:53:31.010
"But basically, they have
the same behavior.",00:53:31.010,00:53:32.390
They have just different scale.,00:53:32.390,00:53:33.800
"So they capture the same principle of
generalizing, or the uncertainty about",00:53:33.800,00:53:38.330
"which hypothesis to pick, or how much
do I lose from going in-sample to",00:53:38.330,00:53:42.720
out-of-sample.,00:53:42.720,00:53:43.400
So they have the same behavior.,00:53:43.400,00:53:45.090
"And the only difference here is that,
here the bias obviously is constant",00:53:45.090,00:53:48.310
"with respect to N. The bias depends
on the hypothesis set.",00:53:48.310,00:53:51.800
Now this is also an assumption.,00:53:51.800,00:53:53.450
"Because it says, I have 2 examples
and take the average.",00:53:53.450,00:53:55.950
I will get an error.,00:53:55.950,00:53:56.950
"If I have 10 examples and take
the average, I get an error.",00:53:56.950,00:53:59.650
Is it the same?,00:53:59.650,00:54:00.480
"Well, in both cases, you effectively used
an infinite number of examples.",00:54:00.480,00:54:03.850
"Because the first one you used two
at a time, and you repeated it",00:54:03.850,00:54:06.610
"an infinite number of times,
and you took an average.",00:54:06.610,00:54:09.430
"This, you used 10 at a time,
and you took an average.",00:54:09.430,00:54:12.380
"I grant you maybe the 10 will
give you a better situation.",00:54:12.670,00:54:14.790
"But again, it's a little bit of
a license, in order to be able to",00:54:14.790,00:54:18.990
"attribute the bias and variance to this
line, which happens to be the",00:54:18.990,00:54:22.710
"best hypothesis proper within
your hypothesis set.",00:54:22.710,00:54:26.210
"So this is the contrast between the
two theoretical approaches we have",00:54:26.210,00:54:28.840
"covered, in this lecture and the
previous three lectures.",00:54:28.840,00:54:31.590
"I am going to end up with the analysis
for the linear regression case.",00:54:34.100,00:54:38.400
"So I'm going to basically go
through it fairly quickly.",00:54:38.400,00:54:41.830
This is a very good exercise to do.,00:54:41.830,00:54:45.610
"And if you read the exercise and you
follow the steps, it will give you",00:54:45.610,00:54:49.110
"very good insight into the
linear regression.",00:54:49.110,00:54:51.190
"I'll try to explain the
highlights of it.",00:54:51.190,00:54:53.780
"Let's start with a reminder
of linear regression.",00:54:53.780,00:54:57.130
"So linear regression,
I'm using a target.",00:54:57.130,00:54:59.250
"For the purpose of simplification, I am
going to use a noisy target, which",00:54:59.250,00:55:04.700
is linear plus noise.,00:55:04.700,00:55:08.280
"So I'm using linear regression to learn
something linear plus noise.",00:55:08.280,00:55:11.450
"If it weren't for the noise,
I would get it perfectly.",00:55:11.450,00:55:13.700
It's already linear.,00:55:13.700,00:55:14.950
"But because of the noise, I will
be deviating a little bit.",00:55:14.950,00:55:17.960
"This is just to make the mathematics
that results easier to handle.",00:55:17.960,00:55:23.360
Now you're given a data set.,00:55:23.360,00:55:25.710
And the data set is a noisy data set.,00:55:25.710,00:55:27.350
"So each of these is picked
independently.",00:55:27.350,00:55:30.190
"This y depends on x, and the only
unknown here is the noise.",00:55:30.190,00:55:34.960
"So you get this value, it gives you
the average, and then you add",00:55:34.960,00:55:37.720
a noise to get the y.,00:55:37.720,00:55:38.970
"Do you remember the linear
regression solution?",00:55:41.310,00:55:43.440
"Regardless of what the target function
is, you look at the data, and this is",00:55:43.440,00:55:46.200
what you get for the solution.,00:55:46.200,00:55:48.770
"You take the input data set,
and the output data set.",00:55:48.770,00:55:52.950
"You do this algebraic combination, and
whatever comes out is your output of",00:55:52.950,00:55:57.480
the linear regression.,00:55:57.480,00:55:58.420
This is your final hypothesis.,00:55:58.420,00:56:00.150
We have done that.,00:56:00.570,00:56:02.590
"And now we are going to think about
a notion of the in-sample error, not",00:56:02.590,00:56:07.050
"in-sample error as a summary quantity,
but the in-sample error pattern.",00:56:07.050,00:56:10.920
"How much error do I get
in the first example?",00:56:10.920,00:56:12.600
"How much error do I get in the
second, third, et cetera?",00:56:12.600,00:56:15.000
Just for our purposes.,00:56:15.000,00:56:16.150
So what would that be?,00:56:16.150,00:56:17.390
"Well, that would be what I got
in the final hypothesis.",00:56:17.390,00:56:22.060
"I apply the final hypothesis
to the input points.",00:56:22.060,00:56:24.900
"I am going to get a pattern of values
that my hypothesis is predicting.",00:56:24.900,00:56:29.020
"I compare them to the actual targets,
which happen to be stored in the y.",00:56:29.020,00:56:32.870
And that would be an error pattern.,00:56:32.870,00:56:34.980
"So it would be plus something
minus something, plus",00:56:34.980,00:56:37.290
something minus something.,00:56:37.290,00:56:38.230
"And if I add the squared values here,
get the average of those, I will",00:56:38.230,00:56:42.110
get what we call the in-sample error.,00:56:42.110,00:56:45.160
"For the out-of-sample error, I am going
to play a simplifying trick here,",00:56:45.160,00:56:48.490
"in order to get the learning
curve in the finite case.",00:56:48.490,00:56:52.650
"Here I am going to consider that, in
order to get the out-of-sample error,",00:56:52.650,00:56:57.270
"what I'm going to do I am going to just
generate the same inputs, which",00:56:57.270,00:57:02.030
is a complete no-no in out-of-sample.,00:57:02.030,00:57:04.060
"Supposedly in out-of-sample, you get
points that you haven't seen before.",00:57:04.060,00:57:07.250
You have seen these x's before.,00:57:07.250,00:57:09.270
"But the redeeming value is that I'm
now going to give you fresh noise.",00:57:09.270,00:57:13.850
"So that's the unknown, and that is what
allows me to say that it plays",00:57:13.850,00:57:18.210
the role of an out-of-sample.,00:57:18.210,00:57:19.710
"I'm going to generate another set of
points with different noises, but on",00:57:19.710,00:57:24.190
"the same inputs in order to
simplify the analysis.",00:57:24.190,00:57:26.770
"You see that the x's
here are involved.",00:57:26.770,00:57:28.860
"And if I use the same inputs,
things will simplify.",00:57:28.860,00:57:31.620
"And in that case, if you ask yourself
what is the out-of-sample error of",00:57:31.620,00:57:34.890
"those, it's exactly the same.",00:57:34.890,00:57:36.010
I evaluated on the points.,00:57:36.010,00:57:37.920
"They happen to be the points
for the out-of-sample.",00:57:37.920,00:57:40.210
And I'm comparing it with y.,00:57:40.210,00:57:41.490
"I'm calling it y dash, which is exactly
the same thing, except with",00:57:41.490,00:57:44.610
"noise dash, another realization
of the noise.",00:57:44.610,00:57:47.590
"This is the outline of the setup to
get us the learning curves we want.",00:57:47.590,00:57:52.950
"When you do the analysis, not that
difficult at all, you will get this",00:57:52.950,00:57:57.210
very interesting curve.,00:57:57.210,00:57:59.530
"This is the learning curve, and
it has very specific values.",00:57:59.530,00:58:02.770
"sigma squared, that's the
variance of the noise.",00:58:02.770,00:58:05.310
This is the best you can do.,00:58:05.310,00:58:06.620
"I expect that, because you told
me the target is linear.",00:58:06.620,00:58:09.460
So I can get that perfectly.,00:58:09.460,00:58:10.970
"But then, there is this added noise.",00:58:10.970,00:58:12.320
I cannot capture the noise.,00:58:12.320,00:58:13.790
"What is the variance of the
noise? sigma squared.",00:58:13.790,00:58:15.490
"So this is the error
that is inevitable.",00:58:15.490,00:58:18.580
Look at the in-sample error.,00:58:18.580,00:58:20.000
"Up to d plus 1, you were perfect.",00:58:20.000,00:58:21.570
"Yeah, of course I am perfect.",00:58:21.570,00:58:22.640
"Because I have d plus 1 parameters in
linear, and I am fitting less than",00:58:22.640,00:58:26.720
"those, so I can fit them perfectly.",00:58:26.720,00:58:28.630
"It doesn't mean much for the
out-of-sample error, but",00:58:28.630,00:58:31.350
that's what I get.,00:58:31.350,00:58:32.310
"I start compromising when
I get more points.",00:58:32.310,00:58:34.610
"And as I go with more points,
here I'm fitting the noise.",00:58:34.610,00:58:39.680
I am fitting the noise less.,00:58:39.680,00:58:40.450
The noise is averaging out.,00:58:40.450,00:58:41.540
"Now I'm getting very, very close,
to as if there was no noise.",00:58:41.540,00:58:44.110
"Because the pattern persists,
which is the linear guy.",00:58:44.110,00:58:47.300
"And the noise, if I get more
examples, more or less cancels out in",00:58:47.300,00:58:50.800
the fitting.,00:58:50.800,00:58:51.440
"I don't have enough degrees of
freedom to fit them all.",00:58:51.440,00:58:53.760
"So I get to average, until
eventually I get to as if",00:58:53.760,00:58:56.760
I am doing it perfectly.,00:58:56.760,00:58:58.030
And out-of-sample goes down.,00:58:58.030,00:59:00.170
"There is a very specific formula that
you can get, which is interesting.",00:59:00.170,00:59:02.960
So let me finish with this.,00:59:02.960,00:59:04.760
"The best approximation error
is sigma squared.",00:59:04.760,00:59:07.270
"That's the line, right?",00:59:07.270,00:59:10.320
What is the expected in-sample error?,00:59:10.320,00:59:12.275
"It has a very simple formula, which is--
everything is scaled by sigma squared.",00:59:12.275,00:59:18.250
"So What you have here is,
it's almost perfect.",00:59:18.250,00:59:21.700
"And you are doing better than perfect, by
this amount, the ratio of d plus 1.",00:59:21.700,00:59:27.210
Remember what d plus 1 was?,00:59:27.210,00:59:28.500
"For the perceptron, it
was the VC dimension.",00:59:28.500,00:59:31.000
"Here it's also a VC dimension of sorts,
the degrees of freedom that",00:59:31.000,00:59:33.930
linear regression has.,00:59:33.930,00:59:35.410
"So we divide the degrees of freedom
by the number of examples.",00:59:35.410,00:59:38.280
That is the factor that you get.,00:59:38.280,00:59:40.720
"And you realize here that this
is the best you can do.",00:59:40.720,00:59:45.220
"And here you are doing
better than the best.",00:59:45.220,00:59:47.440
Why is it better than the best?,00:59:47.440,00:59:48.320
"Because I'm not trying to
fit the whole function.",00:59:48.320,00:59:49.780
I am only fitting the finite sample.,00:59:49.780,00:59:53.020
"So I'm doing very well, and I'm very
happy about it, little that I know",00:59:53.020,00:59:56.260
that I'm actually harming myself.,00:59:56.260,00:59:58.280
"Because what I'm doing here,
I am fitting the noise.",00:59:58.280,01:00:00.400
"And as a result of that, I am deviating
from the optimal guy.",01:00:00.400,01:00:02.870
"And I am paying the price
in out-of-sample error.",01:00:02.870,01:00:06.570
"What is the price I am paying
in out-of-sample error?",01:00:06.570,01:00:08.870
It is the mirror image.,01:00:08.870,01:00:11.500
"I lose exactly in out-of-sample
what I gained in-sample.",01:00:11.500,01:00:16.030
"And the most interesting quantity
is the summary quantity.",01:00:16.030,01:00:18.230
"What is the expected generalization
error?",01:00:18.230,01:00:20.100
"Well, the generalization error is the
difference between this and that.",01:00:20.100,01:00:22.160
I have the formula for them.,01:00:22.160,01:00:23.330
So all I need to do is write this.,01:00:23.330,01:00:26.220
Let me magnify this.,01:00:26.220,01:00:29.510
This is the generalization error.,01:00:29.510,01:00:31.072
"It has the form of the VC dimension
divided by the number of examples.",01:00:31.072,01:00:37.510
"In this case, it's exact.",01:00:37.510,01:00:40.180
And this is what I promised last time.,01:00:40.180,01:00:42.220
"I told you that this rule of
proportionality between a VC dimension",01:00:42.220,01:00:45.260
"and a number of examples persists to
the level where sometimes, you just",01:00:45.260,01:00:49.440
"divide the VC dimension by the number of
examples, and that will give you",01:00:49.440,01:00:52.200
a generalization error.,01:00:52.200,01:00:54.000
"This is the concrete version of it, in
spite of the fact that here is not",01:00:54.000,01:00:57.780
a VC dimension.,01:00:57.780,01:00:58.560
This is real-valued.,01:00:58.560,01:00:59.670
"But it's degrees of freedom,
so it plays the role.",01:00:59.670,01:01:01.970
"We could actually solve for it and
realize that this is indeed the",01:01:01.970,01:01:05.230
"compromise between the degrees of
freedom I have, in the case of linear",01:01:05.230,01:01:08.350
"regression, and the number
of examples I am using.",01:01:08.350,01:01:12.510
So we will stop here.,01:01:12.510,01:01:13.690
"And we will go into questions and
answers after a short break.",01:01:13.690,01:01:17.518
Let's go into the questions.,01:01:22.660,01:01:25.410
MODERATOR: Right.,01:01:25.410,01:01:25.770
"The first question is if you
can go back to slide 19.",01:01:25.770,01:01:28.754
PROFESSOR: 19.,01:01:28.754,01:01:30.004
"MODERATOR: The question is if you can
explain how complex models are better",01:01:34.130,01:01:39.670
than simple models.,01:01:39.670,01:01:41.210
PROFESSOR: OK.,01:01:41.210,01:01:43.260
Better in something.,01:01:43.260,01:01:44.670
"I think the key issue in the theory
is, there is a tradeoff.",01:01:44.670,01:01:49.530
"Nothing is better on all fronts, and
nothing is worse on all fronts.",01:01:49.530,01:01:53.790
"So let's compare the simple model
and the complex model.",01:01:53.790,01:01:57.930
"In terms of the ability to approximate,
whether that ability to",01:01:57.930,01:02:01.730
"approximate is in-sample, or whether
the ability to approximate is",01:02:01.730,01:02:04.870
"absolute, what is the ability to
approximate in the absolute?",01:02:04.870,01:02:09.000
"Here is my hypothesis set, and
I have a target function.",01:02:09.000,01:02:11.640
"The horizontal line, that height gives
you the error of approximation.",01:02:11.640,01:02:19.110
"So if you go from a simple model to
a complex model, you will be able to",01:02:19.110,01:02:23.160
approximate better.,01:02:23.160,01:02:24.550
That is obvious.,01:02:24.550,01:02:25.900
"And that also is inherited if your
approximation is focused only on the",01:02:25.900,01:02:29.960
training examples.,01:02:29.960,01:02:30.730
"In this case, you are comparing
not the horizontal lines,",01:02:30.730,01:02:33.530
but the blue curves.,01:02:33.530,01:02:34.710
"This is the error you make in
approximating the sample you get.",01:02:34.710,01:02:40.050
"And again, the approximation for the
simple model is worse than the",01:02:40.050,01:02:43.790
approximation for the complex model.,01:02:43.790,01:02:45.890
"So if your game is approximation, and
that's your purpose, then obviously",01:02:45.890,01:02:49.820
the complex model is better.,01:02:49.820,01:02:51.852
"In this particular case, you
can also ask yourself about the",01:02:51.852,01:02:55.620
generalization ability.,01:02:55.620,01:02:57.090
"The generalization ability will be the
discrepancy between, either the blue",01:02:57.090,01:03:02.800
and red curve.,01:03:02.800,01:03:03.750
That would be the VC analysis.,01:03:03.750,01:03:05.105
"This would be how much I lose from going
from in-sample to out-of-sample.",01:03:05.105,01:03:09.810
"Or how much I lose from a perfect
approximation, in the case of the",01:03:09.810,01:03:14.920
"bias-variance analysis, to getting E_out,
because of my inability to zoom in on",01:03:14.920,01:03:21.850
the right hypothesis.,01:03:21.850,01:03:22.910
This would be that area here.,01:03:22.910,01:03:25.860
"So whether you are taking the difference
between the blue and red",01:03:25.860,01:03:28.550
"curve, or the difference between the
red curve and the black line, that",01:03:28.550,01:03:32.600
area is smaller here than here.,01:03:32.600,01:03:35.470
"Therefore, the simple model
is better, as far as the",01:03:35.470,01:03:39.310
generalization is concerned.,01:03:39.310,01:03:41.730
"Now because it's a tradeoff, and I have
one of them better and one of",01:03:41.730,01:03:44.880
"them worse, then the question is, when
I put them together, which is better?",01:03:44.880,01:03:48.840
"Because the bottom line in learning
is the red curve.",01:03:48.840,01:03:52.980
That's what I care about.,01:03:52.980,01:03:54.360
"This is the performance of the system
that I'm going to deliver to my",01:03:54.360,01:03:57.070
"customer, and they're going
to test it out-of-sample.",01:03:57.070,01:03:59.760
"And if they get it right,
they will be happy.",01:03:59.760,01:04:03.010
"So now because I have two quantities
that I'm adding, and one of them is",01:04:03.010,01:04:07.140
"going down, and one of them is going
up, then it is obvious that the sum",01:04:07.140,01:04:12.020
could go either way.,01:04:12.020,01:04:13.390
"And in this case, you can see
that it is going either way.",01:04:13.390,01:04:15.860
"For example, if you have few examples,
then E_out here is OK.",01:04:15.860,01:04:20.750
"It's not great, but it's decent.",01:04:20.750,01:04:22.240
"If you have the same number of examples
here, E_out is a disaster.",01:04:22.240,01:04:26.830
"So if you have few examples,
you simply cannot",01:04:26.830,01:04:28.640
afford the complex model.,01:04:28.640,01:04:30.280
"You are better off working with a simple
model, and you will get better",01:04:30.280,01:04:33.210
out-of-sample error.,01:04:33.210,01:04:34.810
"If I give you much bigger resource
of the examples-- if you are here, now",01:04:34.810,01:04:38.260
"this one is limited by the
fact that it's simple.",01:04:38.260,01:04:40.460
It cannot get any better.,01:04:40.460,01:04:41.600
It has all the information.,01:04:41.600,01:04:42.465
"It zooms in perfectly, but
it cannot get any better.",01:04:42.465,01:04:45.040
"This guy now gets to use its degrees of
freedom properly, and gets you to",01:04:45.040,01:04:48.930
a smaller value.,01:04:48.930,01:04:49.860
"So for larger number of points, you
get a better performance here.",01:04:49.860,01:04:53.800
"That's why we are saying that you should
match the complexity of the",01:04:53.800,01:04:57.070
"model to the data resources you have,
which in this case are represented by",01:04:57.070,01:05:02.530
"N. We're talking about different target
functions and different things.",01:05:02.530,01:05:06.790
"But in choosing this model or another,
what really dictates the performance",01:05:06.790,01:05:10.920
"is the number of examples versus
the complexity of the model.",01:05:10.920,01:05:14.246
MODERATOR: OK.,01:05:17.660,01:05:18.910
"When you did the analysis for linear
regression, if you did it using the",01:05:21.340,01:05:25.290
"perceptron model, would you get
the same generalization error?",01:05:25.290,01:05:29.510
PROFESSOR: Let's go for that.,01:05:29.510,01:05:31.085
"The analysis of the bias-variance, and
it's also inherited in the learning",01:05:35.590,01:05:41.440
"curves-- the analysis is very clean
when you use mean squared error.",01:05:41.440,01:05:46.400
"Obviously, you can use mean squared
error in the perceptron.",01:05:46.400,01:05:50.050
There will be a correspondence here.,01:05:50.050,01:05:52.460
"But the ability to get such a clean
formula here really depends on the",01:05:52.460,01:05:57.350
very particulars of linear regression.,01:05:57.350,01:06:01.610
"If you go back to the previous slide,
where the assumption is, it was very",01:06:01.610,01:06:05.480
"critical to make the assumption that the
out-of-sample is this way, and to",01:06:05.480,01:06:09.830
"make the target very specifically linear
plus noise, in order to be able",01:06:09.830,01:06:13.180
to simplify.,01:06:13.180,01:06:14.160
"The result, by the way, holds
in general, asymptotically.",01:06:14.160,01:06:17.240
"So if you take genuine out-of-sample,
which means that you pick different",01:06:17.240,01:06:20.460
"points, you will get
a different matrix X.",01:06:20.460,01:06:22.880
"So you will apply w that you got from
in-sample, you'll apply it to X dash in",01:06:22.880,01:06:27.690
"this case, which is this,
and then y dash.",01:06:27.690,01:06:30.230
"And the problem is that when you plug
it in here, and try to get a formula",01:06:30.230,01:06:33.510
"for that, the formula will depend on
how the X dash relates to the X.",01:06:33.510,01:06:38.670
"When it's the same, they cancel
out neatly, and you get the",01:06:38.670,01:06:41.200
formula that I had.,01:06:41.200,01:06:42.280
"But asymptotically, if you make certain
assumptions about how X is",01:06:42.280,01:06:45.720
"generated and you take the asymptotic
result, you will get the same thing.",01:06:45.720,01:06:49.560
So the short answer is the following.,01:06:49.560,01:06:51.350
"The analysis in the exact form that I
gave, which gives me these very neat",01:06:51.350,01:06:55.190
"results, is very specific to linear
regression, very specific to the",01:06:55.190,01:06:59.030
"choice of out-of-sample as I did it,",01:06:59.030,01:07:00.680
"if you want to give the answer
exactly in a finite case.",01:07:00.680,01:07:03.810
"If you use a perceptron, you will be
able to find a parallel, but it may",01:07:03.810,01:07:06.990
not be as neat.,01:07:06.990,01:07:10.140
MODERATOR: Quick clarification.,01:07:10.140,01:07:11.190
"sigma squared is the variance
of the noise in the--",01:07:11.190,01:07:14.220
PROFESSOR: Yeah.,01:07:14.220,01:07:15.700
I just realized that.,01:07:15.990,01:07:17.810
I have been using bias-variance.,01:07:17.810,01:07:19.820
"The lecture is called bias-variance, and
now we have variance of the noise.",01:07:19.820,01:07:22.830
"So obviously, I am so used to these
things that I didn't notice.",01:07:22.830,01:07:26.000
"When I say the variance here, this has
absolutely nothing to do with the",01:07:26.000,01:07:29.700
"bias-variance analysis
that I talked about.",01:07:29.700,01:07:31.610
It's a noise.,01:07:31.610,01:07:32.700
"I am trying to measure
the energy of it.",01:07:32.700,01:07:35.380
"It's a zero-mean noise, so the energy of
it is proportional to the variance.",01:07:35.380,01:07:41.360
"So I should have called it-- the energy of
the noise is sigma squared, in order",01:07:41.360,01:07:45.080
not to confuse people.,01:07:45.080,01:07:46.000
"But I hope that I did not
confuse too many people.",01:07:46.000,01:07:49.830
"MODERATOR: Can the bias-variance
analysis be",01:07:49.830,01:07:51.820
used for model selection?,01:07:51.820,01:07:55.580
"PROFESSOR: Bias-variance
analysis, just because it is so",01:07:55.580,01:07:58.040
"specific, it actually assumes that you
know the target function, if you want",01:07:58.040,01:08:02.230
to get the quantities explicitly.,01:08:02.230,01:08:03.900
"So for example linear regression,
I assume the form is linear plus noise.",01:08:03.900,01:08:07.700
"For the sinusoidal case, we got the
answers, and we were able to choose.",01:08:07.700,01:08:11.100
"But you actually knew that
it was a sinusoid.",01:08:11.100,01:08:13.410
"So the bias-variance analysis
is taken as a guide.",01:08:13.410,01:08:17.010
But it's a very important guide.,01:08:17.010,01:08:18.689
"Because I can ask myself how do I
affect-- I want to",01:08:18.689,01:08:22.920
get E_out to be down.,01:08:22.920,01:08:24.120
"Now I know that there
are two contributing",01:08:24.609,01:08:26.100
"factors, bias and variance.",01:08:26.100,01:08:28.260
"Can I get the variance down, without
getting the bias up?",01:08:28.260,01:08:33.140
That's a bunch of techniques.,01:08:33.140,01:08:34.140
"Regularization will belong
to that category.",01:08:34.140,01:08:36.479
Can I get both of them down?,01:08:36.479,01:08:38.880
That will be learning from hints.,01:08:38.880,01:08:40.130
"There will be something that affects
both of them, and so on.",01:08:40.130,01:08:43.060
"So you can map different techniques
to how they are affecting",01:08:43.060,01:08:46.450
the bias and variance.,01:08:46.450,01:08:48.620
"I would say that, in terms of any
application to learning situation,",01:08:48.620,01:08:51.689
"it's a guideline rather than something
that I'm going to plug in, and tell you",01:08:51.689,01:08:55.120
what the model is.,01:08:55.120,01:08:58.090
"The answer for the model selection is
mostly through validation, which we're",01:08:58.090,01:09:02.819
going to talk about in a few lectures.,01:09:02.819,01:09:05.170
"And this is the gold standard for the
choices you make in a learning",01:09:05.170,01:09:09.410
"situation, including
choosing the model.",01:09:09.410,01:09:13.425
"MODERATOR: I have a question
getting a little bit ahead.",01:09:13.425,01:09:15.560
"In ensemble methods, like boosting or
something, is there a reason under",01:09:18.520,01:09:23.000
these analyses why those methods work?,01:09:23.000,01:09:26.140
"PROFESSOR: I almost included
this in the lecture, but I thought it",01:09:26.140,01:09:29.130
was one too many.,01:09:29.130,01:09:31.120
If you look at the idea of g bar.,01:09:31.120,01:09:37.706
Let me try to get to its definition.,01:09:37.706,01:09:41.300
"This was just a theoretical
tool of analysis.",01:09:41.300,01:09:43.899
"I have g bar equals the expected
value of that.",01:09:43.899,01:09:46.350
"And if I want to do it with a finite
number of sets, I sum up this, and",01:09:46.350,01:09:53.510
normalize by 1 over K.,01:09:53.510,01:09:55.810
"Although this was just
a theoretical way of getting the",01:09:55.810,01:09:59.080
"bias-variance decomposition, and this is
a conceptual way of understanding",01:09:59.080,01:10:01.750
"what it is, there is an ensemble
learning method that builds exactly on",01:10:01.750,01:10:07.070
"this, which is called Bagging--
bootstrap aggregation.",01:10:07.070,01:10:10.950
"And the idea is, what do I need
in order to get g bar?",01:10:10.950,01:10:14.130
"We said g bar is great,
if I can get it.",01:10:14.130,01:10:15.700
"But it requires an infinite number
of data sets, and I have",01:10:15.700,01:10:19.340
only one data set.,01:10:19.340,01:10:20.530
"So the idea of Bagging is that, I am
going to use my data set to generate",01:10:20.530,01:10:25.790
a large number of different data sets.,01:10:25.790,01:10:28.690
How am I going to do that?,01:10:28.690,01:10:29.710
"Well, that's bootstrapping.",01:10:29.710,01:10:31.080
Bootstrapping always looks like magic.,01:10:31.080,01:10:34.350
You know where the expression comes from?,01:10:34.350,01:10:36.280
"Bootstrapping, you try to lift
yourself by pulling on your",01:10:36.280,01:10:42.360
bootstraps.,01:10:42.360,01:10:43.030
"Which obviously, you cannot do,
because you are pulling on it.",01:10:43.030,01:10:45.320
But that's what you do.,01:10:45.320,01:10:46.830
"Here we are trying to create something,
where it isn't there.",01:10:46.830,01:10:49.820
"And in this particular case, what you
do is you sample randomly from your",01:10:49.820,01:10:56.460
"data set, in order to get different
data sets, and then average.",01:10:56.460,01:10:59.580
"And believe it or not, that gives
you actually a dividend.",01:10:59.580,01:11:01.640
"It gives you something about
the ensemble learning.",01:11:01.640,01:11:04.920
"There are other, obviously
more sophisticated,",01:11:04.920,01:11:06.520
methods of ensemble learning.,01:11:06.520,01:11:07.800
"And one way or the other, they appeal to
the fact that you are reducing the",01:11:07.800,01:11:11.160
"variance by averaging
a bunch of stuff.",01:11:11.160,01:11:14.410
"So you can say that it's either taken
outright, like Bagging, or inspired",01:11:14.410,01:11:18.470
"in some sense, that it's a good idea
to average because you cancel out",01:11:18.470,01:11:21.380
fluctuations.,01:11:21.380,01:11:22.630
"MODERATOR: If we use the Bayesian
approach, does this bias-variance",01:11:26.420,01:11:31.180
dilemma still appear?,01:11:31.180,01:11:34.550
"PROFESSOR: Repeat
the question, please.",01:11:34.550,01:11:35.300
"MODERATOR: If you use a Bayesian
approach, does this bias-variance",01:11:35.300,01:11:39.070
still appear?,01:11:39.070,01:11:39.740
PROFESSOR: OK.,01:11:39.740,01:11:40.990
The bias-variance is there to stay.,01:11:43.325,01:11:45.800
It's a fact.,01:11:45.800,01:11:47.050
"And we can take a particular approach,
and then we are going to perhaps find",01:11:47.050,01:11:54.030
"an explicit expression for
the bias, and an explicit",01:11:54.030,01:11:56.290
expression for the variance.,01:11:56.290,01:11:57.060
"But nothing will change about the
nature of things because of the",01:11:57.060,01:11:59.310
approach I have.,01:11:59.310,01:12:00.830
"Now the Bayesian approach is very
particular, because the Bayesian",01:12:00.830,01:12:04.250
approach makes certain assumptions.,01:12:04.250,01:12:06.810
"And after you make these assumptions,
you can answer",01:12:06.810,01:12:09.090
all questions perfectly.,01:12:09.090,01:12:11.100
"You can answer questions like that,
and other questions as well.",01:12:11.100,01:12:13.980
"And I will talk about the Bayesian
approach in the very last lecture of",01:12:13.980,01:12:18.270
the course.,01:12:18.270,01:12:19.030
"So I will defer answers, that are specific
to that, until that point.",01:12:19.030,01:12:23.530
"But basically, the answer to this very
specific question, it's like if you",01:12:23.530,01:12:28.610
"ask, does the VC dimension change
if you apply the Bayesian?",01:12:28.610,01:12:31.390
"Well, you apply the Bayesian, this
is just a bunch of assumptions.",01:12:31.390,01:12:34.800
The VC dimension is there.,01:12:34.800,01:12:36.440
"Maybe by using the Bayesian you'll be
able to find more direct quantities to",01:12:36.440,01:12:39.840
predict what you want.,01:12:39.840,01:12:40.920
"But the VC dimension is there, because
it's defined in a general setup.",01:12:40.920,01:12:46.960
"MODERATOR: A question about relation
with numerical function approximation.",01:12:46.960,01:12:52.510
"In that field, there's interpolation
and extrapolation.",01:12:52.510,01:12:55.950
"When is there extrapolation
in machine learning?",01:12:55.950,01:13:00.440
"PROFESSOR: Function
approximation is one of the fields",01:13:00.440,01:13:01.970
that is very much related.,01:13:01.970,01:13:03.110
"Because we are given a finite sample,
and they're coming from a function, and",01:13:03.110,01:13:06.430
you're trying to approximate it.,01:13:06.430,01:13:07.350
And this is one of the applications.,01:13:07.350,01:13:09.690
"In general, interpolation is
easier than extrapolation,",01:13:09.690,01:13:12.480
because you have a handle.,01:13:12.480,01:13:14.570
"And if you want to articulate that in
terms of the stuff we have, the",01:13:14.570,01:13:18.600
"variance in interpolation is smaller
than the variance in",01:13:18.600,01:13:21.020
extrapolation in general.,01:13:21.020,01:13:22.280
"Remember, the lines in the sinusoid?",01:13:22.280,01:13:25.440
They were all over the place.,01:13:25.440,01:13:27.330
"If you take between the two points-- so
I have the sinusoid, and I have the",01:13:27.330,01:13:30.040
"two points, I'm connecting
them with a line.",01:13:30.040,01:13:31.620
"Between the two points, I am very much
in good shape, because the sine is",01:13:31.620,01:13:36.010
"this way, and I am this way.",01:13:36.010,01:13:38.050
So it's not that big a deal.,01:13:38.050,01:13:39.840
"The further out you go, then there
is a lot of fluctuation.",01:13:39.840,01:13:43.300
"And that is reflected in
the extrapolation.",01:13:43.300,01:13:47.830
MODERATOR: OK.,01:13:47.830,01:13:49.080
"When the variance is big, we
know we're extrapolating.",01:13:51.400,01:13:53.820
Is that the answer?,01:13:53.820,01:13:54.925
PROFESSOR: No.,01:13:54.925,01:13:56.330
"I will say there is an association
between them.",01:13:56.330,01:13:58.980
"To answer this specifically, you need
to understand the particular case.",01:13:58.980,01:14:01.720
"There may be cases, where the
extrapolation doesn't have a lot of",01:14:01.720,01:14:04.650
variance and whatnot.,01:14:04.650,01:14:05.430
"I'm just trying to map in general,
what the quantity here",01:14:05.430,01:14:09.510
"corresponds to, in that.",01:14:09.510,01:14:11.740
"The problem with extrapolation can be
posed, in this case, in terms of more",01:14:11.740,01:14:17.070
variance than interpolation.,01:14:17.070,01:14:19.670
"But I'm not making a mathematical
statement that this is guaranteed to",01:14:19.670,01:14:22.200
be the case.,01:14:22.200,01:14:23.450
"MODERATOR: Could you explain what the
literature means by the bias-variance",01:14:25.950,01:14:30.530
covariance dilemma?,01:14:30.530,01:14:33.050
PROFESSOR: OK.,01:14:33.050,01:14:34.300
"You can pursue this analysis a little
bit further to the cases where you",01:14:37.610,01:14:42.730
have cross terms.,01:14:42.730,01:14:44.160
"Particularly for boosting,
this is the case.",01:14:44.160,01:14:47.500
"And then there is a question of, I
am trying to get these guys that I'm",01:14:47.500,01:14:52.870
"going to average in order to
get the final hypothesis.",01:14:52.870,01:14:55.090
That's my game.,01:14:55.090,01:14:56.570
"Now it would be nice if I can
get them to be independent.",01:14:56.570,01:15:00.190
"Because when I get them to be
independent, then adding them up",01:15:00.190,01:15:03.800
"reduces the variance
in a very good way.",01:15:03.800,01:15:07.890
"But then, in general, when you
actually apply some of these",01:15:07.890,01:15:10.250
"algorithms, there is a correlation
between one and another.",01:15:10.250,01:15:11.830
So there's a covariance.,01:15:11.830,01:15:13.120
"So there's a question of the
balance between the two.",01:15:13.120,01:15:16.850
"But it really is, in terms of
application, related more to ensemble",01:15:16.850,01:15:21.470
"learning than to just the general
bias-variance analysis as I did it.",01:15:21.470,01:15:25.360
"Because in the bias-variance analysis,
I had the luxury of picking",01:15:25.360,01:15:30.580
"independently generated data sets,
generating independent guys, and then",01:15:30.580,01:15:34.140
"averaging them, because it's
a conceptual aspect.",01:15:34.140,01:15:36.960
"But when you actually are using
a technique, where you are constructing",01:15:36.960,01:15:39.930
"these guys based on variations of the
data set, then the covariance starts",01:15:39.930,01:15:44.960
playing a role.,01:15:44.960,01:15:46.210
"MODERATOR: A question about,
I guess, naming the things.",01:15:49.675,01:15:52.810
"Is linear regression
actually learning?",01:15:52.810,01:15:54.820
"Or is it just fitting along the lines
of function approximation?",01:15:54.820,01:15:59.980
"PROFESSOR: Linear regression,
is a learning technique.",01:15:59.980,01:16:02.610
"And fitting is the first
part of learning.",01:16:02.610,01:16:05.980
"So you always fit, in order to learn.",01:16:05.980,01:16:08.130
"The only added thing is that you want
to make sure that, as you fit, you",01:16:08.130,01:16:11.260
always perform well out-of-sample.,01:16:11.260,01:16:12.660
That's what the theory was about.,01:16:12.660,01:16:14.610
"I've been spending four lectures trying
to make sure, that when you do",01:16:14.610,01:16:17.250
"the intuitive thing, I give
you data, you fit them.",01:16:17.250,01:16:19.740
"You could do that without taking
a machine learning course.",01:16:19.740,01:16:22.050
"Now I'm telling you that you have to
have the checks in place, such that",01:16:22.050,01:16:24.640
"when you fit in-sample, something good
happens in what you care about, which",01:16:24.640,01:16:28.410
is out-of-sample.,01:16:28.410,01:16:29.320
So that's the--,01:16:29.320,01:16:31.990
MODERATOR: All right.,01:16:31.990,01:16:32.488
I think that's it.,01:16:32.488,01:16:33.700
PROFESSOR: Very good.,01:16:33.700,01:16:34.240
We'll see you next week.,01:16:34.500,01:16:35.750

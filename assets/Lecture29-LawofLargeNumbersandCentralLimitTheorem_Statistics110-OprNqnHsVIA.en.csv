text,start,stop
"All right, so let's get started.",00:00:00.420,00:00:02.410
"So today,
we're gonna talk about what are probably",00:00:02.410,00:00:06.990
"the two most famous theorems in
the entire history of probably.",00:00:06.990,00:00:11.490
"They're called the law of large
numbers and the central limit theorem.",00:00:11.490,00:00:15.890
"They're closely related, so makes sense
to do them together, kind of compare and",00:00:15.890,00:00:20.330
contrast them.,00:00:20.330,00:00:21.100
"I don't, I can't think of a more famous
probability theorem than these two.",00:00:22.320,00:00:27.640
"So the setup for today is that
we have i.i.d random variables.",00:00:27.640,00:00:34.690
"Let's just call them X1, X2 i.i.d.",00:00:34.690,00:00:39.400
"Since they're i.i.d they have
the same mean and variance.",00:00:39.400,00:00:44.920
"If the mean and variance exists but
we'll assume they do.",00:00:44.920,00:00:49.060
"So the mean, we'll just call it Mu.",00:00:49.060,00:00:50.600
"And the variants, sigma squared.",00:00:50.600,00:00:53.670
"So we're assuming that
these are finite for now.",00:00:56.460,00:00:59.200
The mean and variants exist.,00:00:59.200,00:01:00.760
And both of these theorems tell us,00:01:00.760,00:01:05.420
"what happens to the sample
mean as n gets large.",00:01:05.420,00:01:09.200
"So, the sample mean is
just defined as Xn bar.",00:01:09.200,00:01:12.470
"Standard notation in statistics
is put a bar to mean averages and",00:01:14.090,00:01:17.960
that's just the average of the first n.,00:01:17.960,00:01:20.000
"So to take the first n random variables,
and average them, so",00:01:21.130,00:01:24.718
that's just called the sample mean.,00:01:24.718,00:01:26.934
"So the question is, what can we
say about Xn bar as n gets large?",00:01:32.491,00:01:37.980
"So the way we would interpret this or
use this is we get to observe.",00:01:37.980,00:01:44.180
"These Xs, they're random variables but
after we observe them they become data.",00:01:44.180,00:01:47.540
"We're never going to have
an infinite amount of data so",00:01:47.540,00:01:51.230
at some point we stop it at n.,00:01:51.230,00:01:52.870
"We can think of that as the sample size
and hopefully we get a large sample size.",00:01:52.870,00:01:57.440
"Of course, it depends on the problem.",00:01:57.440,00:01:58.700
"Some problems,
you may not be able to get large n.",00:01:58.700,00:02:01.440
"Well, we assume n is large, and",00:02:01.440,00:02:03.230
"just take the average,
question is just, what can we say?",00:02:03.230,00:02:07.110
"All right, so first,
here's what the law of large numbers says.",00:02:07.110,00:02:12.217
It's a very simple statement.,00:02:16.689,00:02:17.860
"And hopefully pretty intuitive, too.",00:02:19.870,00:02:22.140
Law of Large Numbers says that Xn bar,00:02:22.140,00:02:27.510
"converges to mu, as n goes to infinity.",00:02:27.510,00:02:33.600
With probability 1.,00:02:36.217,00:02:37.726
"That's the fine print, probability 1.",00:02:39.745,00:02:44.152
"With probability 0, so
something really crazy could happen.",00:02:44.152,00:02:47.380
"But we don't worry too much about it,
because it has probability 0.",00:02:47.380,00:02:50.170
"With probability 1,
this is the sample mean, and",00:02:50.170,00:02:54.568
"it says that the sample mean
converges to the true mean.",00:02:54.568,00:02:59.394
"So, that is a pretty nice,
intuitive, easy to remember result.",00:03:03.673,00:03:11.750
"That is,
by true I mean the theoretical mean.",00:03:11.750,00:03:14.920
"That is the expected value of Xj for
any j is the true expected value.",00:03:14.920,00:03:21.410
"Whereas this, is a random variable.",00:03:21.410,00:03:24.070
"Right?
We're taking an average of",00:03:24.070,00:03:25.200
random variables.,00:03:25.200,00:03:25.850
That's a random variable.,00:03:25.850,00:03:26.940
"So this is just a constant but
this is a random variable.",00:03:26.940,00:03:30.790
"But it's gonna converge and
I should say a little bit more,",00:03:30.790,00:03:35.600
"what is this convergence
statement actually mean.",00:03:35.600,00:03:39.000
"You've all seen limit of sequences, but
when we are talking about limits of random",00:03:39.000,00:03:43.690
"variables we have to be
a little more careful.",00:03:43.690,00:03:45.290
How do we actually define this.,00:03:45.290,00:03:47.690
"The definition of this statement
is just pointwise which means,",00:03:47.690,00:03:53.030
remember Xn bar is a random variable.,00:03:54.150,00:03:56.050
"Random variable mathematically
speaking is a function.",00:03:56.050,00:03:58.390
"So it's say for each possible,
if you evaluate this at some",00:03:58.390,00:04:02.260
"specific outcome of the experiment,
then you'll get a sequence of numbers.",00:04:02.260,00:04:06.140
"That is if you actually observed the
values and this kind of crystallizes into",00:04:06.140,00:04:11.030
"numbers when you evaluate it at
the outcome of the experiment.",00:04:11.030,00:04:14.810
And so those numbers converge to mu.,00:04:14.810,00:04:20.730
"In other words, this is an event.",00:04:20.730,00:04:23.430
"Either these random variables converge or
they don't.",00:04:23.430,00:04:27.720
And we say that event has probability 1.,00:04:27.720,00:04:31.375
"That' what the statement
of the theorem is.",00:04:31.375,00:04:34.900
So to just give a simple example.,00:04:34.900,00:04:38.470
"Let's think about what happens
if we have Bernoulli p.",00:04:41.160,00:04:45.326
"So if Xj is Bernoulli p,
then intuitively we're",00:04:45.326,00:04:50.429
"just imagining a infinite
sequence of coin tosses.",00:04:50.429,00:04:56.500
"Where the probability of heads is p, and",00:04:56.500,00:05:00.367
"then this says that if we add up
all of these Bernoullis up to n,",00:05:00.367,00:05:06.279
"that it's just in the first coin flips,
how many times did the coin land heads,",00:05:06.279,00:05:13.899
"divided by the number of flips should
convert to p with probability 1.",00:05:13.899,00:05:20.778
"So for example, so
this is a very intuitive statement.",00:05:25.294,00:05:28.800
"If it's a fair coin and
you flip the coin a million times, well,",00:05:28.800,00:05:33.730
"you're not really expecting that it
will be 500,000 heads and 500,00 tails.",00:05:33.730,00:05:39.130
"But you do think that,
in the long run, it should be the case",00:05:39.130,00:05:44.000
"that it's going to be essentially
half heads, half tails.",00:05:44.000,00:05:48.670
"Not exactly, but essentially.",00:05:48.670,00:05:50.930
"And the proportion should get closer and
closer to the true value.",00:05:50.930,00:05:54.570
"This qualification would probably 1 is
needed because mathematically speaking",00:05:56.170,00:05:59.730
"even if you have a fair coin,
there's nothing in the math that says",00:05:59.730,00:06:04.750
"it's impossible that the coin would land
heads, heads, heads, heads, heads forever.",00:06:04.750,00:06:09.340
"You know that that's never
actually gonna happen in reality.",00:06:09.340,00:06:12.580
It's just not gonna happen.,00:06:13.810,00:06:15.170
It's a fair coin.,00:06:15.170,00:06:16.710
"It might land heads, heads, heads for
a time if you're very lucky or",00:06:16.710,00:06:20.740
unlucky or whatever.,00:06:20.740,00:06:21.920
"But it's not gonna be heads,
heads, heads forever.",00:06:21.920,00:06:25.180
"But there's nothing in the math that
says that's an invalid sequence.",00:06:26.400,00:06:30.040
"So there's some weird
pathological cases like that.",00:06:31.660,00:06:35.570
"But with probability one,
we get what we expect.",00:06:35.570,00:06:39.050
"If we didn't have this result,
how we would ever even estimate p?",00:06:39.050,00:06:44.650
"You might imagine if you
didn't know what p was,",00:06:45.820,00:06:48.460
"kind of the obvious thing to do is
flip the coin a lot of times and",00:06:48.460,00:06:51.040
"take the proportion of heads and
use that as your approximation for p.",00:06:51.040,00:06:54.950
But what justification could you have for,00:06:54.950,00:06:57.130
"doing that approximation
if you didn't have this.",00:06:57.130,00:07:00.070
"So this is a very, very necessary result.",00:07:00.070,00:07:02.880
"But I guess to comment a little bit
more about what does it actually say for",00:07:06.140,00:07:09.890
"the coin, because this is kind of
related to gambler's fallacy, and",00:07:09.890,00:07:13.650
things like that.,00:07:13.650,00:07:14.350
"The gambler's fallacy is the idea
that like let's say your gambling and",00:07:15.660,00:07:19.880
"you lose like ten times in a row and then
it's the feeling that your due to win.",00:07:19.880,00:07:25.840
"You lost all these times then and
you might try to justify that using a lot",00:07:27.040,00:07:32.110
"of large numbers and say well you
know the coin might landed let's say,",00:07:32.110,00:07:36.910
"heads you win money, tails you lose money,
you just lost money ten times in a row.",00:07:36.910,00:07:41.510
"But the law of large numbers says,
in the long run,",00:07:41.510,00:07:44.330
"it's gonna go back to
one-half if it's fair.",00:07:44.330,00:07:47.030
"So somehow you need to start
winning a lot to compensate.",00:07:47.030,00:07:50.160
That's not the way it works.,00:07:51.560,00:07:53.000
The coin is memoryless.,00:07:54.540,00:07:56.000
"The coin does not care how many failures
or how many losses you had before.",00:07:56.000,00:08:00.290
"So the way it works is not through If
you're unlucky at the beginning that",00:08:00.290,00:08:04.370
"somehow it gets offset later
by an increase in heads.",00:08:04.370,00:08:09.900
"The way it works is through
what we might call swamping.",00:08:09.900,00:08:14.415
"And let's say the coin landed
tails a 100 times in a row.",00:08:14.415,00:08:19.577
"It doesn't mean that the probability
has changed for 101st flip.",00:08:19.577,00:08:24.050
"What it means though, is that we're
letting n go to infinity here, okay?",00:08:24.050,00:08:29.920
"So no matter how unlucky you
were in the first 100 or",00:08:29.920,00:08:32.570
"the first million trials, that's
nothing compared to infinity, right?",00:08:32.570,00:08:37.620
"So those first million just get swamped
out by the entire infinite future,",00:08:37.620,00:08:42.992
so that what's going on here.,00:08:42.992,00:08:47.960
"Yeah, so
to tell you one little story about the law",00:08:50.875,00:08:55.950
"of large numbers,
a colleague of mine told me this story.",00:08:55.950,00:09:01.130
"He had a student once who
said he hated statistics.",00:09:01.130,00:09:05.450
"And of course,
my colleague was very shocked,",00:09:06.460,00:09:08.770
like how can anyone hate statistics?,00:09:08.770,00:09:11.520
"And so he asked, why?",00:09:11.520,00:09:12.653
"How is it possible that
you hate statistics?",00:09:12.653,00:09:15.465
"And then the student who was an athlete,
and he was training everyday and",00:09:15.465,00:09:19.744
"he had just learned
the law of large numbers.",00:09:19.744,00:09:22.425
"And he was very, very depressed by this
because he said, the law of large numbers",00:09:22.425,00:09:26.962
"says in the long run, I'm gonna only
be average and I can't improve.",00:09:26.962,00:09:30.917
"So well, of course the fallacy there,
we assumed iid right now.",00:09:30.917,00:09:38.390
"Now there are generalizations
of this theorem beyond iid, but",00:09:38.390,00:09:41.574
we can't just get rid of iid.,00:09:41.574,00:09:43.084
"So the iid is saying that the distribution
is not changing with time.",00:09:43.084,00:09:48.435
"That doesn't mean that you can't actually
improve your own distribution then it",00:09:48.435,00:09:53.093
would not be iid.,00:09:53.093,00:09:54.140
"So don't be depressed by this,
and in fact this theorem",00:09:54.140,00:09:59.227
"I think is crucial in order for
science to actually be possible.",00:09:59.227,00:10:05.640
"Because if you kind of
imagine kind of hypothetical",00:10:05.640,00:10:08.850
"counter factual world where this
theorem was actually false.",00:10:08.850,00:10:13.800
"That would be really depressing to try
to ever learn about the world, right?",00:10:13.800,00:10:18.682
"Cuz this is saying,
you're collecting more and more data.",00:10:18.682,00:10:21.481
"You're letting your sample
size go to infinity.",00:10:21.481,00:10:24.183
"And this says,
you converged to the truth, right?",00:10:24.183,00:10:28.032
"And it would be some weird setting, where
you get more and more data, and more and",00:10:28.032,00:10:31.908
"more data, and yet you're not able
to converge to the truth, right?",00:10:31.908,00:10:35.283
So that would be really bad.,00:10:35.283,00:10:36.760
"So this is very intuitive, very important.",00:10:36.760,00:10:39.070
"Okay, so let's prove this
at least a similar version.",00:10:40.610,00:10:47.390
"So this is actually sometimes called
the strong law of large numbers.",00:10:47.390,00:10:51.480
"And we're actually gonna
prove what's sometimes called",00:10:53.280,00:10:56.240
the weak law of large numbers.,00:10:56.240,00:10:57.620
"I don't really like the terminology
strong and weak here, but",00:10:57.620,00:11:02.309
that's kind of a standard.,00:11:02.309,00:11:04.444
"Strong law of large numbers
is what I just said,",00:11:04.444,00:11:07.407
"where it's converging
point-wise with probability 1.",00:11:07.407,00:11:11.125
"That is just these random variables
converged to this constant,",00:11:13.583,00:11:19.235
"except on some bad event
that has probability 0.",00:11:19.235,00:11:23.563
"The weak law of large
numbers says that for",00:11:23.563,00:11:26.303
"any, C greater than 0,",00:11:26.303,00:11:32.337
the probability that Xn bar minus,00:11:32.337,00:11:37.097
the mean is greater than c goes to 0.,00:11:37.097,00:11:42.367
So it's a very similar looking statement.,00:11:42.367,00:11:46.290
It's not exactly equivalent.,00:11:47.830,00:11:49.410
"It's possible to show, you have to
go through some real analysis for",00:11:49.410,00:11:53.330
"this that is not necessary for
our purposes.",00:11:53.330,00:11:55.600
"But it turns out that, this statement,",00:11:55.600,00:11:57.970
"once you've proven this thing it
implies this form of convergence.",00:11:57.970,00:12:01.940
"This is called convergence in probability,
but",00:12:01.940,00:12:06.676
the intuition is very similar.,00:12:06.676,00:12:09.801
"So just to interpret this statement
in words it says, so we can chose,",00:12:09.801,00:12:14.257
"we should interpret c as
being some small number.",00:12:14.257,00:12:17.439
"So let's say we chose c to be 0.001, okay?",00:12:17.439,00:12:21.409
"And then it says that this thing
goes to 0, so in other words, this,",00:12:21.409,00:12:26.110
as n goes to infinity again.,00:12:26.110,00:12:28.070
"So this says that if n is large enough,
then",00:12:29.100,00:12:33.020
"it's extremely unlikely that
these are more than 0.001 apart.",00:12:34.090,00:12:38.860
"In other words, if n is large,",00:12:38.860,00:12:41.150
"it's extremely likely that this is
extremely close to this, right?",00:12:41.150,00:12:46.900
"So it's a very similar statement,
n is large,",00:12:46.900,00:12:49.510
"it's extremely likely that the sample
mean is very close to the true mean.",00:12:49.510,00:12:53.200
"Okay, so that's what it says.",00:12:54.490,00:12:55.880
"So we'll prove this one,",00:12:55.880,00:12:58.107
"because to prove this one takes
a lot of work and a lot of time.",00:12:58.107,00:13:03.279
"This one,
it looks like it's a nice-looking theorem.",00:13:03.279,00:13:06.450
"And it is a nice theorem, but",00:13:06.450,00:13:07.915
"we can prove it very easily
using Chebyshev's inequality.",00:13:07.915,00:13:11.040
"Okay, so
let's prove the weak law of large numbers.",00:13:15.355,00:13:18.528
"So all we need to do is show
that this goes to 0, right?",00:13:23.507,00:13:26.610
That's what the statement is.,00:13:26.610,00:13:28.460
"So let's just bound it using, this looks
pretty similar to what we were doing last",00:13:28.460,00:13:32.690
"time, where we did Markov's inequality,
Chebyshev's inequality.",00:13:32.690,00:13:36.400
"This looks similar to that
kind of stuff from last time,",00:13:36.400,00:13:39.761
"which is why I did that, well,
one reason for doing that last time.",00:13:39.761,00:13:42.790
"We need the inequalities anyway,
but it's especially useful here.",00:13:42.790,00:13:46.604
"So we just need to show
this thing goes to 0.",00:13:46.604,00:13:48.818
"Xn bar minus mu greater than c, goes to 0,",00:13:48.818,00:13:53.206
"By Chebyshev's inequality,
this is less than or",00:13:55.062,00:13:59.316
"equal to the variance of Xn
bar divided by c squared,",00:13:59.316,00:14:03.773
"that's just exactly
Chebyshev from last time.",00:14:03.773,00:14:07.950
"Now we just need the variance of Xn bar,
variance of Xn bar,",00:14:09.650,00:14:15.040
"well, just stare at the definition
of Xn bar for a second.",00:14:15.040,00:14:18.910
"There's a 1 over n in front,
that comes out as 1 over n squared.",00:14:18.910,00:14:23.220
"And then since I'm assuming
they're iid an then dependent,",00:14:25.080,00:14:28.450
"the variance of the sum is just n
times the variance of one term.",00:14:28.450,00:14:32.030
"So that's n sigma squared
divided by c squared,",00:14:32.030,00:14:36.508
which is sigma squared over nc squared.,00:14:36.508,00:14:40.230
"Sigma is a constant, c is a constant,
n goes to infinity, so this goes to 0.",00:14:41.870,00:14:48.070
"So that proved the weak law of large
numbers, just only a one line thing.",00:14:48.070,00:14:54.360
"Okay, so that tells us what happens
point-wise when we average a bunch",00:14:59.819,00:15:06.566
"of iid random variables, and
it converges to the mean.",00:15:06.566,00:15:11.860
So let me just rewrite that statement.,00:15:11.860,00:15:14.190
"Then we'll write the central limit
theorem and kind of compare them.",00:15:14.190,00:15:17.800
"So another way to write
what we just showed",00:15:17.800,00:15:22.821
"is that Xn bar minus mu
goes to 0 as n goes to",00:15:22.821,00:15:27.842
"infinity, which is a good thing to know.",00:15:27.842,00:15:32.870
"However, it doesn't tell us what
the distribution of Xn bar looks like.",00:15:33.910,00:15:40.360
"So this is true with probability one,
but what is the distribution?",00:15:40.360,00:15:48.300
"What is the distribution
of Xn bar look like?",00:15:52.000,00:15:57.579
"So this says it's getting closer,
Xn bar is getting closer and",00:16:00.753,00:16:05.086
closer to this constant mu.,00:16:05.086,00:16:07.700
"Okay, but that's not really
telling us the shape, and",00:16:07.700,00:16:10.438
it's not really telling us the rate.,00:16:10.438,00:16:12.310
"This goes to 0, but at what rate?",00:16:12.310,00:16:16.183
"So one way to think about problems like
that, when you have something going to 0,",00:16:16.183,00:16:22.826
"and you wanna study something about,
how fast does it go to 0?",00:16:22.826,00:16:27.949
"Then one might, not just in here, but",00:16:27.949,00:16:30.094
"just as a general approach
to that kind of problem.",00:16:30.094,00:16:33.360
"We know this goes to 0, but
we don't know how fast.",00:16:33.360,00:16:37.380
"One way to study that would be multiply it
by something that goes to infinity, right.",00:16:37.380,00:16:42.461
"Now, if we multiply it by
something that goes to infinity,",00:16:42.461,00:16:47.240
"such that this times
this goes to infinity.",00:16:47.240,00:16:50.789
"Then we know that this part that blows
up is dominating over this part.",00:16:50.789,00:16:55.080
"And if we multiply by something
that goes to infinity, but",00:16:55.080,00:16:58.475
"this whole thing still goes to 0,
then that's more informative, right?",00:16:58.475,00:17:02.930
"So what's gonna happen is that we
can imagine multiplying here by",00:17:02.930,00:17:08.004
"n to some power and we're gonna
show that there's a power here,",00:17:08.004,00:17:12.985
"and to some power, fill in the blank.",00:17:12.985,00:17:15.958
"What we're gonna show is that,",00:17:15.958,00:17:18.812
"if the power here is above some
threshold and to the big powers,",00:17:18.812,00:17:24.414
"its gonna go to infinity fast,
this thing will just blow up.",00:17:24.414,00:17:29.820
"And if we put a smaller power than the
threshold here, then this is still going",00:17:29.820,00:17:34.566
"to infinity as long as this is a positive
power of n, this is still going to",00:17:34.566,00:17:39.021
"infinity, this parts going to 0,
but this part's dominating, right?",00:17:39.021,00:17:43.641
So this term is competing with this term.,00:17:43.641,00:17:46.728
"This one goes to infinity,
this one goes to 0, okay?",00:17:46.728,00:17:49.520
"So then the question is what's
that magic threshold value?",00:17:49.520,00:17:53.548
And the answer is one-half.,00:17:53.548,00:17:57.343
"So that's what we're
gonna study right now.",00:17:57.343,00:17:58.797
"So we're gonna take the square
root of n times xn bar minus mu.",00:17:58.797,00:18:04.287
"This is kind of the happy medium,",00:18:04.287,00:18:06.363
"where we're gonna get a non-degenerate
distribution, that this is gonna converge",00:18:06.363,00:18:11.705
"in distribution to an actual distribution,
it's not gonna just get killed to 0 or",00:18:11.705,00:18:16.896
"blow up to infinity, it's actually
gonna give us a nice distribution.",00:18:16.896,00:18:22.180
"Okay, and I'm also gonna divide by the
sigma here, makes it a little bit cleaner.",00:18:22.180,00:18:28.313
So this is the central limit theorem now.,00:18:28.313,00:18:31.169
"I'm stating it, then we'll prove it.",00:18:31.169,00:18:33.375
"Central limit theorem says,
if you take this and",00:18:37.266,00:18:40.452
"look at what happens
as n goes to infinity.",00:18:40.452,00:18:43.260
"Converges to standard
normal in distribution.",00:18:47.910,00:18:55.075
"[SOUND] By convergence and
distribution, what we mean is that",00:18:55.075,00:19:00.344
"the distribution of this converges
to the standard normal distribution.",00:19:00.344,00:19:06.870
"In other words, you could take the CDF.",00:19:06.870,00:19:09.811
"I mean these may be discrete or continuous
or a mixture of discreet and continuous.",00:19:09.811,00:19:14.890
"So it doesn't necessarily have a PDF,
but every random variable has a CDF.",00:19:14.890,00:19:20.000
"So it says if you take the CDF of this,",00:19:20.000,00:19:22.959
"it's gonna converge to capital 5,
the standard normal.",00:19:22.959,00:19:27.742
"So I think this is kind of an amazing
result that this holds in such generality,",00:19:27.742,00:19:33.070
"right, because I mean the normal is just
this one, standard normal is just this",00:19:33.070,00:19:38.480
"one particular, it's a nice looking bell
curve, but that's just one distribution.",00:19:38.480,00:19:44.484
"And those x's they could be discrete,
they could be continuous,",00:19:44.484,00:19:48.757
"they could be extremely nasty
looking distributions, right?",00:19:48.757,00:19:52.889
"It could look like anything,",00:19:52.889,00:19:54.506
"the only thing we assumed was
that there was a finite variance.",00:19:54.506,00:19:57.940
"Other than that,
they could have an incredibly complicated,",00:19:59.050,00:20:03.470
messy distribution.,00:20:03.470,00:20:06.150
"But it's always gonna
go to standard normal.",00:20:06.150,00:20:08.460
"So this is one of the reasons why
the standard normal distribution is so",00:20:09.510,00:20:14.416
"important on the one hand and so,
widely used, because this is a theorem",00:20:14.416,00:20:19.489
"as n goes to infinity is what it says,
but the way it's used in practice is then",00:20:19.489,00:20:24.894
"people use normal approximations all the
time and a lot of the justification for",00:20:24.894,00:20:30.465
"normal approximations is coming from this,
because this says that if n is large,",00:20:30.465,00:20:36.206
"then the sample mean will approximately
have a normal distribution.",00:20:36.206,00:20:41.164
"Even if the original data did not look
like they came from a normal distribution,",00:20:44.016,00:20:50.034
"when you average lots and
lots of them, it looks normal, okay.",00:20:50.034,00:20:55.060
"So this is in a sense is a better
theorem than the law of large numbers,",00:20:55.060,00:20:59.570
"but because it's kind of more
informative to know the distribution,",00:20:59.570,00:21:03.550
"know something about the rate, and
you know it's interesting that it's,",00:21:03.550,00:21:07.000
"square root of n is kind of the power
of n that's just right, right?",00:21:07.000,00:21:11.340
"A larger power it's gonna blow up,
a smaller power it's gonna go to 0.",00:21:11.340,00:21:14.600
"N to the one-half is the compromise,
then you always get a normal distribution.",00:21:15.880,00:21:20.410
"It's more informative in some sense, but",00:21:20.410,00:21:22.627
"you should also keep in mind,
it is a different sense of convergence.",00:21:22.627,00:21:27.070
"Up here, we're talking about the random
variables actually converging,",00:21:27.070,00:21:32.160
"literally the random variables
converge the sample mean converges",00:21:32.160,00:21:36.692
"literally to point-wise with
probability 1, to the true mean.",00:21:36.692,00:21:41.163
"Here, we're talking about
convergence in distribution.",00:21:41.163,00:21:43.787
"So we're not talking about
convergence of random variables.",00:21:43.787,00:21:47.350
"We're just saying the distribution of this
converges to the normal 0, 1 distribution.",00:21:47.350,00:21:52.604
"So that's a different sense
of convergence, but anyway,",00:21:52.604,00:21:57.704
"both of them are telling us what's gonna
happen to Xn bar when n is large, okay?",00:21:57.704,00:22:04.570
"So well, let's prove this theorem.",00:22:04.570,00:22:07.807
"Here's another way to write this,
by the way,",00:22:07.807,00:22:11.676
it's good to be familiar with both ways.,00:22:11.676,00:22:15.080
"It's just algebra to go
from one to the other, but",00:22:15.080,00:22:18.361
"they're both useful enough
to be worth mentioning.",00:22:18.361,00:22:21.890
"Let's just write the central limit
theorem in terms of the sum of X's",00:22:21.890,00:22:26.497
rather than in terms of the sample mean.,00:22:26.497,00:22:29.760
"So I'm just gonna take the sum of Xj,
j equals 1 to n.",00:22:29.760,00:22:34.444
"And so, we can either think of
the central limit theorem as,",00:22:34.444,00:22:38.019
"either think of it as telling us what
happens to the sample mean or we",00:22:38.019,00:22:41.944
"can think of it as telling us what happens
to the sum, or the convolution, okay?",00:22:41.944,00:22:46.733
"It's equivalent because
they're just a factor of,",00:22:46.733,00:22:50.108
"we just have to be careful not
to mess up the factor of n,",00:22:50.108,00:22:53.558
"b ut we can go from one to the other
cuz it's just a factor of n.",00:22:53.558,00:22:57.386
"So the claim is that this is
approximately normal when n is large,",00:22:57.386,00:23:02.980
"but if we just have this thing,
this could easily just blow up.",00:23:02.980,00:23:08.384
You're just adding more and more terms.,00:23:08.384,00:23:10.510
"But somehow we wanna
standardize this first.",00:23:10.510,00:23:15.250
"So if we take this thing,
because this thing has mean and",00:23:15.250,00:23:20.104
"mu, right, so let's subtract n mu.",00:23:20.104,00:23:23.425
"Because then it has zero mean,
because I just want to match.",00:23:26.889,00:23:30.042
"I wanna make the mean 0 and
the variance 1, so",00:23:30.042,00:23:32.773
"that it kind of matches up with that,
rather than just letting it blow up.",00:23:32.773,00:23:37.128
"So this is called centering,
we just subtracted by linearity,",00:23:37.128,00:23:41.267
"the mean is n mu, so
just subtract it n mu.",00:23:41.267,00:23:43.961
"And then let's divide by
the standard deviation,",00:23:43.961,00:23:47.005
"this is just how we did
standard deviation before.",00:23:47.005,00:23:50.060
"So over there we showed that the variants
of Xn bar is sigma-squared over n.",00:23:50.060,00:23:57.534
"And the variance of this sum
is just n sigma squared.",00:23:57.534,00:24:02.050
"So let's just divide by
the standard deviation, right,",00:24:02.050,00:24:07.520
"which is square root of n Times sigma,
okay?",00:24:07.520,00:24:12.517
Cuz the variance is n sigma squared.,00:24:12.517,00:24:15.110
So that's just the standardized version.,00:24:15.110,00:24:17.520
"And the statement is again that this
converges to the standard normal",00:24:17.520,00:24:22.110
in distribution.,00:24:22.110,00:24:23.500
"So if we take this sum and standardize it,
then it's gonna go standard normal.",00:24:23.500,00:24:28.101
"Okay, so, all right, so
now we're ready to prove this theorem.",00:24:33.722,00:24:39.418
"And, sort of just a calculation,
but it's kind of a nice",00:24:41.797,00:24:46.316
"calculation in some ways,
we're gonna prove it, well.",00:24:46.316,00:24:50.940
"This theorem is always true as
long as the variance exist.",00:24:53.287,00:24:57.276
"We don't need to assume that, the third
moment or the fourth moment exist.",00:24:57.276,00:25:01.770
"But the proof is much more complicated
to do it in that generality.",00:25:01.770,00:25:05.691
"So we're gonna assume that the MGF exists,
then we can actually work with the MGFs.",00:25:05.691,00:25:11.231
"Because when you see this thing,
sum of independent random variables,",00:25:11.231,00:25:15.806
"then we know the MGF is gonna be
something useful if it exists.",00:25:15.806,00:25:20.450
"And there's ways to extend this proof
to cases where the MGF doesn't exist.",00:25:20.450,00:25:23.870
"But for our purposes,
we may as well just assume MGF exists.",00:25:23.870,00:25:30.065
"So assuming MGF, let's call it M(t).",00:25:30.065,00:25:34.812
"Of Xj, they're iid, so if one of them
has an MGF, they all have the same MGF.",00:25:38.193,00:25:44.760
We'll just assume that that exists.,00:25:44.760,00:25:46.563
"Once we have MGFs, then our strategy
is to show that the MGFs converge.",00:25:54.744,00:26:01.139
"So that's a theorem about MGFs, that
if the MGFs converge to some other MGF,",00:26:01.139,00:26:07.557
"then the random variables
converge in distribution, right?",00:26:07.557,00:26:12.980
"We had a homework problem related to that,
where you found that the MGFs converged",00:26:12.980,00:26:18.040
"to some MGF, and that implies
convergence of the distributions, right?",00:26:18.040,00:26:22.678
"Okay, so that's the whole strategy.",00:26:22.678,00:26:24.490
"So that means all we need to
do is find the MGF of this and",00:26:24.490,00:26:28.470
"then take the limit, okay?",00:26:28.470,00:26:30.603
"So basically at this point,
it's just like, write down the MGF,",00:26:30.603,00:26:35.796
"take the limit, and
use a few facts about MGFs, okay?",00:26:35.796,00:26:40.086
"So first of all, we can assume.",00:26:40.086,00:26:46.091
"That, let's just assume mu = 0 and",00:26:50.208,00:26:54.610
"sigma = 1, just to simplify the notation.",00:26:54.610,00:27:00.015
"This is without loss of generality,",00:27:00.015,00:27:04.000
"because we could write this as,
all we have to do is consider.",00:27:04.000,00:27:10.689
"I wrote the standardized thing this way,
but",00:27:10.689,00:27:14.049
"I could've just written it as
standardizing each X separately.",00:27:14.049,00:27:19.170
I could've written Xj- mu over sigma.,00:27:19.170,00:27:24.835
"So this would be standardizing each
of them separately, j = 1 to n, and",00:27:24.835,00:27:29.140
then we have a 1 over root n.,00:27:29.140,00:27:30.848
"That will be the same thing
that we're looking at.",00:27:34.304,00:27:36.530
"This just says standardize
them separately first.",00:27:36.530,00:27:39.630
"But then you could just, I mean if
you want, just call this thing Yj.",00:27:39.630,00:27:43.394
"And once you have the central limit term
for Yj, then you know that that's true.",00:27:43.394,00:27:47.980
"So you might as well just assume that
they've already been standardized.",00:27:47.980,00:27:50.450
"And so just to have some notation,
let's just let Sn equal the sum,",00:27:51.660,00:27:57.340
"S for sum, of the first n terms.",00:27:57.340,00:28:00.350
And what we wanna show is that the MGF,00:28:00.350,00:28:03.080
"of Sn over root n,
that's what we're looking at, right?",00:28:04.530,00:28:08.975
"That let mu equal zero, sigma equals one,
so we're looking at Sn over root n.",00:28:08.975,00:28:12.395
"And we wanna show that that goes
to the standard normal MGF.",00:28:12.395,00:28:17.942
"Right, so we just need to find this MGF,
take a limit.",00:28:22.166,00:28:25.390
"Okay, so let's just find the MGF.",00:28:27.060,00:28:30.977
"So by definition, that's the expected
value of e to the t times Sn over root n.",00:28:30.977,00:28:38.060
And Sn is just the sum.,00:28:42.177,00:28:44.993
"So, and we're assuming independence,
which means that these, you can",00:28:44.993,00:28:50.205
"write this as e to the t x1 over root n, e
to the t x2 over root n, blah, blah, blah.",00:28:50.205,00:28:56.188
"All of those factors are independent,
therefore, they're uncorrelated.",00:28:56.188,00:29:02.350
"So we can just split it up as a product,
X1/ over root n.",00:29:02.350,00:29:07.389
"Blah, blah, blah, same thing,",00:29:09.814,00:29:13.125
"just e to the Xj over root n
is the general term, right?",00:29:13.125,00:29:18.390
"I'm just using the fact that
those are uncorrelated, so",00:29:18.390,00:29:23.346
"we can write e of the product
of the expectations.",00:29:23.346,00:29:28.590
"But since these X's are iid,",00:29:28.590,00:29:30.480
"these are really just the same
thing written, n times.",00:29:30.480,00:29:33.720
"So really,
this is just this thing to the nth power.",00:29:33.720,00:29:40.340
"And this thing,
that should remind you of an MGF, right?",00:29:40.340,00:29:44.520
"That's just the MGF of X1,",00:29:44.520,00:29:46.410
"except that instead of evaluated at t,
it's evaluated at t over root n.",00:29:46.410,00:29:50.140
"So really, that's just the MGF,",00:29:51.300,00:29:54.453
"evaluated at t over root n
raised to the nth power.",00:29:54.453,00:29:59.033
So that's what we have.,00:30:00.853,00:30:02.297
"Now we need to take the limit
as n goes to infinity.",00:30:04.516,00:30:06.925
"So let's just look at what's gonna
happen here, n is going to infinity.",00:30:06.925,00:30:10.820
This thing on the inside becomes M of 0.,00:30:11.830,00:30:16.048
"M of 0 is 1 for any MGF, right?",00:30:16.048,00:30:19.950
Cuz e to the 0 is 1.,00:30:19.950,00:30:21.650
"So this is of the form 1 to the infinity
which is in indeterminate form, right?",00:30:21.650,00:30:28.840
It could evaluate to anything.,00:30:28.840,00:30:31.670
"So going back to calculus,
how do you deal with 1 to the infinity,",00:30:31.670,00:30:35.620
"or 0 over 0, or whatever.",00:30:35.620,00:30:37.230
"Usually we try to reduce it to something
where we can use L'Hopital's Rule for",00:30:37.230,00:30:41.156
"those problems, right?",00:30:41.156,00:30:42.473
"Or we can use a Taylor
series type of thing.",00:30:42.473,00:30:44.390
"So, how do we get into that form?",00:30:45.980,00:30:48.621
"Take the log,
because this looks like 1 to infinity.",00:30:51.640,00:30:56.240
"If we take the log,
it'll look like infinity times log of 1.",00:30:56.240,00:31:00.725
"So it'll look like infinity times 0,
take logs.",00:31:00.725,00:31:04.710
"Then we just have to remember to
exponentiate at the end to undo the log.",00:31:04.710,00:31:10.110
"Okay, so
let's write down then what we have.",00:31:10.110,00:31:13.581
"After taking the log, and
we're trying to do a limit, so",00:31:18.477,00:31:22.130
"we're doing the limit as n goes
to infinity, and we take the log.",00:31:22.130,00:31:26.331
It's n log M(t,00:31:26.331,00:31:31.401
over root n).,00:31:31.401,00:31:36.472
So that's of the form infinity times 0.,00:31:36.472,00:31:41.208
"If we want 0 over 0 or
infinity over infinity,",00:31:41.208,00:31:44.568
"we can just write it as 1
over n in the denominator.",00:31:44.568,00:31:48.187
"Okay, and now it's of the form 0 over 0.",00:31:54.437,00:31:57.140
"So we can almost use L'Hopital's Rule,
but not quite.",00:31:57.140,00:32:00.812
We have to be a little bit careful.,00:32:00.812,00:32:02.007
"Because first of all,
I'm assuming n is an integer,",00:32:02.007,00:32:05.717
and you can't do calculus on integers.,00:32:05.717,00:32:08.400
"Secondly, it's just kind of, even if we
pretended that n is a real number and",00:32:10.000,00:32:14.610
"then the derivative of n would
be- 1 over n squared and",00:32:14.610,00:32:18.600
that's kind of annoying to deal with.,00:32:18.600,00:32:20.570
"And it's kind of annoying to
deal with this square root here.",00:32:20.570,00:32:23.290
So let's first make a change of variables.,00:32:23.290,00:32:25.250
"Let's just let y = 1 over root n and
also let y be real, not necessarily,",00:32:26.970,00:32:33.963
"Not necessarily of the form 1 over
square root of an integer, okay?",00:32:37.524,00:32:42.356
"So it's the same limit, just written
in terms of y instead of in terms of n.",00:32:42.356,00:32:46.980
"So as n goes to infinity y goes to 0 and
1 over n is y squared,",00:32:48.290,00:32:54.212
so it's denominator is just y squared.,00:32:54.212,00:32:58.246
"The reason I do it this way is
that 1 over root n is just y",00:32:58.246,00:33:02.856
"by definition but
then the numerator is just log m of yt.",00:33:02.856,00:33:07.689
"That's a lot easier to deal with
because we got rid of the square roots.",00:33:07.689,00:33:11.109
So it's still of the form 0 over 0.,00:33:13.540,00:33:16.920
So we're gonna use L'Hospital's Rule.,00:33:16.920,00:33:21.582
"So limit, y goes to 0.",00:33:21.582,00:33:23.457
"Take the derivative of the numerator and
the denominator separately.",00:33:23.457,00:33:28.262
The derivative of the denominator is 2y.,00:33:28.262,00:33:31.834
"The derivative of the numerator,",00:33:31.834,00:33:32.907
"well we're just going to
have to use the chain rule.",00:33:32.907,00:33:35.020
"Derivative of log something
is 1 over that thing.",00:33:35.020,00:33:39.740
"So that's M of yt hence the derivative
of that thing which again",00:33:39.740,00:33:44.750
"by the chain rule is M prime of
yt times the derivative of yt.",00:33:44.750,00:33:50.610
"We're treating t as constant,
we're differentiating with respect to y.",00:33:50.610,00:33:55.540
So t comes out.,00:33:55.540,00:33:56.840
And now let's see what we have.,00:34:00.178,00:34:02.880
"Let's just summarize
a couple facts about MGFs.",00:34:02.880,00:34:06.850
"So M of t is the expected
value of E to the tX1.",00:34:08.110,00:34:14.470
So M of 0 = 1 Okay.,00:34:14.470,00:34:22.520
"And when we first started doing MGF we
said that we take derivatives of the MGF",00:34:22.520,00:34:26.510
and evaluate it at 0.,00:34:26.510,00:34:28.200
"We get the moments, that is why it's
called the moment generating function.",00:34:28.200,00:34:31.120
"So the first derivative at 0 is the mean,
but we assume that mu is 0.",00:34:31.120,00:34:36.380
"So this is 0, here.",00:34:36.380,00:34:38.524
"And the second derivative,
while we're doing this.",00:34:38.524,00:34:42.740
"Secondary derivative is the second moment,
but since we assumed that the variance is",00:34:42.740,00:34:46.295
"1 and the mean is 0,
the second moment is 1, okay?",00:34:46.295,00:34:51.240
"So over here, as we let y go to 0,
denominator's still going to 0.",00:34:51.240,00:34:56.720
"Numerator's also going to 0,
because M prime of 0 is 0,",00:34:56.720,00:35:01.750
"so its still on the form 0 over 0, so
let's just do what we were told again.",00:35:01.750,00:35:06.560
"So first I can simplify it a little bit,
this t can come out,",00:35:08.980,00:35:13.430
"because that's acting as a constant,
and the 2 can come out.",00:35:13.430,00:35:17.658
"And limit y goes to 0 and
this M of yt part,",00:35:17.658,00:35:24.067
that's just going to 1.,00:35:24.067,00:35:28.900
"So we can write that as part
of a separate limit, but",00:35:28.900,00:35:31.910
that other limit is just going to 1.,00:35:31.910,00:35:34.087
"You can think of it as just
the limit of this part times",00:35:34.087,00:35:36.614
the limit of the rest of it.,00:35:36.614,00:35:37.944
"But that part's just going to 1,
so we can get rid of that.",00:35:37.944,00:35:41.991
"So really is just, what's left is just",00:35:41.991,00:35:47.905
the limit of M prime yt divided by y.,00:35:47.905,00:35:53.289
"Everything else is gone, so
it's actually pretty nicely simplified.",00:35:53.289,00:35:59.149
"Now, using L'Hospital's Rule
a second time,",00:35:59.149,00:36:02.967
"now the derivative of
the denominator is just 1, okay?",00:36:02.967,00:36:07.446
"And for the numerator,
chain rule, M double prime of yt.",00:36:07.446,00:36:12.136
"That was a t not a t squared,
but now it's a t squared,",00:36:16.067,00:36:19.737
"because by the chain rule, derivative of
yt is t, so we have a t squared over 2.",00:36:19.737,00:36:25.221
"Now when we let y go to 0,
now it's just M double prime 0 is 1, so",00:36:25.221,00:36:29.601
now this limit is just 1.,00:36:29.601,00:36:31.378
"So we get t squared over 2,",00:36:31.378,00:36:35.555
"that's what we wanted,",00:36:35.555,00:36:39.371
because t squared over 2 is the log.,00:36:39.371,00:36:45.017
"Of e to the t squared over 2, but",00:36:45.017,00:36:48.678
"e to the t square over 2 is
exactly the normal 0,1 MGF.",00:36:48.678,00:36:55.199
"Okay so,",00:37:02.523,00:37:03.131
"to prove that theorem that's the end of
the proof of the central limit theorem.",00:37:03.131,00:37:08.234
"All we had to do was just basic facts
out MGF, use, L'Hospital's Rule twice.",00:37:08.234,00:37:14.467
"And there we have one of the most famous
important theorems in statistics.",00:37:14.467,00:37:20.207
"Now so
there are more general versions of this,",00:37:20.207,00:37:23.796
"like you can extend this in various
ways where it's not an IID,",00:37:23.796,00:37:28.552
"but it still has to satisfy
some assumptions, right.",00:37:28.552,00:37:32.699
"But anyway,
this is the basic central limit theorem.",00:37:32.699,00:37:36.910
"Okay, so that's pretty good.",00:37:36.910,00:37:40.007
"Let's do an example,
like how do we actually use this,",00:37:40.007,00:37:45.340
"for the sake of approximations,
things like that.",00:37:45.340,00:37:50.341
"Last time I was talking about
the difference between inequalities and",00:37:50.341,00:37:53.414
"approximations, right?",00:37:53.414,00:37:54.621
"And we talked about Poisson
approximation before.",00:37:54.621,00:37:57.218
"We haven't really talked
about normal approximation.",00:37:57.218,00:37:59.760
"This result is giving us the ability
to use normal approximations",00:38:01.270,00:38:06.756
"when we're studying sample mean and
is large, okay?",00:38:06.756,00:38:11.354
"So historically, though,
the first version of",00:38:11.354,00:38:16.326
"the central limit theorem
that was ever proven,",00:38:16.326,00:38:21.299
"I think was for binomials, okay?",00:38:21.299,00:38:24.830
So what we're saying is that,00:38:24.830,00:38:28.900
"binomial np under some conditions
will be approximately normal.",00:38:28.900,00:38:33.380
"And well in the old days that was
incredibly important fact because",00:38:33.380,00:38:38.400
"they didn't have computers to
binomials how to deal with",00:38:38.400,00:38:42.840
"like n choose k, and n is large, and
k, you have all these factorials.",00:38:42.840,00:38:47.180
You can't do these things by hand.,00:38:47.180,00:38:49.182
"Now we have fast computers,
so it's a little bit better.",00:38:49.182,00:38:53.150
"But it's still a lot easier working
with normal distributions than",00:38:53.150,00:38:57.100
"binomial distributions most of the time,
right?",00:38:57.100,00:39:00.080
"And even now factorials still grow so
fast that even with",00:39:00.080,00:39:05.020
"a fast computer with large memory and
everything, you may quickly",00:39:05.020,00:39:09.320
"exceed its ability when you're doing
some big complicated binomial problem.",00:39:09.320,00:39:13.760
"And normals have a lot of nice properties,
as we've seen, okay?",00:39:13.760,00:39:18.040
"The question is,
when can we approximate a binomial",00:39:18.040,00:39:24.132
"using a normal, and
how do we do that, okay?",00:39:24.132,00:39:29.397
So this is just the binomial approximation,00:39:29.397,00:39:34.466
"to the normal, other way around.",00:39:34.466,00:39:38.557
"Normal approximation,
I'll say binomial approximated by normal,",00:39:38.557,00:39:41.338
the normal approximation to the binomial.,00:39:41.338,00:39:43.090
When is that valid?,00:39:48.151,00:39:48.990
"To contrast it with
the Poisson approximation,",00:39:52.320,00:39:55.320
"that we've seen before, okay?",00:39:55.320,00:39:58.270
"So, if x is, let's x be binomial np",00:39:58.270,00:40:03.065
And as we've done many times before,00:40:05.060,00:40:10.070
"we can represent x as
a sum of iid Bernoulli.",00:40:10.070,00:40:15.680
"Right?
Well these are just 1, if success on the J",00:40:18.116,00:40:23.284
"trials 0 otherwise, so
the XJ are iid Bernoulli P.",00:40:23.284,00:40:29.232
"So this does fit into
the framework of the central limit",00:40:33.460,00:40:37.096
"theorem that is we are adding
up iid random variables.",00:40:37.096,00:40:40.742
"So the central limit theorem says that,
if the N is large this will be",00:40:40.742,00:40:45.603
"approximately normal, at least after
we have standardized it, okay?",00:40:45.603,00:40:50.828
"So suppose we wanted to approximate,
suppose we're",00:40:50.828,00:40:55.944
"interested in the probability
that x is between A and B.",00:40:55.944,00:41:01.300
"And I want to approximate that,",00:41:04.996,00:41:07.061
"first we'll do equality then
we're approximating it.",00:41:07.061,00:41:10.738
"So, I mean if you had to do this on
a computer what you would do or by hand,",00:41:10.738,00:41:15.695
"which you wouldn't want to,
would be to take the PMF and",00:41:15.695,00:41:19.580
"sum up all the values of
the PMF from A to B, right.",00:41:19.580,00:41:23.073
"So okay, you would not want to do
that by hand most of the time.",00:41:23.073,00:41:28.220
"But suppose we just want an approximation
for this, not the exact thing.",00:41:28.220,00:41:32.338
"So first, the strategy is just gonna
be to take x and standardize it first.",00:41:32.338,00:41:38.627
"So we're gonna subtract the mean,
so we know that the mean is NP,",00:41:38.627,00:41:44.130
"and we're gonna divide by
the standard deviation,",00:41:44.130,00:41:48.535
"which we know as the square root of NPQ or
Q is 1 minus P.",00:41:48.535,00:41:53.156
"So, I'm just standardizing it right now.",00:41:53.156,00:41:55.320
"So this is still equal,
we haven't done any approximations yet.",00:41:55.320,00:41:58.253
"And then, now that we've standardized it,",00:42:02.622,00:42:05.559
"we can apply the central limit theorem,
if N is large enough, right?",00:42:05.559,00:42:10.135
"If N is, if central limit
theorem said N goes to infinity,",00:42:10.135,00:42:12.932
"that doesn't answer the question
of how large does N have to be.",00:42:12.932,00:42:16.030
"And for that, there's various theorems and
various rules of thumb.",00:42:16.030,00:42:20.190
"A lot of books will say,
how large does N have to be?",00:42:20.190,00:42:23.597
"And some books at least will say 30,
and that's just a rule of thumb.",00:42:23.597,00:42:30.391
"That's not always gonna work for all,
there's separate rules of thumb for",00:42:30.391,00:42:36.322
"the binomial, like you want N
times P to be reasonably large and",00:42:36.322,00:42:41.232
"N times 1 minus P to be large,
there are different rules of thumb.",00:42:41.232,00:42:46.780
"But anyway, if N is large enough,",00:42:46.780,00:42:49.149
"then what we've just proven is that
this is gonna look like it has",00:42:49.149,00:42:53.563
"a normal distribution because
that's a sum of IID things.",00:42:53.563,00:42:57.673
"And we standardized it correctly, because
we already knew the mean and the variance,",00:42:57.673,00:43:02.237
so we just standardized it.,00:43:02.237,00:43:03.748
"Okay, so this is approximately.",00:43:03.748,00:43:05.794
"Now we're going to use
the normal approximation,",00:43:08.166,00:43:10.388
"we're going to say this
is approximately normal.",00:43:10.388,00:43:13.280
"And if I want the probability that
the normal is between something and",00:43:13.280,00:43:17.898
"something, that's just the CDF
here minus the CDF here, right?",00:43:17.898,00:43:22.368
"Because for the normal, I mean this
is discrete but we're approximating",00:43:22.368,00:43:27.748
"using something continuous and we just
say, integrate the PDF from here to here.",00:43:27.748,00:43:34.170
"But fundamental theorem calculus,
that just says take the CDF and go, okay.",00:43:34.170,00:43:37.922
So we're just gonna do Phi of B minus,00:43:37.922,00:43:43.142
NP over square root of NPQ minus Phi,00:43:43.142,00:43:48.188
of A minus NP over square root of NPQ.,00:43:48.188,00:43:53.700
"So that would be the basic
normal approximation,",00:43:53.700,00:43:57.362
"I'll talk a little bit about how
to improve this approximation.",00:43:57.362,00:44:02.171
"But to contrast it with
the Poisson approximation.",00:44:02.171,00:44:06.620
"We talked before about the fact that,
and we proved the fact",00:44:11.483,00:44:16.490
"that if N goes to infinity, and
P goes to 0, and N times P is fixed.",00:44:16.490,00:44:22.520
"Then the binomial distribution
converts to the Poisson distribution,",00:44:22.520,00:44:26.790
we proved that before.,00:44:26.790,00:44:28.196
"So in the Poisson approximation, so for",00:44:28.196,00:44:31.570
"the Poisson approximation what we had was
N is large but P was very small, right?",00:44:31.570,00:44:38.329
"And we let lambda equal NP and
x as moderate.",00:44:38.329,00:44:42.540
"And most important thing is that
P is small here, P is close to 0.",00:44:46.084,00:44:51.340
"We proved it in the case where this goes
to infinity and this goes to 0, okay?",00:44:51.340,00:44:55.911
"So Poisson is relevant when we're
dealing with a large number of very",00:44:55.911,00:45:00.801
rare unlikely things.,00:45:00.801,00:45:02.880
"That's really in contrast to this,",00:45:02.880,00:45:06.497
in this case for the normal approximation.,00:45:06.497,00:45:10.823
"Then, while we still want N to be large,
but",00:45:10.823,00:45:14.196
"if you kind of think intuitively
about when is this gonna work well,",00:45:14.196,00:45:19.396
"we actually want P to
be close to one half.",00:45:19.396,00:45:22.613
"Because think about the symmetry, if you
have a binomial of P equals one half,",00:45:25.131,00:45:30.506
that's a symmetric distribution.,00:45:30.506,00:45:33.510
"The normal is symmetric, no matter,
every normal distribution is symmetric.",00:45:33.510,00:45:39.035
"If P is far from one half, then the
binomial is very, very skewed, and in that",00:45:39.035,00:45:44.965
"case it's kind of doesn't make that much
sense to approximate using a normal.",00:45:44.965,00:45:51.110
"So this is gonna work as an approximation,
that's normal approximation,",00:45:52.780,00:45:58.829
"as an approximation if P is very small,
this makes a lot more sense than this.",00:45:58.829,00:46:04.981
"However, think about the statement
of the central limit theorem.",00:46:04.981,00:46:08.912
"In that theorem I never said
P was close to one half,",00:46:08.912,00:46:12.112
"in fact that was just a general theorem,
we didn't even have P in the statement",00:46:12.112,00:46:17.025
"of the central limit theorem, but
somehow this still has to eventually work.",00:46:17.025,00:46:22.470
"But as a practical matter
as an approximation,",00:46:22.470,00:46:26.020
"if P is close to one half this
is going to work quite well,",00:46:26.020,00:46:30.188
"if N is like 30 or 50 or
100, it will work fine.",00:46:30.188,00:46:34.204
"But if P is .001,
the central limit theorem is still true,",00:46:34.204,00:46:38.449
"that as N goes to infinity
it's gonna work, okay.",00:46:38.449,00:46:41.930
"But if N is kind of not
that enormous of a number,",00:46:41.930,00:46:46.090
"then it's gonna be a pretty
bad approximation.",00:46:46.090,00:46:50.354
"And let's just try to reconcile these
statements though, is there a case?",00:46:50.354,00:46:58.430
"If we let N go to infinity and
P be very small,",00:46:58.430,00:47:02.247
"I still said, if N is going to infinity,",00:47:02.247,00:47:05.754
"it's still gonna converge to
normal just much slower, right?",00:47:05.754,00:47:11.349
"So, how could the binomial
look both normal and Poisson?",00:47:11.349,00:47:18.180
"Well, the answer is that
the Poisson also looks normal.",00:47:18.180,00:47:21.408
"So if you've Poisson lambda
where lambda's very large,",00:47:21.408,00:47:24.916
"that's also gonna look normal, so
there is a case where those come together.",00:47:24.916,00:47:30.450
"Okay, one last thing about this
is that there is something kind",00:47:30.450,00:47:35.468
"of weird about this in the sense
that we're approximating",00:47:35.468,00:47:40.107
"a discrete distribution
using something continuous.",00:47:40.107,00:47:44.572
"And if we wanted to get,",00:47:44.572,00:47:47.285
"what if we wanted to just
approximate same problem?",00:47:47.285,00:47:53.270
I just wanna add something to this.,00:47:53.270,00:47:54.887
"Well, let's just look at that just to see
what more of like what could go wrong",00:47:54.887,00:47:59.145
with this.,00:47:59.145,00:47:59.809
What if we look at the case A equals B?,00:47:59.809,00:48:02.159
"So then we're just saying
the probability that x equals A,",00:48:02.159,00:48:06.634
that is approximate the Binomial PMF.,00:48:06.634,00:48:10.060
"And one kind of weird thing about this is,
this thing would change if",00:48:10.060,00:48:14.547
"we changed these to strict inequality but
this part would not.",00:48:14.547,00:48:18.728
"As soon as we say that this is
approximately normal than we don't care",00:48:18.728,00:48:22.237
about that anymore.,00:48:22.237,00:48:24.030
"So there's something called the continuity
correction which I just wanted to",00:48:24.030,00:48:27.642
briefly mention.,00:48:27.642,00:48:28.524
"Which is an improvement to deal with
the fact that you're using something",00:48:28.524,00:48:31.858
"continuous to approximate
something discrete.",00:48:31.858,00:48:34.900
"And it's often not explained very well but
if you understand what",00:48:34.900,00:48:39.755
"it does in this simple case,
then it's not hard to see the idea.",00:48:39.755,00:48:44.534
"The idea is that if you just said this is
approximately normal then you would just",00:48:44.534,00:48:49.127
"say zero, right?",00:48:49.127,00:48:50.670
"Because it would be zero for continuous,
that's not very useful, right?",00:48:50.670,00:48:53.800
We want something more useful than zero.,00:48:53.800,00:48:56.278
"So the idea is just
simply to write this as,",00:48:56.278,00:49:00.327
"here let's assume A is
an integer x is discreet well,",00:49:00.327,00:49:05.389
"x equals A is the same thing
as saying that x is between",00:49:05.389,00:49:10.337
A plus one-half and A minus one-half.,00:49:10.337,00:49:14.179
Right?,00:49:17.740,00:49:19.489
So just use this first.,00:49:19.489,00:49:21.359
"So for each value in this range,",00:49:24.220,00:49:26.368
"replace it by an interval
of length 1 centered there,",00:49:26.368,00:49:30.029
"that's exactly the same thing because x
is an integer anyway, so that's true.",00:49:30.029,00:49:35.457
"But here at least we're giving it
an interval to work with instead of",00:49:35.457,00:49:40.188
"just saying zero, so
that improves this approximation.",00:49:40.188,00:49:44.261
"Anyway, it's just central limit theorem.",00:49:44.261,00:49:45.790
"All right, so see you next time.",00:49:45.790,00:49:47.370

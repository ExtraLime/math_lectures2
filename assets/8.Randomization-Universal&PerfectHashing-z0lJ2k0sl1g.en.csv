text,start,stop
"The following content is
provided under a Creative",00:00:00.090,00:00:02.500
Commons license.,00:00:02.500,00:00:04.019
"Your support will help
MIT OpenCourseWare",00:00:04.019,00:00:06.360
"continue to offer high quality
educational resources for free.",00:00:06.360,00:00:10.730
"To make a donation or
view additional materials",00:00:10.730,00:00:13.340
"from hundreds of MIT courses,
visit MIT OpenCourseWare",00:00:13.340,00:00:17.217
at ocw.mit.edu.,00:00:17.217,00:00:17.842
"ERIK DEMAINE: All right,
let's get started.",00:00:22.420,00:00:25.460
"Today we're going to continue
the theme of randomization",00:00:25.460,00:00:29.060
and data structures.,00:00:29.060,00:00:30.320
Last time we saw skip lists.,00:00:30.320,00:00:31.880
"Skip lists solve the
predecessor-successor problem.",00:00:31.880,00:00:36.010
"You can search for an item
and if it's not there,",00:00:36.010,00:00:38.080
"you get the closest item
on either side in log n",00:00:38.080,00:00:41.660
with high probability.,00:00:41.660,00:00:43.840
"But we already knew how to
do that deterministically.",00:00:43.840,00:00:46.650
"Today we're going to solve a
slightly different problem,",00:00:46.650,00:00:49.400
"the dictionary problem
with hash tables.",00:00:49.400,00:00:52.000
"Something you already
think you know.",00:00:52.000,00:00:54.610
"But we're going to show you
how much you didn't know.",00:00:54.610,00:00:57.450
But after today you will know.,00:00:57.450,00:01:00.080
"And we're going to get
constant time and not",00:01:00.080,00:01:04.114
with high probability.,00:01:04.114,00:01:05.030
That's hard.,00:01:05.030,00:01:06.110
"But we'll do constant
expected time.",00:01:06.110,00:01:08.640
So that's in some sense better.,00:01:08.640,00:01:11.709
"It's going to solve
a weaker problem.",00:01:11.709,00:01:13.250
"But we're going to get
tighter bound constant instead",00:01:13.250,00:01:16.230
of logarithmic.,00:01:16.230,00:01:17.940
"So for starters let me remind
you what problem we're solving",00:01:17.940,00:01:22.830
"and the basics of hashing
which you learned in 6006.",00:01:22.830,00:01:30.770
"I'm going to give this problem
a name because it's important",00:01:30.770,00:01:34.600
"and we often forget
to distinguish",00:01:34.600,00:01:37.880
between two types of things.,00:01:37.880,00:01:41.210
"This is kind of an old
term, but I would call this",00:01:44.070,00:01:47.350
an abstract data type.,00:01:47.350,00:01:51.400
"This is just the
problem specification",00:01:51.400,00:01:54.290
of what you're trying to do.,00:01:54.290,00:01:56.580
"You might call this an
interface or something.",00:01:56.580,00:01:58.604
"This is the problem statement
versus the data structure",00:01:58.604,00:02:00.895
is how you actually solve it.,00:02:00.895,00:02:02.220
"The hash tables are
the data structure.",00:02:02.220,00:02:04.330
"The dictionary is the problem
or the abstract data type.",00:02:04.330,00:02:08.300
"So what we're
trying to do today,",00:02:08.300,00:02:11.480
"as in most data
structures, is maintain",00:02:11.480,00:02:13.530
a dynamic set of items.,00:02:13.530,00:02:14.695
"And here I'm going to
distinguish between the items",00:02:18.150,00:02:20.920
and their keys.,00:02:20.920,00:02:21.695
Each item has a key.,00:02:24.310,00:02:25.857
"And normally you'd
think of there also",00:02:25.857,00:02:27.440
being a value like in Python.,00:02:27.440,00:02:29.640
"But we're just
worrying about the keys",00:02:29.640,00:02:31.500
and moving the items around.,00:02:31.500,00:02:33.760
"And we want to support
three operations.",00:02:33.760,00:02:36.990
"We want to be able to insert
an item, delete an item,",00:02:39.770,00:02:53.170
and search for an item.,00:02:53.170,00:02:54.230
"But search is going to
be different from what",00:02:58.920,00:03:02.340
"we know from AVL trees
or skip lists or even",00:03:02.340,00:03:04.770
"Venom [INAUDIBLE] That was a
predecessor-successor search.",00:03:04.770,00:03:08.820
"Here we just want
to know-- sorry,",00:03:08.820,00:03:10.779
your not searching for an item.,00:03:10.779,00:03:12.070
"Usually you're searching
for just a key-- here",00:03:12.070,00:03:17.760
"you just want to know is
there any item with that key,",00:03:17.760,00:03:20.840
and return it.,00:03:20.840,00:03:22.190
"This is often called
an exact search",00:03:28.880,00:03:31.980
"because if the key
is not in there,",00:03:31.980,00:03:33.600
you learn absolutely nothing.,00:03:33.600,00:03:36.310
You can't find the nearest key.,00:03:36.310,00:03:38.400
"And for whatever reason this
is called a dictionary problem",00:03:38.400,00:03:41.480
"though it's unlike
a real dictionary.",00:03:41.480,00:03:43.150
"Usually when you search for a
word you do find its neighbors.",00:03:43.150,00:03:45.820
"Here we're just going to
either-- if the key's there",00:03:45.820,00:03:48.510
"we find that, otherwise not.",00:03:48.510,00:03:50.630
"And this is exactly what a
Python dictionary implements.",00:03:50.630,00:03:55.930
"So I guess that's why Python
dictionaries are called dicts.",00:03:55.930,00:04:00.980
"So today I'm going to assume
all items have distinct keys.",00:04:00.980,00:04:07.540
"So in the insertion I will
assume key is not already",00:04:07.540,00:04:13.480
in the table.,00:04:13.480,00:04:14.240
"With a little bit
of work, you can",00:04:17.760,00:04:21.769
"allow inserting an item
with an existing key,",00:04:21.769,00:04:24.040
"and you just overwrite
that existing item.",00:04:24.040,00:04:27.240
"But I don't want to
worry about that here.",00:04:27.240,00:04:28.990
"So we could, of course,
solve this using an AVL tree",00:04:31.540,00:04:34.300
in log n time.,00:04:34.300,00:04:36.010
"But our goal is to do better
because it's an easier problem.",00:04:36.010,00:04:40.720
"And I'm going to remind you
of the simplest way you learn",00:04:40.720,00:04:44.540
"to do this which was hashing
with chaining in 006.",00:04:44.540,00:04:50.950
"And the catch is you didn't
really analyze this in 006.",00:04:50.950,00:04:57.930
"So we're going make a
constant time per operation.",00:04:57.930,00:05:02.880
"It's going to be expected or
something and linear space.",00:05:06.950,00:05:14.570
"And remember the
variables we care",00:05:18.230,00:05:22.340
"about, there's u, n, and m.",00:05:22.340,00:05:27.330
"So u is the size
of the universe.",00:05:27.330,00:05:28.860
This is the all possible keys.,00:05:28.860,00:05:30.970
The space of all possible keys.,00:05:30.970,00:05:32.490
"n is the size of the set
your currently storing.",00:05:38.900,00:05:42.460
"So that's the number
of items or keys",00:05:42.460,00:05:47.130
currently in the data structure.,00:05:47.130,00:05:49.060
"And then m is the
size of your table.",00:05:54.510,00:05:57.950
"So say it's the number
of slots in the table.",00:05:57.950,00:06:01.390
So you remember the picture.,00:06:04.460,00:06:05.760
You have a table of slots.,00:06:05.760,00:06:10.630
Let's say 0 to m minus 1.,00:06:10.630,00:06:13.340
"Each of them is a
pointer to a linked list.",00:06:13.340,00:06:15.320
"And if you have,
let's say over here",00:06:18.610,00:06:21.140
"is your universe of
all possible keys,",00:06:21.140,00:06:24.660
"then we have a hash function
which maps each universe",00:06:24.660,00:06:28.810
item into one of these slots.,00:06:28.810,00:06:32.950
"And then the linked
list here is storing",00:06:32.950,00:06:35.100
"all of the items that
hash to that slot.",00:06:35.100,00:06:38.720
"So we have a hash function
which maps the universe.",00:06:38.720,00:06:49.810
"I'm going to assume the
universe has already been mapped",00:06:49.810,00:06:52.340
into integers 0 to u minus 1.,00:06:52.340,00:06:54.685
And it maps to slots.,00:06:54.685,00:06:56.390
"And when we do
hashing with chaining,",00:07:02.660,00:07:07.280
"I think I mentioned this
last week, the bounds",00:07:07.280,00:07:09.780
"you get, we achieve
a bound of 1 plus",00:07:09.780,00:07:23.950
"alpha where alpha is
the load factor n/m.",00:07:23.950,00:07:29.450
"The average number of items
you'd expect to hash to a slot",00:07:29.450,00:07:33.300
"is the number of items divided
by the number of slots.",00:07:33.300,00:07:36.710
OK.,00:07:36.710,00:07:38.810
"And you proved this
in 6006 but you",00:07:38.810,00:07:41.770
"assumed something called
simple uniform hashing.",00:07:41.770,00:07:47.200
"Simple uniform hashing
is an assumption,",00:08:05.590,00:08:08.900
I think invented for CLRS.,00:08:08.900,00:08:10.750
"It makes the
analysis very simple,",00:08:10.750,00:08:13.380
"but it's also
basically cheating.",00:08:13.380,00:08:15.480
"So today our goal
is to not cheat.",00:08:15.480,00:08:17.820
It's nice as a warm up.,00:08:17.820,00:08:19.690
But we don't like cheating.,00:08:19.690,00:08:21.520
"So you may recall the assumption
is about the hash function.",00:08:21.520,00:08:34.270
You want a good hash function.,00:08:34.270,00:08:37.740
And good means this.,00:08:37.740,00:08:43.080
"I want the probability
of two distinct keys",00:08:43.080,00:08:46.480
"mapping to the same slot to
be 1/m if there are m slots.",00:08:46.480,00:08:51.520
"If everything was
completely random,",00:08:51.520,00:08:53.430
"if h was basically choosing a
random number for every key,",00:08:53.430,00:08:57.290
"then that's what we
would expect to happen.",00:08:57.290,00:09:00.090
"So this is like the
idealized scenario.",00:09:00.090,00:09:02.550
"Now, we can't have
a hash function",00:09:02.550,00:09:04.270
"could choosing a random
number for every key",00:09:04.270,00:09:07.000
"because it has to choose the
same value if you give it",00:09:07.000,00:09:09.250
the same key.,00:09:09.250,00:09:10.520
"So it has to be some kind
of deterministic strategy",00:09:10.520,00:09:14.029
"or at least repeatable
strategy where",00:09:14.029,00:09:15.570
"if you plug in the same
key you get the same thing.",00:09:15.570,00:09:18.570
"So really what this
assumption is saying",00:09:18.570,00:09:20.520
"is that the key's that you
give are in some sense random.",00:09:20.520,00:09:28.750
"If I give you random keys
and I have not-too-crazy hash",00:09:28.750,00:09:33.100
function then this will be true.,00:09:33.100,00:09:36.460
"But I don't like assuming
anything about the keys maybe.",00:09:36.460,00:09:39.630
"I want my keys to
be worst case maybe.",00:09:39.630,00:09:44.190
"There are lots of examples in
the real world where you apply",00:09:44.190,00:09:47.872
"some hash function
and it turns out",00:09:47.872,00:09:49.330
"your data has some very
particular structure.",00:09:49.330,00:09:51.660
"And if you choose a
bad hash function,",00:09:51.660,00:09:53.260
"then your hash table
gets really, really slow.",00:09:53.260,00:09:56.350
"Maybe everything hashes
to the same slot.",00:09:56.350,00:10:00.250
"Or say you take--
well yeah, there",00:10:00.250,00:10:02.920
are lots of examples of that.,00:10:02.920,00:10:05.440
We want to avoid that.,00:10:05.440,00:10:06.360
"After today you will know how to
achieve constant expected time",00:10:06.360,00:10:10.650
"no matter what your keys
are, for worst case keys.",00:10:10.650,00:10:14.580
"But it's going to take
some work to do that.",00:10:14.580,00:10:18.140
"So this assumption
requires assuming",00:10:18.140,00:10:26.080
that the keys are random.,00:10:26.080,00:10:28.060
"And this is what we would
call an average case analysis.",00:10:32.447,00:10:34.780
"You might think that
average case analysis is",00:10:41.296,00:10:43.170
"necessary for
randomized algorithms,",00:10:43.170,00:10:45.800
but that's not true.,00:10:45.800,00:10:47.800
"And we saw that last
week with quicksort.",00:10:47.800,00:10:50.580
"Quicksort, if you say I
will always choose a of 1",00:10:50.580,00:10:54.660
"to be my partition
element, that's",00:10:54.660,00:10:57.130
"what the textbook calls
basic quicksort, then",00:10:57.130,00:11:00.770
"for an average input
that will do really well.",00:11:00.770,00:11:03.830
"If you have a uniform
random permutation of items",00:11:03.830,00:11:07.610
"and you sort with the method of
always choosing the first item",00:11:07.610,00:11:10.450
"as your partition, then that
will be n log n on average",00:11:10.450,00:11:16.090
if your data is average.,00:11:16.090,00:11:18.230
"But we saw we could
avoid that assumption",00:11:18.230,00:11:21.270
by choosing a random pivot.,00:11:21.270,00:11:24.000
"If you choose a
random pivot, then you",00:11:24.000,00:11:25.920
"don't need to assume
anything about the input.",00:11:25.920,00:11:27.836
"You just need to assume
that the pivots are random.",00:11:27.836,00:11:30.060
"So it's a big difference between
assuming your inputs are random",00:11:30.060,00:11:32.726
"versus assuming your
coin flips are random.",00:11:32.726,00:11:35.230
"It's pretty reasonable to
assume you can flip coins.",00:11:35.230,00:11:39.569
"If you've got enough
dexterity in your thumb then",00:11:39.569,00:11:41.610
you can do it.,00:11:41.610,00:11:43.341
"But it's not so
reasonable to assume",00:11:43.341,00:11:44.840
that your input is random.,00:11:44.840,00:11:45.923
"So we'd like to avoid average
case analysis whenever we can,",00:11:45.923,00:11:50.012
and that's the goal of today.,00:11:50.012,00:11:51.220
"So what you saw in 006 was
essentially assuming the inputs",00:11:51.220,00:11:54.470
are random.,00:11:54.470,00:11:55.089
"We're going to get rid of that
unreasonable assumption today.",00:11:55.089,00:11:57.630
"So that's, in some
sense, review from 006.",00:12:03.860,00:12:07.780
"I'm going to take a
brief pause and tell you",00:12:07.780,00:12:10.430
"about the etymology of the word
hash in case you're curious.",00:12:10.430,00:12:14.220
"Hash is an English word since
the 1650's, so it's pretty old.",00:12:14.220,00:12:25.020
"It means literally
cut into small pieces.",00:12:25.020,00:12:29.070
"It's usually used
in a culinary sense,",00:12:29.070,00:12:31.520
"like these days you have
corned beef hash or something.",00:12:31.520,00:12:36.412
"I'll put the
definition over here.",00:12:36.412,00:12:37.828
"It comes from French, hacher,
which means to chop up.",00:12:45.480,00:13:01.130
"You know it in English
from the word hatchet.",00:13:04.320,00:13:08.660
So it's the same derivation.,00:13:08.660,00:13:10.260
"And it comes from old French--
I don't actually know whether",00:13:12.800,00:13:20.520
"that's ""hash-ay"" or ""hash""
but-- which means axe.",00:13:20.520,00:13:30.916
So you can see the derivation.,00:13:30.916,00:13:32.165
"If you look this
up in OED or pick",00:13:35.940,00:13:38.020
"your favorite dictionary or even
Google, that's what you find.",00:13:38.020,00:13:41.760
"But in fact there's a
new prevailing theory",00:13:41.760,00:13:45.120
"that in fact hash comes
from another language which",00:13:45.120,00:13:53.570
"is Vulcan, la'ash, I mean you
can see the derivation right?",00:13:53.570,00:14:01.140
Actually means axe.,00:14:01.140,00:14:03.160
"So maybe French got it
from Vulcan or vice versa",00:14:03.160,00:14:06.800
but I think that's pretty clear.,00:14:06.800,00:14:10.600
"Live long and prosper,
and farewell to Spock.",00:14:10.600,00:14:13.960
Sad news of last week.,00:14:16.720,00:14:17.865
So enough about hashing.,00:14:20.880,00:14:22.770
"We'll come back to
that in a little bit.",00:14:22.770,00:14:24.870
"But hash functions
essentially take up",00:14:24.870,00:14:27.050
"this idea of taking your
key, chopping up into pieces,",00:14:27.050,00:14:30.410
"and mixing it like
in a good dish.",00:14:30.410,00:14:35.910
"All right, so we're going
to cover two ways to get",00:14:35.910,00:14:39.620
strong constant time bounds.,00:14:39.620,00:14:43.186
"Probably the most useful one
is called universal hashing.",00:14:43.186,00:14:45.560
"We'll spend most of
our time on that.",00:14:45.560,00:14:47.440
"But the theoretically cooler
one is called perfect hashing.",00:14:47.440,00:14:50.379
"Universal hashing,
we're going to guarantee",00:14:50.379,00:14:52.170
"there are very few
conflicts in expectation.",00:14:52.170,00:14:54.580
"Perfect hashing , we're going
to guarantee there are zero",00:14:54.580,00:14:56.990
conflicts.,00:14:56.990,00:14:58.240
"The catch is, at least
in its obvious form,",00:14:58.240,00:15:01.570
it only works for static sets.,00:15:01.570,00:15:04.290
"If you forbid, insert,
and delete and just want",00:15:04.290,00:15:07.630
"to do search, then perfect
hashing is a good method.",00:15:07.630,00:15:10.894
"So like if you're
actually storing",00:15:10.894,00:15:12.310
"a dictionary, like
the OED, English",00:15:12.310,00:15:15.464
doesn't change that quickly.,00:15:15.464,00:15:16.630
"So you can afford to recompute
your data structure whenever",00:15:16.630,00:15:19.650
you release a new edition.,00:15:19.650,00:15:22.285
"But let's start with
universal hashing.",00:15:22.285,00:15:23.910
"This is a nice
powerful technique.",00:15:23.910,00:15:27.600
It works for dynamic data.,00:15:27.600,00:15:30.030
"Insert, delete, and
search will be constant",00:15:30.030,00:15:32.430
"expected time with no
assumptions about the input.",00:15:32.430,00:15:36.340
So it will not be average case.,00:15:36.340,00:15:37.800
"It's in some sense worse
case but randomized.",00:15:37.800,00:15:40.045
"So the idea is we need
to do something random.",00:15:43.000,00:15:46.320
"If you just say, well, I
choose one hash function",00:15:46.320,00:15:48.900
"once and for all, and I
use that for my table,",00:15:48.900,00:15:51.240
"OK maybe my table
doubles in size",00:15:51.240,00:15:52.750
and I change the hash function.,00:15:52.750,00:15:54.070
But there's no randomness there.,00:15:54.070,00:15:57.400
"We need to introduce
randomness somehow",00:15:57.400,00:15:59.740
into this data structure.,00:15:59.740,00:16:01.760
"And the way we're
going to do that",00:16:01.760,00:16:04.090
"is in how we choose
the hash function.",00:16:04.090,00:16:07.160
"We're going to choose our
hash function randomly",00:16:07.160,00:16:17.580
from some set of hash functions.,00:16:17.580,00:16:19.745
Call it h.,00:16:19.745,00:16:22.430
"This is going to be a
universal hash family.",00:16:22.430,00:16:25.845
"We're going to imagine
there are many possible hash",00:16:25.845,00:16:27.970
functions we could choose.,00:16:27.970,00:16:29.260
"If we choose one of them
uniformly at random,",00:16:29.260,00:16:31.780
that's a random choice.,00:16:31.780,00:16:33.190
"And that randomness
is going to be enough",00:16:33.190,00:16:35.500
"that we no longer need to
assume anything about the keys.",00:16:35.500,00:16:39.750
"So for that to work, we need
some assumption about h.",00:16:39.750,00:16:46.800
"Maybe it's just a set
of one hash function.",00:16:46.800,00:16:48.900
"That wouldn't add
much randomness.",00:16:48.900,00:16:50.600
"Two also would not
add much randomness.",00:16:50.600,00:16:52.540
We need a lot of them.,00:16:52.540,00:16:54.090
"And so we're going to require
H to have this property.",00:16:54.090,00:16:56.340
"And we're going to call it
the property universality.",00:16:59.070,00:17:02.040
"Generally you would call
it a universal hash family.",00:17:04.770,00:17:07.785
Just a set of hash functions.,00:17:11.589,00:17:14.920
"What we want is that-- so we're
choosing our hash function",00:17:14.920,00:17:21.390
"h from H. And
among those choices",00:17:21.390,00:17:25.490
"we want the probability that
two keys hash to the same value",00:17:25.490,00:17:31.180
to be small.,00:17:31.180,00:17:31.910
"I'll say-- and this
is very similar",00:17:42.520,00:17:51.610
"looking to simple
uniform hashing.",00:17:51.610,00:17:55.380
"Looks almost the same here
except I switched from k1",00:17:55.380,00:17:59.130
"and k2 to k and
k', but same thing.",00:17:59.130,00:18:03.470
"But what we're taking
the probability over,",00:18:03.470,00:18:05.770
"what we're assuming is
random is different.",00:18:05.770,00:18:08.190
"Here we're assuming k1 and
k2 a are because h was fixed.",00:18:08.190,00:18:12.520
"This was an assumption
about the inputs.",00:18:12.520,00:18:15.820
"Over here we're thinking
of k and k' as being fixed.",00:18:15.820,00:18:19.970
"This has to work for every
pair of distinct keys.",00:18:19.970,00:18:23.400
"And the probability
we're considering",00:18:23.400,00:18:25.400
is the distribution of h.,00:18:25.400,00:18:28.010
"So we're trying all the
different h's Or we're trying",00:18:28.010,00:18:31.470
little h uniformly at random.,00:18:31.470,00:18:33.330
"We want the probability that a
random h makes k and k' collide",00:18:33.330,00:18:37.730
to be at most 1/m.,00:18:37.730,00:18:40.160
"The other difference is we
switch from equals to at most.",00:18:40.160,00:18:44.030
I mean less would be better.,00:18:44.030,00:18:45.900
"And there are ways to make
it less for a couple pairs",00:18:45.900,00:18:48.160
but it doesn't really matter.,00:18:48.160,00:18:50.310
"But of course anything
less than or equal to 1/m",00:18:50.310,00:18:52.310
will be just as good.,00:18:52.310,00:18:55.170
"So this is an
assumption about H.",00:18:55.170,00:18:57.960
"We'll see how to achieve this
assumption in a little bit.",00:18:57.960,00:19:00.700
"Let me first prove to
you that this is enough.",00:19:00.700,00:19:04.510
"It's going to be basically
the same as the 006 analysis.",00:19:04.510,00:19:09.280
"But it's worth repeating just
so we are sure everything's OK.",00:19:09.280,00:19:13.870
"And so I can be more precise
about what we're assuming.",00:19:20.230,00:19:24.835
"The key difference between this
theorem and the 006 theorem is",00:19:39.240,00:19:42.600
"we get to make no
assumptions about the keys.",00:19:42.600,00:19:44.740
They are arbitrary.,00:19:44.740,00:19:45.790
"You get to choose
them however you want.",00:19:45.790,00:19:48.550
"But then I choose a
random hash function.",00:19:48.550,00:19:51.680
"The hash function cannot
depend on these keys.",00:19:51.680,00:19:54.160
But it's going to be random.,00:19:54.160,00:19:55.980
"And I choose the hash function
after you choose the keys.",00:19:55.980,00:19:59.860
That's important.,00:19:59.860,00:20:01.275
"So we're going to
choose a random h and H.",00:20:07.570,00:20:11.430
"And we're assuming
H is universal.",00:20:11.430,00:20:14.110
"Then the expected number of keys
in a slot among those n keys",00:20:19.060,00:20:35.680
is at most 1 plus alpha.,00:20:35.680,00:20:39.600
Alpha is n/m.,00:20:39.600,00:20:41.260
"So this is exactly
what we had over here.",00:20:41.260,00:20:45.470
"Here we're talking
about time bound.",00:20:45.470,00:20:47.560
"But the time bound
followed because the length",00:20:47.560,00:20:49.720
"of each chain was expected
to be 1 plus alpha.",00:20:49.720,00:20:53.630
"And here the expectation
is over the choice of h.",00:20:53.630,00:20:57.136
"Not assuming anything
about the keys.",00:20:57.136,00:21:01.910
So let's prove this theorem.,00:21:01.910,00:21:04.010
It's pretty easy.,00:21:08.920,00:21:09.660
"But I'm going to introduce
some analysis techniques",00:21:09.660,00:21:12.170
"that we will use for
more interesting things.",00:21:12.170,00:21:16.880
So let's give the keys a name.,00:21:16.880,00:21:20.280
"I'll just call
them-- I'll be lazy.",00:21:20.280,00:21:28.380
Use k1 up to kn.,00:21:28.380,00:21:30.965
"And I just want to
compute that expectation.",00:21:35.680,00:21:41.100
"So I want to compute let's say
the number of keys colliding",00:21:53.620,00:22:01.820
"with one of those
keys, let's say ki.",00:22:01.820,00:22:06.000
"So this is of course the size of
the slot that ki happens to go.",00:22:12.840,00:22:16.760
This is going to work for all i.,00:22:16.760,00:22:18.220
"And so if I can say that this
is at most 1/alpha for each i,",00:22:18.220,00:22:22.200
then I have my theorem.,00:22:22.200,00:22:23.920
"Just another way
to talk about it.",00:22:23.920,00:22:25.760
"Now the number of keys
colliding with ki, here's",00:22:25.760,00:22:29.100
"a general trick, whenever
you want to count something",00:22:29.100,00:22:32.100
"in expectation, a
very helpful tool",00:22:32.100,00:22:34.820
is indicator random variables.,00:22:34.820,00:22:37.850
"Let's name all of the different
events that we want to count.",00:22:37.850,00:22:42.530
"And then we're basically
summing those variables.",00:22:42.530,00:22:45.860
"So I'm going to say-- I'm going
to use I ij to be an indicator",00:22:45.860,00:22:53.514
random variable.,00:22:53.514,00:22:54.180
It's going to be 1 or 0.,00:22:54.180,00:22:56.320
"1 if hash function of ki
equals the hash function of kj.",00:22:56.320,00:23:06.665
"So there's a collision
between ki and kj j and 0",00:23:06.665,00:23:10.790
if they hash to different slots.,00:23:10.790,00:23:12.280
"Now this is, it's a random
variable because it depends",00:23:14.830,00:23:17.660
on h and h is a random thing.,00:23:17.660,00:23:19.930
ki and kj are not random.,00:23:19.930,00:23:22.020
They're given to you.,00:23:22.020,00:23:24.290
"And then I want to know
when does h back those two",00:23:24.290,00:23:28.620
keys to the same slot.,00:23:28.620,00:23:30.660
"And so this number is really
just the sum of Iij over all j.",00:23:30.660,00:23:39.070
This is the same thing.,00:23:39.070,00:23:42.150
"The number in here is the sum
for j not equal to i of Iij.",00:23:42.150,00:23:50.620
"Because we get a 1 every time
they collide, zero otherwise.",00:23:50.620,00:23:53.730
So that counts how many collide.,00:23:53.730,00:23:57.170
"Once we have it
in this notation,",00:23:57.170,00:23:58.870
"we can use all the great
dilemmas and theorems",00:23:58.870,00:24:02.600
"about in this case,
E, expectation.",00:24:02.600,00:24:06.600
What should I use here?,00:24:06.600,00:24:07.580
STUDENT: What?,00:24:10.442,00:24:12.236
"ERIK DEMAINE: What's
a good-- how can I",00:24:12.236,00:24:13.860
simplify this formula?,00:24:13.860,00:24:14.970
"STUDENT: The linearity
of expectation.",00:24:14.970,00:24:16.625
"ERIK DEMAINE: The
linearity of expectation.",00:24:16.625,00:24:16.950
Thank you.,00:24:16.950,00:24:17.450
"If you don't know
all these things,",00:24:20.170,00:24:21.640
"read the probability
appendix in the textbook.",00:24:21.640,00:24:25.340
"So we want to talk
about expectation",00:24:25.340,00:24:29.280
of the simplest thing possible.,00:24:29.280,00:24:31.450
"So linearity let's us
put the E inside the sum",00:24:31.450,00:24:35.910
without losing anything.,00:24:35.910,00:24:37.710
"Now the expectation of an
indicator random variable",00:24:37.710,00:24:41.600
"is pretty simple
because the zeros don't",00:24:41.600,00:24:44.050
contribute to the expectation.,00:24:44.050,00:24:45.810
The 1's contribute 1.,00:24:45.810,00:24:47.310
"So this is the same thing
as just the probability",00:24:47.310,00:24:50.140
of this being 1.,00:24:50.140,00:24:51.971
"So we get sum of j9 equal
to I of the probability",00:24:51.971,00:24:59.700
that Iij equals 1.,00:24:59.700,00:25:04.980
"And the probability
that Iij equals 1,",00:25:04.980,00:25:07.270
"well, that's the probability
that this happens.",00:25:07.270,00:25:11.570
"And what's the probability
that that happens?",00:25:11.570,00:25:15.010
At most 1/m our universality.,00:25:15.010,00:25:18.520
"So I'm going to--
I'll write it out.",00:25:18.520,00:25:22.665
"This is sum j not
equal to I. Probability",00:25:22.665,00:25:26.450
"that h maps ki and
kj to the same slot.",00:25:26.450,00:25:31.420
So that's the definition of Iij.,00:25:34.870,00:25:37.340
"And this is at most
sum j not equal to i",00:25:37.340,00:25:41.950
of 1/m by universality.,00:25:41.950,00:25:44.569
So here's where we're using it.,00:25:44.569,00:25:45.860
"And sum of j not equal to
I, well that's basically n.",00:25:50.450,00:25:56.030
But I made a mistake here.,00:26:04.920,00:26:07.190
Slightly off.,00:26:07.190,00:26:08.960
From here-- yeah.,00:26:08.960,00:26:11.410
So this line is wrong.,00:26:11.410,00:26:14.450
Sorry.,00:26:14.450,00:26:14.950
Let me fix it.,00:26:14.950,00:26:16.130
"Because this
assumption only works",00:26:16.130,00:26:18.430
when the keys are distinct.,00:26:18.430,00:26:20.810
"So in fact-- how did I get
j-- yeah. , Yeah, sorry.",00:26:20.810,00:26:32.070
"This should have been
this-- actually everything",00:26:32.070,00:26:34.290
"I said is true, but if you want
to count the number of keys--",00:26:34.290,00:26:37.960
"I really wanted to count the
total number of keys that",00:26:37.960,00:26:40.210
hash to the same place as ki.,00:26:40.210,00:26:43.630
"So there's one more
which is ki itself.",00:26:43.630,00:26:46.060
"Always hashes to
wherever ki hashes.",00:26:46.060,00:26:48.920
"So I did a summation
j not equal i",00:26:48.920,00:26:50.800
"but I should also have
a plus Iii-- captain.",00:26:50.800,00:26:57.670
"So there's the case when I
hashing to the same place which",00:26:57.670,00:27:03.650
"of course is always going to
happen so you get basically",00:27:03.650,00:27:07.150
plus 1 everywhere.,00:27:07.150,00:27:08.570
"So that makes me
happier because then I",00:27:11.390,00:27:13.310
"actually get with the theorem
said which is 1 plus alpha.",00:27:13.310,00:27:15.835
"There is always going to be
the one guy hashing there",00:27:15.835,00:27:18.540
"when I assume that ki
hashed to wherever it does.",00:27:18.540,00:27:21.970
"So this tells you that if we
could find a universal hash",00:27:24.650,00:27:27.980
"family, then we're guaranteed
insert, delete, and search",00:27:27.980,00:27:32.800
"cost order 1 plus
alpha in expectation.",00:27:32.800,00:27:35.535
"And the expectation is
only over the choice of h,",00:27:35.535,00:27:38.520
not over the inputs.,00:27:38.520,00:27:39.470
"I think I've stressed
that enough times.",00:27:39.470,00:27:42.466
"But the remaining question
is can we actually",00:27:42.466,00:27:44.340
design a universal hash family?,00:27:44.340,00:27:46.390
"Are there any universal
hash families?",00:27:46.390,00:27:48.015
"Yes, as you might
expect there are.",00:27:53.960,00:27:56.672
"Otherwise this wouldn't
be very interesting.",00:27:56.672,00:27:58.505
"Let me give you an example of
a bad universal hash family.",00:28:07.140,00:28:12.990
"Sort of an oxymoron
but it's possible.",00:28:12.990,00:28:15.505
Bad.,00:28:24.190,00:28:25.380
"Here's a hash family
that's universal.",00:28:25.380,00:28:27.600
"h is the set of
all hash functions.",00:28:27.600,00:28:32.360
"h from 0,1 to u minus 1.",00:28:32.360,00:28:36.370
"This is what's normally
called uniform hashing.",00:28:44.010,00:28:46.790
"It makes analysis
really easy because you",00:28:46.790,00:28:50.350
"get to assume-- I
mean this says ahead",00:28:50.350,00:28:53.240
"of time for every
universe item, I'm",00:28:53.240,00:28:55.500
"going to choose a
random slot to put it.",00:28:55.500,00:28:59.300
"And then I'll just
remember that.",00:28:59.300,00:29:01.510
"And so whenever you give me
the key, I'll just map it by h.",00:29:01.510,00:29:06.030
"And I get a consistent slot
and definitely it's universal.",00:29:06.030,00:29:10.420
"What's bad about
this hash function?",00:29:10.420,00:29:13.460
Many things but--,00:29:13.460,00:29:14.541
"STUDENT: [INAUDIBLE] That's
just as hard as the problem I'm",00:29:17.427,00:29:22.520
solving.,00:29:22.520,00:29:23.020
ERIK DEMAINE: Sort of.,00:29:23.020,00:29:23.820
"I'm begging the
question that it's just",00:29:23.820,00:29:25.460
"as hard as the
problem I'm solving.",00:29:25.460,00:29:27.240
"And what, algorithmically,
what goes wrong here?",00:29:27.240,00:29:31.022
There are two things I guess.,00:29:31.022,00:29:32.230
Yeah?,00:29:38.304,00:29:38.804
STUDENT: It's not deterministic?,00:29:38.804,00:29:40.730
"ERIK DEMAINE: It's
not deterministic.",00:29:40.730,00:29:42.650
"That's OK because we're
allowing randomization",00:29:42.650,00:29:45.275
in this algorithm.,00:29:45.275,00:29:46.960
"So I mean how I
would compute this",00:29:46.960,00:29:49.100
"is I would do a four loop
over all universe items.",00:29:49.100,00:29:52.610
"And I assume I have a way
to generate a random number",00:29:52.610,00:29:54.980
between 0 and m minus 1.,00:29:54.980,00:29:56.840
That's legitimate.,00:29:56.840,00:29:58.570
"But there's something
bad about that algorithm.",00:29:58.570,00:30:01.342
STUDENT: It's not consistent.,00:30:01.342,00:30:02.550
ERIK DEMAINE: Not consistent?,00:30:02.550,00:30:03.758
"It is consistent if I precompute
for every universe item",00:30:03.758,00:30:06.470
where to map it.,00:30:06.470,00:30:07.700
That's good.,00:30:07.700,00:30:08.720
"So all these things
are actually OK.",00:30:08.720,00:30:10.670
"STUDENT: It takes too
much time and space.",00:30:10.670,00:30:12.540
"ERIK DEMAINE: It takes
too much time and space.",00:30:12.540,00:30:14.498
Yeah.,00:30:14.498,00:30:16.460
That's the bad thing.,00:30:16.460,00:30:19.380
"It's hard to isolate in a bad
thing what is so bad about it.",00:30:19.380,00:30:22.640
"But we need u time to compute
all those random numbers.",00:30:22.640,00:30:29.710
"And we need u space to
store that hash function.",00:30:29.710,00:30:32.540
"In order to get to the
consistency we have to-- Oops.",00:30:32.540,00:30:37.270
Good catch.,00:30:37.270,00:30:38.850
"In order to get
consistency, we need",00:30:38.850,00:30:40.350
"to keep track of all those
hash function values.",00:30:40.350,00:30:43.840
And that's not good.,00:30:43.840,00:30:47.524
"You could try to not
store them all, you know,",00:30:47.524,00:30:49.440
use a hash table.,00:30:49.440,00:30:50.400
"But you can't use a hash table
to store a hash function.",00:30:50.400,00:30:53.620
"That would be-- that would
be infinite recursion.",00:30:53.620,00:30:58.180
"So but at least
they're out there.",00:30:58.180,00:31:00.100
"So the challenge is to find
an efficient hash family that",00:31:00.100,00:31:03.510
"doesn't take much space
to store and doesn't",00:31:03.510,00:31:05.690
take much time to compute.,00:31:05.690,00:31:07.850
"OK, we're allowing randomness.",00:31:07.850,00:31:09.786
"But we don't want
to much randomness.",00:31:19.720,00:31:21.280
"We can't afford u units
of time of randomness.",00:31:21.280,00:31:23.620
I mean u could be huge.,00:31:23.620,00:31:25.630
"We're only doing n operations
probably on this hash table.",00:31:25.630,00:31:28.800
u could be way bigger than n.,00:31:28.800,00:31:31.030
"We don't want to have to
precompute this giant table",00:31:31.030,00:31:33.400
"and then use it for
like five steps.",00:31:33.400,00:31:35.170
"It would be really, really
slow even amortized.",00:31:35.170,00:31:38.220
"So here's one that
I will analyze.",00:31:38.220,00:31:42.542
"And there's another one in the
textbook which I'll mention.",00:31:42.542,00:31:45.000
"This one's a little
bit simpler to analyze.",00:31:49.800,00:31:53.359
"We're going to need a little
bit of number theory, just",00:31:53.359,00:31:55.650
prime numbers.,00:31:55.650,00:31:57.610
"And you've probably heard of
the idea of your hash table size",00:31:57.610,00:32:02.240
being prime.,00:32:02.240,00:32:03.400
"Here you'll see
why that's useful,",00:32:03.400,00:32:05.729
at least for this family.,00:32:05.729,00:32:06.770
"You don't always need
primality, but it's",00:32:06.770,00:32:08.860
going to make this family work.,00:32:08.860,00:32:11.320
"So I'm going to assume that
my table size is prime.",00:32:11.320,00:32:14.430
"Now really my table
size is doubling,",00:32:14.430,00:32:17.716
so that's a little awkward.,00:32:17.716,00:32:18.840
"But luckily there are
algorithms given a number",00:32:18.840,00:32:21.550
to find a nearby prime number.,00:32:21.550,00:32:23.170
"We're not going to
cover that here,",00:32:23.170,00:32:25.150
"but that's an algorithmic
number theory thing.",00:32:25.150,00:32:27.500
"And in polylogarithmic
time, I guess",00:32:27.500,00:32:29.860
"you can find a
nearby prime number.",00:32:29.860,00:32:33.340
"So you want it to
be a power of 2.",00:32:33.340,00:32:35.220
"And you'll just look around
for nearby prime numbers.",00:32:35.220,00:32:38.390
"And then we have a prime that's
about the same size so that",00:32:38.390,00:32:41.090
"will work just as well from
a table doubling perspective.",00:32:41.090,00:32:45.550
"Then furthermore,
for convenience,",00:32:45.550,00:32:49.810
"I'm going to assume that u
is an integer power of m.",00:32:49.810,00:32:53.740
"I want my universe to be
a power of that prime.",00:33:01.404,00:33:06.489
"I mean, if it isn't, just
make u a little bigger.",00:33:06.489,00:33:08.530
"It's OK if u gets
bigger as long as it",00:33:08.530,00:33:10.113
covers all of the same items.,00:33:10.113,00:33:13.450
"Now once I view my universe
as a power of the table size,",00:33:13.450,00:33:19.340
"a natural thing to do is
take my universe items,",00:33:19.340,00:33:23.140
"to take my input integers,
and think of them in base m.",00:33:23.140,00:33:27.530
So that's what I'm going to do.,00:33:27.530,00:33:29.730
"I'm going to view
a key k in base m.",00:33:29.730,00:33:37.880
"Whenever I have a
key, I can think of it",00:33:37.880,00:33:41.640
"as a vector of subkeys,
k1 up to kr minus 1.",00:33:41.640,00:33:51.850
"There are digits in base m
because of this relation.",00:33:51.850,00:33:57.024
"And I don't even care which
is the least significant",00:33:57.024,00:33:59.190
"and which is the
most significant.",00:33:59.190,00:34:00.606
"That won't matter so
whatever, whichever order",00:34:00.606,00:34:02.630
you want to think of it.,00:34:02.630,00:34:05.130
"And each of the
ki's here I guess",00:34:05.130,00:34:08.830
is between 0 and m minus 1.,00:34:08.830,00:34:11.550
So far so good.,00:34:17.480,00:34:18.680
"So with this perspective,
the base m perspective,",00:34:37.760,00:34:40.670
"I can define a dot product
hash function as follows.",00:34:40.670,00:34:45.469
"It's going to be
parametrized by another key,",00:34:45.469,00:34:48.520
"I'll call it a, which we can
think of again as a vector.",00:34:48.520,00:34:52.865
I want to define h sub a of k.,00:34:57.380,00:35:03.040
"So this is parametrized
by a, but it's",00:35:03.040,00:35:04.790
"a function of a given
key k as the dot product",00:35:04.790,00:35:10.910
of those two vectors mod m.,00:35:10.910,00:35:13.135
"So remember dot products
are just the sum from i",00:35:16.390,00:35:19.930
"equals 0 to r minus
1 of a1 times ki.",00:35:19.930,00:35:26.800
"I want to do all
of that modulo m.",00:35:26.800,00:35:31.230
"We'll worry about
how long this takes",00:35:31.230,00:35:33.992
to compute in a moment I guess.,00:35:33.992,00:35:37.710
Maybe very soon.,00:35:37.710,00:35:40.680
"But the hash family h is
just all of these ha's",00:35:40.680,00:35:45.690
for all possible choices of a.,00:35:45.690,00:35:48.560
"a was a key so it comes
from the universe u.",00:35:52.276,00:35:56.860
"And so what that means is
to do universal hashing,",00:36:01.770,00:36:04.530
"I want to choose one of these
ha's uniformly at random.",00:36:04.530,00:36:07.650
How do I do that?,00:36:07.650,00:36:08.700
"I just choose a
uniformly at random.",00:36:08.700,00:36:11.410
Pretty easy.,00:36:11.410,00:36:12.390
"It's one random value
from one random key.",00:36:12.390,00:36:16.230
"So that should take constant
time and constant space",00:36:16.230,00:36:19.500
to store one number.,00:36:19.500,00:36:22.660
"In general we're in a world
called the Word RAM model.",00:36:22.660,00:36:28.100
"This is actually-- I
guess m stands for model",00:36:28.100,00:36:31.800
so I shouldn't write model.,00:36:31.800,00:36:33.610
"Random access machine
which you may have heard.",00:36:33.610,00:36:38.240
"The word RAM assumes
that in general we're",00:36:38.240,00:36:42.980
manipulating integers.,00:36:42.980,00:36:45.180
And the integers fit in a word.,00:36:45.180,00:36:49.550
"And the computational
assumption is",00:36:49.550,00:36:51.430
"that manipulating a
constant number of words",00:36:51.430,00:36:55.176
"and doing essentially
any operation you",00:36:55.176,00:36:56.800
"want on constant number of
words takes constant time.",00:36:56.800,00:37:01.060
"And the other part
of the word RAM model",00:37:06.530,00:37:08.430
"is to assume that the things
you care about fit in a word.",00:37:08.430,00:37:11.770
"Say individual data values,
here we're talking about keys,",00:37:16.950,00:37:24.950
fit in a word.,00:37:24.950,00:37:28.190
"This is what you need
to assume in [INAUDIBLE]",00:37:28.190,00:37:30.590
"that you can compute high
of x in constant time or low",00:37:30.590,00:37:33.830
of x in constant time.,00:37:33.830,00:37:35.470
"Here I'm going to use it to
assume that we can compute",00:37:35.470,00:37:38.250
h sub a of k in constant time.,00:37:38.250,00:37:41.790
"In practice this would
be done by implementing",00:37:41.790,00:37:44.010
"this computation, this
dot product computation,",00:37:44.010,00:37:46.870
in hardware.,00:37:46.870,00:37:48.540
"And the reason a 64-bit
edition on a modern processor",00:37:48.540,00:37:53.420
"or a 32-bit on most
phones takes constant time",00:37:53.420,00:37:56.359
"is because there's
hardware that's designed",00:37:56.359,00:37:58.150
to do that really fast.,00:37:58.150,00:38:00.050
"And in general we're assuming
that the things we care about",00:38:00.050,00:38:03.990
fit in a single word.,00:38:03.990,00:38:06.137
"And we're assuming random access
and that we can have a raise.",00:38:06.137,00:38:08.720
"That's what we need in
order to store a table.",00:38:08.720,00:38:10.720
"And same thing in [INAUDIBLE],
we needed to assume we",00:38:10.720,00:38:12.930
had a raise.,00:38:12.930,00:38:13.430
"And I think this
operation is actually",00:38:16.772,00:38:18.400
"pretty-- exists in Intel
architectures in some form.",00:38:18.400,00:38:22.540
"But it's certainly not
a normal operation.",00:38:22.540,00:38:25.117
"If you're going to
do this explicitly,",00:38:25.117,00:38:26.700
"adding up and
multiplying things this",00:38:26.700,00:38:28.340
"would be r is the log base m of
u, so it's kind of logish time.",00:38:28.340,00:38:34.900
"Maybe I'll mention
another hash family that's",00:38:34.900,00:38:39.970
more obviously computable.,00:38:39.970,00:38:41.985
But I won't analyze here.,00:38:45.499,00:38:46.540
It's analyzed in the textbook.,00:38:46.540,00:38:48.000
"So if you're curious you
can check it out there.",00:38:48.000,00:38:52.450
Let's call this just another.,00:38:52.450,00:38:56.600
"It's a bit weird
because it has two mods.",00:39:15.620,00:39:17.520
You take mod p and then mod m.,00:39:17.520,00:39:19.100
"But the main computation
is very simple.",00:39:19.100,00:39:22.010
"You choose a uniformly
random value a.",00:39:22.010,00:39:24.390
"You multiply it by your key
in usual binary multiplication",00:39:24.390,00:39:29.640
instead of dot product.,00:39:29.640,00:39:30.900
"And then you add another
uniformly random key.",00:39:30.900,00:39:34.070
This is also universal.,00:39:34.070,00:39:36.360
"So H is hab for all a
and b that are keys.",00:39:36.360,00:39:44.660
"So if you're not happy
with this assumption",00:39:48.629,00:39:50.420
"that you can compute
this in constant time,",00:39:50.420,00:39:52.272
"you should be happy
with this assumption.",00:39:52.272,00:39:53.980
"If you believe in addition and
multiplication and division",00:39:53.980,00:39:56.396
"being constant time, then
this will be constant time.",00:39:56.396,00:39:58.690
"So both of these
families are universal.",00:40:01.860,00:40:03.640
"I'm going to prove that this
one is universal because it's",00:40:03.640,00:40:06.150
a little bit easier.,00:40:06.150,00:40:06.790
Yeah?,00:40:06.790,00:40:07.290
"STUDENT: Is this p a
choice that you made?",00:40:07.290,00:40:09.640
"ERIK DEMAINE: OK, right.",00:40:09.640,00:40:10.640
What is p?,00:40:10.640,00:40:11.704
"P just has to be bigger than
m, and it should be prime.",00:40:11.704,00:40:19.030
It's not random.,00:40:19.030,00:40:20.940
"You can just choose one prime
that's bigger than your table",00:40:20.940,00:40:24.550
"size, and this will work.",00:40:24.550,00:40:26.120
STUDENT: [INAUDIBLE],00:40:26.120,00:40:29.340
"ERIK DEMAINE: I
forget whether you",00:40:32.390,00:40:33.860
have to assume that m is prime.,00:40:33.860,00:40:35.160
I'd have to check.,00:40:35.160,00:40:37.630
"I'm guessing not, but
don't quote me on that.",00:40:37.630,00:40:43.250
"Check the section
in the textbook.",00:40:43.250,00:40:46.760
So good.,00:40:46.760,00:40:47.920
Easy to compute.,00:40:47.920,00:40:50.112
"The analysis is simpler, but
it's a little bit easier here.",00:40:50.112,00:40:52.570
"Essentially this is
very much like products",00:40:52.570,00:40:56.070
"but there's no
carries here from one.",00:40:56.070,00:40:59.720
"When we do the dot product
instead of just multiplying",00:40:59.720,00:41:02.140
"in base m we multiply
them based on that",00:41:02.140,00:41:04.500
"would give the same thing
as multiplying in base 2,",00:41:04.500,00:41:07.150
"but we get carries from one
m-sized digit to the next one.",00:41:07.150,00:41:10.330
"And that's just more
annoying to think about.",00:41:10.330,00:41:12.280
"So here we're essentially
getting rid of carries.",00:41:12.280,00:41:14.535
"So it's in some sense
even easier to compute.",00:41:14.535,00:41:17.170
"And in both cases,
it's universal.",00:41:17.170,00:41:20.305
"So we want to prove
this property.",00:41:24.370,00:41:34.140
"That if we choose a random
a then the probability",00:41:34.140,00:41:39.200
"of two keys, k and k'
which are distinct mapping",00:41:39.200,00:41:42.170
"via h to the same value is at
most 1/m So let's prove that.",00:41:42.170,00:41:49.450
So we're given two keys.,00:42:06.450,00:42:12.422
"We have no control
over them because this",00:42:12.422,00:42:14.130
"has to work for all
keys that are distinct.",00:42:14.130,00:42:16.645
"The only thing we know
is that they're distinct.",00:42:22.430,00:42:24.550
"Now if two keys are
distinct, then their vectors",00:42:24.550,00:42:27.267
must be distinct.,00:42:27.267,00:42:27.975
"If two vectors
are distinct, that",00:42:27.975,00:42:29.360
"means at least one
item must be different.",00:42:29.360,00:42:32.269
Should sound familiar.,00:42:32.269,00:42:33.185
"So this was like in the matrix
multiplication verification",00:42:39.870,00:42:43.240
"algorithm that
[INAUDIBLE] taught.",00:42:43.240,00:42:46.420
"So k and k' differ
in some digit.",00:42:46.420,00:42:54.855
Let's call that digit d.,00:42:58.190,00:42:59.440
"So k sub d is different
from k sub d'.",00:43:02.902,00:43:06.830
"And I want to compute
this probability.",00:43:09.370,00:43:14.590
We'll rewrite it.,00:43:14.590,00:43:15.530
The probability is over a.,00:43:33.970,00:43:36.450
"I'm choosing a
uniformly at random.",00:43:36.450,00:43:38.400
"I want another
probability that that",00:43:38.400,00:43:39.900
maps k and k' to the same slot.,00:43:39.900,00:43:43.520
"So let me just write
out the definition.",00:43:43.520,00:43:47.210
"It's probability over a that
the dot product of a and k",00:43:47.210,00:43:58.750
"is the same thing as when I do
the dot product with k' mod m.",00:43:58.750,00:44:12.180
"These two, that sum should
come out the same, mod m.",00:44:12.180,00:44:15.620
"So let me move this part over to
this side because in both cases",00:44:19.570,00:44:25.210
we have the same ai.,00:44:25.210,00:44:26.490
"So I can group
terms and say this",00:44:26.490,00:44:28.920
"is the probability--
probability sum over i",00:44:28.920,00:44:45.640
"equals 0 to r minus 1
of ai times ki minus",00:44:45.640,00:44:50.900
ki prime equals 0.,00:44:50.900,00:44:54.420
Mod m.,00:44:57.660,00:44:58.160
"OK, no pun intended.",00:45:12.380,00:45:14.750
Now we care about this digit d.,00:45:14.750,00:45:19.430
"d is a place where we know
that this is non-zero.",00:45:19.430,00:45:22.210
"So let me separate out the terms
for d and everything but d.",00:45:22.210,00:45:28.270
"So this is the same as ability
of, let's do the d term first,",00:45:28.270,00:45:34.630
"so we have ad times
kd minus kd prime.",00:45:34.630,00:45:41.920
That's one term.,00:45:41.920,00:45:43.240
"I'm going to write
the summation of i",00:45:43.240,00:45:46.860
"not equal to d of ai
ki minus ki prime.",00:45:46.860,00:45:56.485
"These ones, some of
them might be zero.",00:45:56.485,00:45:58.110
Some are not.,00:45:58.110,00:45:58.990
"We're not going
to worry about it.",00:45:58.990,00:46:00.850
"It's enough to just isolate
one term that is non-zero.",00:46:00.850,00:46:03.105
"So this thing we know
does not equal zero.",00:46:08.550,00:46:11.520
Cool.,00:46:14.370,00:46:15.350
"Here's where I'm going to use
a little bit of number theory.",00:46:15.350,00:46:17.850
"I haven't yet used
that m is prime.",00:46:17.850,00:46:20.360
"I required m is prime because
when you're working modulo m,",00:46:20.360,00:46:27.120
"you have multiplicative
inverses.",00:46:27.120,00:46:30.470
"Because this is
not zero, there is",00:46:30.470,00:46:32.430
"something I can
multiply on both sides",00:46:32.430,00:46:35.190
"and get this to cancel
out and become one.",00:46:35.190,00:46:40.800
"For every value x
there is a value y.",00:46:40.800,00:46:43.650
So x times y equals 1 modulo m.,00:46:43.650,00:46:46.170
"And you can even compute
it in constant time",00:46:46.170,00:46:48.070
in a reasonable model.,00:46:48.070,00:46:50.410
"So then I can say I want the
probability that ad is minus",00:46:50.410,00:47:08.290
kd minus kd prime inverse.,00:47:08.290,00:47:12.030
"This is the multiplicative
inverse I was talking about.",00:47:12.030,00:47:14.520
"And then the sum i not equal
to d whatever, I don't actually",00:47:14.520,00:47:20.680
"care what this is too much, I've
already done the equals part.",00:47:20.680,00:47:27.264
I still need to write mod m.,00:47:27.264,00:47:28.430
"The point is this
is all about ad.",00:47:31.240,00:47:36.520
"Remember we're choosing
a uniformly at random.",00:47:36.520,00:47:38.850
"That's the same
thing as choosing",00:47:38.850,00:47:40.700
"each of the ai's independently
uniformly at random.",00:47:40.700,00:47:45.896
Yeah?,00:47:45.896,00:47:47.372
"STUDENT: Is the second line over
there isolating d [INAUDIBLE]?",00:47:47.372,00:47:53.276
Second from the top.,00:47:53.276,00:47:54.667
ERIK DEMAINE: Which?,00:47:54.667,00:47:55.500
This one?,00:47:55.500,00:47:56.020
STUDENT: No up.,00:47:56.020,00:47:56.520
ERIK DEMAINE: This?,00:47:56.520,00:47:57.311
STUDENT: Down.,00:47:57.311,00:47:58.398
That one.,00:47:58.398,00:47:58.898
No.,00:47:58.898,00:47:59.380
The one below that.,00:47:59.380,00:48:00.171
ERIK DEMAINE: Yes.,00:48:00.171,00:48:01.790
"STUDENT: Is that line
isolating d or is that--",00:48:01.790,00:48:03.730
ERIK DEMAINE: No.,00:48:03.730,00:48:04.438
I haven't isolated d yet.,00:48:04.438,00:48:05.490
This is all the terms.,00:48:05.490,00:48:06.970
"And then going from
this line to this one,",00:48:06.970,00:48:08.970
"I'm just pulling out
the i equals d term.",00:48:08.970,00:48:12.910
That's this term.,00:48:12.910,00:48:13.650
"And then separating out
the i not equal to d.",00:48:13.650,00:48:16.324
STUDENT: I get it.,00:48:16.324,00:48:17.090
ERIK DEMAINE: Right?,00:48:17.090,00:48:17.380
"This sum is just the
same as that sum.",00:48:17.380,00:48:18.980
"But I've done the
d term explicitly.",00:48:18.980,00:48:20.480
STUDENT: Sure.,00:48:20.480,00:48:21.063
I get it.,00:48:21.063,00:48:21.780
"ERIK DEMAINE: So I've
done all this rewriting",00:48:24.310,00:48:27.150
"because I know that ad is
chosen uniformly at random.",00:48:27.150,00:48:29.780
"Here we have this
thing, this monstrosity,",00:48:29.780,00:48:34.340
but it does not depend on ad.,00:48:34.340,00:48:36.890
In fact it is independent of ad.,00:48:36.890,00:48:39.300
"I'm going to write this
as a function of k and k'",00:48:39.300,00:48:44.570
"because those are
given to us and fixed.",00:48:44.570,00:48:46.850
"And then it's also a
function of a0 and a1.",00:48:46.850,00:48:50.310
Everything except d.,00:48:50.310,00:48:53.520
"So ad minus 1, ad plus 1,
and so on up to ar minus 1.",00:48:53.520,00:49:01.920
This is awkward to write.,00:49:01.920,00:49:03.700
"But everything except
ad appears here",00:49:03.700,00:49:06.230
"because we have
i not equal to d.",00:49:06.230,00:49:09.230
"And these ai's are
random variables.",00:49:09.230,00:49:13.040
"But we're assuming that they're
all chosen independently",00:49:13.040,00:49:16.460
from each other.,00:49:16.460,00:49:17.910
"So I don't really care what's
going on in this function.",00:49:17.910,00:49:21.720
It's something.,00:49:21.720,00:49:22.660
"And if I rewrite
this probability,",00:49:22.660,00:49:24.390
"it's the probability
over the choice of a.",00:49:24.390,00:49:27.636
"I can separate out the
choice of all these things",00:49:27.636,00:49:31.720
from the choice of ad.,00:49:31.720,00:49:35.320
"And this is just
a useful formula.",00:49:35.320,00:49:39.560
"I'm going to write
a not equal to d.",00:49:39.560,00:49:43.500
"All the other-- maybe I'll
write a sub i not equal to d.",00:49:43.500,00:49:48.400
"All the choices of
those guys separately",00:49:48.400,00:49:51.080
"from the probability
of choosing ad of ad",00:49:51.080,00:49:59.700
equal to this function.,00:49:59.700,00:50:00.895
"If you just think about the
definition of expectation,",00:50:05.090,00:50:08.200
this is doing the same thing.,00:50:08.200,00:50:09.560
"We're thinking of first
choosing the ai's where",00:50:09.560,00:50:12.780
i is not equal to d.,00:50:12.780,00:50:14.370
And then we choose ad.,00:50:14.370,00:50:15.970
"And this computational will
come out the same as that.",00:50:15.970,00:50:19.470
"But this is the probability
of a uniformly random number",00:50:25.110,00:50:28.720
equaling something.,00:50:28.720,00:50:31.680
"So we just need to
think about-- sorry.",00:50:31.680,00:50:35.950
Important.,00:50:35.950,00:50:37.470
"That would be pretty
unlikely that would be 1/u,",00:50:37.470,00:50:39.810
"but this is all
working modulo m.",00:50:39.810,00:50:42.970
"So if I just take a
uniformly random integer",00:50:42.970,00:50:45.760
"and the chance of it hitting any
particular value mod m is 1/m.",00:50:45.760,00:50:49.530
And that's universality.,00:50:53.011,00:50:54.010
"So in this case, you get exactly
1/m, no less than or equal to.",00:50:57.430,00:51:02.500
"Sorry, I should have written
it's the expectation of 1/m,",00:51:02.500,00:51:06.440
"but that's 1/m because 1/m
has no random parts in it.",00:51:06.440,00:51:12.540
Yeah?,00:51:12.540,00:51:13.412
"STUDENT: How do
we know that the,",00:51:13.412,00:51:15.220
"that this expression doesn't
have any biases in the sense",00:51:15.220,00:51:19.735
"that it doesn't give more,
more, like if you give it",00:51:19.735,00:51:23.832
"the uniform
distribution of numbers,",00:51:23.832,00:51:26.718
"it doesn't spit out
more numbers than others",00:51:26.718,00:51:28.642
and that could potentially--,00:51:28.642,00:51:30.514
"ERIK DEMAINE: Oh,
so you're asking",00:51:30.514,00:51:31.930
"how do we know that
this hash family doesn't",00:51:31.930,00:51:35.360
"prefer some slots
over others, I guess.",00:51:35.360,00:51:38.085
"STUDENT: Of course like
after the equals sign,",00:51:38.085,00:51:41.378
"like in this middle
line in the middle.",00:51:41.378,00:51:46.219
Middle board.,00:51:46.219,00:51:46.760
ERIK DEMAINE: This one?,00:51:46.760,00:51:47.718
"Oh, this one.",00:51:47.718,00:51:49.304
STUDENT: Middle board.,00:51:49.304,00:51:50.220
ERIK DEMAINE: Middle board.,00:51:50.220,00:51:51.531
Here.,00:51:51.531,00:51:52.030
STUDENT: Yes.,00:51:52.030,00:51:53.056
"So how do we know
that if you give it--",00:51:53.056,00:51:54.680
ERIK DEMAINE: This function.,00:51:54.680,00:51:55.846
"STUDENT: --random variables,
it won't prefer certain numbers",00:51:55.846,00:51:59.760
over others?,00:51:59.760,00:52:00.480
"ERIK DEMAINE: So this function
may prefer some numbers",00:52:00.480,00:52:03.560
over others.,00:52:03.560,00:52:04.940
But it doesn't matter.,00:52:04.940,00:52:06.300
"All we need is
that this function",00:52:06.300,00:52:08.310
"is independent of
our choice of ad.",00:52:08.310,00:52:10.285
"So you can think
of this function,",00:52:10.285,00:52:12.500
"you choose all of these
random-- actually k and k'",00:52:12.500,00:52:15.010
"are not random-- but you choose
all these random numbers.",00:52:15.010,00:52:18.179
Then you evaluate your f.,00:52:18.179,00:52:19.220
Maybe it always comes out to 5.,00:52:19.220,00:52:20.970
Who knows.,00:52:20.970,00:52:21.470
It could be super biased.,00:52:21.470,00:52:23.090
"But then you choose ad
uniformly at random.",00:52:23.090,00:52:26.430
"So the chance of ad
equalling 5 is the same",00:52:26.430,00:52:29.350
as the chance of ad equaling 3.,00:52:29.350,00:52:31.730
"So in all cases, you get
the probability is 1/m.",00:52:31.730,00:52:34.410
What we need is independence.,00:52:34.410,00:52:36.020
"We need that the ad is chosen
independently from the other",00:52:36.020,00:52:39.220
ai's.,00:52:39.220,00:52:39.810
"But we don't need to know
anything about f other",00:52:39.810,00:52:42.210
than it doesn't depend on ad.,00:52:42.210,00:52:44.640
"So and we made it not depend
on ad because I isolated ad",00:52:44.640,00:52:48.690
"by pulling it out
of that summation.",00:52:48.690,00:52:50.600
"So we know there's
no ad's over here.",00:52:50.600,00:52:53.370
Good question.,00:52:53.370,00:52:56.110
"You get a bonus Frisbee
for your question.",00:52:56.110,00:52:58.825
All right.,00:53:01.500,00:53:02.890
That ends universal hashing.,00:53:02.890,00:53:06.520
Any more questions?,00:53:06.520,00:53:08.010
"So at this point we
have at least one",00:53:08.010,00:53:10.390
universal hash family.,00:53:10.390,00:53:12.350
"So we're just choosing, in this
case, a uniformly at random.",00:53:12.350,00:53:15.790
"In the other method, we choose
a and b uniformly at random.",00:53:15.790,00:53:19.400
"And then we build
our hash table.",00:53:19.400,00:53:23.040
"And the hash function
depends on m.",00:53:23.040,00:53:25.667
"So also every time we
double our table size,",00:53:25.667,00:53:27.500
"we're going to have to
choose a new hash function",00:53:27.500,00:53:29.570
for the new value of m.,00:53:29.570,00:53:32.340
And that's about it.,00:53:32.340,00:53:34.440
"So this will give us constant
expected time-- or in general 1",00:53:34.440,00:53:38.870
"plus alpha if you're not doing
table doubling-- for insert,",00:53:38.870,00:53:42.480
"delete, and exact search.",00:53:42.480,00:53:45.230
"Just building on the
hashing with chaining.",00:53:45.230,00:53:49.230
And so this is a good method.,00:53:49.230,00:53:50.760
Question?,00:53:50.760,00:53:51.551
"STUDENT: Why do you say expected
value of the probability?",00:53:51.551,00:53:54.497
"Isn't it sufficient to just say
the probability of [INAUDIBLE]?",00:53:54.497,00:53:58.430
"ERIK DEMAINE: Uh, yeah,
I wanted to isolate--",00:53:58.430,00:54:02.210
"it is the overall probability
of this happening.",00:54:02.210,00:54:05.400
"I rewrote it this
way because I wanted",00:54:05.400,00:54:07.140
"to think about first choosing
the ai's where i does not",00:54:07.140,00:54:09.640
equal d and then choosing ad.,00:54:09.640,00:54:12.225
"So this probability
was supposed to be only",00:54:12.225,00:54:14.180
over the choice of ad.,00:54:14.180,00:54:15.626
"And you have to do something
with the other ai's",00:54:15.626,00:54:17.700
because they're random.,00:54:17.700,00:54:18.470
"You can't just say,
what's the probability ad",00:54:18.470,00:54:20.345
equaling a random variable?,00:54:20.345,00:54:21.800
That's a little sketchy.,00:54:21.800,00:54:23.300
"I wanted to have no
random variables over all.",00:54:23.300,00:54:25.255
"So I have to kind of bind
those variables with something.",00:54:25.255,00:54:28.460
"And I just want to see what
the-- This doesn't really",00:54:28.460,00:54:32.480
"affect very much, but to
make this algebraically",00:54:32.480,00:54:35.860
"correct I need to say
what the ai's, i not",00:54:35.860,00:54:38.610
equal to d are doing.,00:54:38.610,00:54:41.490
Other questions?,00:54:41.490,00:54:43.214
Yeah.,00:54:43.214,00:54:43.714
"STUDENT: Um, I'm a bit
confused about your definition",00:54:43.714,00:54:45.922
"of the collision in
the lower left board.",00:54:45.922,00:54:50.546
"Why are you adding
i's [INAUDIBLE]?",00:54:50.546,00:54:53.357
"ERIK DEMAINE: Yeah, sorry.",00:54:53.357,00:54:54.440
"This is a funny
notion of colliding.",00:54:54.440,00:54:56.320
"I just mean I want to count
the number of keys that",00:54:56.320,00:54:58.560
hash to the same slot as ki.,00:54:58.560,00:55:00.350
"STUDENT: So it's not necessarily
like a collision [INAUDIBLE].",00:55:00.350,00:55:04.106
"ERIK DEMAINE: You
may not call it",00:55:04.106,00:55:05.480
"a collision when it
collides with itself, yeah.",00:55:05.480,00:55:08.250
Whatever you want to call it.,00:55:08.250,00:55:11.050
"But I just mean hashing
to the same slot is ki.",00:55:11.050,00:55:14.920
Yeah.,00:55:14.920,00:55:15.470
"Just because I want to count
the total length of the chain.",00:55:15.470,00:55:17.960
"I don't want to count the number
of collisions in the chain.",00:55:17.960,00:55:21.210
Sorry.,00:55:21.210,00:55:21.710
Probably a poor choice of word.,00:55:21.710,00:55:23.220
"We're hashing because
we're taking our key,",00:55:27.720,00:55:31.179
"we're cutting it up
into little bits,",00:55:31.179,00:55:32.720
"and then we're mixing them up
just like a good corned beef",00:55:32.720,00:55:35.480
hash or something.,00:55:35.480,00:55:38.550
"All right let's move
on to perfect hashing.",00:55:38.550,00:55:41.140
"This is more
exciting I would say.",00:55:41.140,00:55:44.950
"Even cooler-- this was cool
from a probability perspective,",00:55:44.950,00:55:48.690
"depending on your
notion of cool.",00:55:48.690,00:55:50.590
"This method will be cool from
a data structures perspective",00:55:50.590,00:55:53.280
and a probability perspective.,00:55:53.280,00:55:54.610
"But so far data structures
are what we know from 006.",00:55:57.630,00:56:01.600
"Now we're going to go
up a level, literally.",00:56:01.600,00:56:06.080
We're going to have two levels.,00:56:06.080,00:56:08.930
"So here we're solving-- you
can actually make this data",00:56:08.930,00:56:12.410
structure dynamic.,00:56:12.410,00:56:13.420
"But we're going to solve
the static dictionary",00:56:13.420,00:56:15.800
"problem which is when you
have no inserts and deletes.",00:56:15.800,00:56:24.530
You're given the keys up front.,00:56:24.530,00:56:26.195
You're given n keys.,00:56:29.760,00:56:30.900
"You want to build a table
that supports search.",00:56:30.900,00:56:34.560
And that's it.,00:56:38.424,00:56:39.590
"You want search to
be constant time",00:56:39.590,00:56:42.550
"and perfect hashing,
also known as FKS hashing",00:56:42.550,00:56:51.820
"because it was invented by
Fredman, Komlos, and Szemeredi",00:56:51.820,00:56:55.390
in 1984.,00:56:55.390,00:56:59.270
"What we will achieve is constant
time worst case for search.",00:56:59.270,00:57:09.520
"So that's a little
better because here we're",00:57:16.670,00:57:19.010
"just doing constant
expected time for search.",00:57:19.010,00:57:22.000
"But it's worse in that we have
to know the keys up in advance.",00:57:22.000,00:57:26.260
"We're going to take the linear
space in the worst case.",00:57:26.260,00:57:30.620
"And then the
remaining question is",00:57:40.250,00:57:41.750
"how long does it take you to
build this data structure?",00:57:41.750,00:57:44.870
"And for now I'll just
say it's polynomial time.",00:57:44.870,00:57:47.570
"It's actually going
to be nearly linear.",00:57:47.570,00:57:49.820
"And this is also
an expected bounds.",00:57:56.750,00:58:00.150
"Actually with high probability
could be a little more strong",00:58:00.150,00:58:05.111
here.,00:58:05.111,00:58:05.610
"So it's going to take
us a little bit of time",00:58:07.745,00:58:09.620
"to build this structure,
but once you have it,",00:58:09.620,00:58:11.536
you have the perfect scenario.,00:58:11.536,00:58:12.925
"There's going to
be in some sense",00:58:12.925,00:58:14.300
"no collisions in our hash
table so it would be constant",00:58:14.300,00:58:16.591
"times first search
and linear space.",00:58:16.591,00:58:19.936
So that part's great.,00:58:19.936,00:58:20.810
The only catch is it's static.,00:58:20.810,00:58:24.710
"But beggars can't
be choosers I guess.",00:58:24.710,00:58:30.500
All right.,00:58:30.500,00:58:31.040
"I'm not sure who's begging
in that analogy but.",00:58:34.102,00:58:36.060
The keys who want to be stored.,00:58:40.370,00:58:41.900
I don't know.,00:58:41.900,00:58:43.580
"All right, so the big
idea for perfect hashing",00:58:43.580,00:58:48.170
is to use two levels.,00:58:48.170,00:58:49.350
So let me draw a picture.,00:58:55.710,00:58:57.980
"We have our universe, and we're
mapping that via hash function",00:58:57.980,00:59:04.240
h1 into a table.,00:59:04.240,00:59:07.180
Look familiar?,00:59:07.180,00:59:08.640
"Exactly the diagram
I drew before.",00:59:08.640,00:59:11.540
"It's going to have
some table size m.",00:59:11.540,00:59:14.970
"And we're going to set m to be
within a constant factor of n.",00:59:14.970,00:59:22.090
"So right now it looks
exactly like regular--",00:59:22.090,00:59:25.410
"and it's going to
be a universal,",00:59:25.410,00:59:27.380
"h1 is chosen from a
universal hash family,",00:59:27.380,00:59:30.160
so universal hashing applies.,00:59:30.160,00:59:34.080
"The trouble is we're going
to get some lists here.",00:59:34.080,00:59:38.760
"And we don't want to store
the set of colliding elements,",00:59:38.760,00:59:44.755
"the set of elements that hash to
that place, with a linked list",00:59:44.755,00:59:47.380
because linked lists are slow.,00:59:47.380,00:59:50.550
"Instead we're going to store
them using a hash table.",00:59:50.550,00:59:53.692
It sounds crazy.,00:59:53.692,00:59:56.000
"But we're going to have--
so this is position 1.",00:59:56.000,01:00:01.340
"This is going to be h2,1.",01:00:01.340,01:00:04.470
"There's going to be another hash
function h2,0 that maps to some",01:00:04.470,01:00:10.500
other hash table.,01:00:10.500,01:00:11.230
"These hash tables are going
to be of varying sizes.",01:00:11.230,01:00:14.180
"Some of them will be of size 0
because nothing hashes there.",01:00:14.180,01:00:19.300
"But in general
each of these slots",01:00:19.300,01:00:21.420
"is going to map instead of to
a linked list to a hash table.",01:00:21.420,01:00:25.570
"So this would be h2, m minus 1.",01:00:25.570,01:00:31.260
"I'm going to guarantee in
the second level of hashing",01:00:31.260,01:00:33.840
there are zero collisions.,01:00:33.840,01:00:34.960
Let that sink in a little bit.,01:00:50.590,01:00:53.130
"Let me write down a little
more carefully what I'm doing.",01:00:53.130,01:00:56.100
"So h1 is picked from a
universal hash family.",01:01:09.050,01:01:12.330
Where m is theta n.,01:01:20.220,01:01:25.420
"I want to put a theta-- I
mean I could m equals n,",01:01:25.420,01:01:27.680
"but sometimes we
require m to be a prime.",01:01:27.680,01:01:29.810
"So I'm going to give you some
slop in how you choose m.",01:01:29.810,01:01:32.164
"So it can be prime
or whatever you want.",01:01:32.164,01:01:33.830
"And then at the
first level we're",01:01:36.370,01:01:37.880
"basically doing
hashing with chaining.",01:01:37.880,01:01:40.810
"And now I want to look at
each slot in that hash table.",01:01:40.810,01:01:50.580
So between 0 and m-1.,01:01:50.580,01:01:51.650
"I'm going to let lj be the
number of keys that hash,",01:01:55.520,01:02:02.150
"it's the length of the
list that would go there.",01:02:02.150,01:02:05.520
"It's going to be
the number of keys,",01:02:05.520,01:02:07.130
"among just the n keys, Number
of, keys hashing to slot j.",01:02:07.130,01:02:23.730
"So now the big question
is, if I have lj keys here,",01:02:26.760,01:02:30.200
how big do I make that table?,01:02:30.200,01:02:31.830
"You might say, well
I make a theta lj.",01:02:31.830,01:02:33.750
That's what I always do.,01:02:33.750,01:02:34.750
"But that's not what
I'm going to do.",01:02:34.750,01:02:36.640
That wouldn't help.,01:02:36.640,01:02:38.380
"We get exactly, I
think, the same number",01:02:38.380,01:02:40.490
"of collisions if we did that,
more or less, in expectation.",01:02:40.490,01:02:44.450
"So we're going do
something else.",01:02:44.450,01:02:49.340
"We're going to pick a hash
function from a universal",01:02:49.340,01:02:54.150
"family, h2,j.",01:02:54.150,01:02:55.300
It again maps the same universe.,01:02:58.720,01:03:00.170
"The key thing is the
size of the hash table",01:03:05.510,01:03:08.200
"I'm going to choose
which is lj squared.",01:03:08.200,01:03:13.227
"So if there are 3 elements that
happen to hash to this slot,",01:03:32.510,01:03:37.890
this table will have size 9.,01:03:37.890,01:03:42.730
So it's mostly empty.,01:03:42.730,01:03:43.920
"Only square root fraction--
if that's a word, if that's",01:03:43.920,01:03:47.180
a phrase-- will be full.,01:03:47.180,01:03:48.930
Most of it's empty.,01:03:48.930,01:03:50.050
Why squared?,01:03:50.050,01:03:50.735
Any ideas?,01:03:53.370,01:03:55.820
"I claim this will guarantee zero
collisions with decent chance.",01:03:55.820,01:03:59.460
Yeah.,01:03:59.460,01:03:59.960
"STUDENT: With 1/2
probability you're",01:03:59.960,01:04:01.860
"going to end up
with no collisions.",01:04:01.860,01:04:03.474
"ERIK DEMAINE: With
1/2 probability",01:04:03.474,01:04:04.890
"I'm going to end up
with no collisions.",01:04:04.890,01:04:05.880
Why?,01:04:05.880,01:04:06.290
What's it called?,01:04:06.290,01:04:06.998
STUDENT: Markov [INAUDIBLE],01:04:09.516,01:04:11.219
"ERIK DEMAINE: Markov's
inequality would prove it.",01:04:11.219,01:04:13.260
"But it's more commonly
known as the, whoa,",01:04:13.260,01:04:17.970
as the birthday paradox.,01:04:17.970,01:04:21.020
"So the whole name of the game
here is the birthday paradox.",01:04:21.020,01:04:25.280
"If I have, how's
it go, if I have n",01:04:25.280,01:04:29.315
"squared people with n
possible birthdays then--",01:04:29.315,01:04:33.450
is that the right way?,01:04:33.450,01:04:35.430
"No, less.",01:04:35.430,01:04:36.240
"If I have n people and n
squared possible birthdays,",01:04:36.240,01:04:40.280
"the probability of getting a
collision, a shared birthday,",01:04:40.280,01:04:42.700
is 1/2.,01:04:42.700,01:04:44.390
"Normally we think of
that as a funny thing.",01:04:44.390,01:04:46.740
"You know, if I choose a
fair number of people,",01:04:46.740,01:04:48.860
"then I get immediately
a collision.",01:04:48.860,01:04:51.330
"I'm going to do it
the opposite way.",01:04:51.330,01:04:52.830
"I'm going to guarantee that
there's so many birthdays",01:04:52.830,01:04:56.130
"that no 2 of them will collide
with probability of 1/2 No,",01:04:56.130,01:04:59.560
1/2 is not great.,01:04:59.560,01:05:00.430
We're going to fix that.,01:05:00.430,01:05:01.430
"So actually I haven't given
you the whole algorithm yet.",01:05:08.230,01:05:11.880
"There are two steps, 1 and 2.",01:05:11.880,01:05:14.050
"But there are also two
other steps 1.5 and 2.5.",01:05:14.050,01:05:19.920
"But this is the right
idea and this will make",01:05:19.920,01:05:22.290
things work in expectation.,01:05:22.290,01:05:23.660
"But I'm going to
tweak it a little bit.",01:05:23.660,01:05:26.020
"So first let me
tell you step 1.5.",01:05:28.810,01:05:30.650
It fits in between the two.,01:05:30.650,01:05:33.170
"I want that the space of this
data structure is linear.",01:05:33.170,01:05:38.100
So I need to make sure it is.,01:05:38.100,01:05:40.050
"If the sum j equals 0 to
m minus 1 of lj squared",01:05:40.050,01:05:48.840
"is bigger than
some constant times",01:05:48.840,01:05:50.570
"n-- we'll figure out what the
constant is later-- then redo",01:05:50.570,01:05:55.250
step 1.,01:05:55.250,01:05:58.020
"So after I do step 1, I know
how big all these tables",01:05:58.020,01:06:01.360
are going to be.,01:06:01.360,01:06:02.240
"If the sum of those squares is
bigger than linear, start over.",01:06:02.240,01:06:07.140
"I need to prove
that this will only",01:06:07.140,01:06:09.180
"have to take-- this
will happen an expected",01:06:09.180,01:06:12.180
constant number of times.,01:06:12.180,01:06:13.690
"log n times with
high probability.",01:06:13.690,01:06:16.120
"In fact why don't we-- yeah,
let's worry about that later.",01:06:16.120,01:06:21.290
"Let me first tell
you step 2.5 which",01:06:24.150,01:06:27.690
"is I want there to be
zero collisions in each",01:06:27.690,01:06:30.650
of these tables.,01:06:30.650,01:06:31.720
"It's only going to happen
with probability of 1/2",01:06:31.720,01:06:34.170
"So if it doesn't
happen, just try again.",01:06:34.170,01:06:37.900
"So 2.5 is while there's some
hash function h2,j that maps 2",01:06:37.900,01:06:50.160
"keys that we're given to the
same slot at the second level,",01:06:50.160,01:07:02.310
"this is for some j and let's
say ki different from ki prime.",01:07:02.310,01:07:17.290
"But they map to the same place
by the first hash function.",01:07:17.290,01:07:20.310
"So if two keys map to
the same secondary table",01:07:26.350,01:07:29.680
"and there's a
conflict, then I'm just",01:07:29.680,01:07:32.100
going to redo that construction.,01:07:32.100,01:07:36.020
"So I'm going to repick h2,j.",01:07:36.020,01:07:40.420
"h2,j was a random choice.",01:07:40.420,01:07:42.020
"So if I get a bad choice,
I'll just try another one.",01:07:42.020,01:07:47.230
"Just keep randomly
choosing the a",01:07:47.230,01:07:50.045
"or randomly choosing
this hash function",01:07:50.045,01:07:51.910
"until there are zero collisions
in that secondary table.",01:07:51.910,01:07:55.780
"And I'm going to do
this for each table.",01:07:55.780,01:07:57.920
"So we worry about how
long these will take,",01:07:57.920,01:08:00.600
"but I claim expected
constant number of trials.",01:08:00.600,01:08:02.745
"So let's do the
second one first.",01:08:05.560,01:08:07.250
"After we do this y loop
there are no collisions",01:08:13.040,01:08:16.870
"with the proper notion of
the word collisions, which",01:08:16.870,01:08:19.050
"is two different keys
mapping to the same value.",01:08:19.050,01:08:21.750
"So at this point
we have guaranteed",01:08:35.970,01:08:41.470
"that searches are
constant time worst",01:08:41.470,01:08:43.220
"case after we do all these
4 steps because we apply h1,",01:08:43.220,01:08:48.740
"we figure out which
slot we fit in.",01:08:48.740,01:08:51.029
"Say it's slot j,
then we apply h2j",01:08:51.029,01:08:53.930
"and if your item's
in the overall table,",01:08:53.930,01:08:56.689
"it should be in that
secondary table.",01:08:56.689,01:08:58.410
"Because there are no
collisions you can see,",01:08:58.410,01:09:00.243
"is that one item the
one I'm looking for?",01:09:00.243,01:09:02.130
"If so, return it.",01:09:02.130,01:09:02.920
"If not, it's not anywhere.",01:09:02.920,01:09:04.699
"If there are no
collisions then I",01:09:04.699,01:09:07.829
"don't need chains coming out
of here because it is just",01:09:07.829,01:09:10.120
a single item.,01:09:10.120,01:09:10.800
"The big question-- so
constant worst case space",01:09:13.750,01:09:16.760
because 1.5 guarantees that.,01:09:16.760,01:09:19.130
"Constant worst case
time first search.",01:09:19.130,01:09:20.964
"The big question is, how
long does it take to build?",01:09:20.964,01:09:23.130
"How many times do
we have to redo",01:09:23.130,01:09:25.330
"steps 1 and 2 before we
get a decent-- before we",01:09:25.330,01:09:28.890
get a perfect hash table.,01:09:28.890,01:09:30.130
"So let me remind
you of the birthday",01:09:32.979,01:09:35.880
"paradox, why it works here.",01:09:35.880,01:09:37.750
"As mentioned earlier this is
going to be a union bounds.",01:09:54.530,01:09:59.847
"We want to know the
probability of collision",01:09:59.847,01:10:01.680
at that second level.,01:10:01.680,01:10:02.930
"Well that's at most the sum
of all possible collisions,",01:10:02.930,01:10:06.754
probabilities of collisions.,01:10:06.754,01:10:07.920
"So I'm going to say
the sum over all i",01:10:07.920,01:10:09.910
"not equal to ij of
the probability.",01:10:09.910,01:10:14.340
"Now this is over our choice
of the hash function h2,j.",01:10:14.340,01:10:16.800
"Of h2,j of ki equaling
h2,j of ki prime.",01:10:19.848,01:10:29.120
"So union bounds says, of course.",01:10:29.120,01:10:30.970
"The probability of any
of them happening--",01:10:30.970,01:10:33.080
"we don't know about
interdependence or whatnot--",01:10:33.080,01:10:35.380
"but certainly almost the sum of
each of these possible events.",01:10:35.380,01:10:39.730
"There are a lot of
possible events.",01:10:39.730,01:10:42.150
"If there 's li
things, that there",01:10:42.150,01:10:43.620
"are going to be li choose
2 possible collisions",01:10:43.620,01:10:47.462
we have to worry about.,01:10:47.462,01:10:48.420
"We know i is not
equal to i prime.",01:10:48.420,01:10:49.836
"So the number of terms
here is li choose 2.",01:10:53.360,01:10:57.710
And what's this probability?,01:11:00.890,01:11:02.115
STUDENT: [INAUDIBLE],01:11:06.120,01:11:07.990
"ERIK DEMAINE: 1/li at most
because we're assuming h2,j is",01:11:07.990,01:11:14.420
"a universal hash function so
the probability of choosing--",01:11:14.420,01:11:17.880
sorry?,01:11:17.880,01:11:18.780
li squared.,01:11:18.780,01:11:19.530
Thank you.,01:11:19.530,01:11:20.640
The size of the table.,01:11:20.640,01:11:23.020
"1/m but m in this case, the
size of our table is li squared.",01:11:23.020,01:11:27.230
"So the probability that we
choose a good hash function",01:11:27.230,01:11:30.455
"and that these
particular keys don't hit",01:11:30.455,01:11:32.790
is at most 1/li squared.,01:11:32.790,01:11:34.740
This is basically li squared/ 2.,01:11:34.740,01:11:37.740
And so this is at most 1/2.,01:11:37.740,01:11:40.690
"It's a slightly less
than li squared/2.",01:11:40.690,01:11:43.030
So this is at most 1/2.,01:11:43.030,01:11:45.274
"And this is basically
a birthday paradox",01:11:45.274,01:11:46.940
in this particular case.,01:11:46.940,01:11:48.375
"That means there
is a probability",01:11:48.375,01:11:49.750
"of at least a half that there
is zero collisions in one",01:11:49.750,01:11:53.610
of these tables.,01:11:53.610,01:11:54.620
"So that means I'm basically
flipping a fair coin.",01:11:54.620,01:11:57.160
If I ever get a heads I'm happy.,01:11:57.160,01:11:58.922
"Each time I get a
tails I have to reflip.",01:11:58.922,01:12:00.630
"This should sound
familiar from last time.",01:12:00.630,01:12:03.060
"So this is 2 expected trials
or log n with high probability.",01:12:03.060,01:12:14.045
"We've proved log n
with high probability.",01:12:20.710,01:12:23.600
"That's the same as saying the
number of levels in a skip list",01:12:23.600,01:12:26.360
is log n with high probability.,01:12:26.360,01:12:28.330
"How many times do I have to flip
a coin before I get a heads?",01:12:28.330,01:12:30.959
Definitely at most log n.,01:12:30.959,01:12:32.000
"Now we have to do this
for each secondary table.",01:12:35.620,01:12:38.530
"There are m equal theta
and secondary tables.",01:12:38.530,01:12:41.700
"There's a slight question of how
big are the secondary tables.",01:12:50.110,01:12:53.490
"If one of these tables
is like linear size,",01:12:53.490,01:12:56.770
"then I have to spend
linear time for a trial.",01:12:56.770,01:12:59.600
"And then I multiply that
by the number of trials",01:12:59.600,01:13:02.450
"and also the number of different
things that would be like n",01:13:02.450,01:13:05.050
squared log n n.,01:13:05.050,01:13:06.670
"But you know a secondary table
better not have linear sides.",01:13:06.670,01:13:11.460
"I mean a linear
number of li equal n.",01:13:11.460,01:13:14.450
"That would be bad because
then li squared is n squared",01:13:14.450,01:13:16.850
"and we guaranteed that
we had linear space.",01:13:16.850,01:13:20.540
"So in fact you can prove
with another Chernoff bound.",01:13:20.540,01:13:25.790
Let me put this over here.,01:13:25.790,01:13:27.450
"That all the li's
are pretty small.",01:13:34.330,01:13:36.940
Not constant but logarithmic.,01:13:36.940,01:13:42.730
"So li is order log n with
high probability for each i",01:13:42.730,01:13:50.400
and therefore for all i.,01:13:50.400,01:13:51.850
"So I can just change the alpha
my minus 1 n to the alpha",01:13:51.850,01:13:56.550
"and get that for
all i this happens.",01:13:56.550,01:14:00.654
"In fact, the right answer
is log over log log,",01:14:00.654,01:14:02.570
"if you want to do some
really messy analysis.",01:14:02.570,01:14:04.620
"But we just, logarithmic
is fine for us.",01:14:04.620,01:14:08.430
"So what this means
is we're doing",01:14:08.430,01:14:10.920
"n different things
for each of them",01:14:10.920,01:14:14.010
"with high probability
li is of size log n.",01:14:14.010,01:14:17.960
"And then maybe we'll have
to do like log n trials",01:14:17.960,01:14:20.470
"repeating until we get a
good hash function there.",01:14:20.470,01:14:23.200
"And so the total build
time for steps 1 and 2.5",01:14:23.200,01:14:29.320
"is going to be at most
n times log squared n.",01:14:29.320,01:14:34.240
"You can prove a tighter
bound but it's polynomial.",01:14:34.240,01:14:37.420
"That's all I wanted to go
for and it's almost linear.",01:14:37.420,01:14:41.200
"So I'm left with one thing
to analyze which is step 1.5.",01:14:41.200,01:14:46.855
"This to me is maybe the
most surprising thing",01:14:46.855,01:14:48.730
that it works out.,01:14:48.730,01:14:50.120
"I mean here we designed--
we did this li to li",01:14:50.120,01:14:53.490
"squared so the birthday
paradox would happen.",01:14:53.490,01:14:55.480
This is not surprising.,01:14:55.480,01:14:56.854
"I mean it's a cool idea,
but once you have the idea,",01:14:56.854,01:14:59.020
"it's not surprising
that it works.",01:14:59.020,01:15:01.370
"What's a little more
surprising is that squaring",01:15:01.370,01:15:03.370
is OK from a space perspective.,01:15:03.370,01:15:05.670
"1.5 says we're going
to have to rebuild",01:15:05.670,01:15:07.310
"that first table until the
sum of these squared lengths",01:15:07.310,01:15:10.380
is at most linear.,01:15:10.380,01:15:11.470
"I can guarantee
that each of these",01:15:11.470,01:15:13.540
"is logarithmic so the sum of the
squares is at most like n log",01:15:13.540,01:15:16.840
squared n.,01:15:16.840,01:15:17.950
But I claim I can get linear.,01:15:17.950,01:15:19.315
Let's do that.,01:15:22.360,01:15:25.580
"So for step 1.5
we're looking at what",01:15:25.580,01:15:29.880
"is the expectation of the
sum of the lj squareds being",01:15:29.880,01:15:35.410
more than linear.,01:15:35.410,01:15:37.640
Sorry.,01:15:37.640,01:15:38.750
Expectation.,01:15:38.750,01:15:39.852
"Let's first compute
the expectation",01:15:39.852,01:15:41.310
"and then we'll talk
about a tail bound",01:15:41.310,01:15:43.710
"which is the probability
that we're much",01:15:43.710,01:15:45.420
bigger than the expectation.,01:15:45.420,01:15:46.980
"First thing is I claim
the expectation is linear.",01:15:46.980,01:15:50.280
"So again whenever we're
counting something--",01:15:50.280,01:15:56.000
"I mean this is basically
the total number of pairs",01:15:56.000,01:15:59.370
"of items that collide
at the first level",01:15:59.370,01:16:02.580
with double counting.,01:16:02.580,01:16:05.060
"So I mean if you think of lj
and then I make a complete graph",01:16:05.060,01:16:08.940
"on those lj items,
that's going to have",01:16:08.940,01:16:11.910
"like the squared
number of edges, so,",01:16:11.910,01:16:14.400
if I also multiply by 2.,01:16:14.400,01:16:16.700
"So this is the same
thing as counting",01:16:16.700,01:16:19.060
"how many pairs of items map to
the same spot, the same slot.",01:16:19.060,01:16:25.180
"So this is going to-- and that
I can write as an indicator",01:16:25.180,01:16:28.890
"random variable which
lets me use linearity",01:16:28.890,01:16:30.820
"of expectation
which makes me happy",01:16:30.820,01:16:33.830
because then everything simple.,01:16:33.830,01:16:36.940
"So I'm going to write Ii,j.",01:16:36.940,01:16:38.090
"This is going to be 1 if each 1
of ki, I guess, equals h1 if kj",01:16:41.210,01:16:54.070
"and it's going to be
0 if h1 otherwise.",01:16:54.070,01:16:59.055
"This is the total number
of pairwise colliding",01:17:05.080,01:17:07.080
items including i versus i.,01:17:07.080,01:17:10.520
"And so like if li equals
1, li squared is also 1.",01:17:10.520,01:17:14.210
"There's 1 item
colliding with itself.",01:17:14.210,01:17:15.820
So this actually works exactly.,01:17:15.820,01:17:19.490
"All right, with the wrong
definition of colliding.",01:17:19.490,01:17:21.840
If you bear with me.,01:17:21.840,01:17:24.140
"So now we can use
linear of expectation",01:17:24.140,01:17:26.735
and put the E in here.,01:17:26.735,01:17:28.890
"So this is sum i equals 1
to n sum j equals 1 to n",01:17:28.890,01:17:35.400
"of the expectation of Ii,j.",01:17:35.400,01:17:39.590
"But we know the expectation
of the Ii,j is the probability",01:17:39.590,01:17:42.982
"of it equaling 1 because it's
an indicator random variable.",01:17:42.982,01:17:45.440
"The probability of this
happening over our choice of h1",01:17:45.440,01:17:48.582
is at most 1/m by universality.,01:17:48.582,01:17:51.170
"Here it actually is m because
we're at the first level.",01:17:51.170,01:17:53.580
"So this is at most
1/m which is theta n.",01:17:53.580,01:17:59.520
"So when i does not equal j,
so it's a little bit annoying.",01:18:02.420,01:18:10.930
"I do have to separate out the Ii
terms from the i and different",01:18:10.930,01:18:16.570
i not equal to j terms.,01:18:16.570,01:18:17.994
"But there's only-- I
mean it's basically",01:18:17.994,01:18:19.660
the diagonal of this matrix.,01:18:19.660,01:18:21.170
"There's n things that will
always collide with themselves.",01:18:21.170,01:18:24.510
"So we're going to get like
n plus the number of i",01:18:24.510,01:18:30.690
"not equal to pairs
double counted.",01:18:30.690,01:18:32.430
So it's like 2 times n choose 2.,01:18:32.430,01:18:35.520
But we get to divide by m.,01:18:35.520,01:18:38.180
So this is like n squared /n.,01:18:38.180,01:18:40.930
So we get order n.,01:18:40.930,01:18:46.070
"So that's not--
well, that's cool.",01:18:46.070,01:18:49.130
Expected space is linear.,01:18:49.130,01:18:51.073
"This is what makes
everything work.",01:18:51.073,01:18:52.531
"Last class was about getting
with high probability bounds",01:18:59.410,01:19:01.840
when we're working with logs.,01:19:01.840,01:19:03.070
"When you want to
get that something",01:19:05.610,01:19:07.570
"is log with high
probability, you",01:19:07.570,01:19:09.280
"have to use, with
respect to n, you",01:19:09.280,01:19:11.690
have to use a turn off bound.,01:19:11.690,01:19:13.670
"But this is about-- now I
want to show that the space is",01:19:13.670,01:19:17.290
linear with high probability.,01:19:17.290,01:19:19.090
Linear is actually really easy.,01:19:19.090,01:19:20.630
"You can use a much weaker
bound called Markov inequality.",01:19:20.630,01:19:24.560
"So I want to claim that the
probability of h1 of this thing",01:19:24.560,01:19:36.200
"lj squareds being bigger
than some constant times",01:19:36.200,01:19:39.980
"n is at most the expectation
of that thing divided by cn.",01:19:39.980,01:19:49.624
This is Markov's inequality.,01:19:49.624,01:19:50.790
It holds for anything here.,01:19:50.790,01:19:52.710
"So I'm just repeating
it over here.",01:19:52.710,01:19:54.320
"So this is nice because we
know that this expectation is",01:19:58.550,01:20:05.170
linear.,01:20:05.170,01:20:06.970
"So we're getting like a
linear function divided by cn.",01:20:06.970,01:20:12.000
Remember we get to choose c.,01:20:12.000,01:20:14.100
"The step said if it's bigger
than some constant times n",01:20:14.100,01:20:16.500
then we're redoing the thing.,01:20:16.500,01:20:18.050
"So I can choose c
to be 100, whatever.",01:20:18.050,01:20:20.270
"I'm going to choose it to
be twice this constant.",01:20:20.270,01:20:23.870
And then this is at most half.,01:20:23.870,01:20:27.460
"So the probability of
my space being too big",01:20:27.460,01:20:29.600
is at most a half.,01:20:29.600,01:20:30.670
We're back to coin flipping.,01:20:30.670,01:20:31.980
"Every time I flip
a coin, if I get",01:20:31.980,01:20:33.590
"heads I have the right amount
of space at less than c times n",01:20:33.590,01:20:40.550
space.,01:20:40.550,01:20:42.100
If I get a tails I try again.,01:20:42.100,01:20:43.650
"So the expected number
of trials is 2 at most",01:20:43.650,01:20:50.000
"not trails, trials.",01:20:50.000,01:20:53.710
"And it's also log n trials
with high probability.",01:20:53.710,01:20:57.605
"How much time do I
spend for each trial?",01:21:01.510,01:21:03.480
Linear time.,01:21:03.480,01:21:04.186
I choose one hash function.,01:21:04.186,01:21:05.310
I hash all the items.,01:21:05.310,01:21:06.850
"I count the number of collision
squared or the sum of lj",01:21:06.850,01:21:10.120
squared.,01:21:10.120,01:21:10.620
That takes linear time to do.,01:21:10.620,01:21:12.420
"And so the total work I'm doing
for these steps is n log n.",01:21:12.420,01:21:16.150
"So n log n to do
step 1 and 1 prime",01:21:20.920,01:21:23.710
"and log squared n to
do steps 2 and 2 prime.",01:21:23.710,01:21:27.000
"Overall n Polylog
or polynomial time.",01:21:27.000,01:21:30.940
"And we get guaranteed no
collisions for static data.",01:21:30.940,01:21:35.190
"Constant worst case search
and linear worst case space.",01:21:35.190,01:21:39.714
"This is kind of surprising
that this works out",01:21:39.714,01:21:41.630
but everything's nice.,01:21:41.630,01:21:43.049
Now you know hashing.,01:21:47.780,01:21:49.910

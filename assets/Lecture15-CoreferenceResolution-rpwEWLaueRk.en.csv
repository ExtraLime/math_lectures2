text,start,stop
Stanford University.,00:00:00.005,00:00:07.761
"&gt;&gt; So we are now heading to
the crucial end phase of the semester,",00:00:07.761,00:00:13.644
"so I guess we're now at
the start of week nine.",00:00:13.644,00:00:18.410
"So first of all if I just
do sort of reminders.",00:00:18.410,00:00:22.222
"Obviously everyone should keep working
on their final projects- assignment 4s.",00:00:22.222,00:00:28.310
A couple of just notes on that.,00:00:28.310,00:00:31.070
"So on Thursday we're just going to
talk about dynamic memory networks.",00:00:31.070,00:00:35.996
"While they are only one of several ways
that you could go about approaching",00:00:35.996,00:00:39.238
assignment 4.,00:00:39.238,00:00:39.884
"They'll certainly be relevant material
if you are doing assignment 4 because,",00:00:39.884,00:00:45.070
"it's an instance of the kind of
architectures of sort of attention based",00:00:45.070,00:00:49.780
"architectures that people can use for
tasks like the reading comprehension",00:00:49.780,00:00:55.128
"question/answering, like the squad
data sets, so watch out for that.",00:00:55.128,00:01:00.628
"We've been trying to keep money
in people's Azure accounts,",00:01:00.628,00:01:04.592
"actually a Microsoft rep Kristine is here
right now if you need to pester her.",00:01:04.592,00:01:09.545
"[LAUGH] Pester her about any problems, and",00:01:09.545,00:01:12.438
"you're certainly contact us on
Piazza if they're any issues, and",00:01:12.438,00:01:17.150
"we've been trying to be proactive
at keeping things restocked.",00:01:17.150,00:01:21.710
"I know it's slightly frustrating if you go
out of money and it then locks you out and",00:01:21.710,00:01:26.609
"we need to reset it, but
we're doing our best.",00:01:26.609,00:01:29.450
"Okay, and it's great that there are now
lots of people that are clearly very",00:01:29.450,00:01:33.236
actively using it and doing stuff.,00:01:33.236,00:01:35.250
"And that's super, we are very
pleased to see that all happening.",00:01:35.250,00:01:39.312
"Okay, then for assignment four, so for",00:01:39.312,00:01:42.295
"the assignment four submissions,
we're doing submissions on CodaLab,",00:01:42.295,00:01:47.828
"which is conveniently tied
right into Azure as well.",00:01:47.828,00:01:52.090
"And so for assignment four
we've set up a leaderboard,",00:01:52.090,00:01:56.200
"at least when I looked this morning it
only had one submission from Chris, and",00:01:56.200,00:02:01.140
the people who set up the leaderboard.,00:02:01.140,00:02:03.100
"And they were only getting 2% on squad,
so if you can get higher than 2%",00:02:03.100,00:02:08.070
"on squad at least temporarily you
could be top of the leaderboard.",00:02:08.070,00:02:12.935
"[LAUGH] And so
I hope people can try that out.",00:02:12.935,00:02:16.644
"And so, a couple of
Percy Liang's RAs have been very",00:02:16.644,00:02:21.071
"actively working at helping
us out of doing this",00:02:21.071,00:02:26.157
"CodaLab-Azure integration for
using an assignment four.",00:02:26.157,00:02:32.091
So big thank you to Percy and his RAs.,00:02:32.091,00:02:36.850
"And so,
they've also made a couple of videos on",00:02:36.850,00:02:40.320
"how to use CodaLab that
there's short videos.",00:02:40.320,00:02:43.220
"There was an announcement about it on
Piazza, so have a look about those.",00:02:43.220,00:02:46.620
"Okay so, then moving right along for
today's lecture, so",00:02:47.650,00:02:52.130
"for today's lecture,
I'm gonna talk about coreference.",00:02:52.130,00:02:54.800
"So when we did the mid quarter survey,
one of",00:02:55.880,00:03:00.890
"the things that a whole bunch of people
complained about was that, we actually",00:03:00.890,00:03:05.740
"weren't doing much linguistics and
natural language content in this class.",00:03:05.740,00:03:11.220
"So today, it's getting a little bit
late since it's the start of week nine.",00:03:11.220,00:03:15.100
"I've actually got a try to have
some more linguistic content",00:03:15.100,00:03:20.120
"in the first half before going back
to deep learning models for the same.",00:03:20.120,00:03:25.320
"I think that sort of comment
in the mid-quarter evaluations",00:03:25.320,00:03:30.340
"was completely fair because the reality
was in the first half of the class.",00:03:30.340,00:03:34.800
"It really was sort of just about all
deep learning models all the time.",00:03:34.800,00:03:38.740
"I mean, I'm not sure I've yet worked out
the perfect solution to that because",00:03:38.740,00:03:42.640
"the fact of the matter was we kind
of felt when organizing the class.",00:03:42.640,00:03:46.140
"That we were sort of on this treadmill
where we had to get through more stuff in",00:03:46.140,00:03:49.840
time for our next assignment.,00:03:49.840,00:03:52.630
"And so that is what it is, but over
the last couple of weeks, we'll try and",00:03:52.630,00:03:56.600
"have a bit more NLP
content as we go along,",00:03:56.600,00:03:59.840
"Coreference Resolution is
an instance of a task.",00:04:03.400,00:04:08.580
"And it's really the only one that we're
going to look at in any depth here.",00:04:08.580,00:04:12.910
"Where we're working on
a larger level of a text so",00:04:12.910,00:04:16.040
"that we're not longer just trying to
look at an individual sentence and say,",00:04:16.040,00:04:20.530
"what's the subject and the object and
parsing or is this a company name.",00:04:20.530,00:04:25.780
"Where we're trying to make
sense of a bigger text and",00:04:25.780,00:04:29.120
work out what's going on about that.,00:04:29.120,00:04:31.250
"And it's not the entirety of
understanding long text but",00:04:31.250,00:04:34.610
"it's sort of one of the most prominent
things you need to do as you go along.",00:04:34.610,00:04:38.920
"So when we're doing coreference
resolution, the first thing that we're",00:04:38.920,00:04:43.200
"doing is working out all the mentions in
the piece of text so that pieces of text,",00:04:43.200,00:04:49.450
"basically noun phrases that refer
to some entity in the world.",00:04:49.450,00:04:56.140
"And then once we've got those,
we're trying to find",00:04:56.140,00:04:59.080
"the ones that refer to the same
real world entity that co-refers.",00:04:59.080,00:05:03.790
"So there's one set them here
which co-refer in this example",00:05:03.790,00:05:07.980
"to Barack Obama and
then there's another set,",00:05:07.980,00:05:10.790
"which are these ones here, and they're
the ones that co-refer to Hillary Clinton.",00:05:10.790,00:05:15.920
"Okay so
that's our task of coreference resolution,",00:05:17.430,00:05:22.710
"and so I thought next we could just go
through, really get a sense of what",00:05:22.710,00:05:27.620
"goes on in coreference resolution,
of go through an example text.",00:05:27.620,00:05:32.360
"And this is where I signal to these guys
here to flip me over to my other screen.",00:05:32.360,00:05:38.050
Look at that!,00:05:39.060,00:05:40.200
"Okay, so here's our example text done,",00:05:40.200,00:05:42.610
"this is from a short story,
story by Shruthi Rao called, The Star.",00:05:42.610,00:05:46.585
"Now, I have to admit,
since this isn't a literature class,",00:05:46.585,00:05:51.135
"I actually made some little cuts and
edits to this story so",00:05:51.135,00:05:54.275
"I could more easily fit it in
a larger font size on my slide.",00:05:54.275,00:05:59.057
"So the text is slightly mangled but
it's basically part of the story,",00:05:59.057,00:06:04.987
"so what have we got here when we do this,
right?",00:06:04.987,00:06:07.677
"So first of all,
we have the named entities,",00:06:07.677,00:06:12.458
"which is precisely what you were
finding in assignment three, right?",00:06:12.458,00:06:17.028
"So we have Vanaja, and Akhila, and",00:06:17.028,00:06:21.126
"there's Akhila, and
there's Prajwal, Akash,",00:06:21.126,00:06:27.000
"Lord Krishna, He is a named entity, Akash.",00:06:27.000,00:06:32.199
"So we've got all of those, but",00:06:32.199,00:06:35.075
"then we have a lot of other kinds
of phrases that refer in the text.",00:06:35.075,00:06:41.169
"And the second prominent category
is that we have pronouns.",00:06:41.169,00:06:46.150
So there's they and there's she.,00:06:46.150,00:06:51.018
"There's this one thats a pronoun,
that's kind of a special pronoun, herself.",00:06:51.018,00:06:57.867
And there's she and him.,00:06:57.867,00:07:00.030
"And she, she, it, it.",00:07:00.030,00:07:03.083
"then I admit, I noticed I missed at",00:07:03.083,00:07:07.740
"least one of the named entities,",00:07:07.740,00:07:12.238
there's another named entity.,00:07:12.238,00:07:16.750
"And there are probably other
things I've missed, so",00:07:16.750,00:07:19.220
you can tell me what I've missed.,00:07:19.220,00:07:20.830
"Okay, so
those are both prominent categories, but",00:07:20.830,00:07:23.720
"that's not all there is,
there's really a third category.",00:07:23.720,00:07:28.070
"Which are then things
that are mentions but",00:07:28.070,00:07:32.680
"are done with common nouns so that they're
neither pronouns or named entities.",00:07:32.680,00:07:37.700
"So that's something like the local park,",00:07:37.700,00:07:42.022
"well, there's her son is such an example,",00:07:42.022,00:07:46.464
"the same school, the preschool play.",00:07:46.464,00:07:50.930
"So there's sort of, you get an interesting
thing is you get embedded ones.",00:07:50.930,00:07:55.300
"So the preschool play is in reference
of a mention of an entity, but",00:07:55.300,00:07:59.970
"inside that there's the preschool,
which is another mention of an entity.",00:07:59.970,00:08:05.030
"Okay so there is the naughty child,",00:08:08.758,00:08:13.545
"a tree, the best tree, a brown T-shirt,",00:08:13.545,00:08:19.286
"brown trousers, the tree trunk,",00:08:19.286,00:08:23.912
a large cardboard cut out.,00:08:23.912,00:08:27.593
"Okay, then there is a circular opening,
there are red balls.",00:08:30.987,00:08:35.347
So they are those ones and,00:08:39.395,00:08:41.084
"then there are some other things that are
common noun phrases and it's not quite so",00:08:41.084,00:08:46.635
"clear whether they're actually
mentions of anything in the world.",00:08:46.635,00:08:51.321
So there's a couple of years.,00:08:51.321,00:08:54.440
"Is that a mention of
anything in the world?",00:08:54.440,00:08:56.271
Not quite so clear.,00:08:56.271,00:08:57.280
"And then there's this,
a tree's foliage which",00:08:57.280,00:09:02.235
"doesn't really seem like it's referring
to anything concrete in the world.",00:09:02.235,00:09:05.975
"So there are various
complicated cases like that.",00:09:05.975,00:09:10.075
"And in particular, there's another one of
those more complicated cases at the end,",00:09:10.075,00:09:14.700
"which I'll maybe come back to later,
this, the nicest tree.",00:09:14.700,00:09:18.650
"Okay, but somehow we work out
what all our mentions are.",00:09:18.650,00:09:22.450
"And then, the task we want to do is to
start to then work out which ones corefer.",00:09:22.450,00:09:31.542
"For a start, there's then Vanaja here.",00:09:31.542,00:09:38.240
"Then, what's the next thing
that refers to Vanaja?",00:09:38.240,00:09:44.609
"Her, yes.",00:09:55.087,00:09:55.850
"So this is her, which I guess I forgot
to mark when I was marking the pronouns.",00:09:55.850,00:10:00.910
"So embedded in the MP, her son, so again,",00:10:00.910,00:10:04.190
"you can get mentions and
side mentions, there's that her.",00:10:04.190,00:10:08.410
Is there anything else that is coreferent?,00:10:10.130,00:10:15.956
"She, okay, so there's that she there.",00:10:15.956,00:10:18.850
"And then the next one is, herself.",00:10:18.850,00:10:24.180
"Right, and so
here we have these reflexive pronouns.",00:10:24.180,00:10:28.280
"And so reflexive pronouns are kind
of special because they always",00:10:28.280,00:10:33.315
"corefer very closely back to
each other as in that example,",00:10:33.315,00:10:38.128
she resigned herself.,00:10:38.128,00:10:41.157
"Okay, so there's she again.",00:10:41.157,00:10:45.090
"Then she made, she attached.",00:10:45.090,00:10:49.300
So that goes right through.,00:10:50.750,00:10:51.699
"Ok, so then we have.",00:10:51.699,00:10:54.030
"Now we have Akhila, which is Akhila.",00:10:54.030,00:10:59.814
"Sometimes you just get names
being repeated as names.",00:10:59.814,00:11:08.800
"Are there other things
that corefer with Akhila?",00:11:08.800,00:11:11.663
Maybe not.,00:11:24.124,00:11:25.950
"Note that this is kind of part
of how it's tricky, right?",00:11:25.950,00:11:29.970
"Because well here at the beginning,
we have the names of two women.",00:11:29.970,00:11:36.950
"Akhila's name is actually
repeated in the second sentence.",00:11:36.950,00:11:40.330
"But somehow we have to understand
enough of the text beyond that",00:11:40.330,00:11:44.090
"to understand that all these
other references to a she and",00:11:44.090,00:11:48.340
"a her aren't referring to Akhila at all,
they're all referring to Vanaja.",00:11:48.340,00:11:53.740
"So we have these two entity chains, and
then we have some more entity chains.",00:11:53.740,00:12:00.740
"So if we go to the next one, the local
park, that one is just a singleton.",00:12:00.740,00:12:08.630
"Nothing else refers,
I believe, to the local parks.",00:12:08.630,00:12:12.920
"There's something for
a lot of background mentions,",00:12:12.920,00:12:15.190
"things have just been mentioned once and
never repeated.",00:12:15.190,00:12:18.150
"And so then we have Prajwal,
who sort of appears twice here.",00:12:19.260,00:12:25.103
"Like, there's Akhila's son which is sort
of a descriptive term and then his name,",00:12:25.103,00:12:29.765
"Prajwal, so that's generally
referred to as apposition.",00:12:29.765,00:12:33.440
So we get two mentions of him right there.,00:12:33.440,00:12:35.940
And then where else is Prajwal appearing?,00:12:35.940,00:12:39.340
"Well, one's his name is right here.",00:12:39.340,00:12:41.340
"Are there other places
that Prajwal appears?",00:12:41.340,00:12:43.941
"Okay, so here's a complicated one.",00:12:46.584,00:12:49.240
"There is this they which
refers to two people, right?",00:12:49.240,00:12:52.680
That refers to Prajwal and Akash.,00:12:52.680,00:12:55.910
"So that's a phenomenon,
maybe I'll start putting Akash in as well.",00:12:55.910,00:13:01.290
"So we have this phenomenon here of
when you have split antecedents.",00:13:02.790,00:13:08.540
"So you can have a plural they that's
referring back to two things that",00:13:08.540,00:13:13.880
"are disassociated with each other, they're
just discontinuous in different places.",00:13:13.880,00:13:19.660
"When we start looking at co-ref algorithms
that NLP people use a bit later,",00:13:19.660,00:13:25.103
"one of the embarrassing things that
you will notice is that the standard",00:13:25.103,00:13:30.205
"algorithms that we use just can't
handle this kind of split antecedents.",00:13:30.205,00:13:35.075
"That you're looking at a mention and
you're trying to decide what to make it",00:13:35.075,00:13:38.851
"coreferent to, and
you just can't get ones like that right.",00:13:38.851,00:13:42.170
So that's a bit of embarrassing but,00:13:42.170,00:13:43.770
"that's the current state of
natural language processing.",00:13:43.770,00:13:47.120
What else is coreferent to Prajwal?,00:13:47.120,00:13:49.564
"Okay, we'll go on.",00:13:58.632,00:14:01.170
So Akash appears a lot.,00:14:01.170,00:14:02.916
"We have Akash, Akash,",00:14:02.916,00:14:09.519
"him, Akash.",00:14:09.519,00:14:17.468
"Okay so, he goes through all the tree.",00:14:19.704,00:14:23.870
"So what other entities are there
that occur multiple times here?",00:14:23.870,00:14:28.537
"The tree, okay.",00:14:34.669,00:14:36.265
"If you think about it,
there's definitely here a tree, but",00:14:39.476,00:14:44.704
"if you think about it which of these
things count as mentions in the real",00:14:44.704,00:14:50.753
"world, and which one do you want to deem
this coreferent, is actually tricky.",00:14:50.753,00:14:57.650
"If you just half look at it,
you just think, okay,",00:14:57.650,00:15:02.490
"anytime I see the word tree in a noun
phrase, I'm just gonna say all of those",00:15:02.490,00:15:07.485
"coreferent with each other, but
that doesn't actually seem to be right.",00:15:07.485,00:15:12.554
"Cuz when it was here,
Akash was to be a tree,",00:15:12.554,00:15:15.592
"that's not talking about any specific
tree that's referred in the world,",00:15:15.592,00:15:20.853
that's some intentional description.,00:15:20.853,00:15:23.750
"She resigned herself to make Akash
the best tree that anybody had ever seen.",00:15:24.900,00:15:30.080
"Again, that doesn't seem to be
referring to any particular tree that's",00:15:30.080,00:15:33.850
"extant at any time,
that's some kind of descriptive text.",00:15:33.850,00:15:38.800
"On the other hand, when it's gone down
to she bought him a brown T-shirt and",00:15:38.800,00:15:42.930
"brown trousers to represent the tree,",00:15:42.930,00:15:46.910
"this is now,
it's an abstract tree costume obviously.",00:15:46.910,00:15:50.940
"But that actually seems to be a real tree
that's a thing in the will that you can",00:15:50.940,00:15:54.990
point at.,00:15:54.990,00:15:56.470
"So, that's a real thing and then, well
what about when it says a tree's foliage?",00:15:56.470,00:16:02.390
"Is that referring to that
tree that she's building?",00:16:02.390,00:16:05.940
Kind of a little bit unclear.,00:16:05.940,00:16:08.290
"But by the time it says,
she attached red balls to it.",00:16:09.290,00:16:13.600
"That's clearly a reference to
the tree that she's making and",00:16:15.380,00:16:20.179
it's a good clear one.,00:16:20.179,00:16:22.400
"But then the last sentence says,
it truly was the nicest tree.",00:16:22.400,00:16:27.781
"So for that one, the it is clearly again,",00:16:27.781,00:16:31.813
"referring to the tree
that she's constructed.",00:16:31.813,00:16:36.840
So that one is clear.,00:16:36.840,00:16:38.235
"The question is what to
do about the nicest tree.",00:16:38.235,00:16:42.270
"And, to be honest, if like for
various other NLP tasks for",00:16:42.270,00:16:47.081
"co-reference resolution,
people have constructed data",00:16:47.081,00:16:52.140
"sets where people have essentially done
what I'm trying to do live in front of you",00:16:52.140,00:16:57.040
"as to identify mentions and
then say which one's a co-reference.",00:16:57.040,00:17:01.800
"And so what we have here for something
like it truly was the nicest tree.",00:17:01.800,00:17:07.230
"The nicest tree is referred
to as a predicate nominal.",00:17:07.230,00:17:10.320
"So it's something in the form
of a noun phrase, but",00:17:10.320,00:17:13.244
"it's actually a property that is
being predicated of the subject.",00:17:13.244,00:17:17.500
It is the nicest tree.,00:17:17.500,00:17:19.920
"And so
some of the data sets that people use for",00:17:19.920,00:17:24.125
"co-reference, they declare
predicate nominals like this",00:17:24.125,00:17:28.678
"to be co-referent to the subject and
therefore it would be purple.",00:17:28.678,00:17:33.500
"But there's kind of an argument
that that's actually just wrong and",00:17:33.500,00:17:36.999
"the predicate nominal is actually kind of
a descriptive property of the nicest tree",00:17:36.999,00:17:41.223
"and it's not actually
referent to anything.",00:17:41.223,00:17:43.600
"And so then it wouldn't be
what you're wanting to do.",00:17:43.600,00:17:48.440
"But I'll leave my purple there for
the moment, okay.",00:17:50.920,00:17:54.912
Are there any other interesting,00:17:54.912,00:18:00.032
things I should comment on?,00:18:00.032,00:18:04.588
"There's obviously more things
that we haven't done yet.",00:18:04.588,00:18:08.863
I could choose a different color.,00:18:08.863,00:18:13.100
"So there's,",00:18:13.100,00:18:14.170
"Yeah, so I mean, there are obviously lot's
of other things that are mentioned that",00:18:22.557,00:18:27.474
"aren't in change, right?",00:18:27.474,00:18:29.149
"So something like Akash's face is
a mention that's a singular mention.",00:18:29.149,00:18:33.506
"There's a circular opening I guess
that's kind of an interesting one cuz",00:18:33.506,00:18:37.759
"it seems like that is
an entity in the real world.",00:18:37.759,00:18:40.408
"But it's an entity that's a hole in
the world as opposed to a thing that's in",00:18:40.408,00:18:44.604
the world.,00:18:44.604,00:18:45.330
"So there are lots of real world
complications as to how things pan out in",00:18:45.330,00:18:50.287
"co-reference, but that's sort of
an idea of that task and the problems.",00:18:50.287,00:18:55.595
Does that make sense?,00:18:55.595,00:18:56.820
"Okay, I will go on from there by
sending back my person, awesome.",00:18:58.250,00:19:06.067
"Okay, right, so
what we've seen from that is basically",00:19:06.067,00:19:11.108
"what we're working with
is the noun phrases.",00:19:11.108,00:19:15.213
"Most of them refer to
entities in the world.",00:19:15.213,00:19:20.260
"There are many of them that in pairs
refer to the same entity of the world and",00:19:20.260,00:19:24.786
"they're the ones we're
gonna call co-referent.",00:19:24.786,00:19:28.014
"And then the other interesting
thing that's different",00:19:28.014,00:19:32.245
"to what we tried to do with
named identity recognition is",00:19:32.245,00:19:36.386
"that there are lots of cases in
which you get nesting, right?",00:19:36.386,00:19:41.080
"So when you have CFO of Prime Corp,
that Prime Corp is a mention,",00:19:41.080,00:19:46.230
but CFO of Prime Corp is also a mention.,00:19:46.230,00:19:50.380
"His pay is a mention, but
the his inside is also a mention.",00:19:50.380,00:19:55.000
And we've got another one there.,00:19:55.000,00:19:55.940
So you got lots of these nested examples.,00:19:55.940,00:19:58.510
"I mean, in truth you'd get
the same thing happening also in",00:19:58.510,00:20:03.083
named identity recognition.,00:20:03.083,00:20:05.980
"And some people, including a former
student of mine, Jennie Finkle,",00:20:05.980,00:20:08.869
actually looked at that.,00:20:08.869,00:20:09.939
"Because there are a whole bunch of cases
that also in sort of names of things,",00:20:09.939,00:20:15.032
"when you have something like
Palo Alto Utility Company,",00:20:15.032,00:20:18.853
"that you have the organization,
which is Palo Alto Utility Company.",00:20:18.853,00:20:23.520
"And inside that you have a location,
that's Palo Alto.",00:20:23.520,00:20:26.742
"Though in general, in NER,
people just do it flat, and",00:20:26.742,00:20:30.243
you kind of lose those embedded locations.,00:20:30.243,00:20:33.200
"But if you're wanting to follow along
co-reference links like John Smith, and",00:20:33.200,00:20:37.580
"his pay, that you're sort of really
losing a lot in your ability to",00:20:37.580,00:20:41.960
"interpret texts if you aren't dealing
with those embedded mentions.",00:20:41.960,00:20:46.110
"And so normally, people do.",00:20:46.110,00:20:47.250
"So co-reference resolution
is a really key task.",00:20:48.920,00:20:52.580
It's used in all sorts of places.,00:20:52.580,00:20:55.898
"Essentially, anywhere where you want to
do a fuller job of text understanding,",00:20:55.898,00:21:00.130
you need to have co-reference.,00:21:00.130,00:21:02.560
"So if you want to sort of understand
a story, like the story of the star,",00:21:02.560,00:21:06.520
"where you definitely need to be able
to follow along the co-reference.",00:21:06.520,00:21:09.520
It helps in lots of other tasks.,00:21:09.520,00:21:11.180
"So if you wanna do machine translation,
if you're doing machine translation from",00:21:11.180,00:21:16.626
"one of the many languages like Turkish
that don't distinguish gender in pronouns.",00:21:16.626,00:21:22.170
"And you want to then translate into say,
English and it does have gender.",00:21:22.170,00:21:26.690
"Then what you need to do is follow along
the co-reference chains to be work",00:21:27.760,00:21:32.390
"out which ones should be he and
which ones should be she.",00:21:32.390,00:21:35.910
"It's been observed and
complained about by a number of people",00:21:35.910,00:21:40.369
"recently that current MT
systems don't do that.",00:21:40.369,00:21:44.300
"If you take Turkish and you translate into
English, everything comes out male, sorry.",00:21:44.300,00:21:48.589
That's a state of NLP on that.,00:21:48.589,00:21:51.210
"So text summarization,
including things like web snippets right,",00:21:52.380,00:21:57.370
"if you're trying to cut out sort of
a little snippet to put on web results.",00:21:57.370,00:22:01.580
"And it contains a pronoun in it, it would
be much cleverer if you could replace",00:22:01.580,00:22:04.742
"it with its reference so
its interpretable.",00:22:04.742,00:22:06.605
"And then also tasks like information
extraction, relation extraction,",00:22:06.605,00:22:11.550
question answering.,00:22:11.550,00:22:14.020
"This doesn't apply to the squad
task the way it's formulated,",00:22:14.020,00:22:19.560
"but a lot of the time we have questions
like, who married Claudia Ross in 1971?",00:22:19.560,00:22:25.010
"And you start searching the text for
the answer to that question.",00:22:25.010,00:22:29.204
"And you say, yeah,
I found the right place to look.",00:22:29.204,00:22:31.866
Here's the sentence.,00:22:31.866,00:22:32.990
He married Claudia Ross in 1971.,00:22:32.990,00:22:35.533
"And you're sure you've got the answer
if only you could work out what he",00:22:35.533,00:22:39.222
"was co-referent to and that's why
you need co-reference resolution.",00:22:39.222,00:22:43.050
"So when we've made all of these
attempts to link things together,",00:22:44.900,00:22:48.410
"I'll just explain now how we go about
evaluating co-reference resolution.",00:22:48.410,00:22:53.295
"So effectively co-referenced scoring
it's kind of like clustering.",00:22:53.295,00:22:58.883
"And basically any metric that people
have used for cluster evaluation,",00:22:58.883,00:23:03.200
"people have also tried to use
the co-reference resolution.",00:23:03.200,00:23:07.200
"And so the one that we're gonna emphasis
in today's lecture is one called B cubed,",00:23:07.200,00:23:11.890
"which is one of the widely used
clustering evaluation metrics.",00:23:11.890,00:23:15.900
"So what you have here I mean, I've
sort of just duplicated on both sides.",00:23:15.900,00:23:19.653
"So I can show you precision, recall, is so",00:23:19.653,00:23:22.937
"the colors of my little balls
are the gold answers of what's correct.",00:23:22.937,00:23:28.850
"And then the circles that I've drawn
around it is how my system has",00:23:28.850,00:23:33.100
"decided to gather things that
it thinks its co-reference.",00:23:33.100,00:23:36.540
"And so what you do for
the B cubed metric is you sort of",00:23:36.540,00:23:42.570
"align system clusters and
the gold clusters.",00:23:42.570,00:23:48.050
"So I've chosen to align this system
cluster with the blue color here,",00:23:48.050,00:23:52.590
"which I've shown by that
black around the circle.",00:23:52.590,00:23:55.630
"And then I say, okay, well of the things
that I put in my system cluster,",00:23:55.630,00:24:01.570
"what is the precision
of what I put in there?",00:24:01.570,00:24:04.190
"And well, it turns out that four out
of the five of them are blue and",00:24:04.190,00:24:08.999
one of them is pink.,00:24:08.999,00:24:10.492
"And so I say my precision is 4 5ths For
this cluster.",00:24:10.492,00:24:15.130
"Then I do it the other way around,
and I work out a recall.",00:24:15.130,00:24:19.247
"So I say, well, I aligned the blue
things with this system cluster.",00:24:19.247,00:24:25.673
"And, well, actually,",00:24:25.673,00:24:27.228
"this system cluster only contains
four out of the six blue things.",00:24:27.228,00:24:31.383
"So my recall for that alignment is
then four-sixths or two-thirds.",00:24:31.383,00:24:36.925
"And I'm gonna put those
together in an F measure and",00:24:36.925,00:24:40.089
"that's then going to give
me the B cubed measure.",00:24:40.089,00:24:44.220
And so that's sort of the main idea.,00:24:44.220,00:24:46.830
"It's just a little bit
more complex than that.",00:24:46.830,00:24:49.023
"I mean, first of all, obviously I want
to do it not only with that cluster.",00:24:49.023,00:24:52.986
"I also want to align this
cluster with the oranges and",00:24:52.986,00:24:55.920
"say they're precision recall one,
cuz they're completely correct.",00:24:55.920,00:24:59.995
"And this cluster and the pinks,",00:24:59.995,00:25:05.226
"and then I wanna say precision,",00:25:05.226,00:25:10.457
"four sixths and recall, four fifths.",00:25:10.457,00:25:16.869
So there are a couple of other tricks.,00:25:16.869,00:25:18.271
"One is that you're weighting
the different precisions and",00:25:18.271,00:25:22.783
recalls based on the size of the clusters.,00:25:22.783,00:25:26.170
"So, it matters more to get high
precision on really big clusters.",00:25:26.170,00:25:31.040
"The other bit that I sort of slightly
glossed over is I said, well, you align",00:25:31.040,00:25:36.260
"these system found clusters
with a gold cluster.",00:25:36.260,00:25:41.390
"And as you might know
from some other class,",00:25:42.510,00:25:46.410
"in the general case if you're
doing a bipartite alignment, or",00:25:47.450,00:25:51.930
"things of that sort,
that's actually an NP-hard problem.",00:25:51.930,00:25:56.320
"So it's sort of almost impossible
to guarantee that you've found",00:25:56.320,00:26:01.390
the optimal B-CUBED score.,00:26:01.390,00:26:03.700
"So normally, what you're
actually finding in your system",00:26:03.700,00:26:06.270
"is sort of a lower bound on
a possible B-CUBED score.",00:26:06.270,00:26:10.920
"But in practice, provided your system is
reasonably good, it's fairly easy and",00:26:10.920,00:26:15.950
"a greedy manner to start aligning
together system clusters and",00:26:15.950,00:26:20.360
"gold clusters,
starting with the ones that you did best.",00:26:20.360,00:26:23.820
"And in practice, there's greedy
matching software that's used for",00:26:23.820,00:26:28.595
"B-CUBED, which seems to nearly always
work and give you the right answer.",00:26:28.595,00:26:32.650
"And so that hasn't been
a huge problem in practice.",00:26:32.650,00:26:35.590
"That's only one measure that's
been used for coreference.",00:26:35.590,00:26:39.720
"There are a whole bunch of
other ones that have been used.",00:26:39.720,00:26:43.320
"Most of which relate to
clustering algorithms,",00:26:43.320,00:26:46.070
"evaluations that people
have used elsewhere.",00:26:46.070,00:26:48.350
"Okay, so before getting to the halfway
point, I then want to say just a little",00:26:49.390,00:26:54.480
"bit more about what goes on in coreference
from sort of a linguistic point of view.",00:26:54.480,00:27:01.700
"And this is actually
a little bit interesting and",00:27:01.700,00:27:03.680
"hasn't actually been much
dealt with by NLP systems.",00:27:03.680,00:27:06.700
So what kinds of things do we have?,00:27:06.700,00:27:09.080
"So we have referring expressions,
so things that directly refer,",00:27:09.080,00:27:13.397
"like John Smith, as named entities or
the President as common nouns.",00:27:13.397,00:27:18.200
"We then have things that aren't
directly referring, but or",00:27:18.200,00:27:21.724
"sort of variables that
are contingent on something else.",00:27:21.724,00:27:25.124
And there ones that are free variables.,00:27:25.124,00:27:27.465
"So his pay,
that's sort of a free variable, but",00:27:27.465,00:27:30.262
"it's reference is dependent
on the reference of Smith.",00:27:30.262,00:27:34.170
"And then we have these reflexives,",00:27:34.170,00:27:35.770
"the bound variables which are sort of
closely connected with something nearby.",00:27:35.770,00:27:42.260
"And so in linguistic theory most of
the work is dealt with these variables and",00:27:42.260,00:27:47.995
"trying to interpret what they
are going to be coreferent with.",00:27:47.995,00:27:52.760
"Whereas, in doing practical
coreference of a real text,",00:27:52.760,00:27:57.295
there's quite a lot of pronouns.,00:27:57.295,00:27:59.986
"But a lot of the actions was actually
dealing with these proper noun and",00:27:59.986,00:28:04.074
common noun referring expressions.,00:28:04.074,00:28:06.560
"And it turns out in practice,
getting these guys right is actually",00:28:06.560,00:28:10.390
"harder than getting these guys right,
which is sort of interesting.",00:28:10.390,00:28:14.170
So things to notice.,00:28:15.910,00:28:17.260
Not all noun phrases are referring.,00:28:17.260,00:28:20.020
"So, if we have every dancer twisted her
knee, her knee does not refer to anything",00:28:20.020,00:28:25.441
"because it's sort of embedded under
this quantifier of every dancer.",00:28:25.441,00:28:30.243
"That's perhaps more clearly seen
if you look at the second example.",00:28:30.243,00:28:33.736
No dancer twisted her knee.,00:28:33.736,00:28:35.950
"There's no her knee being talked about,
right?",00:28:35.950,00:28:38.400
"So that's a clearly
non-referring noun phrase.",00:28:38.400,00:28:42.140
"Okay, and similarly,",00:28:43.590,00:28:44.590
"no dancer isn't a noun phrase
that refers to any dancer either.",00:28:44.590,00:28:49.170
"So in linguistics,
people normally distinguish two relations.",00:28:51.980,00:28:58.980
"So one of them is coreference,",00:28:58.980,00:29:01.560
"which is when two mentions refer
to the same entity in the world.",00:29:01.560,00:29:06.370
"And that has nothing to do
with the structure of text.",00:29:06.370,00:29:09.550
"And the other one is
a relation of anaphora,",00:29:09.550,00:29:12.920
"which is a textual relation when a term,
the anaphor,",00:29:12.920,00:29:17.640
"refer gains reference with respect
to another term, the antecedent.",00:29:17.640,00:29:23.230
So you're using it for its interpretation.,00:29:23.230,00:29:25.510
"So if you go back to Greek roots,",00:29:26.600,00:29:29.369
"an anaphor meant that you had
this word whose interpretation",00:29:29.369,00:29:34.601
"was dependent on something
that preceded it in the text.",00:29:34.601,00:29:39.539
"And so anaphora was distinguished
from the opposite relationship,",00:29:39.539,00:29:43.959
"which was called Cataphora,
where you actually",00:29:43.959,00:29:47.082
"had a dependent term that was dependent
on something after it in the text.",00:29:47.082,00:29:51.828
"So here's a lovely example of
Cataphora from Oscar Wilde.",00:29:51.828,00:29:56.270
"From the corner of the divan of Persian
saddle-bags on which he was lying,",00:29:56.270,00:30:00.750
"smoking, as was his custom,
innumerable cigarettes.",00:30:00.750,00:30:05.240
"Lord Henry Wotton cold just catch
the gleam of the honey-sweet and",00:30:05.240,00:30:08.849
honey-colored blossoms of a laburnum.,00:30:08.849,00:30:11.040
"This is a Laburnum,
in case you were wondering.",00:30:12.380,00:30:14.455
"&gt;&gt; [LAUGH]
&gt;&gt; So this is a beautiful example of",00:30:14.455,00:30:18.662
"cataphora, because the referential
noun phrase is Lord Henry Wotton.",00:30:18.662,00:30:24.921
"And then both he and his then cataphors
on the following Lord Henry Wotton.",00:30:24.921,00:30:31.703
"Again, it turns out in nearly all of our
NLP systems, we never try and do this.",00:30:31.703,00:30:38.220
So we're always coming across mentions and,00:30:38.220,00:30:43.460
"then we're trying to assign
them to something before them.",00:30:43.460,00:30:47.165
"So we always treat them as
backward looking anaphora.",00:30:47.165,00:30:50.893
"So we'd be actually hoping to say this
he doesn't refer to anything before it.",00:30:50.893,00:30:55.706
"And then we'd be, later on, saying Lord
Henry Wotton is coreferent with the he.",00:30:55.706,00:31:02.765
"But that's actually sort of linguistically
bad and doesn't make terribly much sense.",00:31:02.765,00:31:06.826
"So a lot of the time, things that
are anaphoric are coreferencial",00:31:09.861,00:31:15.222
"because the textural
dependence is one of identity.",00:31:15.222,00:31:19.622
"So an anaphor is coreferencial
with its antecedent.",00:31:19.622,00:31:24.940
"But that's not always true, either.",00:31:24.940,00:31:26.710
"So you have things, anaphoric relations
that aren't identity relationships and",00:31:26.710,00:31:31.381
then they're not coreferential.,00:31:31.381,00:31:33.393
And so here's an example of this.,00:31:33.393,00:31:35.940
We went to see a concert last night.,00:31:35.940,00:31:38.160
The tickets were really expensive.,00:31:38.160,00:31:40.154
"So the tickets here is an anaphor that's
dependent on reference to this antecedent.",00:31:40.154,00:31:47.004
"Because it's meaning that the tickets for
the concert were really expensive.",00:31:47.004,00:31:52.530
"But it's not an identity relationship, so",00:31:52.530,00:31:54.860
"those are referred to
as bridging anaphors.",00:31:54.860,00:31:57.720
"And there's been a little NLP work on
trying to interpret bridging anaphors,",00:31:57.720,00:32:02.093
but extremely little.,00:32:02.093,00:32:03.427
"For most of the coreference systems,
a concert is a mention of an entity.",00:32:03.427,00:32:09.983
The tickets are a mention of an entity.,00:32:09.983,00:32:11.810
"And you just don't learn
the relationship between them.",00:32:11.810,00:32:14.790
"Okay, so there are really
kind of two different things.",00:32:15.790,00:32:18.264
"So you can have anaphoric
relationships in the text which may or",00:32:18.264,00:32:22.508
"may not imply (co)reference,
90% of the time they do but",00:32:22.508,00:32:26.673
"not always, and then you have
(co)referential relationships with two",00:32:26.673,00:32:31.639
"things in the text referred to
the same thing in the world.",00:32:31.639,00:32:35.950
"But that may just be because they
refer to the same thing in the world.",00:32:35.950,00:32:39.470
"There isn't necessarily any textual
dependence relationship between them, and",00:32:39.470,00:32:44.640
"so something that you might like to think
about is maybe those two phenomenas should",00:32:44.640,00:32:49.010
"actually be handled somewhat differently
in our models, and the truth is for",00:32:49.010,00:32:55.600
"most of the models we build at the moment
they're really not handled differently.",00:32:55.600,00:32:59.590
"You could hope if you crossed your
fingers really hard, that somehow the way",00:32:59.590,00:33:03.580
"our neural network model works
will end up treating the pronouns,",00:33:03.580,00:33:08.230
"which are normally anaphors, sort of
differently to the way it's treating You",00:33:08.230,00:33:13.770
"know the various mentions of a cache which
were just (co)reference relationships but",00:33:13.770,00:33:18.880
"you know sort of a crush your
fingers really hard there's nothing",00:33:18.880,00:33:21.620
"really that model structure that's sort of
really distinguishing these two notions.",00:33:21.620,00:33:26.610
"Okay, so on the second half of getting to
say how we build (co)reference systems but",00:33:26.610,00:33:31.399
"before we do that we're onto
the research highlight and",00:33:31.399,00:33:34.525
James is going to talk about that.,00:33:34.525,00:33:36.471
"So, hello everyone.",00:33:41.578,00:33:43.230
"Today's research highlight will be
summarizing source code using a neural",00:33:43.230,00:33:46.010
attention model.,00:33:46.010,00:33:47.040
"This paper was published in ACL 2016,
and its by authors from",00:33:47.040,00:33:51.860
"University of Washington, Computer Science
and Engineering Department.",00:33:51.860,00:33:56.360
"So the main task in a dataset that
they define is to generate sentences",00:33:56.360,00:34:00.998
"that describe C# and SQL queries, and
they use a dataset from StackOverflow.",00:34:00.998,00:34:05.180
"Essentially, what they do is they
just query the whole dataset for",00:34:06.660,00:34:09.240
"all the posts that have tags that have C#,
SQL or Database, or Oracle in them.",00:34:09.240,00:34:15.090
"Well, as you would expect,
just doing this naively doesn't work",00:34:15.090,00:34:17.970
"very well because there's lots and
lots of noise in the dataset.",00:34:17.970,00:34:20.920
"So one of the cleaning sets that they do
beforehand is to remove all the posts",00:34:20.920,00:34:24.351
where the question and,00:34:24.351,00:34:25.385
"the text doesn't actually have any
relevance with the content of the code.",00:34:25.385,00:34:29.360
"For example like people often ask
questions about codes such as like how can",00:34:29.360,00:34:33.030
I make these code run faster and,00:34:33.030,00:34:34.670
"in that sense that won't be
a very good summary at all.",00:34:34.670,00:34:37.120
"A second thing that they do in order to,
a more technical thing that they do is",00:34:37.120,00:34:40.560
"they actually try to parse the code
in the sense that like a lot of",00:34:40.560,00:34:45.110
"code contains like literals, they have
specific variable names Things like this,",00:34:45.110,00:34:49.160
"which are not very general, like general
systems, to try to summarize code, and",00:34:49.160,00:34:53.230
"what they do in this case is they actually
replace literals with their types.",00:34:53.230,00:34:56.460
"They also replace the table and
column names with something more generic,",00:34:56.460,00:34:59.540
"and then they also remove in-line
comments in order to make the system",00:34:59.540,00:35:03.310
less reliant on those things.,00:35:03.310,00:35:05.110
"And two examples of the code are shown
on the side, where one is C# code for",00:35:05.110,00:35:10.560
"getting the whiff of a text block in
some view, I think, and the second",00:35:10.560,00:35:15.730
"is source code for SQL where you're
trying to get the second largest element.",00:35:15.730,00:35:19.670
Okay.,00:35:21.040,00:35:21.840
"And specifically, like they define two
tasks that they're actually going to try",00:35:21.840,00:35:25.715
"to attempt, and these are to generate
text, specifically a sentence",00:35:25.715,00:35:30.030
"to describe some code sequence
that maximizes a scoring function.",00:35:30.030,00:35:34.000
"A second task is the information
retrieval task, which is to go through,",00:35:34.000,00:35:37.780
"essentially their corpus, and
find the code snippet that most",00:35:37.780,00:35:43.820
"closely relates to the input question,
which would in a natural language.",00:35:43.820,00:35:47.870
The scoring function is shown to the side.,00:35:47.870,00:35:49.790
"It's essentially the product of
the next word probabilities, and",00:35:49.790,00:35:54.240
"these are proportional to what
the output that we get from their model,",00:35:54.240,00:35:58.570
"which takes into account
the hidden states of the LSTM and",00:35:58.570,00:36:01.580
also some attention on the source code.,00:36:01.580,00:36:03.230
"Specifically, here's an example of how",00:36:04.670,00:36:07.480
"they generate their text using their
model which they call CODE-NN.",00:36:07.480,00:36:11.150
"You feed in some starting token and
then you make some form of prediction",00:36:11.150,00:36:16.010
"based on the attention and based on
the LSTM to get on the next word, N1,",00:36:16.010,00:36:20.790
"and then you keep doing
this iteratively again and",00:36:20.790,00:36:23.400
"again, and this is how they
generate their full sequence.",00:36:23.400,00:36:26.340
"So to evaluate their system, they did,
first on the text generation side they",00:36:27.570,00:36:32.920
"compared against, well they used existing
MT metrics such as METEOR and BLEU, and",00:36:32.920,00:36:37.170
"they took some existing translation
system and information retrieval baseline",00:36:37.170,00:36:42.010
"system called Moses which is a phrase
based translations system and",00:36:42.010,00:36:47.050
"then on a previous model
that also know that and",00:36:47.050,00:36:50.586
"they found that their models get higher
scores on essentially METEOR and",00:36:50.586,00:36:55.983
"BLEU and then they also did
something that's a user study,",00:36:55.983,00:36:58.780
"where they got five people, and
they had them rank the result.",00:36:58.780,00:37:01.370
"Like manually score the results,
in terms of naturalness, which is how well",00:37:01.370,00:37:05.430
"does the sentence actually read,
in terms of fluency and informativeness.",00:37:05.430,00:37:09.660
"In terms of how much of the content
of the code was actually captured",00:37:09.660,00:37:12.910
"by the summary, and",00:37:12.910,00:37:14.860
"they found that their model unsurprisingly
does better than existing approaches.",00:37:14.860,00:37:19.890
"And then their information retrieval
task they use a metric called",00:37:19.890,00:37:23.360
"Mean Reciprocal Rank, and they compared
against some existing previous papers and",00:37:23.360,00:37:28.970
existing baseline out there.,00:37:28.970,00:37:30.100
"So what's more interesting is the actual
example outputs that the model generates.",00:37:32.800,00:37:36.720
Here's an example of a C# code which is to,00:37:36.720,00:37:42.170
"add children to some tree node, and",00:37:42.170,00:37:46.783
"in particular, this is C#, so
treenode is actually part of a TreeView,",00:37:46.783,00:37:51.840
"and in this case,
CODE-NN actually gets pretty close,",00:37:51.840,00:37:54.280
"where they recognize that these tree
nodes are related to tree views but",00:37:54.280,00:37:58.411
"it doesn't quite get the idea that you're
trying to add instead of get them all.",00:37:58.411,00:38:01.570
"On the second example where the CODE-NN
actually got the right result is this",00:38:02.790,00:38:07.126
"query where we're trying to select
random rows from a table and",00:38:07.126,00:38:10.661
CODE-NN actually get's it exactly.,00:38:10.661,00:38:13.015
"Cool, and that's all.",00:38:13.015,00:38:16.584
"&gt;&gt; [APPLAUSE]
&gt;&gt; Okay,",00:38:16.584,00:38:24.780
"so now for the remaining 40 minutes,
it's now algorithms to try and",00:38:24.780,00:38:30.260
"do (co)reference resolution,
and so I guess for",00:38:30.260,00:38:33.990
"about the first 15 minutes, I'm gonna
sort of say something about sort of",00:38:33.990,00:38:38.820
"the history of ways of doing
(co)reference resolution,",00:38:38.820,00:38:43.380
"anaphora resolution in general, and just
the sort of space and traditional methods.",00:38:43.380,00:38:48.060
"And then sort of for the last 25 minutes
I'm gonna talk about one particular",00:38:48.060,00:38:52.570
"way of doing it which is actually
from a paper that Kevin Clark did.",00:38:52.570,00:38:57.860
"The most famous thing in the space
of (co)reference resolution,",00:38:57.860,00:39:03.100
actually just an algorithm for,00:39:03.100,00:39:05.810
"determining the pronominal
anaphora resolution, working out.",00:39:05.810,00:39:10.430
"What, you know, he, him,
she, hers, its, refer to",00:39:10.430,00:39:15.730
"is an algorithm that was proposed
by a long time ago by Jerry Hobbs,",00:39:15.730,00:39:21.550
"which these days is normally
referred to as the Hobbs' Algorithm.",00:39:21.550,00:39:26.540
"But actually in Jerry Hobbs' paper he
refers to as the naive algorithm for",00:39:26.540,00:39:32.590
a reason that I will explain in a minute.,00:39:32.590,00:39:35.010
"And so this algorithm,
I'm not gonna read through it,",00:39:35.010,00:39:39.940
"it's a complex mechanistic procedure for
deciding what a pronoun refers to.",00:39:39.940,00:39:46.060
"So you begin at the noun phrase, so it's
assuming a syntactic pass of the sentence,",00:39:46.060,00:39:50.390
"begin at the noun phrase
immediately above the pronoun.",00:39:50.390,00:39:53.710
Go up the tree to the first NP or S.,00:39:53.710,00:39:55.860
"Call this X, and the path p.",00:39:55.860,00:39:57.470
"Traverse all branches below X,
to the left, blah, blah, blah, and",00:39:57.470,00:40:01.290
this is in all of it.,00:40:01.290,00:40:02.320
"It keeps on going on the next page,
and it's got go-tos, go to step 4.",00:40:02.320,00:40:07.212
"So, the sort of
embarrassing thing is that,",00:40:10.920,00:40:14.208
"not in the system I'm gonna
present at the end part of class.",00:40:14.208,00:40:18.632
"But if you look at the sort of
machine learning approaches to",00:40:18.632,00:40:23.786
"co-reference resolution that
were done in the 2000s and",00:40:23.786,00:40:29.147
"the first half of the 2010s, nearly all
of them used this algorithm as a feature.",00:40:29.147,00:40:37.120
"So if you had a regular
statistical classifier,",00:40:37.120,00:40:40.110
"you can take any kind of
little sub routine and",00:40:40.110,00:40:43.090
"sort of put its judgment in as a feature,
into your logistic regression.",00:40:43.090,00:40:48.290
"And it turned out that what this
calculated was sort of a useful enough",00:40:48.290,00:40:53.132
"approximation to getting out most
likely anaphoric relationships out of",00:40:53.132,00:40:57.935
syntactic trees.,00:40:57.935,00:40:58.911
"That most machine learning
systems use this and",00:40:58.911,00:41:01.991
got value out of that as a feature.,00:41:01.991,00:41:04.225
"So here's the kind of idea
of how it was meant to work.",00:41:04.225,00:41:07.500
"So ""Niall Ferguson is a prolific
well-paid and a snappy dresser.",00:41:07.500,00:41:14.550
"Stephen Moss hated him."" Okay,
so here's a him and",00:41:14.550,00:41:20.080
"you're wanting to work out
what that's co-referent to.",00:41:20.080,00:41:23.860
And so you start at this noun phrase here.,00:41:23.860,00:41:27.520
"And so what it said was that you
started from this noun phrase and",00:41:27.520,00:41:30.930
"you went up to the next S or
NP and you called that X, and",00:41:30.930,00:41:36.400
the path that you'd gone up p.,00:41:36.400,00:41:38.630
"And then it says, traverse",00:41:39.820,00:41:43.060
"branches below X to the left of p,",00:41:44.480,00:41:48.940
"propose as antecedent any NP that
has an NP or S between it and X.",00:41:48.940,00:41:55.700
"So I traverse things to the left and
I can find here a noun phrase.",00:41:55.700,00:42:01.980
"But that doesn't have anything
else in between that and so,",00:42:01.980,00:42:05.700
"therefore, it's not a candidate.",00:42:05.700,00:42:08.220
"And so, at that point,",00:42:08.220,00:42:11.260
"I'm going to be at the highest S in
the sentence and I'm gonna traverse",00:42:11.260,00:42:15.705
"the parse trees of the previous
sentences in the order of recency.",00:42:15.705,00:42:20.675
"And I traverse each tree left-to-right,
breadth first.",00:42:20.675,00:42:25.245
"So there's a lot of stuff embedded in
this very complex mechanistic procedure.",00:42:25.245,00:42:31.302
"But most of it is sort of correct, and
gets first order linguistics right.",00:42:31.302,00:42:37.212
And so what's going on here?,00:42:37.212,00:42:39.002
So what's going on here?,00:42:39.002,00:42:40.192
"So when we see him,
the first thing to suspect",00:42:41.530,00:42:44.900
"is maybe it refers to something to
the left in this same sentence.",00:42:44.900,00:42:49.010
"But that's where the kind of linguistic
constraints on pronouns come in, right?",00:42:49.010,00:42:53.370
"So that if Stephen Moss referred to him,
he couldn't of said him,",00:42:53.370,00:42:59.050
"he would of needed to say himself, right,
you have to use a reflexive there.",00:42:59.050,00:43:06.050
"So that's why it says,
unless there's some intervening NP or",00:43:06.050,00:43:10.570
"S in between,
it's not a candidate anymore.",00:43:10.570,00:43:15.020
"But precisely,",00:43:15.020,00:43:15.840
"if there was a more complex sentence
structure, it would be a candidate.",00:43:15.840,00:43:20.320
"So if you had something like a more
complex noun phrase in which",00:43:20.320,00:43:24.140
"Stephen Moss was a modifier,
then co-reference would become possible.",00:43:24.140,00:43:28.660
"So if it was something like,
Stephen Moss' brother hated him,",00:43:28.660,00:43:32.490
"then it'd be possible for
him to refer to Stephen Moss, right?",00:43:32.490,00:43:35.830
"And so that will be captured
because then there would an extra",00:43:35.830,00:43:40.850
"NP node in between here
then it would be okay.",00:43:40.850,00:43:45.070
"So that didn't work and so
we went backwards and so",00:43:45.070,00:43:48.030
"then again now instead of
using software heuristics.",00:43:48.030,00:43:51.180
"So the heuristics are usually right,
it said to go backwards to sentences and",00:43:51.180,00:43:56.059
order of recency.,00:43:56.059,00:43:57.270
"So if the antecedent isn't
in the current sentence,",00:43:57.270,00:44:00.260
"it's mostly in the immediately preceding
sentence, so you look there first.",00:44:00.260,00:44:04.150
"And then it says, within the sentence,
go from left to right.",00:44:04.150,00:44:09.310
"Well, what's going on there?",00:44:09.310,00:44:10.790
"There's sort of an obliqueness
hierarchy in sentences.",00:44:10.790,00:44:14.230
"And actually within a sentence, a subject,
which at least in English is on the left",00:44:14.230,00:44:19.280
"side, is more likely to be the antecedent
than something that's an object or",00:44:19.280,00:44:23.810
"an indirect object or object of
a preposition that's buried down here.",00:44:23.810,00:44:27.990
"So it's saying the first thing you
should try is Niall Ferguson and so",00:44:27.990,00:44:32.260
that's then a candidate.,00:44:32.260,00:44:33.930
"It doesn't get disqualified on
page two from reasons of gender or",00:44:33.930,00:44:38.120
anything like that.,00:44:38.120,00:44:39.360
"And so we propose it as an answer and
the Hobbs Algorithm gets it right.",00:44:39.360,00:44:44.828
"And so the Hobbs Algorithm
gets the right answer for",00:44:44.828,00:44:48.855
"pronouns about 80% of the time,
it's actually pretty good.",00:44:48.855,00:44:54.017
"Of course, sometimes it gets it wrong,",00:44:54.017,00:44:55.980
"it's easy to come up with sentences
that won't get it right for.",00:44:55.980,00:44:59.170
"So I just wanted to,
before going on, deviate for",00:45:00.220,00:45:04.330
"a minute and talk about what
Jerry Hobbs was actually interested in.",00:45:04.330,00:45:09.690
"So what Jerry Hobbs was actually
interested in was knowledge-based",00:45:09.690,00:45:13.952
pronominal coreference.,00:45:13.952,00:45:15.660
"And so, from the early days of AI,",00:45:15.660,00:45:18.410
"there's been sort of observations about
how to actually get coreference right",00:45:18.410,00:45:23.110
"in many cases,
you actually have to understand sentences.",00:45:23.110,00:45:27.960
"And so there was this famous
pair of sentences, which was",00:45:27.960,00:45:32.410
"proposed by Terry Winograd, who until very
recently, was on the Stanford faculty.",00:45:32.410,00:45:36.670
"Though he had kind of dropped out
of doing NLPN and moved onto HCI.",00:45:36.670,00:45:40.840
"And so,
Terry contrasted these two sentences.",00:45:40.840,00:45:44.530
"The city council refused the women
a permit because they feared violence.",00:45:44.530,00:45:48.730
"And the city council refused the women
a permit because they advocated violence.",00:45:48.730,00:45:53.490
"This was back in the 60s and 70s, when
there was more protests around, I guess.",00:45:53.490,00:45:57.640
"So anyway, so in the first sentence,
the natural reading is",00:45:57.640,00:46:03.790
"the they,
is coreferent with the city council.",00:46:03.790,00:46:09.030
"And in the second sentence,
the natural reasoning with reading is",00:46:09.030,00:46:12.350
"with the they being coreferent
with the woman, right?",00:46:12.350,00:46:16.010
"And the crucial thing to notice is
this isn't something that the Hobbs",00:46:16.010,00:46:20.710
"naive algorithm could possibly get
right cuz both of these sentences",00:46:20.710,00:46:24.860
have completely identical structure.,00:46:24.860,00:46:27.280
"And so the answer that was
suggested at the time was well,",00:46:29.120,00:46:32.870
"what we actually need to do is
have knowledge of the world and",00:46:32.870,00:46:38.320
"being able to sort of
represent these actions, and",00:46:38.320,00:46:41.920
"representing relationships
that are likely to occur.",00:46:41.920,00:46:45.700
"And that we just need to know, we have to
sort of understand about city councils and",00:46:45.700,00:46:50.335
permits.,00:46:50.335,00:46:50.949
"To understand that if
you're refusing a permit,",00:46:50.949,00:46:55.007
"that would happen if someone
was advocating violence but",00:46:55.007,00:46:59.646
"it would happen if the people
who would be getting the permit",00:46:59.646,00:47:04.478
were doing the advocation of violence.,00:47:04.478,00:47:08.370
"And so this is an idea that people have
actually tried to resurrect recently.",00:47:08.370,00:47:12.140
"So, Hector Levesque a good
old-fashioned AI guy.",00:47:12.140,00:47:17.700
"And I guess he gave an invited talk
in 2013 where he sort of suggested,",00:47:17.700,00:47:23.220
"gee, we should kind of try and get back
to these kind of Winograd sentences and",00:47:23.220,00:47:27.350
"actually be trying to understand them as
interesting, co-referenced challenges.",00:47:27.350,00:47:32.020
"And so people have tried to, more
recently, run a Winograd schema challenge.",00:47:32.020,00:47:35.890
"So, really this is what
Jerry Hobbs was interested in.",00:47:37.390,00:47:41.950
"And so actually why he
proposed his naive algorithm,",00:47:41.950,00:47:45.550
"it was actually one of the first
instances of NLP when someone said, gee,",00:47:45.550,00:47:50.510
"before proposing something really complex,
I should have a baseline.",00:47:50.510,00:47:55.400
"So I've got a good baseline to compare
against as to how well something",00:47:55.400,00:47:59.328
simple works.,00:47:59.328,00:48:00.980
And what he discovered.,00:48:00.980,00:48:02.990
"Was the kind of systems he built
couldn't possibly beat his baseline,",00:48:02.990,00:48:08.380
"because trying to do knowledge-based
pronominal coreference was way too",00:48:08.380,00:48:13.850
"hard for
what could be done back in the 1970s.",00:48:13.850,00:48:19.250
But this is what he wrote about it.,00:48:19.250,00:48:21.130
"So he said,
the naive approach is quite good.",00:48:21.130,00:48:24.760
"Computationally speaking, it will be
a long time before a semantically based",00:48:24.760,00:48:28.660
"algorithm is sophisticated
enough to perform as well.",00:48:28.660,00:48:32.330
"And these results set a very high standard
for any other approach to aim for.",00:48:32.330,00:48:37.060
"Yet there is every reason to pursue
a semantically based approach.",00:48:37.060,00:48:40.610
The naive algorithm does not work.,00:48:40.610,00:48:42.900
"Anyone can think of
examples where it fails.",00:48:42.900,00:48:45.880
"In these cases it not only fails, it gives
no indication that it has failed and",00:48:45.880,00:48:50.542
"offers no hope in finding
the real antecedent.",00:48:50.542,00:48:53.427
"So in one sense,
since 1978 we have progress because we now",00:48:53.427,00:48:58.780
"have algorithms that are significantly
better than the Hobbs' naive algorithm.",00:48:58.780,00:49:06.150
"So we've passed that bar for
at least a decade, so that's a good news.",00:49:06.150,00:49:11.540
"On the other hand,
Jerry Hobbs could very viably argue,",00:49:11.540,00:49:17.490
"that actually the second
paragraph that I quoted there",00:49:17.490,00:49:21.370
"hasn't been addressed whatsoever,
cuz we're writing.",00:49:21.370,00:49:25.610
"They might have more machine learning than
them but we're writing the same kind of",00:49:25.610,00:49:28.720
"mechanistic algorithms that usually get
things right, sometimes get things wrong.",00:49:28.720,00:49:33.580
And that's just how it is and,00:49:33.580,00:49:37.030
"we don't really have any way
of telling when it's failing.",00:49:37.030,00:49:40.970
"Okay, so how do people do coreference.",00:49:42.200,00:49:44.830
"So there are different ways that people
approach the coreference problem.",00:49:44.830,00:49:49.650
"So actually the most common
way of doing it is what's",00:49:49.650,00:49:53.330
referred to as mentioned pair models.,00:49:53.330,00:49:55.250
"And that's what we gonna look at for
the end part of this class.",00:49:55.250,00:49:58.820
"So we try and
work out all the coreference relationships",00:49:58.820,00:50:02.910
"by just making a sequence
of pairwise links.",00:50:02.910,00:50:06.690
"So we're gonna take pairs of mentions and
say, are these coreference or",00:50:06.690,00:50:10.340
"not, yes or no.",00:50:10.340,00:50:11.420
"So we're doing binary classification
decisions independently.",00:50:11.420,00:50:16.730
"And then as a result of those
binary decisions we sort of",00:50:16.730,00:50:21.410
"induce a kind of clustering of
mentions into entities, and",00:50:21.410,00:50:25.860
"we just do that in a simple
deterministic way.",00:50:25.860,00:50:28.852
"We just join everything together into
a lump that's been put together by",00:50:28.852,00:50:33.505
these binary decisions.,00:50:33.505,00:50:35.045
"And we just say, and
they all close together by transitivity.",00:50:35.045,00:50:39.625
"There are a couple of other
approaches that people have used for",00:50:39.625,00:50:42.643
coreference resolution.,00:50:42.643,00:50:44.845
"Rather than simply doing
binary yes/no decisions,",00:50:44.845,00:50:48.225
"another choice is to say that you can
actually use a ranking algorithm.",00:50:48.225,00:50:52.515
"Something that's gone very prominent
in certain areas of machine learning",00:50:52.515,00:50:58.019
"that you kind of don't cover in your basic
ML class, is doing ranking problems.",00:50:58.019,00:51:03.880
But they have come up in a lot of places.,00:51:03.880,00:51:06.000
"Think things like,
Netflix recommending you a movie.",00:51:06.000,00:51:11.230
"Google recommending you a web page,
all of those things are ranking problems.",00:51:11.230,00:51:16.200
"And so you can think of coref as a ranking
problem, because if you have a pronoun,",00:51:16.200,00:51:21.290
"well it should be
coreferent with something.",00:51:21.290,00:51:24.760
"And maybe there are seven prior
mentions in the document.",00:51:24.760,00:51:29.430
"And then you're doing a ranking task as
to which one of those seven it would be.",00:51:29.430,00:51:33.320
"And then there's a third way of
doing coreference resolution,",00:51:33.320,00:51:37.950
which is arguably really the right way.,00:51:37.950,00:51:40.650
"Which is, what are referred
to as entity mention models.",00:51:40.650,00:51:43.501
"And that's just explicitly
think about the entities.",00:51:43.501,00:51:48.930
"They're actual real entities
that your discourse is about.",00:51:48.930,00:51:53.380
"And when you see a mention, you should be
saying, that's a mention of a particular",00:51:53.380,00:51:58.340
"entity, or maybe this mention introduces
a new entity in to your discourse and",00:51:58.340,00:52:02.920
"you've got this set of
underlying entities.",00:52:02.920,00:52:05.720
"So in some sense, your entities
are your clusters or mentions, but",00:52:05.720,00:52:09.280
"you're actually giving them first
class status as objects in your model",00:52:09.280,00:52:13.850
"rather than them just appearing as
a result of some linking process.",00:52:13.850,00:52:18.780
"And so, a number of people then tried to
work on models that explicitly represent",00:52:18.780,00:52:23.900
"entities and then do some kind
of joint inference, or have some",00:52:23.900,00:52:28.852
"kind of generative model of how the
mentions are created from the entities.",00:52:28.852,00:52:32.959
"But the simplest case and what we're look
at mainly, are these mention-pair models.",00:52:33.960,00:52:41.610
"And so mention-pair models are normally
trained to supervise learning models.",00:52:41.610,00:52:46.850
"What you do is you have some data,
you have mentions.",00:52:46.850,00:52:52.636
"And so there's this prior problem
of finding the mentions, but",00:52:52.636,00:52:56.400
"we can roughly think of
the mentions as our noun phrases.",00:52:56.400,00:53:00.820
"And then here's a He, and",00:53:00.820,00:53:02.234
"what we're gonna say is that gonna
be coreferent with something.",00:53:02.234,00:53:06.130
"Well, if we have gold standard data
we'll know that the right answer would",00:53:06.130,00:53:12.903
"be either Mr. Obama or the president,
cuz both of them are coreferent.",00:53:12.903,00:53:18.310
"And if you have multiple choices, you'd
normally just choose the nearest one.",00:53:18.310,00:53:21.900
"And you say, the correct answer for
this one is the president.",00:53:21.900,00:53:24.780
"And then you have negative examples which
are things that are not coreference.",00:53:26.160,00:53:30.275
So Milwaukee is a negative example.,00:53:30.275,00:53:33.580
So you get positive and,00:53:33.580,00:53:35.550
"negative examples, you train
a binary classifier and you're done.",00:53:35.550,00:53:40.610
"So if a conventional coref
people then used all",00:53:40.610,00:53:45.610
"sorts of features,
that were indicators of coreference.",00:53:45.610,00:53:50.170
"So for pronouns in English, they're things
like person, number, and gender agreement.",00:53:50.170,00:53:55.890
So Jack gave Mary a gift.,00:53:55.890,00:53:57.310
She was excited.,00:53:57.310,00:53:58.690
"That has to be Mary because of gender,
rather than Jack.",00:53:58.690,00:54:03.240
"There are softer notions
like semantic compatibility.",00:54:03.240,00:54:07.190
"So if there's a reference
to the mining conglomerate,",00:54:07.190,00:54:09.340
"that could be coreferent with the company
because that's sorta semantically",00:54:09.340,00:54:13.780
"compatible, that's much harder to do.",00:54:13.780,00:54:16.610
"Some things that we've already mentioned
are hard syntactic constraints.",00:54:16.610,00:54:21.350
"So John bought him a new car, him can't
be John, that'd have to be himself.",00:54:21.350,00:54:26.510
So that's a feature we can use.,00:54:26.510,00:54:28.690
"But there are lots of softer things
which I was mentioning before.",00:54:28.690,00:54:31.959
"So, recency is a good indicator.",00:54:31.959,00:54:36.150
John went to a movie.,00:54:36.150,00:54:37.210
Jack went as well.,00:54:37.210,00:54:38.420
He was not busy.,00:54:38.420,00:54:39.820
"That sort of sounds like it was probably
Jack that wasn't busy, at least to me.",00:54:39.820,00:54:44.392
"And that's presumably a recency effect,
but",00:54:44.392,00:54:47.370
"it's not really categorical, it has to be.",00:54:47.370,00:54:51.920
Subjects are commonly preferred.,00:54:51.920,00:54:53.980
John went to a movie with Jack.,00:54:53.980,00:54:55.600
He was not busy.,00:54:55.600,00:54:56.480
"I think the most natural reading
of that is that John was not busy,",00:54:56.480,00:55:00.890
so that's preferring subjects.,00:55:00.890,00:55:03.560
"Parallelism, John went
with Jack to a movie.",00:55:03.560,00:55:06.560
Joe went with him to a bar.,00:55:06.560,00:55:08.760
"I think the most natural reading of that
is that that is Jack that Joe went with.",00:55:08.760,00:55:14.632
"And that seems to make sense not
according to grammatical role preference,",00:55:14.632,00:55:18.430
"which would give you John, but",00:55:18.430,00:55:20.100
"in terms of the parallelism of the two
sentences and interpreting it that way.",00:55:20.100,00:55:24.158
"So there are lot's linguistic features
that you would start to build",00:55:24.158,00:55:28.000
"features from and
put them into a classifier and try and",00:55:28.000,00:55:32.050
"determine coreference, and
people built these things where loosely,",00:55:32.050,00:55:36.932
"they are big logistic regression
classifiers with hundreds of thousands of",00:55:36.932,00:55:41.740
"features that try to capture some
of these kind of relationships.",00:55:41.740,00:55:46.675
"But for the last 25 minutes
what I want to tell you then is",00:55:46.675,00:55:49.900
"about what people have done with
deep learning and coreference.",00:55:49.900,00:55:53.390
"And the answer to that in two words is,
not much.",00:55:53.390,00:55:57.050
"And so at this point in time There
are basically four papers that have tried",00:55:57.050,00:56:03.161
"to use neural networks, deep learning,
to do coreference by two authors.",00:56:03.161,00:56:08.468
"So there's Sam Wiseman at Harvard
who's worked on the problem in",00:56:08.468,00:56:13.213
a couple of papers.,00:56:13.213,00:56:14.690
"And then there's Kevin who's worked
on the problem in a couple of papers.",00:56:14.690,00:56:19.240
"And so
there is some sort of connections and",00:56:19.240,00:56:21.745
there's some different approaches here.,00:56:21.745,00:56:24.326
"I mean, in particular, there are a couple
of papers, both sorta Sam's second",00:56:24.326,00:56:29.382
"paper and Kevin's first paper, which we're
both trying to do entity-mention models.",00:56:29.382,00:56:35.132
"And actually try to have explicit
representations for entities and",00:56:35.132,00:56:39.842
"doing more global inference
in terms of entities.",00:56:39.842,00:56:43.660
"And I think most people who have tried to
do coreference a bit really do believe",00:56:43.660,00:56:48.296
"that surely they should be good power from
doing things jointly over these entities",00:56:48.296,00:56:53.289
and that should give some real advantages.,00:56:53.289,00:56:55.951
"In practice, it's repeatedly sort of
proven hard to get sustained advantages",00:56:55.951,00:57:00.918
"from doing that and so there's sort of
been this continuing use of entity pair,",00:57:00.918,00:57:05.881
"sorry, mention-pair models,
which are very simple to implement, and",00:57:05.881,00:57:10.478
"you keep on thinking to work
out how to make them work well.",00:57:10.478,00:57:14.560
"So Kevin's most recent paper is actually
back to a mention-pair model and",00:57:14.560,00:57:20.150
that produces great results.,00:57:20.150,00:57:21.930
"And I'd thought I'd actually show that
one, not only because it's the most recent",00:57:21.930,00:57:26.880
"and best, but because it might be kind of
a good chance to sort of show a couple",00:57:26.880,00:57:31.020
"of other techniques of doing things,
in the context of deep learning.",00:57:31.020,00:57:35.100
"Okay, so here we go, so the first couple
of bits may be fairly similar, right?",00:57:35.100,00:57:42.030
"So, we wanna find these
coreference clusters.",00:57:42.030,00:57:45.810
"And so we're gonna be doing it
simply as a mention-ranking model",00:57:45.810,00:57:50.150
"where you want to assign a score
to each candidate antecedent.",00:57:50.150,00:57:54.780
"So, we want to be saying,
what does my refer to?",00:57:54.780,00:57:59.332
"And we're picking the preceding mentions,
and then we add on one extra candidate,",00:57:59.332,00:58:05.438
"cuz for any mention you have one
possibility is this is a new referent in",00:58:05.438,00:58:10.513
"the discourse and it's not co-referent
with anything that appears before it.",00:58:10.513,00:58:16.380
"So, we then have this new up the end.",00:58:16.380,00:58:20.096
So you can say this a NEW referent.,00:58:20.096,00:58:22.870
"And so for each of these mention pairs,",00:58:22.870,00:58:25.699
"we're gonna build a model
that scores them.",00:58:25.699,00:58:29.260
"And so, it's just gonna
score a pair of mentions for",00:58:29.260,00:58:33.480
"coreference, independently still,
and give them a score.",00:58:33.480,00:58:37.790
"And then what we're gonna do is say,
well, which one has the best score?",00:58:37.790,00:58:41.950
"Okay, that's putting I and my together.",00:58:41.950,00:58:45.090
"And so that one, we're going to rejoin.",00:58:45.090,00:58:47.690
"And so then we can literally just go
through the mentions in the discourse from",00:58:47.690,00:58:52.140
"left to right and
run this mention-pair classifier",00:58:52.140,00:58:57.050
"on each successive mention and
sort of then assign them.",00:58:57.050,00:59:00.510
And that will give us our model.,00:59:00.510,00:59:02.630
"And so then the question is,
how can we go about designing and",00:59:03.750,00:59:08.790
training a good mention-pair classifier?,00:59:08.790,00:59:12.699
"Then yeah, cuz at the end of the day,
our actual set of",00:59:12.699,00:59:17.595
"coreferent things will just be
the result of these local decisions.",00:59:17.595,00:59:22.770
"So if we say, I as a new thing,
Nader as a new thing,",00:59:22.770,00:59:27.140
"he refers to Nader,
my refers to I, she refers to my.",00:59:27.140,00:59:32.320
"Then the result of that is we've
sort of constructed these two",00:59:32.320,00:59:36.140
"coreferent clusters as a result
of these local decisions.",00:59:36.140,00:59:40.870
And as a result of imputing transitivity.,00:59:40.870,00:59:43.350
"So we're then saying that
she is also coreferent to I.",00:59:43.350,00:59:48.832
"Okay, so I hope the setting
in general is clear enough.",00:59:48.832,00:59:51.890
How is that done?,00:59:53.580,00:59:54.810
"And so for
doing the neural mention-pair model,",00:59:54.810,00:59:58.770
"this is being done as
a feed-forward network.",00:59:58.770,01:00:02.630
"It's sort of, in some sense,
it's no more complicated than that.",01:00:02.630,01:00:06.440
But what are the parts that go into it?,01:00:06.440,01:00:08.821
"So down at the bottom we
have two kinds of things.",01:00:08.821,01:00:12.940
"So firstly, for both the mention and",01:00:12.940,01:00:16.094
"the candidate antecedent,
we have embeddings of words.",01:00:16.094,01:00:21.090
"And so this model didn't use any
kind of recurrent neural network or",01:00:22.150,01:00:27.500
"something like that that
goes through the mentions.",01:00:27.500,01:00:30.440
"I mean, Kevin actually experimented
with that a little bit and",01:00:30.440,01:00:34.309
found no particular value in it.,01:00:34.309,01:00:36.356
"And so it actually kind of like
the dependency powers of Danqis",01:00:36.356,01:00:40.490
"that you did in assignment two if
you remember back to that one.",01:00:40.490,01:00:44.631
"And so it picks out particular words and
uses their word representations.",01:00:44.631,01:00:49.580
"So it will use the head word of the
mention, the last word of the mention, and",01:00:49.580,01:00:53.730
"things like that, and so that gives
you some word embedding features.",01:00:53.730,01:00:59.230
"And so the word embedding features are
gonna be good for capturing similarities.",01:00:59.230,01:01:05.322
"I mean, certainly when it's
just the same word, right?",01:01:05.322,01:01:07.834
"They both say Akash, you'll get that.",01:01:07.834,01:01:10.270
"But you hope to also get things like
conglomerate and company having similar",01:01:10.270,01:01:14.890
"word representations that
you can do things with.",01:01:14.890,01:01:19.880
"But there are some relationships
that that's clearly not capturing.",01:01:19.880,01:01:23.015
"If you think of some of those properties
that we've already mentioned like recency",01:01:23.015,01:01:27.457
"and grammatical role and things like that,
they're not being captured.",01:01:27.457,01:01:31.461
"So there are also, then,
a few features that are calculated for",01:01:31.461,01:01:35.434
each mention that are also put into it.,01:01:35.434,01:01:38.280
"So after that it's really
a straightforward architecture.",01:01:38.280,01:01:42.180
"It's a deep feed-forward network
of sort of ReLus at every level",01:01:42.180,01:01:46.680
"that take you up to the top, and then at
the top you're turning that into a score",01:01:46.680,01:01:51.490
"which is a numeric score of how
likely it is to be coreferant.",01:01:51.490,01:01:56.340
"Yeah, so the tradition,
compared to traditional systems,",01:01:56.340,01:02:02.220
"the number of handcrafted features
is getting smaller over time,",01:02:02.220,01:02:05.790
"but there are still just
the number that really help.",01:02:05.790,01:02:08.750
Distance is one that really helps.,01:02:08.750,01:02:11.570
"And if you have any kind of dialogue
doing tracking of speakers and",01:02:11.570,01:02:15.407
"change of speakers also really helps you,
and you don't just get for",01:02:15.407,01:02:19.453
free out of word embeddings.,01:02:19.453,01:02:21.227
"Okay, users pretrained word embeddings.",01:02:22.792,01:02:25.900
"I mentioned the no RNNs,
deep network, dropout.",01:02:25.900,01:02:30.010
"So, that part is all
pretty straightforward.",01:02:30.010,01:02:33.210
"So what's a novel more
interesting part of the model?",01:02:33.210,01:02:38.278
"And so that was to say, well,
you aren't necessarily gonna do well",01:02:38.278,01:02:43.360
"if you just train such a model
as a straightforward classifier.",01:02:43.360,01:02:48.450
"You either got this decision right or
wrong.",01:02:48.450,01:02:50.920
"You could do that but that's non
optimal and the reason why it's non",01:02:50.920,01:02:55.860
"optimal is that some mistakes matter much,
much more than others.",01:02:55.860,01:03:00.940
"Because even though
the mention pair classifier",01:03:00.940,01:03:05.410
"is just an independent classifier
of a pair of mentions.",01:03:05.410,01:03:09.250
"The reality is that as a result
of making a sequence of those",01:03:09.250,01:03:13.840
"mention pair classifications,
you're then going to end up",01:03:15.100,01:03:18.970
"with these clusterings of
mentions that are your entities.",01:03:18.970,01:03:22.710
"And you have the quality of your
co-reference is going to be decided",01:03:22.710,01:03:27.450
"by how good are those
clusters mentions that your",01:03:27.450,01:03:30.890
"entities that you formed
at the end of the day.",01:03:30.890,01:03:33.500
And so what that means in particular,01:03:33.500,01:03:35.950
"is that some mistakes you can
make are really bad mistakes.",01:03:35.950,01:03:40.190
"So if you've started along saying
Bill Clinton coreferent with he,",01:03:40.190,01:03:44.173
"Clinton coreferent with he, and
Hillary is coreferent with Clinton, her.",01:03:44.173,01:03:48.649
"If you then said let's make
these two Clinton's coreferent,",01:03:48.649,01:03:52.694
that sort of collapses your.,01:03:52.694,01:03:55.120
"Two partial clusterings together
into one huge hair ball.",01:03:55.120,01:04:00.000
"And that sort of destroys your ability
to kind of do discourse interpretation",01:04:00.000,01:04:04.716
"because you've collapsed two individuals
in your discourse into one individual.",01:04:04.716,01:04:10.120
So that's gonna really destroy you.,01:04:10.120,01:04:12.130
"But there are other ways in
which you can make little errors",01:04:12.130,01:04:15.360
which don't really matter.,01:04:15.360,01:04:16.230
"So, it was raining, but the car stayed
dry because it was under cover.",01:04:16.230,01:04:20.550
"So this first it is what's
referred to as the pleonastic it.",01:04:20.550,01:04:24.280
"That it's just not really
referential at all.",01:04:24.280,01:04:26.600
"Just somehow in English
we like to have subject.",01:04:26.600,01:04:29.510
"So rather than just saying it's raining
as you would in many other languages,",01:04:29.510,01:04:33.310
you say it is raining.,01:04:33.310,01:04:34.940
"But it's not really referring to anything,
so It was",01:04:34.940,01:04:39.925
"a mistake to make that co-reference, the
car, but it sort of doesn't really matter.",01:04:39.925,01:04:44.785
"It's not gonna destroy your
understanding of the dialogue,",01:04:44.785,01:04:48.065
you've just got sort of one thing wrong.,01:04:48.065,01:04:50.025
So that's a minor error.,01:04:50.025,01:04:51.792
"And so the question is couldn't
we train a model so that it's",01:04:51.792,01:04:56.722
"actually sensitive to these ideas of what
a major error is and minor error is.",01:04:56.722,01:05:01.732
"and the secret of that is to say Well,",01:05:01.732,01:05:05.256
"that means that we can't work out
the loss simply of individual decisions.",01:05:05.256,01:05:11.406
"We've gotta work out what impact those
decisions have at the end of the day.",01:05:11.406,01:05:17.940
"And if you're in that situation
in which you can't locally",01:05:17.940,01:05:22.960
"work out the loss of individual decisions,
but you have to wait around and",01:05:22.960,01:05:28.190
"say how does things turn out later in
the day cuz I'll have to use that.",01:05:28.190,01:05:33.150
"That's the space in which you need
to use reinforcement learning.",01:05:33.150,01:05:36.790
"And so, that's one possibility
that this paper talks about.",01:05:37.930,01:05:43.710
"So, previously people had done
something about this problem.",01:05:43.710,01:05:50.230
"And I'll show you that in just
a minute which is to say, gee,",01:05:50.230,01:05:53.640
"some of these decisions
are more important than others.",01:05:53.640,01:05:56.960
"So we could come up with heuristics to
decide how important different things are.",01:05:56.960,01:06:02.500
"And then we could For
different kinds of errors.",01:06:02.500,01:06:05.090
"We can kind of set hyperparameters
to weight those kind of errors and",01:06:05.090,01:06:09.860
adjust those to maximize our performance.,01:06:09.860,01:06:13.100
"And to some extent, you can do that.",01:06:13.100,01:06:15.060
"But it seems like the more right and
principled way to do it would be to say,",01:06:15.060,01:06:18.910
"no, this can be done,
this reinforcement learning problem.",01:06:18.910,01:06:21.990
"And if we too wanna be
finding local decisions,",01:06:21.990,01:06:25.320
"which lead to the end of
the day as a good clustering.",01:06:25.320,01:06:29.560
Say that our reward function for,01:06:29.560,01:06:31.840
"reinforcement learning is do we a get
good clustering at the end of the day?.",01:06:31.840,01:06:35.480
"And if we do that we can get rid
of having to, manually find,",01:06:35.480,01:06:43.060
"things to weight and setting the weights
of them with hyper parameters.",01:06:43.060,01:06:47.180
"And we can get some gains, not huge gains,
but some gains, from doing that.",01:06:47.180,01:06:51.532
"So the thing that are being done
in prior work is to say, well,",01:06:51.532,01:06:55.110
"there are different classes
of co-reference decisions and",01:06:55.110,01:06:59.250
"their importance we might
want to weight differently.",01:06:59.250,01:07:03.062
"So the mistakes you can make
is you can do a false new.",01:07:03.062,01:07:07.810
"You can claim that
something is a new cluster",01:07:07.810,01:07:10.680
"when really it should have been
made coreference to something.",01:07:10.680,01:07:13.680
"So that's failing to cluster
when you should have.",01:07:13.680,01:07:16.360
There's a false anaphoric.,01:07:16.360,01:07:18.570
"Something should have been
it's own cluster, but",01:07:18.570,01:07:21.170
you've joined it to an existing cluster.,01:07:21.170,01:07:23.790
"And then there's a wrong link where you
should have been joining it with some",01:07:23.790,01:07:27.690
"previously established cluster and
you chose a different one.",01:07:27.690,01:07:32.640
"So these notions still don't really get
at sort of making a big scale mistake,",01:07:32.640,01:07:37.280
"like the Clinton mistake,
versus small scale mistakes.",01:07:37.280,01:07:40.530
"But they sort of distinguish
different decisions.",01:07:40.530,01:07:43.440
And some of these are worse than others.,01:07:43.440,01:07:46.830
"So in general, doing a Wrong Link
is worse than doing a False New.",01:07:46.830,01:07:52.300
"Cuz a False New doesn't thave the same
knock on effects that a Wrong Link has.",01:07:52.300,01:07:57.990
"Yeah, so what prior work had said is okay,
we have these four kinds of things.",01:07:57.990,01:08:03.320
They can actually be coreferent.,01:08:03.320,01:08:05.620
"And done correctly or you can make
these three different kinds of errors.",01:08:05.620,01:08:10.800
"And so what we could do is
manually set weights for",01:08:10.800,01:08:14.240
"these different kinds of errors and
adjust them to try and maximize our score.",01:08:14.240,01:08:18.930
"And so Sam Wiseman had proposed doing
that In the kind of margin loss scenario.",01:08:18.930,01:08:25.180
"So that we are taking
the maximum choice of",01:08:25.180,01:08:29.100
candidate antecedents for each mentioned.,01:08:29.100,01:08:33.460
"And then here we have
the usual kind of margin loss",01:08:33.460,01:08:38.070
"that we are looking at the score
difference of the model.",01:08:38.070,01:08:41.620
"For the true antecedent
versus this candidate.",01:08:41.620,01:08:47.120
"And they're both being
scored by a current model.",01:08:47.120,01:08:49.900
"And so, again, we wanna adjust the scores
according to our model to sort of",01:08:49.900,01:08:54.280
minimize that kind of large margin loss.,01:08:54.280,01:08:57.710
"But we add this one extra factor to our
loss function which we say is well,",01:08:57.710,01:09:02.780
"scale how much it cost's
you to make this mistake",01:09:02.780,01:09:05.900
"based on what kind of
an error you're making.",01:09:05.900,01:09:08.850
"Which is sort of classifying it as
one of these different kinds errors.",01:09:08.850,01:09:12.980
"Okay and so that kind of idea wasn't
actually original to Sam Weisman.",01:09:15.840,01:09:20.220
"So really sort of a whole bunch of
papers really kind of just about all",01:09:20.220,01:09:25.240
"the coref papers that were done in
the 2010s, had used this kind of idea.",01:09:25.240,01:09:30.840
"Because there's a way to push
up coreference numbers a bit.",01:09:30.840,01:09:34.720
"But it doesn't seem perfect firstly you
have to do the hyper parameter search.",01:09:34.720,01:09:40.540
"And secondly those error types
are a bit correlated with badness,",01:09:40.540,01:09:46.340
"but they don't seem to be very
directly correlated with badness.",01:09:46.340,01:09:50.580
"And so Kevin was wanting to try and
do things better than that.",01:09:50.580,01:09:54.030
"These are the ways he approached it and
he introduced two approaches.",01:09:55.040,01:09:59.190
"What we can say is when
we are doing coreference,",01:09:59.190,01:10:02.930
"what we're doing is we're
taking a sequence of actions.",01:10:02.930,01:10:06.790
"And so each action is looking at
one mention as we head through and",01:10:06.790,01:10:11.846
"choosing something as
it to be coreferent to,",01:10:11.846,01:10:15.497
"where one possibility is
you are co-referent to NEW.",01:10:15.497,01:10:20.540
So you're making this sequence of actions.,01:10:20.540,01:10:22.766
"So you are deciding what to do with I,
deciding what to do with Nader,",01:10:22.766,01:10:26.280
deciding what to do with he.,01:10:26.280,01:10:27.880
That these are your sequence of actions.,01:10:27.880,01:10:30.740
"And so what we'd like to do is chose
the sequence of actions that maximizes",01:10:30.740,01:10:36.250
"getting a good coreference
clustering at the end of the day.",01:10:36.250,01:10:42.040
"So how do we decide what's
a good coreference clustering?",01:10:42.040,01:10:46.300
"Well, what we do is we actually
just believe our metric.",01:10:46.300,01:10:50.770
"So I showed you the BQ metric for
coreference and",01:10:50.770,01:10:54.208
"that seemed kind of
sensible it was sort of",01:10:54.208,01:10:56.840
"is F measure of getting your links
right and precision and recall.",01:10:56.840,01:11:00.250
So we call that our reward function.,01:11:00.250,01:11:03.090
"So if you kind if you get everything
right, your BQ metric Is 100 or",01:11:03.090,01:11:07.743
"a 1 depending on whether you make
it a point or make it a percentage.",01:11:07.743,01:11:12.580
"And so we then have no loss and
if you make some mistakes,",01:11:12.580,01:11:17.930
"we can then work out the reward for
different coreference algorithm.",01:11:17.930,01:11:22.240
"The reward will then be a lower award
corresponding the b cubed score.",01:11:22.240,01:11:28.340
"And so
Kevin explored two methods of doing this.",01:11:28.340,01:11:30.880
"One's sort of the REINFORCE algorithm
which is the most common reinforcement",01:11:30.880,01:11:34.910
"learning algorithm that's used in deep
learning techniques and elsewhere.",01:11:34.910,01:11:41.040
And then reward-rescaling.,01:11:41.040,01:11:44.360
"So for the REINFORCE algorithm,",01:11:44.360,01:11:46.100
"what you're doing is you're defining
a probability distribution over actions.",01:11:46.100,01:11:50.920
"And the way he was doing that was
sort of taking the scores from",01:11:50.920,01:11:55.140
"the mentioned pair model and
exponentiating those and normalizing them,",01:11:55.140,01:12:00.139
so sort of standard soft max function.,01:12:00.139,01:12:02.748
"And saying that's the probability for
taking different actions.",01:12:02.748,01:12:07.390
"And then what you're wanting
to do is work out four action",01:12:07.390,01:12:12.450
"sequences with their probabilities,
you want to maximize your expected reward.",01:12:12.450,01:12:19.350
"So the REINFORCE Algorithm
maximizes your expected reward.",01:12:19.350,01:12:22.780
"So you're taking the expectation
over action sequences,",01:12:22.780,01:12:26.571
"according to this probability distribution
and then working out the reward,",01:12:26.571,01:12:31.575
"the B-cubed score,
having taken that action sequence.",01:12:31.575,01:12:35.316
"The problem, is of course,
that there are sort of an, oops,",01:12:35.316,01:12:39.616
"the problem is that there are an
exponential number of different action",01:12:39.616,01:12:44.584
sequences that you can take here.,01:12:44.584,01:12:47.008
"And so
you can't actually explore all of them.",01:12:47.008,01:12:49.910
"But what you can do is then sample
trajectories to estimate that expectation",01:12:49.910,01:12:55.560
"and to approximate the gradient and
you can learn according to that.",01:12:55.560,01:12:59.000
So using the REINFORCE Algorithm for,01:13:01.090,01:13:04.026
"reinforcement learning,
it basically worked.",01:13:04.026,01:13:08.375
"It's sort of competitive
with the heuristic",01:13:08.375,01:13:11.080
loss functions that people had found.,01:13:11.080,01:13:13.210
"But it still seemed to
have a small disadvantage,",01:13:13.210,01:13:16.748
"which is that the REINFORCE Algorithm
maximizes performance in expectation but",01:13:16.748,01:13:22.481
that's not what we actually want here.,01:13:22.481,01:13:25.360
"We actually want to sorta maximize
the highest scoring action sequence,",01:13:25.360,01:13:29.120
"cuz that's where we're actually
gonna use in practice.",01:13:29.120,01:13:31.648
"And so Kevin explored this other idea,
which is to sort of say,",01:13:31.648,01:13:36.890
let's actually continue with this idea,01:13:36.890,01:13:40.070
"of incorporating rewards into the max
margin objective slack rescaling.",01:13:40.070,01:13:45.130
"But instead of using these sort of
handset hyperparameters like before,",01:13:45.130,01:13:50.770
"what we will do is actually we
can work out how to set those",01:13:50.770,01:13:56.600
"sort of losses for
rescaling the large margin objective.",01:13:58.090,01:14:02.790
"So the idea there is for
our training data,",01:14:02.790,01:14:06.340
"we can actually just look at
the effect of different decisions.",01:14:06.340,01:14:10.730
"So since each action is independent
from every other action,",01:14:10.730,01:14:15.480
"we can change one action and
see what effect it had for the reward.",01:14:15.480,01:14:20.700
"So, this is the correct set of
actions when our Reward = 100.",01:14:20.700,01:14:25.049
"And so, we can just say for AI there,
suppose we made that decision differently,",01:14:25.049,01:14:31.929
"suppose we had said AI then
declared that mention to be NEW.",01:14:31.929,01:14:36.932
"Well, we can just say this is what our
system returned what's the B-cubed",01:14:36.932,01:14:41.720
score for that?,01:14:41.720,01:14:42.860
"And the answer is 85, and so",01:14:42.860,01:14:45.790
"our Regret = 15 because we could have
gotten the right answer and 100.",01:14:45.790,01:14:50.000
"And then we could say, well,
let's consider a different possibility.",01:14:50.000,01:14:53.130
"We could have put my as coreferent to he,
what's the B-cubed score then?",01:14:53.130,01:14:58.478
"And the B-cubed score is 66 and so
now our Regret is larger, our Regret = 34.",01:14:58.478,01:15:04.808
"So we can actually empirically
over the training data",01:15:04.808,01:15:08.300
"work out what the cost of different
mistakes is in terms of B-cubed score.",01:15:08.300,01:15:13.197
"So, then what we can do,
is sort of incorporate that,",01:15:14.560,01:15:19.951
"so that now, the sort of,
the scaling factor over our max",01:15:19.951,01:15:25.579
"margin loss function is being
taken as the difference",01:15:25.579,01:15:30.856
"between the best action we
could have taken at that point.",01:15:30.856,01:15:36.627
"Which may no longer be the perfect action
because we might have previously made",01:15:36.627,01:15:41.585
"mistakes versus the actions
that we did choose.",01:15:41.585,01:15:44.700
"So that the cost is then sort of
the regret for taking a particular action,",01:15:44.700,01:15:49.560
"and that replaces the heuristic cost
we used previously, for actually",01:15:49.560,01:15:53.770
"what is the actual cost of this mistake
in the context of a particular sentence?",01:15:53.770,01:15:58.140
"Okay, so that was the system or
the second system that was built.",01:15:59.860,01:16:04.340
And so then this was evaluated on coref.,01:16:04.340,01:16:06.660
"So most of the recent work on coreference
has used, there were these CoNLL shared",01:16:06.660,01:16:12.070
"task in 2011 and 2012, the coreference.",01:16:12.070,01:16:16.905
"It was, it had English and
Chinese in it and",01:16:16.905,01:16:21.540
"these are scored with
the sort of CoNLL score.",01:16:21.540,01:16:24.510
"The people who did the CoNLL competition,
I guess they didn't wanna take sides as to",01:16:24.510,01:16:28.190
"which was the best metric for co-reference
so they came up with the CoNLL score for",01:16:28.190,01:16:32.315
"co-reference, which was actually
just the arithmetic mean",01:16:32.315,01:16:35.680
"of three co-reference metrics,
B-cubed and two other ones.",01:16:35.680,01:16:40.270
And so these are how things performed.,01:16:40.270,01:16:42.500
"So the kind of heuristic losses
that people had used previously",01:16:42.500,01:16:46.540
actually work quite well.,01:16:46.540,01:16:48.550
"So using the REINFORCE algorithm
is a smidgen better but",01:16:48.550,01:16:54.700
"not really better than using
the current heuristic loss functions.",01:16:54.700,01:16:59.180
"But what you could find is
that using Reward Rescaling",01:16:59.180,01:17:04.690
"actually did work significantly
better because you could then",01:17:04.690,01:17:10.330
"sort of actually use the real losses that
were incurred in different environments.",01:17:10.330,01:17:16.090
"Now, these results and
differences may not look very impressive.",01:17:16.090,01:17:20.040
"But that's partly because
even the heuristic loss",01:17:20.040,01:17:23.230
"is being run on a good
neural coreference system.",01:17:23.230,01:17:26.360
"So I should also show
you these other results,",01:17:26.360,01:17:29.020
just to give you a sense of things.,01:17:29.020,01:17:31.290
"So this is the sort of progress
that's really been made",01:17:31.290,01:17:34.720
in coreference resolution.,01:17:34.720,01:17:37.050
"So at the time of CoNLL 2012,
the best Chinese system was",01:17:37.050,01:17:42.318
"this Chen &amp; Ng which got 57 on the CoNLL
score, the best English system got 60.",01:17:42.318,01:17:49.571
"There had been some work on non-neural
systems since then, so there's",01:17:49.571,01:17:54.767
"a better Chinese system, and there's also
a bit better English in this system.",01:17:54.767,01:18:01.480
"So the Wiseman was sort of
the first neural system,",01:18:01.480,01:18:05.860
"so that was actually now
starting to do a lot better.",01:18:05.860,01:18:09.140
And then here is Kevin's two system and,01:18:09.140,01:18:12.585
"it sort of starting to get some
decent gains beyond that going up.",01:18:12.585,01:18:16.355
So the neural systems actually have given,01:18:16.355,01:18:18.945
"a nice new level of gain beyond
previous coreference systems.",01:18:18.945,01:18:24.105
"And so I just wanted to end in the last
minute it by saying, well why is that?",01:18:24.105,01:18:28.950
"So, one of the biggest gains is just
the sort of general goodness of embedding.",01:18:28.950,01:18:34.980
"So, one of the places where you get the
biggest gains is what turns out to be one",01:18:34.980,01:18:39.634
"the hardest cases of coreference in
practice, is when you have these common",01:18:39.634,01:18:44.216
"noun nominals and
you have to realize they're coreferent.",01:18:44.216,01:18:47.598
"And so that's things like
the country's leftist rebels and",01:18:47.598,01:18:51.836
"the guerillas, the gun, the rifle,
216 sailors from the USS Cole,",01:18:51.836,01:18:56.902
"the crew, these are the kind of ones
that are very hard to get, right.",01:18:56.902,01:19:01.823
"If you're just doing conventional system
with word features and things like that.",01:19:01.823,01:19:06.775
"But that's precisely the kind of place
where having our word vectors actually",01:19:06.775,01:19:10.845
"does give us some purchase that these
are still hardest cases to get right.",01:19:10.845,01:19:15.665
But the other place you were getting gains,01:19:15.665,01:19:18.695
"is from using this
Reward Rescaling algorithm.",01:19:18.695,01:19:21.925
"And the kind of interesting thing that the
results actually turned out, is compared",01:19:21.925,01:19:26.410
"to the heuristic loss function,
that using Reward Rescaling, that",01:19:26.410,01:19:32.280
"the Reward Rescaling system actually made
more mistake than heuristic loss function.",01:19:32.280,01:19:38.708
"But was cleverer at deciding
where to make its mistakes and so",01:19:38.708,01:19:43.031
it made mistakes that were less important.,01:19:43.031,01:19:46.476
"So, even thought it made more mistakes,
overall it was able to achieve",01:19:46.476,01:19:51.880
"a better B-cubed score by concentrating
on making less important mistakes.",01:19:51.880,01:19:57.660
"And that reflects the fact that is for
different mistakes,",01:19:57.660,01:20:02.340
"there is a wide variety
of different costs.",01:20:02.340,01:20:05.160
"So this is an empirical graph of
looking at all cases of false new,",01:20:05.160,01:20:10.385
"and then this is the distribution
over how much they cost You.",01:20:10.385,01:20:15.230
"Even though, as you can see,
there is a clear mode to this graph.",01:20:15.230,01:20:19.397
"So if you were doing
a heuristic loss you would say,",01:20:19.397,01:20:22.708
"okay, customer false new is about 0.28 or
something like that.",01:20:22.708,01:20:27.540
"For different situations there's
an enormous distribution as",01:20:27.540,01:20:31.550
"to what the real cost
of a false new is and",01:20:31.550,01:20:34.060
"that's precisely what could be
captured by the Reward Rescaling.",01:20:34.060,01:20:37.680
"Okay, that's it for coreference and
then back on Thursday for",01:20:37.680,01:20:42.410
doing the dynamic memory networks.,01:20:42.410,01:20:44.690
